{
  "raw_text": "Introductory Statistics for the\nLife and Biomedical Sciences\nFirst Edition\nJulie Vu\nPreceptor in Statistics\nHarvard University\nDavid Harrington\nProfessor of Biostatistics (Emeritus)\nHarvard T.H. Chan School of Public Health\nDana-Farber Cancer Institute\nCopyright © 2020. First Edition.\nVersion date: August 8th, 2021.\nThis textbook and its supplements, including slides and labs, may be downloaded for free at\nopenintro.org/book/biostat .\nThis textbook is a derivative of OpenIntro Statistics 3rd Edition by Diez, Barr, and Çetinkaya-\nRundel, and it is available under a Creative Commons Attribution-ShareAlike 3.0 Unported United\nStates license. License details are available at the Creative Commons website:\ncreativecommons.org .\nSource ﬁles for this book may be found on Github at\ngithub.com/OI-Biostat/oi_biostat_text .\n3\nTable of Contents\n1 Introduction to data 10\n1.1 Case study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n1.2 Data basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n1.3 Data collection principles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n1.4 Numerical data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n1.5 Categorical data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n1.6 Relationships between two variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38\n1.7 Exploratory data analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n1.8 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74\n1.9 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75\n2 Probability 88\n2.1 Deﬁning probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90\n2.2 Conditional probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\n2.3 Extended example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117\n2.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126\n2.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127\n3 Distributions of random variables 138\n3.1 Random variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140\n3.2 Binomial distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147\n3.3 Normal distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\n3.4 Poisson distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168\n3.5 Distributions related to Bernoulli trials . . . . . . . . . . . . . . . . . . . . . . . . . . . 170\n3.6 Distributions for pairs of random variables . . . . . . . . . . . . . . . . . . . . . . . . 177\n3.7 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184\n3.8 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185\n4 Foundations for inference 198\n4.1 Variability in estimates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201\n4.2 Conﬁdence intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205\n4.3 Hypothesis testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212\n4.4 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225\n4.5 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227\n4 TABLE OF CONTENTS\n5 Inference for numerical data 236\n5.1 Single-sample inference with the t-distribution . . . . . . . . . . . . . . . . . . . . . . 238\n5.2 Two-sample test for paired data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244\n5.3 Two-sample test for independent data . . . . . . . . . . . . . . . . . . . . . . . . . . . 247\n5.4 Power calculations for a di ﬀerence of means . . . . . . . . . . . . . . . . . . . . . . . . 257\n5.5 Comparing means with ANOV A . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264\n5.6 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272\n5.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274\n6 Simple linear regression 290\n6.1 Examining scatterplots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293\n6.2 Estimating a regression line using least squares . . . . . . . . . . . . . . . . . . . . . . 295\n6.3 Interpreting a linear model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298\n6.4 Statistical inference with regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308\n6.5 Interval estimates with regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312\n6.6 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 316\n6.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317\n7 Multiple linear regression 330\n7.1 Introduction to multiple linear regression . . . . . . . . . . . . . . . . . . . . . . . . . 332\n7.2 Simple versus multiple regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 334\n7.3 Evaluating the ﬁt of a multiple regression model . . . . . . . . . . . . . . . . . . . . . 338\n7.4 The general multiple linear regression model . . . . . . . . . . . . . . . . . . . . . . . 342\n7.5 Categorical predictors with several levels . . . . . . . . . . . . . . . . . . . . . . . . . . 347\n7.6 Reanalyzing the PREVEND data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 350\n7.7 Interaction in regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352\n7.8 Model selection for explanatory models . . . . . . . . . . . . . . . . . . . . . . . . . . 358\n7.9 The connection between ANOV A and regression . . . . . . . . . . . . . . . . . . . . . 368\n7.10 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 370\n7.11 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 372\n8 Inference for categorical data 386\n8.1 Inference for a single proportion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 388\n8.2 Inference for the di ﬀerence of two proportions . . . . . . . . . . . . . . . . . . . . . . 395\n8.3 Inference for two or more groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 401\n8.4 Chi-square tests for the ﬁt of a distribution . . . . . . . . . . . . . . . . . . . . . . . . 414\n8.5 Outcome-based sampling: case-control studies . . . . . . . . . . . . . . . . . . . . . . 416\n8.6 Notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 420\n8.7 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 421\nA End of chapter exercise solutions 435\nB Distribution tables 463\nIndex 469\n5\nForeword\nThe past year has been challenging for the health sciences in ways that we could not have imagined\nwhen we started writing 5 years ago. The rapid spread of the SARS coronavirus (SARS-CoV-2)\nworldwide has upended the scientiﬁc research process and highlighted the need for maintaining a\nbalance between speed and reliability. Major medical journals have dramatically increased the pace\nof publication; the urgency of the situation necessitates that data and research ﬁndings be made\navailable as quickly as possible to inform public policy and clinical practice. Yet it remains essential\nthat studies undergo rigorous review; the retraction of two high-proﬁle coronavirus studies1, 2\nsparked widespread concerns about data integrity, reproducibility, and the editorial process.\nIn parallel, deepening public awareness of structural racism has caused a re-examination of\nthe role of race in published studies in health and medicine. A recent review of algorithms used to\ndirect treatment in areas such as cardiology, obstetrics and oncology uncovered examples of race\nused in ways that may lead to substandard care for people of color.3The SARS-CoV-2 pandemic\nhas reminded us once again that marginalized populations are disproportionately at risk for bad\nhealth outcomes. Data on 17 million patients in England4suggest that Blacks and South Asians\nhave a death rate that is approximately 50% higher than white members of the population.\nUnderstanding the SARS coronavirus and tackling racial disparities in health outcomes are\nbut two of the many areas in which Biostatistics will play an important role in the coming decades.\nMuch of that work will be done by those now beginning their study of Biostatistics. We hope this\nbook provides an accessible point of entry for students planning to begin work in biology, medicine,\nor public health. While the material presented in this book is essential for understanding the\nfoundations of the discipline, we advise readers to remember that a mastery of technical details is\nsecondary to choosing important scientiﬁc questions, examining data without bias, and reporting\nresults that transparently display the strengths and weaknesses of a study.\n1Mandeep R. Mehra et al. “Retraction: Cardiovascular Disease, Drug Therapy, and Mortality in Covid-19. N Engl J\nMed. DOI: 10.1056/NEJMoa2007621.” In: New England Journal of Medicine 382.26 (2020), pp. 2582–2582. doi:10.1056/\nNEJMc2021225 .\n2Mandeep R Mehra et al. “RETRACTED:Hydroxychloroquine or chloroquine with or without a macrolide for treatment\nof COVID-19: a multinational registry analysis”. In: The Lancet (2020). doi:https://doi.org/10.1016/S0140-6736(20)\n31180-6 .\n3Darshali A. Vyas et al. “Hidden in Plain Sight — Reconsidering the Use of Race Correction in Clinical Algorithms”.\nIn:New England Journal of Medicine (2020). doi:10.1056/NEJMms2004740 .\n4Elizabeth J. Williamson et al. “OpenSAFELY: factors associated with COVID-19 death in 17 million patients”. In:\nNature (2020). issn: 1476-4687.\n6\nPreface\nThis text introduces statistics and its applications in the life sciences and biomedical research. It is\nbased on the freely available OpenIntro Statistics , and, like OpenIntro , it may be downloaded at no\ncost.5In writing Introduction to Statistics for the Life and Biomedical Sciences , we have added sub-\nstantial new material, but also retained some examples and exercises from OpenIntro that illustrate\nimportant ideas even if they do not relate directly to medicine or the life sciences. Because of its\nlink to the original OpenIntro project, this text is often referred to as OpenIntro Biostatistics in the\nsupplementary materials.\nThis text is intended for undergraduate and graduate students interested in careers in biology\nor medicine, and may also be proﬁtably read by students of public health or medicine. It cov-\ners many of the traditional introductory topics in statistics, in addition to discussing some newer\nmethods being used in molecular biology.\nStatistics has become an integral part of research in medicine and biology, and the tools for\nsummarizing data and drawing inferences from data are essential both for understanding the out-\ncomes of studies and for incorporating measures of uncertainty into that understanding. An intro-\nductory text in statistics for students who will work in medicine, public health, or the life sciences\nshould be more than simply the usual introduction, supplemented with an occasional example\nfrom biology or medical science. By drawing the majority of examples and exercises in this text\nfrom published data, we hope to convey the value of statistics in medical and biological research. In\ncases where examples draw on important material in biology or medicine, the problem statement\ncontains the necessary background information.\nComputing is an essential part of the practice of statistics. Nearly everyone entering the\nbiomedical sciences will need to interpret the results of analyses conducted in software; many\nwill also need to be capable of conducting such analyses. The text and associated materials sepa-\nrate those two activities to allow students and instructors to emphasize either or both skills. The\ntext discusses the important features of ﬁgures and tables used to support an interpretation, rather\nthan the process of generating such material from data. This allows students whose main focus\nis understanding statistical concepts not to be distracted by the details of a particular software\npackage. In our experience, however, we have found that many students enter a research setting\nafter only a single course in statistics. These students beneﬁt from a practical introduction to data\nanalysis that incorporates the use of a statistical computing language. The‘ self-paced learning labs\nassociated with the text provide such an introduction; these are described in more detail later in\nthis preface. The datasets used in this book are available via the Ropenintro package available on\nCRAN6and the Roibiostat package available via GitHub.\n5PDF available at https://www.openintro.org/book/biostat/ and source available at https://github.com/\nOI-Biostat/oi_biostat_text .\n6Diez DM, Barr CD, Çetinkaya-Rundel M. 2012. openintro : OpenIntro data sets and supplement functions. http:\n//cran.r-project.org/web/packages/openintro.\n7\nTextbook overview\nThe chapters of this book are as follows:\n1. Introduction to data. Data structures, basic data collection principles, numerical and graphical\nsummaries, and exploratory data analysis.\n2. Probability. The basic principles of probability.\n3. Distributions of random variables. Introduction to random variables, distributions of discrete\nand continuous random variables, and distributions for pairs of random variables.\n4. Foundations for inference. General ideas for statistical inference in the context of estimating a\npopulation mean.\n5. Inference for numerical data. Inference for one-sample and two-sample means with the t-distribution,\npower calculations for a di ﬀerence of means, and ANOV A.\n6. Simple linear regression. An introduction to linear regression with a single explanatory vari-\nable, evaluating model assumptions, and inference in a regression context.\n7. Multiple linear regression. General multiple regression model, categorical predictors with more\nthan two values, interaction, and model selection.\n8. Inference for categorical data. Inference for single proportions, inference for two or more groups,\nand outcome-based sampling.\nExamples, exercises, and appendices\nExamples in the text help with an understanding of how to apply methods:\nEXAMPLE 0.1\nThis is an example. When a question is asked here, where can the answer be found?\nThe answer can be found here, in the solution section of the example.\nWhen we think the reader would beneﬁt from working out the solution to an example, we frame it\nas Guided Practice.\nGUIDED PRACTICE 0.2\nThe reader may check or learn the answer to any Guided Practice problem by reviewing the full\nsolution in a footnote.7\nThere are exercises at the end of each chapter that are useful for practice or homework as-\nsignments. Solutions to odd numbered problems can be found in Appendix A. Readers will notice\nthat there are fewer end of chapter exercises in the last three chapters. The more complicated\nmethods, such as multiple regression, do not always lend themselves to hand calculation, and\ncomputing is increasingly important both to gain practical experience with these methods and to\nexplore complex datasets. For students more interested in concepts than computing, however, we\nhave included useful end of chapter exercises that emphasize the interpretation of output from\nstatistical software.\nProbability tables for the normal, t, and chi-square distributions are in Appendix B, and PDF\ncopies of these tables are also available from openintro.org for anyone to download, print, share, or\nmodify. The labs and the text also illustrate the use of simple Rcommands to calculate probabilities\nfrom common distributions.\n7Guided Practice problems are intended to stretch your thinking, and you can check yourself by reviewing the footnote\nsolution for any Guided Practice.\n8 CHAPTER 0. PREFACE\nSelf-paced learning labs\nThe labs associated with the text can be downloaded from github.com/OI-Biostat/oi_biostat_\nlabs . They provide guidance on conducting data analysis and visualization with the Rstatistical\nlanguage and the computing environment RStudio, while building understanding of statistical\nconcepts. The labs begin from ﬁrst principles and require no previous experience with statistical\nsoftware. Both Rand RStudio are freely available for all major computing operating systems, and\nthe Unit 0 labs ( 00_getting_started ) provide information on downloading and installing them.\nInformation on downloading and installing the packages may also be found at openintro.org .\nThe labs for each chapter all have the same structure. Each lab consists of a set of three\ndocuments: a handout with the problem statements, a template to be used for working through\nthe lab, and a solution set with the problem solutions. The handout and solution set are most\neasily read in PDF format (although Rmd ﬁles are also provided), while the template is an Rmd\nﬁle that can be loaded into RStudio. Each chapter of labs is accompanied by a set of \"Lab Notes\",\nwhich provides a reference guide of all new Rfunctions discussed in the labs.\nLearning is best done, of course, if a student attempts the lab exercises before reading the\nsolutions. The \"Lab Notes\" may be a useful resource to refer to while working through problems.\nOpenIntro, online resources, and getting involved\nOpenIntro is an organization focused on developing free and a ﬀordable education materials. The\nﬁrst project, OpenIntro Statistics , is intended for introductory statistics courses at the high school\nthrough university levels. Other projects examine the use of randomization methods for learning\nabout statistics and conducting analyses ( Introductory Statistics with Randomization and Simulation )\nand advanced statistics that may be taught at the high school level ( Advanced High School Statistics ).\nWe encourage anyone learning or teaching statistics to visit openintro.org and get involved by\nusing the many online resources, which are all free, or by creating new material. Students can test\ntheir knowledge with practice quizzes, or try an application of concepts learned in each chapter\nusing real data and the free statistical software R. Teachers can download the source for course\nmaterials, labs, slides, datasets, Rﬁgures, or create their own custom quizzes and problem sets for\nstudents to take on the website. Everyone is also welcome to download the book’s source ﬁles to\ncreate a custom version of this textbook or to simply share a PDF copy with a friend or on a website.\nAll of these products are free, and anyone is welcome to use these online tools and resources with\nor without this textbook as a companion.\nAcknowledgements\nThe OpenIntro project would not have been possible without the dedication of many people, in-\ncluding the authors of OpenIntro Statistics , the OpenIntro team and the many faculty, students,\nand readers who commented on all the editions of OpenIntro Statistics .\nThis text has beneﬁted from feedback from Andrea Foulkes, Raji Balasubramanian, Curry\nHilton, Michael Parzen, Kevin Rader, and the many excellent teaching fellows at Harvard College\nwho assisted in courses using the book. The cover design was provided by Pierre Baduel.\n9\n10\nChapter 1\nIntroduction to data\n1.1 Case study\n1.2 Data basics\n1.3 Data collection principles\n1.4 Numerical data\n1.5 Categorical data\n1.6 Relationships between two variables\n1.7 Exploratory data analysis\n1.8 Notes\n1.9 Exercises\n11\nMaking observations and recording data form the backbone of empirical research,\nand represent the beginning of a systematic approach to investigating scientiﬁc\nquestions. As a discipline, statistics focuses on addressing the following three\nquestions in a rigorous and e ﬃcient manner: How can data best be collected? How\nshould data be analyzed? What can be inferred from data?\nThis chapter provides a brief discussion on the principles of data collection, and\nintroduces basic methods for summarizing and exploring data.\nFor labs, slides, and other resources, please visit\nwww.openintro.org/book/biostat\n12 CHAPTER 1. INTRODUCTION TO DATA\n1.1 Case study: preventing peanut allergies\nThe proportion of young children in Western countries with peanut allergies has doubled in\nthe last 10 years. Previous research suggests that exposing infants to peanut-based foods, rather\nthan excluding such foods from their diets, may be an e ﬀective strategy for preventing the develop-\nment of peanut allergies. The \"Learning Early about Peanut Allergy\" (LEAP) study was conducted\nto investigate whether early exposure to peanut products reduces the probability that a child will\ndevelop peanut allergies.1\nThe study team enrolled children in the United Kingdom between 2006 and 2009, selecting\n640 infants with eczema, egg allergy, or both. Each child was randomly assigned to either the\npeanut consumption (treatment) group or the peanut avoidance (control) group. Children in the\ntreatment group were fed at least 6 grams of peanut protein daily until 5 years of age, while chil-\ndren in the control group avoided consuming peanut protein until 5 years of age.\nAt 5 years of age, each child was tested for peanut allergy using an oral food challenge (OFC): 5\ngrams of peanut protein in a single dose. A child was recorded as passing the oral food challenge if\nno allergic reaction was detected, and failing the oral food challenge if an allergic reaction occurred.\nThese children had previously been tested for peanut allergy through a skin test, conducted at the\ntime of study entry; the main analysis presented in the paper was based on data from 530 children\nwith an earlier negative skin test.2\nIndividual-level data from the study are shown in Figure 1.1 for 5 of the 530 children—each\nrow represents a participant and shows the participant’s study ID number, treatment group assign-\nment, and OFC outcome.3\nparticipant.ID treatment.group overall.V60.outcome\nLEAP_100522 Peanut Consumption PASS OFC\nLEAP_103358 Peanut Consumption PASS OFC\nLEAP_105069 Peanut Avoidance PASS OFC\nLEAP_994047 Peanut Avoidance PASS OFC\nLEAP_997608 Peanut Consumption PASS OFC\nFigure 1.1: Individual-level LEAP results, for ﬁve children.\nThe data can be organized in the form of a two-way summary table; Figure 1.2 shows the\nresults categorized by treatment group and OFC outcome.\nFAIL OFC PASS OFC Sum\nPeanut Avoidance 36 227 263\nPeanut Consumption 5 262 267\nSum 41 489 530\nFigure 1.2: Summary of LEAP results, organized by treatment group (either\npeanut avoidance or consumption) and result of the oral food challenge at 5 years\nof age (either pass or fail).\n1Du Toit, George, et al. Randomized trial of peanut consumption in infants at risk for peanut allergy. New England\nJournal of Medicine 372.9 (2015): 803-813.\n2Although a total of 542 children had an earlier negative skin test, data collection did not occur for 12 children.\n3The data are available as LEAP in the Rpackage oibiostat .\n1.1. CASE STUDY 13\nThe summary table makes it easier to identify patterns in the data. Recall that the question\nof interest is whether children in the peanut consumption group are more or less likely to develop\npeanut allergies than those in the peanut avoidance group. In the avoidance group, the proportion\nof children failing the OFC is 36 =263 = 0:137 (13.7%); in the consumption group, the proportion\nof children failing the OFC is 5 =267 = 0:019 (1.9%). Figure 1.3 shows a graphical method of dis-\nplaying the study results, using either the number of individuals per category from Figure 1.2 or\nthe proportion of individuals with a speciﬁc OFC outcome in a group.\nPeanut Avoidance Peanut Consumption050100150200250FAIL OFC\nPASS OFC\n(a)\nPeanut Avoidance Peanut Consumption0.00.20.40.60.81.0\nFAIL OFC\nPASS OFC (b)\nFigure 1.3: (a) A bar plot displaying the number of individuals who failed or\npassed the OFC in each treatment group. (b) A bar plot displaying the proportions\nof individuals in each group that failed or passed the OFC.\nThe proportion of participants failing the OFC is 11.8% higher in the peanut avoidance group\nthan the peanut consumption group. Another way to summarize the data is to compute the ratio of\nthe two proportions (0.137/0.019 = 7.31), and conclude that the proportion of participants failing\nthe OFC in the avoidance group is more than 7 times as large as in the consumption group; i.e.,\nthe risk of failing the OFC was more than 7 times as great for participants in the avoidance group\nrelative to the consumption group.\nBased on the results of the study, it seems that early exposure to peanut products may be\nan eﬀective strategy for reducing the chances of developing peanut allergies later in life. It is\nimportant to note that this study was conducted in the United Kingdom at a single site of pediatric\ncare; it is not clear that these results can be generalized to other countries or cultures.\nThe results also raise an important statistical issue: does the study provide deﬁnitive evidence\nthat peanut consumption is beneﬁcial? In other words, is the 11.8% di ﬀerence between the two\ngroups larger than one would expect by chance variation alone? The material on inference in later\nchapters will provide the statistical tools to evaluate this question.\n14 CHAPTER 1. INTRODUCTION TO DATA\n1.2 Data basics\nEﬀective organization and description of data is a ﬁrst step in most analyses. This section\nintroduces a structure for organizing data and basic terminology used to describe data.\n1.2.1 Observations, variables, and data matrices\nIn evolutionary biology, parental investment refers to the amount of time, energy, or other\nresources devoted towards raising o ﬀspring. This section introduces the frog dataset, which orig-\ninates from a 2013 study about maternal investment in a frog species.4Reproduction is a costly\nprocess for female frogs, necessitating a trade-o ﬀbetween individual egg size and total number of\neggs produced. Researchers were interested in investigating how maternal investment varies with\naltitude and collected measurements on egg clutches found at breeding ponds across 11 study sites;\nfor 5 sites, the body size of individual female frogs was also recorded.\naltitude latitude egg.size clutch.size clutch.volume body.size\n1 3,462.00 34.82 1.95 181.97 177.83 3.63\n2 3,462.00 34.82 1.95 269.15 257.04 3.63\n3 3,462.00 34.82 1.95 158.49 151.36 3.72\n150 2,597.00 34.05 2.24 537.03 776.25 NA\nFigure 1.4: Data matrix for the frog dataset.\nFigure 1.4 displays rows 1, 2, 3, and 150 of the data from the 431 clutches observed as part\nof the study.5Each row in the table corresponds to a single clutch, indicating where the clutch\nwas collected ( altitude and latitude ),egg.size ,clutch.size ,clutch.volume , and body.size of\nthe mother when available. \"NA\" corresponds to a missing value, indicating that information on\nan individual female was not collected for that particular clutch. The recorded characteristics are\nreferred to as variables ; in this table, each column represents a variable.\nvariable description\naltitude Altitude of the study site in meters above sea level\nlatitude Latitude of the study site measured in degrees\negg.size Average diameter of an individual egg to the 0.01 mm\nclutch.size Estimated number of eggs in clutch\nclutch.volume Volume of egg clutch in mm3\nbody.size Length of mother frog in cm\nFigure 1.5: Variables and their descriptions for the frog dataset.\nIt is important to check the deﬁnitions of variables, as they are not always obvious. For ex-\nample, why has clutch.size not been recorded as whole numbers? For a given clutch, researchers\ncounted approximately 5 grams’ worth of eggs and then estimated the total number of eggs based\non the mass of the entire clutch. Deﬁnitions of the variables are given in Figure 1.5.6\n4Chen, W., et al. Maternal investment increases with altitude in a frog on the Tibetan Plateau. Journal of evolutionary\nbiology 26.12 (2013): 2710-2715.\n5The frog dataset is available in the Rpackage oibiostat .\n6The data discussed here are in the original scale; in the published paper, some values have undergone a natural log\ntransformation.\n1.2. DATA BASICS 15\nThe data in Figure 1.4 are organized as a data matrix . Each row of a data matrix corresponds\nto an observational unit, and each column corresponds to a variable. A piece of the data matrix for\nthe LEAP study introduced in Section 1.1 is shown in Figure 1.1; the rows are study participants\nand three variables are shown for each participant. Data matrices are a convenient way to record\nand store data. If the data are collected for another individual, another row can easily be added;\nsimilarly, another column can be added for a new variable.\n1.2.2 Types of variables\nThe Functional polymorphisms Associated with human Muscle Size and Strength study (FA-\nMuSS) measured a variety of demographic, phenotypic, and genetic characteristics for about 1,300\nparticipants.7Data from the study have been used in a number of subsequent studies,8such as\none examining the relationship between muscle strength and genotype at a location on the ACTN3\ngene.9\nThe famuss dataset is a subset of the data for 595 participants.10Four rows of the famuss\ndataset are shown in Figure 1.6, and the variables are described in Figure 1.7.\nsex age race height weight actn3.r577x ndrm.ch\n1 Female 27 Caucasian 65.0 199.0 CC 40.0\n2 Male 36 Caucasian 71.7 189.0 CT 25.0\n3 Female 24 Caucasian 65.0 134.0 CT 40.0\n595 Female 30 Caucasian 64.0 134.0 CC 43.8\nFigure 1.6: Four rows from the famuss data matrix.\nvariable description\nsex Sex of the participant\nage Age in years\nrace Race, recorded as African Am (African American), Caucasian ,Asian ,\nHispanic orOther\nheight Height in inches\nweight Weight in pounds\nactn3.r577x Genotype at the location r577x in the ACTN3 gene.\nndrm.ch Percent change in strength in the non-dominant arm, comparing strength\nafter to before training\nFigure 1.7: Variables and their descriptions for the famuss dataset.\nThe variables age,height ,weight , and ndrm.ch arenumerical variables . They take on numer-\nical values, and it is reasonable to add, subtract, or take averages with these values. In contrast,\na variable reporting telephone numbers would not be classiﬁed as numerical, since sums, di ﬀer-\nences, and averages in this context have no meaning. Age measured in years is said to be discrete ,\nsince it can only take on numerical values with jumps; i.e., positive integer values. Percent change\nin strength in the non-dominant arm ( ndrm.ch ) iscontinuous , and can take on any value within a\nspeciﬁed range.\n7Thompson PD, Moyna M, Seip, R, et al., 2004. Functional Polymorphisms Associated with Human Muscle Size and\nStrength. Medicine and Science in Sports and Exercise 36:1132 - 1139.\n8Pescatello L, et al. Highlights from the functional single nucleotide polymorphisms associated with human muscle\nsize and strength or FAMuSS study, BioMed Research International 2013.\n9Clarkson P, et al., Journal of Applied Physiology 99: 154-163, 2005.\n10The subset is from Foulkes, Andrea S. Applied statistical genetics with R: for population-based association studies.\nSpringer Science & Business Media, 2009. The full version of the data is available at http://people.umass.edu/foulkes/\nasg/data.html .\n16 CHAPTER 1. INTRODUCTION TO DATA\nFigure 1.8: Breakdown of variables into their respective types.\nThe variables sex,race , and actn3.r577x arecategorical variables , which take on values that\nare names or labels. The possible values of a categorical variable are called the variable’s levels .11\nFor example, the levels of actn3.r577x are the three possible genotypes at this particular locus:\nCC, CT, or TT. Categorical variables without a natural ordering are called nominal categorical\nvariables ;sex,race , and actn3.r577x are all nominal categorical variables. Categorical variables\nwith levels that have a natural ordering are referred to as ordinal categorical variables . For exam-\nple, age of the participants grouped into 5-year intervals (15-20, 21-25, 26-30, etc.) is an ordinal\ncategorical variable.\nEXAMPLE 1.1\nClassify the variables in the frog dataset: altitude ,latitude ,egg.size ,clutch.size ,\nclutch.volume , and body.size .\nThe variables egg.size ,clutch.size ,clutch.volume , and body.size are continuous numerical\nvariables, and can take on all positive values.\nIn the context of this study, the variables altitude and latitude are best described as categorical\nvariables, since the numerical values of the variables correspond to the 11 speciﬁc study sites where\ndata were collected. Researchers were interested in exploring the relationship between altitude and\nmaternal investment; it would be reasonable to consider altitude an ordinal categorical variable.\nGUIDED PRACTICE 1.2\nCharacterize the variables treatment.group and overall.V60.outcome from the LEAP study (dis-\ncussed in Section 1.1).12\nGUIDED PRACTICE 1.3\nSuppose that on a given day, a research assistant collected data on the ﬁrst 20 individuals visiting a\nwalk-in clinic: age (measured as less than 21, 21 - 65, and greater than 65 years of age), sex, height,\nweight, and reason for the visit. Classify each of the variables.13\n11Categorical variables are sometimes called factor variables .\n12These variables measure non-numerical quantities, and thus are categorical variables with two levels.\n13Height and weight are continuous numerical variables. Age as measured by the research assistant is ordinal categorical.\nSex and the reason for the visit are nominal categorical variables.\n1.2. DATA BASICS 17\n1.2.3 Relationships between variables\nMany studies are motivated by a researcher examining how two or more variables are related.\nFor example, do the values of one variable increase as the values of another decrease? Do the values\nof one variable tend to di ﬀer by the levels of another variable?\nOne study used the famuss data to investigate whether ACTN3 genotype at a particular lo-\ncation (residue 577) is associated with change in muscle strength. The ACTN3 gene codes for a\nprotein involved in muscle function. A common mutation in the gene at a speciﬁc location changes\nthe cytosine (C) nucleotide to a thymine (T) nucleotide; individuals with the TT genotype are un-\nable to produce any ACTN3 protein.\nResearchers hypothesized that genotype at this location might inﬂuence muscle function. As\na measure of muscle function, they recorded the percent change in non-dominant arm strength\nafter strength training; this variable, ndrm.ch , is the response variable in the study. A response\nvariable is deﬁned by the particular research question a study seeks to address, and measures the\noutcome of interest in the study. A study will typically examine whether the values of a response\nvariable di ﬀer as values of an explanatory variable change, and if so, how the two variables are\nrelated. A given study may examine several explanatory variables for a single response variable.14\nThe explanatory variable examined in relation to ndrm.ch in the study is actn3.r557x , ACTN3\ngenotype at location 577.\nEXAMPLE 1.4\nIn the maternal investment study conducted on frogs, researchers collected measurements on egg\nclutches and female frogs at 11 study sites, located at di ﬀering altitudes, in order to investigate\nhow maternal investment varies with altitude. Identify the response and explanatory variables in\nthe study.\nThe variables egg.size ,clutch.size , and clutch.volume are response variables indicative of ma-\nternal investment.\nThe explanatory variable examined in the study is altitude .\nWhile latitude is an environmental factor that might potentially inﬂuence features of the egg\nclutches, it is not a variable of interest in this particular study.\nFemale body size ( body.size ) is neither an explanatory nor response variable.\nGUIDED PRACTICE 1.5\nRefer to the variables from the famuss dataset described in Figure 1.7 to formulate a question about\nthe relationships between these variables, and identify the response and explanatory variables in\nthe context of the question.15\n14Response variables are sometimes called dependent variables and explanatory variables are often called independent\nvariables or predictors.\n15Two sample questions: (1) Does change in participant arm strength after training seem associated with race? The\nresponse variable is ndrm.ch and the explanatory variable is race . (2) Do male participants appear to respond di ﬀerently to\nstrength training than females? The response variable is ndrm.ch and the explanatory variable is sex.\n18 CHAPTER 1. INTRODUCTION TO DATA\n1.3 Data collection principles\nThe ﬁrst step in research is to identify questions to investigate. A clearly articulated research\nquestion is essential for selecting subjects to be studied, identifying relevant variables, and deter-\nmining how data should be collected.\n1.3.1 Populations and samples\nConsider the following research questions:\n1. Do blueﬁn tuna from the Atlantic Ocean have particularly high levels of mercury, such that\nthey are unsafe for human consumption?\n2. For infants predisposed to developing a peanut allergy, is there evidence that introducing\npeanut products early in life is an e ﬀective strategy for reducing the risk of developing a\npeanut allergy?\n3. Does a recently developed drug designed to treat glioblastoma, a form of brain cancer, appear\nmore e ﬀective at inducing tumor shrinkage than the drug currently on the market?\nEach of these questions refers to a speciﬁc target population . For example, in the ﬁrst ques-\ntion, the target population consists of all blueﬁn tuna from the Atlantic Ocean; each individual\nblueﬁn tuna represents a case. It is almost always either too expensive or logistically impossible to\ncollect data for every case in a population. As a result, nearly all research is based on information\nobtained about a sample from the population. A sample represents a small fraction of the popu-\nlation. Researchers interested in evaluating the mercury content of blueﬁn tuna from the Atlantic\nOcean could collect a sample of 500 blueﬁn tuna (or some other quantity), measure the mercury\ncontent, and use the observed information to formulate an answer to the research question.\nGUIDED PRACTICE 1.6\nIdentify the target populations for the remaining two research questions.16\n16In Question 2, the target population consists of infants predisposed to developing a peanut allergy. In Question 3, the\ntarget population consists of patients with glioblastoma.\n1.3. DATA COLLECTION PRINCIPLES 19\n1.3.2 Anecdotal evidence\nAnecdotal evidence typically refers to unusual observations that are easily recalled because of\ntheir striking characteristics. Physicians may be more likely to remember the characteristics of a\nsingle patient with an unusually good response to a drug instead of the many patients who did not\nrespond. The dangers of drawing general conclusions from anecdotal information are obvious; no\nsingle observation should be used to draw conclusions about a population.\nWhile it is incorrect to generalize from individual observations, unusual observations can\nsometimes be valuable. E.C. Heyde was a general practitioner from Vancouver who noticed that a\nfew of his elderly patients with aortic-valve stenosis (an abnormal narrowing) caused by an accu-\nmulation of calcium had also su ﬀered massive gastrointestinal bleeding. In 1958, he published his\nobservation.17Further research led to the identiﬁcation of the underlying cause of the association,\nnow called Heyde’s Syndrome.18\nAn anecdotal observation can never be the basis for a conclusion, but may well inspire the\ndesign of a more systematic study that could be deﬁnitive.\n17Heyde EC. Gastrointestinal bleeding in aortic stenosis. N Engl J Med 1958;259:196.\n18Greenstein RJ, McElhinney AJ, Reuben D, Greenstein AJ. Co-lonic vascular ectasias and aortic stenosis: coincidence or\ncausal relationship? Am J Surg 1986;151:347-51.\n20 CHAPTER 1. INTRODUCTION TO DATA\n1.3.3 Sampling from a population\nSampling from a population, when done correctly, provides reliable information about the\ncharacteristics of a large population. The US Centers for Disease Control (US CDC) conducts sev-\neral surveys to obtain information about the US population, including the Behavior Risk Factor\nSurveillance System (BRFSS).19The BRFSS was established in 1984 to collect data about health-\nrelated risk behaviors, and now collects data from more than 400,000 telephone interviews con-\nducted each year. Data from a recent BRFSS survey are used in Chapter 4. The CDC conducts\nsimilar surveys for diabetes, health care access, and immunization. Likewise, the World Health Or-\nganization (WHO) conducts the World Health Survey in partnership with approximately 70 coun-\ntries to learn about the health of adult populations and the health systems in those countries.20\nThe general principle of sampling is straightforward: a sample from a population is useful for\nlearning about a population only when the sample is representative of the population. In other\nwords, the characteristics of the sample should correspond to the characteristics of the population.\nSuppose that the quality improvement team at an integrated health care system, such as Har-\nvard Pilgrim Health Care, is interested in learning about how members of the health plan perceive\nthe quality of the services o ﬀered under the plan. A common pitfall in conducting a survey is to\nuse a convenience sample , in which individuals who are easily accessible are more likely to be\nincluded in the sample than other individuals. If a sample were collected by approaching plan\nmembers visiting an outpatient clinic during a particular week, the sample would fail to enroll\ngenerally healthy members who typically do not use outpatient services or schedule routine phys-\nical examinations; this method would produce an unrepresentative sample (Figure 1.9).\nFigure 1.9: Instead of sampling from all members equally, approaching members\nvisiting a clinic during a particular week disproportionately selects members who\nfrequently use outpatient services.\nRandom sampling is the best way to ensure that a sample reﬂects a population. In a simple\nrandom sample , each member of a population has the same chance of being sampled. One way to\nachieve a simple random sample of the health plan members is to randomly select a certain number\nof names from the complete membership roster, and contact those individuals for an interview\n(Figure 1.10).\n19https://www.cdc.gov/brfss/index.html\n20http://www.who.int/healthinfo/survey/en/\n1.3. DATA COLLECTION PRINCIPLES 21\nFigure 1.10: Five members are randomly selected from the population to be in-\nterviewed.\nEven when a simple random sample is taken, it is not guaranteed that the sample is represen-\ntative of the population. If the non-response rate for a survey is high, that may be indicative of\na biased sample. Perhaps a majority of participants did not respond to the survey because only a\ncertain group within the population is being reached; for example, if questions assume that par-\nticipants are ﬂuent in English, then a high non-response rate would be expected if the population\nlargely consists of individuals who are not ﬂuent in English (Figure 1.11). Such non-response\nbias can skew results; generalizing from an unrepresentative sample may likely lead to incorrect\nconclusions about a population.\nFigure 1.11: Surveys may only reach a certain group within the population, which\nleads to non-response bias. For example, a survey written in English may only\nresult in responses from health plan members ﬂuent in English.\nGUIDED PRACTICE 1.7\nIt is increasingly common for health care facilities to follow-up a patient visit with an email pro-\nviding a link to a website where patients can rate their experience. Typically, less than 50% of\npatients visit the website. If half of those who respond indicate a negative experience, do you think\nthat this implies that at least 25% of patient visits are unsatisfactory?21\n21It is unlikely that the patients who respond constitute a representative sample from the larger population of patients.\nThis is not a random sample, because individuals are selecting themselves into a group, and it is unclear that each person\nhas an equal chance of answering the survey. If our experience is any guide, dissatisﬁed people are more likely to respond\nto these informal surveys than satisﬁed patients.\n22 CHAPTER 1. INTRODUCTION TO DATA\n1.3.4 Sampling methods\nAlmost all statistical methods are based on the notion of implied randomness. If data are not\nsampled from a population at random, these statistical methods – calculating estimates and errors\nassociated with estimates – are not reliable. Four random sampling methods are discussed in this\nsection: simple, stratiﬁed, cluster, and multistage sampling.\nIn a simple random sample , each case in the population has an equal chance of being included\nin the sample (Figure 1.12). Under simple random sampling, each case is sampled independently of\nthe other cases; i.e., knowing that a certain case is included in the sample provides no information\nabout which other cases have also been sampled.\nInstratiﬁed sampling , the population is ﬁrst divided into groups called strata before cases\nare selected within each stratum (typically through simple random sampling) (Figure 1.12). The\nstrata are chosen such that similar cases are grouped together. Stratiﬁed sampling is especially\nuseful when the cases in each stratum are very similar with respect to the outcome of interest, but\ncases between strata might be quite di ﬀerent.\nSuppose that the health care provider has facilities in di ﬀerent cities. If the range of services\noﬀered di ﬀer by city, but all locations in a given city will o ﬀer similar services, it would be e ﬀective\nfor the quality improvement team to use stratiﬁed sampling to identify participants for their study,\nwhere each city represents a stratum and plan members are randomly sampled from each city.\n1.3. DATA COLLECTION PRINCIPLES 23\nIndex●\n●\n●●\n●●●\n●\n●●●\n●●●●\n●\n●●●\n●●\n●●\n●●\n●●●\n●\n●●\n●●●\n●●\n●●●●\n●\n●\n●\n●●●\n●●\n●●●●●●\n●\n●\n●\n● ●●●●\n●●\n●●\n●\n●●\n●●●\n●●\n●●\n●\n●●\n●●●\n●●●\n●●●●\n●\n●\n●●●\n●\n● ●\n●●●●\n● ●●●\n●●\n●●●\n●\n●●\n●●\n●●\n●\n●●\n●●\n●\n●●\n●●●\n●\n●●\n●●\n●●\n●\n●●\n●●\n●\n●●\n●\n●\n●●●●\n●●●\n●●●\n●●\n●\n●\n●●\n●● ●\n●●●\n●●●\n●●\n●\n●●\n●● ●\n●\n●●\n●\n●●\n●\n●●\n●●●●\n●\n●\n●●\n●●\n●\n●●\n●\n●●●\n●●\n● ● ●●\n●●\n●●●\n●●●\n●\n●●\n●\n●●●\n●●●\n●●●\n●●●●●●●\n●●●\n●●\n●●●\n●●\n●●\n●●\n●●\n●●●\n●●\n●\n●●\n●●●\n●\n●●\n●●●\n●\n●●●\n●\n●●\n●●●\n●●●●●●\n●\n●●●\n●●\n●●\n●●\n●\n●●●●\n●●●\n●●\n●●\n● ●●\n●●●\nStratum 1Stratum 2\nStratum 3Stratum 4\nStratum 5Stratum 6\nFigure 1.12: Examples of simple random and stratiﬁed sampling. In the top\npanel, simple random sampling is used to randomly select 18 cases (circled or-\nange dots) out of the total population (all dots). The bottom panel illustrates\nstratiﬁed sampling: cases are grouped into six strata, then simple random sam-\npling is employed within each stratum.\n24 CHAPTER 1. INTRODUCTION TO DATA\nIn a cluster sample , the population is ﬁrst divided into many groups, called clusters . Then,\na ﬁxed number of clusters is sampled and all observations from each of those clusters are included\nin the sample (Figure 1.13). A multistage sample is similar to a cluster sample, but rather than\nkeeping all observations in each cluster, a random sample is collected within each selected cluster\n(Figure 1.13).\nUnlike with stratiﬁed sampling, cluster and multistage sampling are most helpful when there\nis high case-to-case variability within a cluster, but the clusters themselves are similar to one an-\nother. For example, if neighborhoods in a city represent clusters, cluster and multistage sampling\nwork best when the population within each neighborhood is very diverse, but neighborhoods are\nrelatively similar.\nApplying stratiﬁed, cluster, or multistage sampling can often be more economical than only\ndrawing random samples. However, analysis of data collected using such methods is more com-\nplicated than when using data from a simple random sample; this text will only discuss analysis\nmethods for simple random samples.\nEXAMPLE 1.8\nSuppose researchers are interested in estimating the malaria rate in a densely tropical portion of\nrural Indonesia. There are 30 villages in the area, each more or less similar to the others. The goal\nis to test 150 individuals for malaria. Evaluate which sampling method should be employed.\nA simple random sample would likely draw individuals from all 30 villages, which could make\ndata collection extremely expensive. Stratiﬁed sampling is not advisable, since there is not enough\ninformation to determine how strata of similar individuals could be built. However, cluster sam-\npling or multistage sampling are both reasonable options. For example, with multistage sampling,\nhalf of the villages could be randomly selected, and then 10 people selected from each village. This\nstrategy is more e ﬃcient than a simple random sample, and can still provide a sample representa-\ntive of the population of interest.\n1.3.5 Introducing experiments and observational studies\nThe two primary types of study designs used to collect data are experiments and observational\nstudies.\nIn an experiment , researchers directly inﬂuence how data arise, such as by assigning groups of\nindividuals to di ﬀerent treatments and assessing how the outcome varies across treatment groups.\nThe LEAP study is an example of an experiment with two groups, an experimental group that\nreceived the intervention (peanut consumption) and a control group that received a standard ap-\nproach (peanut avoidance). In studies assessing e ﬀectiveness of a new drug, individuals in the\ncontrol group typically receive a placebo , an inert substance with the appearance of the experi-\nmental intervention. The study is designed such that on average, the only di ﬀerence between the\nindividuals in the treatment groups is whether or not they consumed peanut protein. This allows\nfor observed di ﬀerences in experimental outcome to be directly attributed to the intervention and\nconstitute evidence of a causal relationship between intervention and outcome.\nIn an observational study , researchers merely observe and record data, without interfering\nwith how the data arise. For example, to investigate why certain diseases develop, researchers\nmight collect data by conducting surveys, reviewing medical records, or following a cohort of\nmany similar individuals. Observational studies can provide evidence of an association between\nvariables, but cannot by themselves show a causal connection. However, there are many instances\nwhere randomized experiments are unethical, such as to explore whether lead exposure in young\nchildren is associated with cognitive impairment.\n1.3. DATA COLLECTION PRINCIPLES 25\nIndex●● ●●\n●●●\n●●●\n●\n●●●\n●\n●●●●●\n●●●\n●●\n●●● ●●\n●\n●●\n●\n●●\n●\n●●\n●\n●●\n●●●\n●●●\n●●●\n●●\n●●●\n●●●\n●●●\n●\n●\n●\n●\n●●\n●\n●●\n●●●\n●●\n●\n●●●●●\n●\n●●●\n●\n●\n●●\n●●● ●●\n●\n●●●\n●\n●\n●●\n●●●\n●●●\n●●\n●\n●\n● ●●●\n●\n● ●\n●●\n●\n●●●\n●●●\n●\n●●●●●●\n●●●\n●●●\n●\n●●\n●●\n●●\n●\n●●\n●\n●●●●\n●\n●●●\n●●\n●●●\n●\n●●●\n●\n●●\n●\n●●●●\n●●●●\n●\n●●●\n●\n●●\n●\n●●●●\n●●●\n●●\n●●●\n●●\n●●\n●\nCluster 1Cluster 2\nCluster 3\nCluster 4Cluster 5\nCluster 6Cluster 7\nCluster 8Cluster 9\n●\n●●●\n●●●●\n●●\n●\n●●\n●●\n●●\n●●●●\n●\n●●●\n●●\n●●\n●\n●●\n●●\n●●●\n●\n●●\n●\n●●\n●●●\n●\n●●\n●●●\n●\n●\n●●●●\n●●\n●●●●●\n●●\n●●\n●●●\n●\n●●\n●●●●●\n●●\n●●\n●●●●\n●●●\n●●●\n●●\n●● ●●\n●\n●●\n●●\n●●\n●●●\n●●●\n●●●●\n●●●\n●●●\n●\n●●\n●●\n●●\n●●●\n●●\n●●●\n●\n●●\n●\n●●\n●\n●●\n●\n●●●●\n●●●\n●\n●●●●\n●●\n●\nCluster 1Cluster 2\nCluster 3\nCluster 4Cluster 5\nCluster 6Cluster 7\nCluster 8Cluster 9\nFigure 1.13: Examples of cluster and multistage sampling. The top panel illus-\ntrates cluster sampling: data are binned into nine clusters, three of which are sam-\npled, and all observations within these clusters are sampled. The bottom panel\nillustrates multistage sampling, which di ﬀers from cluster sampling in that only\na subset from each of the three selected clusters are sampled.\n26 CHAPTER 1. INTRODUCTION TO DATA\n1.3.6 Experiments\nExperimental design is based on three principles: control, randomization, and replication.\nControl. When selecting participants for a study, researchers work to control for extraneous vari-\nables and choose a sample of participants that is representative of the population of interest.\nFor example, participation in a study might be restricted to individuals who have a condition\nthat suggests they may beneﬁt from the intervention being tested. Infants enrolled in the\nLEAP study were required to be between 4 and 11 months of age, with severe eczema and/or\nallergies to eggs.\nRandomization. Randomly assigning patients to treatment groups ensures that groups are bal-\nanced with respect to both variables that can and cannot be controlled. For example, random-\nization in the LEAP study ensures that the proportion of males to females is approximately\nthe same in both groups. Additionally, perhaps some infants were more susceptible to peanut\nallergy because of an undetected genetic condition; under randomization, it is reasonable to\nassume that such infants were present in equal numbers in both groups. Randomization al-\nlows di ﬀerences in outcome between the groups to be reasonably attributed to the treatment\nrather than inherent variability in patient characteristics, since the treatment represents the\nonly systematic di ﬀerence between the two groups.\nIn situations where researchers suspect that variables other than the intervention may in-\nﬂuence the response, individuals can be ﬁrst grouped into blocks according to a certain at-\ntribute and then randomized to treatment group within each block; this technique is referred\nto as blocking orstratiﬁcation . The team behind the LEAP study stratiﬁed infants into two\ncohorts based on whether or not the child developed a red, swollen mark (a wheal) after\na skin test at the time of enrollment; afterwards, infants were randomized between peanut\nconsumption and avoidance groups. Figure 1.14 illustrates the blocking scheme used in the\nstudy.\nReplication. The results of a study conducted on a larger number of cases are generally more\nreliable than smaller studies; observations made from a large sample are more likely to be\nrepresentative of the population of interest. In a single study, replication is accomplished by\ncollecting a su ﬃciently large sample. The LEAP study randomized a total of 640 infants.\nRandomized experiments are an essential tool in research. The US Food and Drug Adminis-\ntration typically requires that a new drug can only be marketed after two independently conducted\nrandomized trials conﬁrm its safety and e ﬃcacy; the European Medicines Agency has a similar pol-\nicy. Large randomized experiments in medicine have provided the basis for major public health\ninitiatives. In 1954, approximately 750,000 children participated in a randomized study compar-\ning polio vaccine with a placebo.22In the United States, the results of the study quickly led to the\nwidespread and successful use of the vaccine for polio prevention.\n22Meier, Paul. \"The biggest public health experiment ever: the 1954 ﬁeld trial of the Salk poliomyelitis vaccine.\" Statistics:\na guide to the unknown . San Francisco: Holden-Day (1972): 2-13.\n1.3. DATA COLLECTION PRINCIPLES 27\nFigure 1.14: A simpliﬁed schematic of the blocking scheme used in the LEAP\nstudy, depicting 640 patients that underwent randomization. Patients are ﬁrst\ndivided into blocks based on response to the initial skin test, then each block\nis randomized between the avoidance and consumption groups. This strategy\nensures an even representation of patients in each group who had positive and\nnegative skin tests.\n28 CHAPTER 1. INTRODUCTION TO DATA\n1.3.7 Observational studies\nIn observational studies, researchers simply observe selected potential explanatory and re-\nsponse variables. Participants who di ﬀer in important explanatory variables may also di ﬀer in other\nways that inﬂuence response; as a result, it is not advisable to make causal conclusions about the re-\nlationship between explanatory and response variables based on observational data. For example,\nwhile observational studies of obesity have shown that obese individuals tend to die sooner than\nindividuals with normal weight, it would be misleading to conclude that obesity causes shorter life\nexpectancy. Instead, underlying factors are probably involved; obese individuals typically exhibit\nother health behaviors that inﬂuence life expectancy, such as reduced exercise or unhealthy diet.\nSuppose that an observational study tracked sunscreen use and incidence of skin cancer, and\nfound that the more sunscreen a person uses, the more likely they are to have skin cancer. These\nresults do not mean that sunscreen causes skin cancer. One important piece of missing information\nis sun exposure – if someone is often exposed to sun, they are both more likely to use sunscreen and\nto contract skin cancer. Sun exposure is a confounding variable : a variable associated with both\nthe explanatory and response variables.23There is no guarantee that all confounding variables\ncan be examined or measured; as a result, it is not advisable to draw causal conclusions from\nobservational studies.\nConfounding is not limited to observational studies. For example, consider a randomized\nstudy comparing two treatments (varenicline and buproprion) against a placebo as therapies for\naiding smoking cessation.24At the beginning of the study, participants were randomized into\ngroups: 352 to varenicline, 329 to buproprion, and 344 to placebo. Not all participants successfully\ncompleted the assigned therapy: 259, 225, and 215 patients in each group did so, respectively.\nIf an analysis were based only on the participants who completed therapy, this could introduce\nconfounding; it is possible that there are underlying di ﬀerences between individuals who complete\nthe therapy and those who do not. Including all randomized participants in the ﬁnal analysis\nmaintains the original randomization scheme and controls for di ﬀerences between the groups.25\nGUIDED PRACTICE 1.9\nAs stated in Example 1.4, female body size ( body.size ) in the parental investment study is neither\nan explanatory nor a response variable. Previous research has shown that larger females tend to\nproduce larger eggs and egg clutches; however, large body size can be costly at high altitudes.\nDiscuss a possible reason for why the study team chose to measure female body size when it is not\ndirectly related to their main research question.26\n23Also called a lurking variable ,confounding factor , or a confounder .\n24Jorenby, Douglas E., et al. \"E ﬃcacy of varenicline, an \u000b4\f2 nicotinic acetylcholine receptor partial agonist, vs placebo\nor sustained-release bupropion for smoking cessation: a randomized controlled trial.\" JAMA 296.1 (2006): 56-63.\n25This strategy, commonly used for analyzing clinical trial data, is referred to as an intention-to-treat analysis.\n26Female body size is a potential confounding variable, since it may be associated with both the explanatory variable\n(altitude) and response variables (measures of maternal investment). If the study team observes, for example, that clutch\nsize tends to decrease at higher altitudes, they should check whether the apparent association is not simply due to frogs at\nhigher altitudes having smaller body size and thus, laying smaller clutches.\n1.3. DATA COLLECTION PRINCIPLES 29\nObservational studies may reveal interesting patterns or associations that can be further in-\nvestigated with follow-up experiments. Several observational studies based on dietary data from\ndiﬀerent countries showed a strong association between dietary fat and breast cancer in women.\nThese observations led to the launch of the Women’s Health Initiative (WHI), a large randomized\ntrial sponsored by the US National Institutes of Health (NIH). In the WHI, women were random-\nized to standard versus low fat diets, and the previously observed association was not conﬁrmed.\nObservational studies can be either prospective or retrospective. A prospective study identi-\nﬁes participants and collects information at scheduled times or as events unfold. For example, in\nthe Nurses’ Health Study, researchers recruited registered nurses beginning in 1976 and collected\ndata through administering biennial surveys; data from the study have been used to investigate risk\nfactors for major chronic diseases in women.27Retrospective studies collect data after events have\ntaken place, such as from medical records. Some datasets may contain both retrospectively- and\nprospectively-collected variables. The Cancer Care Outcomes Research and Surveillance Consor-\ntium (CanCORS) enrolled participants with lung or colorectal cancer, collected information about\ndiagnosis, treatment, and previous health behavior, but also maintained contact with participants\nto gather data about long-term outcomes.28\n27www.channing.harvard.edu/nhs\n28Ayanian, John Z., et al. \"Understanding cancer treatment and outcomes: the cancer care outcomes research and\nsurveillance consortium.\" Journal of Clinical Oncology 22.15 (2004): 2992-2996\n30 CHAPTER 1. INTRODUCTION TO DATA\n1.4 Numerical data\nThis section discusses techniques for exploring and summarizing numerical variables, using\nthefrog data from the parental investment study introduced in Section 1.2.\n1.4.1 Measures of center: mean and median\nThe mean , sometimes called the average, is a measure of center for a distribution of data. To\nﬁnd the average clutch volume for the observed egg clutches, add all the clutch volumes and divide\nby the total number of clutches.29\nx=177:8 + 257:0 +\u0001\u0001\u0001+ 933:3\n431= 882:5 mm3:\nThe sample mean is often labeled x, to distinguish it from \u0016, the mean of the entire population x\nsample\nmean\n\u0016\npopulation\nmeanfrom which the sample is drawn. The letter xis being used as a generic placeholder for the variable\nof interest, clutch.volume .\nMEAN\nThe sample mean of a numerical variable is the sum of the values of all observations divided\nby the number of observations:\nx=x1+x2+\u0001\u0001\u0001+xn\nn; (1.10)\nwherex1;x2;:::;xnrepresent the nobserved values.\nThe median is another measure of center; it is the middle number in a distribution after the\nvalues have been ordered from smallest to largest. If the distribution contains an even number of\nobservations, the median is the average of the middle two observations. There are 431 clutches\nin the dataset, so the median is the clutch volume of the 216thobservation in the sorted values of\nclutch.volume : 831:8 mm3.\n29For computational convenience, the volumes are rounded to the ﬁrst decimal.\n1.4. NUMERICAL DATA 31\n1.4.2 Measures of spread: standard deviation and interquartile range\nThe spread of a distribution refers to how similar or varied the values in the distribution are\nto each other; i.e., whether the values are tightly clustered or spread over a wide range.\nThe standard deviation for a set of data describes the typical distance between an observation\nand the mean. The distance of a single observation from the mean is its deviation . Below are the\ndeviations for the 1st, 2nd, 3rd, and 431stobservations in the clutch.volume variable.\nx1\u0000x= 177:8\u0000882:5 =\u0000704:7\nx2\u0000x= 257:0\u0000882:5 =\u0000625:5\nx3\u0000x= 151:4\u0000882:5 =\u0000731:1\n:::\nx431\u0000x= 933:2\u0000882:5 = 50:7\nThe sample variance , the average of the squares of these deviations, is denoted by s2: s2\nsample\nvariance\ns2=(\u0000704:7)2+ (\u0000625:5)2+ (\u0000731:1)2+\u0001\u0001\u0001+ (50:7)2\n431\u00001\n=496;602:09 + 391;250:25 + 534;507:21 +\u0001\u0001\u0001+ 2570:49\n430\n= 143;680:9:\nThe denominator is n\u00001 rather than n; this mathematical nuance accounts for the fact that sample\nmean has been used to estimate the population mean in the calculation. Details on the statistical\ntheory can be found in more advanced texts.\nThe sample standard deviation sis the square root of the variance:\ns=p\n143;680:9 = 379:05mm3: s\nsample\nstandard\ndeviationLike the mean, the population values for variance and standard deviation are denoted by\nGreek letters: \u001b2for the variance and \u001bfor the standard deviation. \u001b2\npopulation\nvariance\n\u001b\npopulation\nstandard\ndeviationSTANDARD DEVIATION\nThe sample standard deviation of a numerical variable is computed as the square root of the\nvariance, which is the sum of squared deviations divided by the number of observations minus\n1.\ns=s\n(x1\u0000x)2+ (x2\u0000x)2+\u0001\u0001\u0001+ (xn\u0000x)2\nn\u00001; (1.11)\nwherex1;x2;:::;xnrepresent the nobserved values.\n32 CHAPTER 1. INTRODUCTION TO DATA\nVariability can also be measured using the interquartile range (IQR). The IQR for a distri-\nbution is the di ﬀerence between the ﬁrst and third quartiles: Q3\u0000Q1. The ﬁrst quartile ( Q1) is\nequivalent to the 25thpercentile; i.e., 25% of the data fall below this value. The third quartile\n(Q3) is equivalent to the 75thpercentile. By deﬁnition, the median represents the second quar-\ntile, with half the values falling below it and half falling above. The IQR for clutch.volume is\n1096:0\u0000609:6 = 486:4 mm3.\nMeasures of center and spread are ways to summarize a distribution numerically. Using nu-\nmerical summaries allows for a distribution to be e ﬃciently described with only a few numbers.30\nFor example, the calculations for clutch.volume indicate that the typical egg clutch has volume\nof about 880 mm3, while the middle 50% of egg clutches have volumes between approximately\n600 mm3and 1100:0 mm3.\n1.4.3 Robust estimates\nFigure 1.15 shows the values of clutch.volume as points on a single axis. There are a few\nvalues that seem extreme relative to the other observations: the four largest values, which appear\ndistinct from the rest of the distribution. How do these extreme values a ﬀect the value of the\nnumerical summaries?\nClutch Volumes100300500700900110013001500170019002100230025002700\nFigure 1.15: Dot plot of clutch volumes from the frog data.\nFigure 1.16 shows the summary statistics calculated under two scenarios, one with and one\nwithout the four largest observations. For these data, the median does not change, while the IQR\ndiﬀers by only about 6 mm3. In contrast, the mean and standard deviation are much more a ﬀected,\nparticularly the standard deviation.\nrobust not robust\nscenario median IQR x s\noriginal data (with extreme observations) 831.8 486.9 882.5 379.1\ndata without four largest observations 831.8 493.9 867.9 349.2\nFigure 1.16: A comparison of how the median, IQR, mean ( x), and standard devi-\nation (s) change when extreme observations are present.\nThe median and IQR are referred to as robust estimates because extreme observations have\nlittle eﬀect on their values. For distributions that contain extreme values, the median and IQR will\nprovide a more accurate sense of the center and spread than the mean and standard deviation.\n30Numerical summaries are also known as summary statistics.\n1.4. NUMERICAL DATA 33\n1.4.4 Visualizing distributions of data: histograms and boxplots\nGraphs show important features of a distribution that are not evident from numerical sum-\nmaries, such as asymmetry or extreme values. While dot plots show the exact value of each obser-\nvation, histograms and boxplots graphically summarize distributions.\nIn a histogram , observations are grouped into bins and plotted as bars. Figure 1.17 shows the\nnumber of clutches with volume between 0 and 200 mm3, 200 and 400 mm3, etc. up until 2,600\nand 2,800 mm3.31These binned counts are plotted in Figure 1.18.\nClutch volumes 0-200 200-400 400-600 600-800 \u0001\u0001\u0001 2400-2600 2600-2800\nCount 4 29 69 99 \u0001\u0001\u0001 2 1\nFigure 1.17: The counts for the binned clutch.volume data.\nClutch VolumeFrequency\n0 500 1000 1500 2000 2500020406080100\nFigure 1.18: A histogram of clutch.volume .\nHistograms provide a view of the data density . Higher bars indicate more frequent obser-\nvations, while lower bars represent relatively rare observations. Figure 1.18 shows that most of\nthe egg clutches have volumes between 500-1,000 mm3, and there are many more clutches with\nvolumes smaller than 1,000 mm3than clutches with larger volumes.\nHistograms show the shape of a distribution. The tails of a symmetric distribution are\nroughly equal, with data trailing o ﬀfrom the center roughly equally in both directions. Asym-\nmetry arises when one tail of the distribution is longer than the other. A distribution is said to be\nright skewed when data trail o ﬀto the right, and left skewed when data trail o ﬀto the left.32Fig-\nure 1.18 shows that the distribution of clutch volume is right skewed; most clutches have relatively\nsmall volumes, and only a few clutches have high volumes.\n31By default in R, the bins are left-open and right-closed; i.e., the intervals are of the form (a, b]. Thus, an observation\nwith value 200 would fall into the 0-200 bin instead of the 200-400 bin.\n32Other ways to describe data that are skewed to the right/left: skewed to the right/left orskewed to the posi-\ntive/negative end .\n34 CHAPTER 1. INTRODUCTION TO DATA\nAmode is represented by a prominent peak in the distribution.33Figure 1.19 shows his-\ntograms that have one, two, or three major peaks. Such distributions are called unimodal ,bi-\nmodal , and multimodal , respectively. Any distribution with more than two prominent peaks is\ncalled multimodal. Note that the less prominent peak in the unimodal distribution was not counted\nsince it only di ﬀers from its neighboring bins by a few observations. Prominent is a subjective term,\nbut it is usually clear in a histogram where the major peaks are.\n051015051015\n05101520051015\n0510152005101520\nFigure 1.19: From left to right: unimodal, bimodal, and multimodal distributions.\nAboxplot indicates the positions of the ﬁrst, second, and third quartiles of a distribution\nin addition to extreme observations.34Figure 1.20 shows a boxplot of clutch.volume alongside a\nvertical dot plot.\nClutch Volume\n5001000150020002500\nlower whiskerQ1  (first quartile)median (second quartile)Q3  (third quartile)upper whiskeroutliers\n−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−\nFigure 1.20: A boxplot and dot plot of clutch.volume . The horizontal dashes\nindicate the bottom 50% of the data and the open circles represent the top 50%.\n33Another deﬁnition of mode, which is not typically used in statistics, is the value with the most occurrences. It is\ncommon that a dataset contains noobservations with the same value, which makes this other deﬁnition impractical for\nmany datasets.\n34Boxplots are also known as box-and-whisker plots.\n1.4. NUMERICAL DATA 35\nIn a boxplot, the interquartile range is represented by a rectangle extending from the ﬁrst\nquartile to the third quartile, and the rectangle is split by the median (second quartile). Extending\noutwards from the box, the whiskers capture the data that fall between Q1\u00001:5\u0002IQR andQ3+1:5\u0002\nIQR . The whiskers must end at data points; the values given by adding or subtracting 1 :5\u0002IQR\ndeﬁne the maximum reach of the whiskers. For example, with the clutch.volume variable,Q3+\n1:5\u0002IQR = 1;096:5+1:5\u0002486:4 = 1;826:1 mm3. However, there was no clutch with volume 1,826.1\nmm3; thus, the upper whisker extends to 1,819.7 mm3, the largest observation that is smaller than\nQ3+ 1:5\u0002IQR .\nAny observation that lies beyond the whiskers is shown with a dot; these observations are\ncalled outliers. An outlier is a value that appears extreme relative to the rest of the data. For\ntheclutch.volume variable, there are several large outliers and no small outliers, indicating the\npresence of some unusually large egg clutches.\nThe high outliers in Figure 1.20 reﬂect the right-skewed nature of the data. The right skew is\nalso observable from the position of the median relative to the ﬁrst and third quartiles; the median\nis slightly closer to the ﬁrst quartile. In a symmetric distribution, the median will be halfway\nbetween the ﬁrst and third quartiles.\nGUIDED PRACTICE 1.12\nUse the histogram and boxplot in Figure 1.21 to describe the distribution of height in the famuss\ndata, where height is measured in inches.35\nHeightFrequency\n60 65 70 75050100150\n(a)\nHeight\n556065707580 (b)\nFigure 1.21: A histogram and boxplot of height in the famuss data.\n35The data are roughly symmetric (the left tail is slightly longer than the right tail), and the distribution is unimodal\nwith one prominent peak at about 67 inches. The middle 50% of individuals are between 5.5 feet and just under 6 feet tall.\nThere is one low outlier and one high outlier, representing individuals that are unusually short/tall relative to the other\nindividuals.\n36 CHAPTER 1. INTRODUCTION TO DATA\n1.4.5 Transforming data\nWhen working with strongly skewed data, it can be useful to apply a transformation , and\nrescale the data using a function. A natural log transformation is commonly used to clarify the\nfeatures of a variable when there are many values clustered near zero and all observations are\npositive.\nIncome (USD)Frequency\n$0 $20k $40k $60k $80k $100k $120k020406080100\n(a)\nIncome (log USD)Frequency\n56789101112051015202530 (b)\nFigure 1.22: (a) Histogram of per capita income. (b) Histogram of the log-\ntransformed per capita income.\nFor example, income data are often skewed right; there are typically large clusters of low to\nmoderate income, with a few large incomes that are outliers. Figure 1.22(a) shows a histogram of\naverage yearly per capita income measured in US dollars for 165 countries in 2011.36The data are\nheavily right skewed, with the majority of countries having average yearly per capita income lower\nthan $10,000. Once the data are log-transformed, the distribution becomes roughly symmetric\n(Figure 1.22(b)).37\nFor symmetric distributions, the mean and standard deviation are particularly informative\nsummaries. If a distribution is symmetric, approximately 70% of the data are within one standard\ndeviation of the mean and 95% of the data are within two standard deviations of the mean; this\nguideline is known as the empirical rule .\nEXAMPLE 1.13\nOn the log-transformed scale, mean log income is 8.50, with standard deviation 1.54. Apply the\nempirical rule to describe the distribution of average yearly per capita income among the 165\ncountries.\nAccording to the empirical rule, the middle 70% of the data are within one standard deviation of\nthe mean, in the range (8.50 - 1.54, 8.50 + 1.54) = (6.96, 10.04) log(USD). 95% of the data are within\ntwo standard deviations of the mean, in the range (8.50 - 2(1.54), 8.50 + 2(1.54)) = (5.42, 11.58)\nlog(USD).\nUndo the log transformation. The middle 70% of the data are within the range ( e6:96,e10:04)\n= ($1,054, $22,925). The middle 95% of the data are within the range ( e5:42,e11:58) = ($226,\n$106,937).\nFunctions other than the natural log can also be used to transform data, such as the square\nroot and inverse.\n36The data are available as wdi.2011 in the Rpackage oibiostat .\n37In statistics, the natural logarithm is usually written log. In other settings it is sometimes written as ln.\n1.5. CATEGORICAL DATA 37\n1.5 Categorical data\nThis section introduces tables and plots for summarizing categorical data, using the famuss\ndataset introduced in Section 1.2.2.\nA table for a single variable is called a frequency table . Figure 1.23 is a frequency table for\ntheactn3.r577x variable, showing the distribution of genotype at location r577x on the ACTN3\ngene for the FAMuSS study participants.\nIn a relative frequency table like Figure 1.24, the proportions per each category are shown\ninstead of the counts.\nCC CT TT Sum\nCounts 173 261 161 595\nFigure 1.23: A frequency table for the actn3.r577x variable.\nCC CT TT Sum\nProportions 0.291 0.439 0.271 1.000\nFigure 1.24: A relative frequency table for the actn3.r577x variable.\nA bar plot is a common way to display a single categorical variable. The left panel of Fig-\nure 1.25 shows a bar plot of the counts per genotype for the actn3.r577x variable. The plot in the\nright panel shows the proportion of observations that are in each level (i.e. in each genotype).\nCC CT TTcount\n050100150200250300\ngenotypeCC CT TT0.000.250.50proportion\ngenotype\nFigure 1.25: Two bar plots of actn3.r577x . The left panel shows the counts, and\nthe right panel shows the proportions for each genotype.\n38 CHAPTER 1. INTRODUCTION TO DATA\n1.6 Relationships between two variables\nThis section introduces numerical and graphical methods for exploring and summarizing re-\nlationships between two variables. Approaches vary depending on whether the two variables are\nboth numerical, both categorical, or whether one is numerical and one is categorical.\n1.6.1 Two numerical variables\nScatterplots\nIn the frog parental investment study, researchers used clutch volume as a primary variable of\ninterest rather than egg size because clutch volume represents both the eggs and the protective\ngelatinous matrix surrounding the eggs. The larger the clutch volume, the higher the energy re-\nquired to produce it; thus, higher clutch volume is indicative of increased maternal investment.\nPrevious research has reported that larger body size allows females to produce larger clutches; is\nthis idea supported by the frog data?\nAscatterplot provides a case-by-case view of the relationship between two numerical vari-\nables. Figure 1.26 shows clutch volume plotted against body size, with clutch volume on the y-axis\nand body size on the x-axis. Each point represents a single case. For this example, each case is one\negg clutch for which both volume and body size (of the female that produced the clutch) have been\nrecorded.\n4.0 4.5 5.0 5.5 6.05001000150020002500Clutch Volume  (mm3)\n●●●●●●●●●●●\n●●●●●\n●●\n●\n●●\n●●\n●●●●●●●●●\n●●●\n●●\n●●●\n●●\n●●●●\n●●●\n●●\n●●\n●\n●●●\n●●●\n●●●●●\n●●●●●●\n●●\n●●●●●\n●●●\n●●●\n●●●●\n●●\n●●\n●●\n●●\n●\n●●●●●\n●●●●\n●●●\n●\n●●●\n●●●\n●●●\n●●●●\n●●\n●●●\n●\nFemale Body Size (cm)\nFigure 1.26: A scatterplot showing clutch.volume (vertical axis) vs. body.size\n(horizontal axis).\nThe plot shows a discernible pattern, which suggests an association , or relationship, between\nclutch volume and body size; the points tend to lie in a straight line, which is indicative of a linear\nassociation . Two variables are positively associated if increasing values of one tend to occur with\nincreasing values of the other; two variables are negatively associated if increasing values of one\nvariable occurs with decreasing values of the other. If there is no evident relationship between two\nvariables, they are said to be uncorrelated orindependent .\nAs expected, clutch volume and body size are positively associated; larger frogs tend to pro-\nduce egg clutches with larger volumes. These observations suggest that larger females are capable\nof investing more energy into o ﬀspring production relative to smaller females.\n1.6. RELATIONSHIPS BETWEEN TWO V ARIABLES 39\nThe National Health and Nutrition Examination Survey (NHANES) consists of a set of surveys\nand measurements conducted by the US CDC to assess the health and nutritional status of adults\nand children in the United States. The following example uses data from a sample of 500 adults\n(individuals ages 21 and older) from the NHANES dataset.38\nEXAMPLE 1.14\nBody mass index (BMI) is a measure of weight commonly used by health agencies to assess whether\nsomeone is overweight, and is calculated from height and weight.39Describe the relationships\nshown in Figure 1.27. Why is it helpful to use BMI as a measure of obesity, rather than weight?\nFigure 1.27(a) shows a positive association between height and weight; taller individuals tend to\nbe heavier. Figure 1.27(b) shows that height and BMI do not seem to be associated; the range of\nBMI values observed is roughly consistent across height.\nWeight itself is not a good measure of whether someone is overweight; instead, it is more reasonable\nto consider whether someone’s weight is unusual relative to other individuals of a comparable\nheight. An individual weighing 200 pounds who is 6 ft tall is not necessarily an unhealthy weight;\nhowever, someone who weighs 200 pounds and is 5 ft tall is likely overweight. It is not reasonable\nto classify individuals as overweight or obese based only on weight.\nBMI acts as a relative measure of weight that accounts for height. Speciﬁcally, BMI is used as an\nestimate of body fat. According to US National Institutes of Health (US NIH) and the World Health\nOrganization (WHO), a BMI between 25.0 - 29.9 is considered overweight and a BMI over 30 is\nconsidered obese.40\n150 160 170 180 19050100150200Weight (kg)\n● ●●●\n●●\n●●\n●\n●\n●●\n●●\n●●\n●●\n●●\n●●● ●\n●●\n●●●●\n●●●\n●\n●● ●● ●●\n●●●●●\n●●\n●●●\n●●\n●●●\n●\n●●●\n●●\n●●●●\n●\n●●●\n●\n●●●\n●●\n●●●\n●\n●●\n●●\n●●●\n● ● ●\n●●●\n●●●\n●●●\n●●●\n●●●●●●●\n●●\n●●\n●\n●●●●\n●\n●●●\n●\n●●●●\n●\n●●\n●●\n●\n●\n●●\n●●\n●●\n●●\n●●\n●\n●● ●\n●●●\n●●●●●\n●\n●●●\n●\n●●●● ●●\n●●\n●●●\n●●\n●\n●●● ●\n● ●●●\n●\n●●\n●●●\n● ●●\n●●●\n●\n●\n● ●●\n●●\n●\n●●\n●●●●\n●●●●\n●●\n●●\n● ●\n●●\n●●●\n●\n●●●●●\n●●●\n●\n●●●\n●●\n●● ●●●\n●●\n●●●\n●●●\n●●●\n●●\n●●●\n●●\n●●\n●\n●●\n●\n●●●●\n●●●\n●●●●\n●●\n●●●●\n●●●\n●●\n● ●\n●●●\n●\n●●●\n●●●\n●●\n●●\n●●\n●●\n● ●●●\n●●\n●\n●●\n●●\n●●●●●●\n●●●\n● ●●\n●●\n●●●●\n●●\n●●\n●●\n●●\n●● ●\n●\n●●\n●●\n●\n●●\n●●\n●●●\n●\n●●\n●●●\n●●●\n● ●●\n●\n●●●●●●\n●\n●●\n●●●\n●●●\n●●\n●●\n●●●\n●●\n●●\n●\n●●\n● ●●●\n●\n●●\n●●●●\n●●●\n●\n●●●●\n●\n●●\n●●\n●\n●●●●●\n●●●\n●●\n●●\n●●●●\n●● ●\n●●\n●●● ●●\n●\n●●\n●● ●\n●●\n●●●\n●●●\n●\n●\n●●●\n●●●\n●●●\n●\n●●●●\n●\n●●\n● ●●\n●●\n●●\n●●\nHeight (cm)\n(a)\n150 160 170 180 190203040506070BMI\n●●\n● ●●●\n●●●\n●\n●●\n●●\n●\n●●●\n●●●●\n●\n●\n●●\n●●●●\n●\n●●●\n●●\n●●●\n●\n●●● ●●\n●●\n●●●\n●●\n●●●\n●\n●●● ●\n●\n●●●●\n● ●●●\n●\n●●●\n●●● ●●\n●\n●\n●●●\n●●●\n●\n●●\n●●●●●●\n●●●\n● ●●\n●●\n●●●●●\n●●\n●●\n●\n●●●●\n●\n●●\n●●\n●●●●\n●\n●●\n●●●\n●\n●●\n●●\n●●\n●●\n●●\n●\n●●\n●\n● ●●\n●●●●●\n●\n●●●\n●\n●●●●\n●●\n●●\n●●●\n●●\n●●●●\n●●●●●\n●\n●●\n●●●\n●\n●●●●\n●\n●\n● ●\n●●\n●●●\n●●\n●●●\n●\n●●●●\n●●\n●●\n●●\n●●●\n●●\n●\n●●\n●●●●●●\n●\n●●●\n●●●●●●\n●\n●●\n●\n●●\n●●●\n●●●\n●●\n●● ●●●\n●●\n● ●●●\n●\n●●\n●●\n●●\n●●\n●●\n●●●\n●●●\n●\n●●\n●●●\n●\n●●●\n●\n●●●\n●●\n●\n●●●●\n●●\n●●\n●● ●●\n●●●\n●●\n●●\n●●\n● ●●●\n●●●\n●●●\n●●\n●●●●\n●●\n●●\n●● ●●\n●●\n●\n●●●\n●●●\n●●\n●●\n●●●\n●\n● ●●●●\n●●●\n●●●\n●\n●●●●\n●●\n●\n●●\n●●●\n●●\n●\n●●\n●●\n●\n●●\n●●\n●●●\n●●\n●\n●\n●● ●\n●●\n●●●\n●●●\n●\n●●\n●●●\n●\n●●\n●●\n●\n●●●●●\n●●● ●●\n●●●●●●\n●●\n●\n●●\n●●\n●\n●● ●\n●●\n●\n●●\n●●\n●●●●●●\n●\n●\n●●●\n●●●\n●●●\n●\n●●●\n●●\n●●\n●●●\n● ●●●\n●●\nHeight (cm) (b)\nFigure 1.27: (a) A scatterplot showing height versus weight from the 500 individ-\nuals in the sample from NHANES . One participant 163.9 cm tall (about 5 ft, 4 in)\nand weighing 144.6 kg (about 319 lb) is highlighted. (b) A scatterplot showing\nheight versus BMI from the 500 individuals in the sample from NHANES . The same\nindividual highlighted in (a) is marked here, with BMI 53.83.\n38The sample is available as nhanes.samp.adult.500 in the Roibiostat package.\n39BMI =weightkg\nheight2m=weightlb\nheight2\nin\u0002703.\n40https://www.nhlbi.nih.gov/health/educational/lose_wt/risk.htm\n40 CHAPTER 1. INTRODUCTION TO DATA\nEXAMPLE 1.15\nFigure 1.28 is a scatterplot of life expectancy versus annual per capita income for 165 countries in\n2011. Life expectancy is measured as the expected lifespan for children born in 2011 and income\nis adjusted for purchasing power in a country. Describe the relationship between life expectancy\nand annual per capita income; do they seem to be linearly associated?\nLife expectancy and annual per capita income are positively associated; higher per capita income\nis associated with longer life expectancy. However, the two variables are not linearly associated.\nWhen income is low, small increases in per capita income are associated with relatively large in-\ncreases in life expectancy. However, once per capita income exceeds approximately $20,000 per\nyear, increases in income are associated with smaller gains in life expectancy.\nIn a linear association, change in the y-variable for every unit of the x-variable is consistent across\nthe range of the x-variable; for example, a linear association would be present if an increase in\nincome of $10,000 corresponded to an increase in life expectancy of 5 years, across the range of\nincome.\nLife Expectancy (years)●●\n●\n●●●●●\n●●\n●●●\n●●●●\n●●●\n●\n●●\n●●\n●●\n●●\n●\n●\n●\n●●●\n●●● ●\n●●\n●●●\n●●\n●●●\n●●●●\n●\n●●●\n●●\n●\n●\n●●\n●●●\n●●●\n●●●●\n●●\n●\n●\n●●\n●\n●\n●●\n●●\n●●●●●\n●\n●\n●●\n●●●●\n●\n●●\n●\n●●●● ●\n●\n●\n●●●\n●\n●●\n●●●\n●●●\n●\n●\n●\n●●\n●●\n●●\n●●\n●\n●●\n●●\n●\n●●\n●●●\n●\n●●\n●\n●●●●\n●\n●●●●\n●\n●\n●●●\n●\n●\n●\n●\n$0 $20k $40k $60k $80k $100k50556065707580\nFigure 1.28: A scatterplot of life expectancy (years) versus annual per capita in-\ncome (US dollars) in the wdi.2011 dataset.\nCorrelation\nCorrelation is a numerical summary statistic that measures the strength of a linear relationship be-\ntween two variables. It is denoted by r, the correlation coe ﬃcient , which takes on values between r\ncorrelation\ncoeﬃcient-1 and 1.\nIf the paired values of two variables lie exactly on a line, r=\u00061; the closer the correlation\ncoeﬃcient is to\u00061, the stronger the linear association. When two variables are positively associated,\nwith paired values that tend to lie on a line with positive slope, r>0. If two variables are negatively\nassociated,r<0. A value of rthat is 0 or approximately 0 indicates no apparent association between\ntwo variables.41\n41If paired values lie perfectly on either a horizontal or vertical line, there is no association and ris mathematically\nundeﬁned.\n1.6. RELATIONSHIPS BETWEEN TWO V ARIABLES 41\nR = 0.33\ny\nR = 0.69\ny\nR = 0.98\ny\nR = 1.00\nR = −0.08\ny\nR = −0.64\ny\nR = −0.92\ny\nR = −1.00\nFigure 1.29: Scatterplots and their correlation coe ﬃcients. The ﬁrst row shows\npositive associations and the second row shows negative associations. From left\nto right, strength of the linear association between xandyincreases.\nThe correlation coe ﬃcient quantiﬁes the strength of a linear trend. Prior to calculating a\ncorrelation, it is advisable to conﬁrm that the data exhibit a linear relationship. Although it is\nmathematically possible to calculate correlation for any set of paired observations, such as the life\nexpectancy versus income data in Figure 1.28, correlation cannot be used to assess the strength of\na nonlinear relationship.\nCORRELATION\nThe correlation between two variables xandyis given by:\nr=1\nn\u00001nX\ni=1 xi\u0000x\nsx! yi\u0000y\nsy!\n; (1.16)\nwhere (x1;y1);(x2;y2);:::;(xn;yn) are thenpaired values of xandy, andsxandsyare the sample\nstandard deviations of the xandyvariables, respectively.\n42 CHAPTER 1. INTRODUCTION TO DATA\nEXAMPLE 1.17\nCalculate the correlation coe ﬃcient ofxandy, plotted in Figure 1.30.\nCalculate the mean and standard deviation for xandy:x= 2,y= 3,sx= 1, andsy= 2:65.\nr=1\nn\u00001nX\ni=1 xi\u0000x\nsx! yi\u0000y\nsy!\n=1\n3\u00001\u0014\u00121\u00002\n1\u0013\u00125\u00003\n2:65\u0013\n+\u00122\u00002\n1\u0013\u00124\u00003\n2:65\u0013\n+\u00123\u00002\n1\u0013\u00120\u00003\n2:65\u0013\u0015\n=\u00000:94:\nThe correlation is -0.94, which reﬂects the negative association visible from the scatterplot in Fig-\nure 1.30.\n1.0 1.5 2.0 2.5 3.0012345\nxy●\n●\n●\nFigure 1.30: A scatterplot showing three points: (1, 5), (2, 4), and (3, 0).\n1.6. RELATIONSHIPS BETWEEN TWO V ARIABLES 43\nEXAMPLE 1.18\nIs it appropriate to use correlation as a numerical summary for the relationship between life ex-\npectancy and income after a log transformation is applied to both variables? Refer to Figure 1.31.\nFigure 1.31 shows an approximately linear relationship; a correlation coe ﬃcient is a reasonable\nnumerical summary of the relationship. As calculated from statistical software, r= 0:79, which is\nindicative of a strong linear relationship.\n6 7 8 910 113.94.04.14.24.34.4Life Expectancy (log years)●●\n●\n●●●●●\n●●\n● ●●\n●●●●\n●●●\n●\n●●\n●●\n●●\n●●\n●●\n●\n●●●\n●●● ●●●\n●●●\n●●\n●●●\n●●●●\n●\n●●●\n●●\n●\n●\n●●\n●●●\n●●●\n●●●●\n●●\n●\n●\n●●\n●\n●\n●●\n●●\n●●●●●\n●\n●\n●●\n●●●●\n●\n●●\n●\n●●●●●\n●\n●\n●●●\n●\n●●\n●●●\n●●●\n●\n●\n●\n●●\n●●\n●●\n●●\n●\n●●\n●●\n●\n●●\n●●●\n●\n●●\n●\n●●● ●\n●\n●●●●\n●\n●\n●●●\n●\n●\n●\n●\nPer Capita Income (log USD)\nFigure 1.31: A scatterplot showing log(income) (horizontal axis) vs.\nlog(life.expectancy) (vertical axis).\n1.6.2 Two categorical variables\nContingency tables\nAcontingency table summarizes data for two categorical variables, with each value in the table\nrepresenting the number of times a particular combination of outcomes occurs.42Figure 1.32\nsummarizes the relationship between race and genotype in the famuss data.\nThe row totals provide the total counts across each row and the column totals are the total\ncounts for each column; collectively, these are the marginal totals .\nCC CT TT Sum\nAfrican Am 16 6 5 27\nAsian 21 18 16 55\nCaucasian 125 216 126 467\nHispanic 4 10 9 23\nOther 7 11 5 23\nSum 173 261 161 595\nFigure 1.32: A contingency table for race and actn3.r577x .\n42Contingency tables are also known as two-way tables .\n44 CHAPTER 1. INTRODUCTION TO DATA\nLike relative frequency tables for the distribution of one categorical variable, contingency\ntables can also be converted to show proportions. Since there are two variables, it is necessary\nto specify whether the proportions are calculated according to the row variable or the column\nvariable.\nFigure 1.33 shows the row proportions for Figure 1.32; these proportions indicate how geno-\ntypes are distributed within each race. For example, the value of 0.593 in the upper left corner\nindicates that of the African Americans in the study, 59.3% have the CC genotype.\nCC CT TT Sum\nAfrican Am 0.593 0.222 0.185 1.000\nAsian 0.382 0.327 0.291 1.000\nCaucasian 0.268 0.463 0.270 1.000\nHispanic 0.174 0.435 0.391 1.000\nOther 0.304 0.478 0.217 1.000\nFigure 1.33: A contingency table with row proportions for the race and\nactn3.r577x variables.\nFigure 1.34 shows the column proportions for Figure 1.32; these proportions indicate the\ndistribution of races within each genotype category. For example, the value of 0.092 indicates that\nof the CC individuals in the study, 9.2% are African American.\nCC CT TT\nAfrican Am 0.092 0.023 0.031\nAsian 0.121 0.069 0.099\nCaucasian 0.723 0.828 0.783\nHispanic 0.023 0.038 0.056\nOther 0.040 0.042 0.031\nSum 1.000 1.000 1.000\nFigure 1.34: A contingency table with column proportions for the race and\nactn3.r577x variables.\nEXAMPLE 1.19\nFor African Americans in the study, CC is the most common genotype and TT is the least common\ngenotype. Does this pattern hold for the other races in the study? Do the observations from the\nstudy suggest that distribution of genotypes at r577x vary between populations?\nThe pattern holds for Asians, but not for other races. For the Caucasian individuals sampled in the\nstudy, CT is the most common genotype at 46.3%. CC is the most common genotype for Asians, but\nin this population, genotypes are more evenly distributed: 38.2% of Asians sampled are CC, 32.7%\nare CT, and 29.1% are TT. The distribution of genotypes at r577x seems to vary by population.\nGUIDED PRACTICE 1.20\nAs shown in Figure 1.34, 72.3% of CC individuals in the study are Caucasian. Do these data suggest\nthat in the general population, people of CC genotype are highly likely to be Caucasian?43\n43No, this is not a reasonable conclusion to draw from the data. The high proportion of Caucasians among CC individuals\nprimarily reﬂects the large number of Caucasians sampled in the study – 78.5% of the people sampled are Caucasian. The\nuneven representation of di ﬀerent races is one limitation of the famuss data.\n1.6. RELATIONSHIPS BETWEEN TWO V ARIABLES 45\nSegmented bar plots\nAsegmented bar plot is a way of visualizing the information from a contingency table. Figure 1.35\ngraphically displays the data from Figure 1.32; each bar represents a level of actn3.r577x and is\ndivided by the levels of race . Figure 1.35(b) uses the row proportions to create a standardized\nsegmented bar plot.\nCC CT TT050100150200250300\nAfrican Am\nAsian\nCaucasian\nHispanic\nOther\n(a)\nCC CT TT0.00.20.40.60.81.0\nAfrican Am\nAsian\nCaucasian\nHispanic\nOther (b)\nFigure 1.35: (a) Segmented bar plot for individuals by genotype, with bars divided\nby race. (b) Standardized version of Figure (a).\nAlternatively, the data can be organized as shown in Figure 1.36, with each bar representing a\nlevel of race . The standardized plot is particularly useful in this case, presenting the distribution\nof genotypes within each race more clearly than in Figure 1.36(a).\nAfrican Am Asian Caucasian Hispanic Other0100200300400500\nCC\nCT\nTT\n(a)\nAfrican Am AsianCaucasian Hispanic Other0.00.20.40.60.81.0\nCC\nCT\nTT (b)\nFigure 1.36: (a) Segmented bar plot for individuals by race, with bars divided by\ngenotype. (b) Standardized version of Figure (a).\n46 CHAPTER 1. INTRODUCTION TO DATA\nTwo-by-two tables: relative risk\nThe results from medical studies are often presented in two-by-two tables (2\u00022 tables), contin-\ngency tables for categorical variables that have two levels. One of the variables deﬁnes two groups\nof participants, while the other represents the two possible outcomes. Figure 1.37 shows a hypo-\nthetical two-by-two table of outcome by group.\nOutcome A Outcome B Sum\nGroup 1 a b a +b\nGroup 2 c d c +d\nSum a+c b +d a +b+c+d=n\nFigure 1.37: A hypothetical two-by-two table of outcome by group.\nIn the LEAP study, participants are divided into two groups based on treatment (peanut\navoidance versus peanut consumption), while the outcome variable records whether an individ-\nual passed or failed the oral food challenge (OFC). The results of the LEAP study as shown in\nFigure 1.2 are in the form of a 2 \u00022 table; the table is reproduced below as Figure 1.38.\nA statistic called the relative risk (RR) can be used to summarize the data in a 2 \u00022 table; the\nrelative risk is a measure of the risk of a certain event occurring in one group relative to the risk of\nthe event occurring in another group.44\nFAIL OFC PASS OFC Sum\nPeanut Avoidance 36 227 263\nPeanut Consumption 5 262 267\nSum 41 489 530\nFigure 1.38: Results of the LEAP study, described in Section 1.1.\nThe question of interest in the LEAP study is whether the risk of developing peanut allergy\n(i.e., failing the OFC) di ﬀers between the peanut avoidance and consumption groups. The relative\nrisk of failing the OFC equals the ratio of the proportion of individuals in the avoidance group who\nfailed the OFC to the proportion of individuals in the consumption group who failed the OFC.\nEXAMPLE 1.21\nUsing the results from the LEAP study, calculate and interpret the relative risk of failing the oral\nfood challenge, comparing individuals in the avoidance group to individuals in the consumption\ngroup.\nRRfailing OFC =proportion in avoidance group who failed OFC\nproportion in consumption group who failed OFC=36=263\n5=267= 7:31:\nThe relative risk is 7.31. The risk of failing the oral food challenge was more than 7 times greater\nfor participants in the peanut avoidance group than for those in the peanut consumption group.\n44Chapter 8 discusses another numerical summary for 2 \u00022 tables, the odds ratio .\n1.6. RELATIONSHIPS BETWEEN TWO V ARIABLES 47\nEXAMPLE 1.22\nAn observational study is conducted to assess the association between smoking and cardiovascular\ndisease (CVD), in which researchers identiﬁed a cohort of individuals and categorized them ac-\ncording to smoking and disease status. If the relative risk of CVD is calculated as the ratio of the\nproportion of smokers with CVD to the proportion of non-smokers with CVD, interpret the results\nof the study if the relative risk equals 1, is less than 1, or greater than 1.\nA relative risk of 1 indicates that the risk of CVD is equal for smokers and non-smokers.\nA relative risk less than 1 indicates that smokers are at a lower risk of CVD than non-smokers;\ni.e., the proportion of individuals with CVD among smokers is lower than the proportion among\nnon-smokers.\nA relative risk greater than 1 indicates that smokers are at a higher risk of CVD than non-smokers;\ni.e., the proportion of individuals with CVD among smokers is higher than the proportion among\nnon-smokers.\nGUIDED PRACTICE 1.23\nFor the study described in Example 1.22, suppose that of the 231 individuals, 111 are smokers. 40\nsmokers and 32 non-smokers have cardiovascular disease. Calculate and interpret the relative risk\nof CVD.45\nRelative risk relies on the assumption that the observed proportions of an event occurring in\neach group are representative of the risk, or incidence, of the event occurring within the popula-\ntions from which the groups are sampled. For example, in the LEAP data, the relative risk assumes\nthat the proportions 33 =263 and 5=267 are estimates of the proportion of individuals who would\nfail the OFC among the larger population of infants who avoid or consume peanut products.\nEXAMPLE 1.24\nSuppose another study to examine the association between smoking and cardiovascular disease\nis conducted, but researchers use a di ﬀerent study design than described in Example 1.22. For\nthe new study, 90 individuals with CVD and 110 individuals without CVD are recruited. 40 of\nthe individuals with CVD are smokers, and 80 of the individuals without CVD are non-smokers.\nShould relative risk be used to summarize the observations from the new study?\nRelative risk should not be calculated for these observations. Since the number of individuals with\nand without CVD is ﬁxed by the study design, the proportion of individuals with CVD within a\ncertain group (smokers or non-smokers) as calculated from the data is not a measure of CVD risk\nfor that population.\n45The relative risk of CVD, comparing smokers to non-smokers, is (40 =111)=(32=120) = 1:35. Smoking is associated\nwith a 35% increase in the probability of CVD; in other words, the risk of CVD is 35% greater in smokers compared to\nnon-smokers.\n48 CHAPTER 1. INTRODUCTION TO DATA\nGUIDED PRACTICE 1.25\nFor a study examining the association between tea consumption and esophageal carcinoma, re-\nsearchers recruited 300 patients with carcinoma and 571 without carcinoma and administered a\nquestionnaire about tea drinking habits.46Of the 47 individuals who reported that they regularly\ndrink green tea, 17 had carcinoma. Of the 824 individuals who reported that they never, or very\nrarely, drink green tea, 283 had carcinoma. Evaluate whether the proportions 17 =47 and 283=824\nare representative of the incidence rate of carcinoma among individuals who drink green tea regu-\nlarly and those who do not.47\nRELATIVE RISK\nThe relative risk of Outcome A in the hypothetical two-by-two table (Figure 1.37) can be cal-\nculated using either Group 1 or Group 2 as the reference group:\nRRA, comparing Group 1 to Group 2 =a=(a+b)\nc=(c+d)\nRRA, comparing Group 2 to Group 1 =c=(c+d)\na=(a+b)\nThe relative risk should only be calculated for data where the proportions a=(a+b) andc=(c+d)\nrepresent the incidence of Outcome A within the populations from which Groups 1 and 2 are\nsampled.\n1.6.3 A numerical variable and a categorical variable\nMethods for comparing numerical data across groups are based on the approaches introduced\nin Section 1.4. Side-by-side boxplots and hollow histograms are useful for directly comparing\nhow the distribution of a numerical variable di ﬀers by category.\nRecall the question introduced in Section 1.2.3: is ACTN3 genotype associated with varia-\ntion in muscle function? Figure 1.39 visually shows the relationship between muscle function\n(measured as percent change in non-dominant arm strength) and ACTN3 genotype in the famuss\ndata with side-by-side boxplots and hollow histograms. The hollow histograms highlight how the\nshapes of the distributions of ndrm.ch for each genotype are essentially similar, although the distri-\nbution for the CC genotype has less right skewing. The side-by-side boxplots are especially useful\nfor comparing center and spread, and reveal that the T allele appears to be associated with greater\nmuscle function; median percent change in non-dominant arm strength increases across the levels\nfrom CC to TT.\nGUIDED PRACTICE 1.26\nUsing Figure 1.40, assess how maternal investment varies with altitude.48\n46Tea drinking habits and oesophageal cancer in a high risk area in northern Iran: population based casecontrol study,\nIslami F, et al., BMJ (2009), doi 10.1136/bmj.b929\n47The proportions calculated from the study data should not be used as estimates of the incidence rate of esophageal\ncarcinoma among individuals who drink green tea regularly and those who do not, since the study selected participants\nbased on carcinoma status.\n48As a general rule, clutches found at higher altitudes have greater volume; median clutch volume tends to increase as\naltitude increases. This suggests that increased altitude is associated with a higher level of maternal investment.\n1.6. RELATIONSHIPS BETWEEN TWO V ARIABLES 49\nGenotypePercent Change in Non−Dominant Arm Strength\nCC CT TT050100150200250\nPercent Change in Non−Dominant Arm Strength050100150200250CC\nCT\nTT\nFigure 1.39: Side-by-side boxplot and hollow histograms for ndrm.ch , split by\nlevels of actn3.r577x .\nAltitudeClutch Volume\n2,035.00 2,926.00 3,080.00 3,189.00 3,479.00 3,493.00050010001500200025003000\nFigure 1.40: Side-by-side boxplot comparing the distribution of clutch.volume\nfor diﬀerent altitudes.\n50 CHAPTER 1. INTRODUCTION TO DATA\n1.7 Exploratory data analysis\nThe simple techniques for summarizing and visualizing data that have been introduced in this\nchapter may not seem especially powerful, but when applied in practice, they can be instrumental\nfor gaining insight into the interesting features of a dataset. This section provides three examples\nof data-driven research questions that can be investigated through exploratory data analysis.\n1.7.1 Case study: discrimination in developmental disability support\nIn the United States, individuals with developmental disabilities typically receive services\nand support from state governments. The State of California allocates funds to developmentally-\ndisabled residents through the California Department of Developmental Services (DDS); individu-\nals receiving DDS funds are referred to as ’consumers’. The dataset dds.discr represents a sample\nof 1,000 DDS consumers (out of a total population of approximately 250,000), and includes infor-\nmation about age, gender, ethnicity, and the amount of ﬁnancial support per consumer provided\nby the DDS.49Figure 1.41 shows the ﬁrst ﬁve rows of the dataset, and the variables are described\nin Figure 1.42.\nA team of researchers examined the mean annual expenditures on consumers by ethnicity,\nand found that the mean annual expenditures on Hispanic consumers was approximately one-\nthird of the mean expenditures on White non-Hispanic consumers. As a result, an allegation of\nethnic discrimination was brought against the California DDS.\nDoes this ﬁnding represent su ﬃcient evidence of ethnic discrimination, or might there be\nmore to the story? This section will illustrate the process behind conducting an exploratory analysis\nthat not only investigates the relationship between two variables of interest, but also considers\nwhether other variables might be inﬂuencing that relationship.\nid age.cohort age gender expenditures ethnicity\n1 10210 13-17 17 Female 2113 White not Hispanic\n2 10409 22-50 37 Male 41924 White not Hispanic\n3 10486 0-5 3 Male 1454 Hispanic\n4 10538 18-21 19 Female 6400 Hispanic\n5 10568 13-17 13 Male 4412 White not Hispanic\nFigure 1.41: Five rows from the dds.discr data matrix.\nvariable description\nid Unique identiﬁcation code for each resident\nage.cohort Age as sorted into six groups, 0-5 years, 6-12 years, 13-17 years, 18-21 years,\n22-50 years, and 51+ years\nage Age, measured in years\ngender Gender, either Female orMale\nexpenditures Amount of expenditures spent by the State on an individual annually, mea-\nsured in USD\nethnicity Ethnic group, recorded as either American Indian ,Asian ,Black ,Hispanic ,\nMulti Race ,Native Hawaiian ,Other , orWhite Not Hispanic\nFigure 1.42: Variables and their descriptions for the dds.discr dataset.\n49The dataset is based on actual attributes of consumers, but has been altered to maintain consumer privacy.\n1.7. EXPLORATORY DATA ANALYSIS 51\nDistributions of single variables\nTo begin understanding a dataset, start by examining the distributions of single variables using\nnumerical and graphical summaries. This process is essential for developing a sense of context; in\nthis case, examining variables individually addresses questions such as \"What is the range of an-\nnual expenditures?\", \"Do consumers tend to be older or younger?\", and \"Are there more consumers\nfrom one ethnic group versus another?\".\nFigure 1.43 illustrates the right skew of expenditures , indicating that for the majority of\nconsumers, expenditures are relatively low; most are within the $0 - $5,000 range. There are\nsome consumers for which expenditures are much higher, such as within the $60,000 - $80,000\nrange. Precise numerical summaries can be calculated using statistical software: the quartiles for\nexpenditures are $2,899, $7,026, and $37,710.\nExpenditures (USD)Frequency\n0 20000 40000 60000 800000100200300400\nFigure 1.43: A histogram of expenditures .\nA consumer’s age is directly recorded as the variable age; in the age.cohort variable, con-\nsumers are assigned to one of six age cohorts. The cohorts are indicative of particular life phases. In\nthe ﬁrst three cohorts, consumers are still living with their parents as they move through preschool\nage, elementary/middle school age, and high school age. In the 18-21 cohort, consumers are transi-\ntioning from their parents’ homes to living on their own or in supportive group homes. From ages\n22-50, individuals are mostly no longer living with their parents but may still receive some support\nfrom family. In the 51+ cohort, consumers often have no living parents and typically require the\nmost amount of support.\n52 CHAPTER 1. INTRODUCTION TO DATA\nFigure 1.44 reveals the right-skewing of age. Most consumers are younger than 30. The plot in\nFigure 1.44(b) graphically shows the number of individuals in each age cohort. There are approx-\nimately 200 individuals in each of the middle four cohorts, while there are about 100 individuals\nin the other two cohorts.\nAge (years)Frequency\n0 20 40 60 80 1000100200300400\n(a)\n0−5 6−12 13−17 18−21 22−50 51+\nAge (years)Frequency\n050100150200 (b)\nFigure 1.44: (a) Histogram of age. (b) Plot of age.cohort .\nThere are eight ethnic groups represented in dds.discr . The two largest groups, Hispanic and\nWhite non-Hispanic, together represent about 80% of the consumers.\nAmerican Indian Asian Black Hispanic Multi Race Native Hawaiian Other White not Hispanic\nEthnicityFrequency\n0100200300400\nFigure 1.45: A plot of ethnicity .\nGUIDED PRACTICE 1.27\nUsing Figure 1.46, does gender appear to be balanced in the dds.discr dataset?50\nFemale Male\nGenderFrequency\n0100200300400500\nFigure 1.46: A plot of gender .\n50Yes, approximately half of the individuals are female and half are male.\n1.7. EXPLORATORY DATA ANALYSIS 53\nRelationships between two variables\nAfter examining variables individually, explore how variables are related to each other. While there\nexist methods for summarizing more than two variables simultaneously, focusing on two variables\nat a time can be surprisingly e ﬀective for making sense of a dataset. It is useful to begin by inves-\ntigating the relationships between the primary response variable of interest and the exploratory\nvariables. In this case study, the response variable is expenditures , the amount of funds the DDS\nallocates annually to each consumer. How does expenditures vary by age, ethnicity, and gender?\nFigure 1.47 shows a side-by-side boxplot of expenditures by age cohort. There is a clear\nupward trend, in which older individuals tend to receive more DDS funds. This reﬂects the under-\nlying context of the data. The purpose of providing funds to developmentally disabled individuals\nis to help them maintain a quality of life similar to those without disabilities; as individuals age, it\nis expected their ﬁnancial needs will increase. Some of the observed variation in expenditures can\nbe attributed to the fact that the dataset includes a wide range of ages. If the data included only\nindividuals in one cohort, such as the 22-50 cohort, the distribution of expenditures would be less\nvariable, and range between $30,000 and $60,000 instead of from $0 and $80,000.\nAge (years)Expenditures (USD)\n0−5 6−12 13−17 18−21 22−50 51+020000400006000080000\nFigure 1.47: A plot of expenditures byage.cohort .\n54 CHAPTER 1. INTRODUCTION TO DATA\nHow does the distribution of expenditures vary by ethnic group? Does there seem to be a dif-\nference in the amount of funding that a person receives, on average, between di ﬀerent ethnicities?\nA side-by-side boxplot of expenditures byethnicity (Figure 1.48) reveals that the distribution of\nexpenditures is quite di ﬀerent between ethnic groups. For example, there is very little variation\ninexpenditures for the Multi Race, Native Hawaiian, and Other groups. Additionally, the me-\ndian expenditures are not the same between groups; the medians for American Indian and Native\nHawaiian individuals are about $40,000, as compared to medians of approximately $10,000 for\nAsian and Black consumers.\nEthnicityExpenditures (USD)\nAmerican Indian Asian Black Hispanic Multi Race Native Hawaiian OtherWhite not Hispanic020000400006000080000\nFigure 1.48: A plot of expenditures byethnicity .\nThe trend visible in Figure 1.48 seems potentially indicative of ethnic discrimination. Before\nproceeding with the analysis, however, it is important to take into account the fact that two of\nthe groups, Hispanic and White non-Hispanic, comprise the majority of the data; some ethnic\ngroups represent less than 10% of the observations (Figure 1.45). For ethnic groups with relatively\nsmall sample sizes, it is possible that the observed samples are not representative of the larger\npopulations. The rest of this analysis will focus on comparing how expenditures varies between\nthe two largest groups, White non-Hispanic and Hispanic.\nGUIDED PRACTICE 1.28\nUsing Figure 1.49, do annual expenditures seem to vary by gender?51\nExpenditures (USD)\nFemale Male020000400006000080000\nGender\nFigure 1.49: A plot of expenditures bygender .\n51No, the distribution of expenditures within males and females is very similar; both are right skewed, with approxi-\nmately equal median and interquartile range.\n1.7. EXPLORATORY DATA ANALYSIS 55\nFigure 1.50 compares the distribution of expenditures between Hispanic and White non-\nHispanic consumers. Most Hispanic consumers receive between about $0 to $20,000 from the\nCalifornia DDS; individuals receiving amounts higher than this are upper outliers. However, for\nWhite non-Hispanic consumers, median expenditures is at $20,000, and the middle 50% of con-\nsumers receive between $5,000 and $40,000. The precise summary statistics can be calculated\nfrom computing software, as shown in the corresponding Rlab. The mean expenditures for His-\npanic consumers is $11,066, while the mean expenditures for White non-Hispanic consumers is\nover twice as large at $24,698. On average, a Hispanic consumer receives less ﬁnancial support\nfrom the California DDS than a White non-Hispanic consumer. Does this represent evidence of\ndiscrimination?\nEthnicityExpenditures (USD)\nHispanic White not Hispanic020000400006000080000\nFigure 1.50: A plot of expenditures byethnicity , showing only Hispanics and\nWhite Non-Hispanics.\nRecall that expenditures is strongly associated with age—older individuals tend to receive\nmore ﬁnancial support. Is there also an association between age and ethnicity, for these two ethnic\ngroups? When using data to investigate a question, it is important to explore not only how explana-\ntory variables are related to the response variable(s), but also how explanatory variables inﬂuence\neach other.\nFigures 1.51 and 1.52 show the distribution of age within Hispanics and White non-Hispanics.\nHispanics tend to be younger, with most Hispanic consumers falling into the 6-12, 13-17, and 18-\n21 age cohorts. In contrast, White non-Hispanics tend to be older; most consumers in this group\nare in the 22-50 age cohort, and relatively more White non-Hispanic consumers are in the 51+ age\ncohort as compared to Hispanics.\n0−5 6−12 13−17 18−21 22−50 51+\nAge (years)Frequency\n020406080100\n(a)\n0−5 6−12 13−17 18−21 22−50 51+\nAge (years)Frequency\n020406080100120 (b)\nFigure 1.51: (a) Plot of age.cohort within Hispanics. (b) Plot of age.cohort within\nWhite non-Hispanics.\n56 CHAPTER 1. INTRODUCTION TO DATA\nAge Cohort Hispanic White Non-Hispanic\n0-5 44/376 = 12% 20/401 = 5%\n6-12 91/376 = 24% 46/401 = 11%\n13-17 103/376 = 27% 67/401 = 17%\n18-21 78/376 = 21% 69/401 = 17%\n22-50 43/376 = 11% 133/401 = 33%\n51+ 17/376 = 5% 66/401 = 16%\nSum 376/376 = 100% 401/401 = 100%\nFigure 1.52: Consumers by ethnicity and age cohort, shown both as counts and\nproportions.\nRecall that a confounding variable is a variable that is associated with the response variable\nand the explanatory variable under consideration; confounding was initially introduced in the\ncontext of sunscreen use and incidence of skin cancer, where sun exposure is a confounder. In this\nsetting, age is a confounder for the relationship between expenditures and ethnicity . Just as it\nwould be incorrect to claim that sunscreen causes skin cancer, it is essential here to recognize that\nthere is more to the story than the apparent association between expenditures and ethnicity .\nFor a closer look at the relationship between age, ethnicity, and expenditures, subset the data\nfurther to compare how expenditures diﬀers by ethnicity within each age cohort. If age is indeed\nthe primary source of the observed variation in expenditures , then there should be little di ﬀerence\nin average expenditures between individuals in di ﬀerent ethnic groups but the same age cohort.\nFigure 1.53 shows the average expenditures within each age cohort, for Hispanics versus\nWhite non-Hispanics. The last column contains the di ﬀerence between the two averages (calcu-\nlated as White Non-Hispanics average - Hispanics average).\nAge Cohort Hispanics White non-Hispanics Diﬀerence\n0-5 1,393 1,367 -26\n6-12 2,312 2,052 -260\n13-17 3,955 3,904 -51\n18-21 9,960 10,133 173\n22-50 40,924 40,188 -736\n51+ 55,585 52,670 -2915\nAverage 11,066 24,698 13,632\nFigure 1.53: Average expenditures by ethnicity and age cohort, in USD ($). For all\nage cohorts except 18-21 years, average expenditures for White non-Hispanics is\nlower than for Hispanics.\nWhen expenditures is compared within age cohorts, there are not large di ﬀerences between\nmean expenditures for White non-Hispanics versus Hispanics. Comparing individuals of similar\nages reveals that the association between ethnicity and expenditures is not nearly as strong as it\nseemed from the initial comparison of overall averages.\n1.7. EXPLORATORY DATA ANALYSIS 57\nInstead, it is the di ﬀerence in age distributions of the two populations that is driving the\nobserved discrepancy in expenditures . The overall average of expenditures for the Hispanic con-\nsumers is lower because the population of Hispanic consumers is relatively young compared to the\npopulation of White non-Hispanic consumers, and the amount of expenditures for younger con-\nsumers tends to be lower than for older consumers. Based on an exploratory analysis that accounts\nfor age as a confounding variable, there does not seem to be evidence of ethnic discrimination.\nIdentifying confounding variables is essential for understanding data. Confounders are often\ncontext-speciﬁc; for example, age is not necessarily a confounder for the relationship between eth-\nnicity and expenditures in a di ﬀerent population. Additionally, it is rarely immediately obvious\nwhich variables in a dataset are confounders; looking for confounding variables is an integral part\nof exploring a dataset.\nChapter 7 introduces multiple linear regression, a method that can directly summarize the re-\nlationship between ethnicity, expenditures, and age, in addition to the tools for evaluating whether\nthe observed discrepancies within age cohorts are greater than would be expected by chance vari-\nation alone.\nSimpson’s paradox\nThese data represent an extreme example of confounding known as Simpson’s paradox , in which\nan association observed in several groups may disappear or reverse direction once the groups are\ncombined. In other words, an association between two variables XandYmay disappear or re-\nverse direction once data are partitioned into subpopulations based on a third variable Z(i.e., a\nconfounding variable).\nFigure 1.53 shows how mean expenditures is higher for Hispanics than White non-Hispanics\nin all age cohorts except one. Yet, once all the data are aggregated, the average expenditures for\nWhite non-Hispanics is over twice as large as the average for Hispanics. The paradox can be ex-\nplored from a mathematical perspective by using weighted averages, where the average expendi-\nture for each cohort is weighted by the proportion of the population in that cohort.\nEXAMPLE 1.29\nUsing the proportions in Figure 1.52 and the average expenditures for each cohort in Figure 1.53,\ncalculate the overall weighted average expenditures for Hispanics and for White non-Hispanics.52\nFor Hispanics:\n1;393(:12) + 2;312(:24) + 3;955(:27) + 9;960(:21) + 40;924(:11) + 55;585(:05) = $11;162:\nFor White non-Hispanics:\n1;367(0:05) + 2;052(:11) + 3;904(:17) + 10;133(:17) + 40;188(:33) + 52;760(:16) = $24;384:\nThe weights for the youngest four cohorts, which have lower expenditures, are higher for the His-\npanic population than the White non-Hispanic population; additionally, the weights for the oldest\ntwo cohorts, which have higher expenditures, are higher for the White non-Hispanic population.\nThis leads to overall average expenditures for the White non-Hispanics being higher than for His-\npanics.\n52Due to rounding, the overall averages calculated via this method will not exactly equal $11,066 and $24,698.\n58 CHAPTER 1. INTRODUCTION TO DATA\n1.7.2 Case study: molecular cancer classiﬁcation\nThe genetic code stored in DNA contains the necessary information for producing the proteins\nthat ultimately determine an organism’s observable traits (phenotype). Although nearly every cell\nin an organism contains the same genes, cells may exhibit di ﬀerent patterns of gene expression. Not\nonly can genes be switched on or o ﬀin certain tissues, but they can also be expressed at varying\nlevels. These variations in gene expression underlie the wide range of physical, biochemical, and\ndevelopmental di ﬀerences that characterize speciﬁc cells and tissues.\nOriginally, scientists were limited to monitoring the expression of only a single gene at a time.\nThe development of microarray technology in the 1990’s made it possible to examine the expres-\nsion of thousands of genes simultaneously. While newer genomic technologies have started to\nreplace microarrays for gene expression studies, microarrays continue to remain clinically relevant\nas a tool for genetic diagnosis. For example, a 2002 study examined the e ﬀectiveness of gene ex-\npression proﬁling as a tool for predicting disease outcome in breast cancer patients, reporting that\nthe expression data from 70 genes constituted a more powerful predictor of survival than standard\nsystems based on clinical criteria.53\nThis section introduces the principles behind DNA microarrays and discusses the 1999 Golub\nleukemia study, which represents one of the earliest applications of microarray technology for\ndiagnostic purposes.\nDNA microarrays\nMicroarray technology is based on hybridization, a basic property of nucleic acids in which com-\nplementary nucleotide sequences speciﬁcally bind together. Each microarray consists of a glass or\nsilicon slide dotted with a grid of short (25-40 base pairs long), single-stranded DNA fragments,\nknown as probes. The probes in a single spot are present in millions of copies, and optimized to\nuniquely correspond to a gene.\nTo measure the gene expression proﬁle of a sample, mRNA is extracted from the sample and\nconverted into complementary-DNA (cDNA). The cDNA is then labeled with a ﬂuorescent dye and\nadded to a microarray. When cDNA from the sample encounters complementary DNA probes, the\ntwo strands will hybridize, allowing the cDNA to adhere to speciﬁc spots on the slide. Once the\nchip is illuminated (to activate the ﬂuorescence) and scanned, the intensity of ﬂuorescence detected\nat each spot corresponds to the amount of bound cDNA.\nMicroarrays are commonly used to compare gene expression between an experimental sample\nand a reference sample. Suppose that the reference sample is taken from healthy cells and the\nexperimental sample from cancer cells. First, the cDNA from the samples are di ﬀerentially labeled,\nsuch as green dye for the healthy cells and red dye for the cancer cells. The samples are then mixed\ntogether and allowed to bind to the slide. If the expression of a particular gene is higher in the\nexperimental sample than in the reference sample, then the corresponding spot on the microarray\nwill appear red. In contrast, the spot will appear green if expression in the experimental sample\nis lower than in the reference sample. Equal expression levels result in a yellow spot, while no\nexpression in either sample shows as a black dot. The ﬂuorescence intensity data provide a relative\nmeasure of gene expression, showing which genes on the chip seem to be more or less active in\nrelation to each other.\n53van de Vijver MJ, He YD, van’t Veer LJ, et al. A gene-expression sign as a predictor of survival in breast cancer. New\nEngland Journal of Medicine 2002;347:1999-2009.\n1.7. EXPLORATORY DATA ANALYSIS 59\nThe raw data produced by a microarray is messy, due to factors such as imperfections dur-\ning chip manufacturing or unpredictable probe behavior. It is also possible for inaccuracies to\nbe introduced from cDNA binding to probes that are not precise sequence matches; this nonspe-\nciﬁc binding will contribute to observed intensity, but not reﬂect the expression level of a gene.\nMethods to improve microarray accuracy by reducing the frequency of nonspeciﬁc binding include\nusing longer probes or multiple probes per gene that correspond to di ﬀerent regions of the gene\nsequence.54The Aﬀymetrix company developed a di ﬀerent strategy involving the use of probe\npairs; one set of probes are a perfect match to the gene sequence (PM probes), while the mismatch\nprobes contain a single base di ﬀerence in the middle of the sequence (MM probes). The MM probes\nact as a control for any cDNA that exhibit nonspeciﬁc binding; subtracting the MM probe intensity\nfrom the PM intensity (PM - MM) provides a more accurate measure of ﬂuorescence produced by\nspeciﬁc hybridization.\nConsiderable research has been done to develop methods for pre-processing microarray data\nto adjust for various errors and produce data that can be analyzed. When analyzing \"cleaned\" data\nfrom any experiment, it is important to be aware that the reliability of any conclusions drawn from\nthe data depends, to a large extent, on the care that has been taken in collecting and processing the\ndata.\nGolub leukemia study\nAccurate cancer classiﬁcation is critical for determining an appropriate course of therapy. The\nchemotherapy regimens for acute leukemias di ﬀers based on whether the leukemia a ﬀects blood-\nforming cells (acute myeloid leukemia, AML) or white blood cells (acute lymphoblastic leukemia,\nALL). At the time of the Golub study, no single diagnostic test was su ﬃcient for distinguishing\nbetween AML and ALL. To investigate whether gene expression proﬁling could be a tool for clas-\nsifying acute leukemia type, Golub and co-authors used A ﬀymetrix DNA microarrays to measure\nthe expression level of 7,129 genes from children known to have either AML or ALL.55\nThe original data (after some initial pre-processing) are available from the Broad Institute.56\nThe version of the data presented in this text have undergone further processing; the expression\nlevels have been normalized to adjust for the variability between the separate arrays used for each\nsampled individual.57Figure 1.54 describes the variables in the ﬁrst six columns of the Golub data.\nThe last 7,129 columns of the dataset contain the expression data for the genes examined in the\nstudy; each column is named after the probe corresponding to a speciﬁc gene.\nvariable description\nSamples Sample number; unique to each patient.\nBM.PB Type of patient material. BM for bone marrow; PB for peripheral blood.\nGender F for female, M for male.\nSource Hospital where the patient was treated.\ntissue.mf Combination of BM.PB and Gender\ncancer Leukemia type; aml is acute myeloid leukemia, allB is acute lymphoblastic\nleukemia with B-cell origin, and allT is acute lymphoblastic leukemia with\nT-cell origin.\nFigure 1.54: Variables and their descriptions for the patient descriptors in Golub\ndataset.\n54Chou, C.C. et al. Optimization of probe length and the number of probes per gene for optimal microarray analysis of\ngene expression. Nucleic Acids Research 2004; 32: e99.\n55Golub, Todd R., et al. Molecular classiﬁcation of cancer: class discovery and class prediction by gene expression\nmonitoring. Science 286 (1999): 531-537.\n56http://www-genome.wi.mit.edu/mpr/data_set_ALL_AML.html\n57John Maindonald, W. John Braun. Data Analysis and Graphics using R: An Example-Based Approach.\n60 CHAPTER 1. INTRODUCTION TO DATA\nFigure 1.55 shows ﬁve rows and seven columns from the dataset. Each row corresponds to a\npatient. These ﬁve patients were all treated at the Dana Farber Cancer Institute (DFCI) ( Source ) for\nALL with B-cell origin ( cancer ), and samples were taken from bone marrow ( BM.PB ). Four of the\npatients were female and one was male ( Gender ). The last row in the table shows the normalized\ngene expression level for the gene corresponding to the probe AFFX.BioB.5.at.\nSamples BM.PB Gender Source tissue.mf cancer AFFX-BioB-5_at\n39 BM F DFCI BM:f allB -1363.28\n40 BM F DFCI BM:f allB -796.29\n42 BM F DFCI BM:f allB -679.14\n47 BM M DFCI BM:m allB -1164.40\n48 BM F DFCI BM:f allB -1299.65\nFigure 1.55: Five rows and seven columns from the Golub data.\nThe goal of the Golub study was to develop a procedure for distinguishing between AML\nand ALL based only on the gene expression levels of a patient. There are two major issues to be\naddressed:\n1.Which genes are the most informative for making a prediction? If a gene is di ﬀerentially ex-\npressed between individuals with AML versus ALL, then measuring the expression level of\nthat gene may be informative for diagnosing leukemia type. For example, if a gene tends to be\nhighly expressed in AML individuals, but only expressed at low levels in ALL individuals, it\nis more likely to be a good predictor of leukemia type than a gene that is expressed at similar\nlevels in both AML and ALL patients.\n2.How can leukemia type be predicted from expression data? Suppose that a patient’s expression\nproﬁle is measured for a group of genes. In an ideal scenario, all the genes measured would\nexhibit AML-like expression, or ALL-like expression, making a prediction obvious. In reality,\nhowever, a patient’s expression proﬁle will not follow an idealized pattern. Some of the\ngenes may have expression levels more typical of AML, while others may suggest ALL. It is\nnecessary to clearly deﬁne a strategy for translating raw expression data into a prediction of\nleukemia type.\nEven though the golub dataset is relatively small by modern standards, it is already too large\nto feasibly analyze without the use of statistical computing software. In this section, conceptual\ndetails will be demonstrated with a small version of the dataset ( golub.small ) that contains only\nthe data for 10 patients and 10 genes. Figure 1.56 shows the cancer type and expression data in\ngolub.small ; the expression values have been rounded to the nearest whole number, and the gene\nprobes are labeled A-J for convenience.\ncancer A B C D E F G H I J\nallB 39308 35232 41171 35793 -593 -1053 -513 -537 1702 1120\nallT 32282 41432 59329 49608 -123 -511 265 -272 3567 -489\nallB 47430 35569 56075 42858 -208 -712 32 -313 433 400\nallB 25534 16984 28057 32694 89 -534 -24 195 3355 990\nallB 35961 24192 27638 22241 -274 -632 -488 20 2259 348\naml 46178 6189 12557 34485 -331 -776 -551 -48 4074 -578\naml 43791 33662 38380 29758 -47 124 1118 3425 7018 1133\naml 53420 26109 31427 23810 396 108 1040 1915 4095 -709\naml 41242 37590 47326 30099 15 -429 784 -532 1085 -1912\naml 41301 49198 66026 56249 -418 -948 -340 -905 877 745\nFigure 1.56: Leukemia type and expression data from golub.small .\n1.7. EXPLORATORY DATA ANALYSIS 61\nTo start understanding how gene expression di ﬀers by leukemia type, summarize the data\nseparately for AML patients and for ALL patients, then make comparisons. For example, how\ndoes the expression of Gene A di ﬀer between individuals with AML versus ALL? Among the 5\nindividuals with AML, the mean expression for Gene A is 45,186; among the 5 ALL individuals,\nmean expression for Gene A is 36,103.\nFigure 1.57 shows mean expression values for each gene among AML patients and Figure 1.58\namong ALL patients.\nAML A B C D E F G H I J\n46178 6189 12557 34485 -331 -776 -551 -48 4074 -578\n43791 33662 38380 29758 -47 124 1118 3425 7018 1133\n53420 26109 31427 23810 396 108 1040 1915 4095 -709\n41242 37590 47326 30099 15 -429 784 -532 1085 -1912\n41301 49198 66026 56249 -418 -948 -340 -905 877 745\nMean 45186 30550 39143 34880 -77 -384 410 771 3430 -264\nFigure 1.57: Expression data for AML patients, where the last row contains mean\nexpression value for each gene among the 5 AML patients. The ﬁrst ﬁve rows are\nduplicated from the last ﬁve rows in Figure 1.56.\nALL A B C D E F G H I J\n39308 35232 41171 35793 -593 -1053 -513 -537 1702 1120\n32282 41432 59329 49608 -123 -511 265 -272 3567 -489\n47430 35569 56075 42858 -208 -712 32 -313 433 400\n25534 16984 28057 32694 89 -534 -24 195 3355 990\n35961 24192 27638 22241 -274 -632 -488 20 2259 348\nMean 36103 30682 42454 36639 -222 -689 -146 -181 2263 474\nFigure 1.58: Expression data for ALL patients, where the last row contains mean\nexpression value for each gene among the 5 ALL patients. The ﬁrst ﬁve rows are\nduplicated from the ﬁrst ﬁve rows in Figure 1.56.\nEXAMPLE 1.30\nOn average, which genes are more highly expressed in AML patients? Which genes are more highly\nexpressed in ALL patients?\nFor each gene, compare the mean expression value among ALL patients to the mean among AML\npatients. For example, the di ﬀerence in mean expression levels for Gene A is\nxAML\u0000xALL= 45186\u000036103 = 9083 :\nThe diﬀerences in means for each gene are shown in Figure 1.59. Due to the order of subtraction\nused, genes with a positive di ﬀerence value are more highly expressed in AML patients: A, E, F, G,\nH, and I. Genes B, C, D, and J are more highly expressed in ALL patients.\nA B C D E F G H I J\nAML mean 45186 30550 39143 34880 -77 -384 410 771 3430 -264\nALL mean 36103 30682 42454 36639 -222 -689 -146 -181 2263 474\nDiﬀerence 9083 -132 -3310 -1758 145 304 556 952 1167 -738\nFigure 1.59: The di ﬀerence in mean expression levels by leukemia type for each\ngene in golub.small .\n62 CHAPTER 1. INTRODUCTION TO DATA\nThe most informative genes for predicting leukemia type are ones for which the di ﬀerence\nin means seems relatively large, compared to the entire distribution of di ﬀerences. Figure 1.60\nvisually displays the distribution of di ﬀerences; the boxplot indicates that there is one large outlier\nand one small outlier.\nDifference in Mean Expression (AML−ALL)Frequency\n−4000−2000 02000400060008000100000123456\n(a)\nDifference in Mean Expression (AML−ALL)−4000−20000200040006000800010000 (b)\nFigure 1.60: A histogram and boxplot of the di ﬀerences in mean expression level\nbetween AML and ALL in the golub.small data.\nIt is possible to identify the outliers from simply looking at the list of di ﬀerences, since the\nlist is short: Genes A and C, with di ﬀerences of 9,083 and -3,310, respectively.58It is important to\nremember that Genes A and C are only outliers out of the speciﬁc 10 genes in golub.small , where\nmean expression has been calculated using data from 10 patients; these genes do not necessarily\nshow outlier levels of expression relative to the complete dataset.\nWith the use of computing software, the same process of calculating means, di ﬀerences of\nmeans, and identifying outliers can easily be applied to the complete version of the data. Fig-\nure 1.61 shows the distribution of di ﬀerences in mean expression level between AML and ALL pa-\ntients for all 7,129 genes in the dataset, from 62 patients. The vast majority of genes are expressed\nat similar levels in AML and ALL patients; most genes have a di ﬀerence in mean expression within\n-5,000 to 5,000. However, there are many genes that show extreme di ﬀerences, as much as higher\nby 20,000 in AML or lower by 30,000 in ALL. These genes may be useful for di ﬀerentiating between\nAML and ALL. The corresponding Rlab illustrates the details of using Rto identify these genes.59\nNote how Figure 1.61 uses data from only 62 patients out of the 72 in the Golub dataset; this\nsubset is called golub.train . The remaining 10 patients have been set aside as a \"test\" dataset\n(golub.test ). Based on what has been learned about expression patterns from the 62 patients in\ngolub.train , how well can the leukemia type of the 10 patients in golub.test be predicted?60\n58For a numerical approach, calculate the outlier boundaries deﬁned by 1 :5\u0002IQR .\n59Lab 3, Chapter 1.\n60The original analysis used data from 38 patients to identify informative genes, then tested predictions on an indepen-\ndent collection of data from 34 patients.\n1.7. EXPLORATORY DATA ANALYSIS 63\nDifference in Mean Expression (AML−ALL)Frequency\n−10000 −5000 05000 10000 15000 200000100020003000\n(a)\nDifference in Mean Expression (AML−ALL)\n−10000−500005000100001500020000\n(b)\nFigure 1.61: A histogram and boxplot of the di ﬀerences in mean expression level\nbetween AML and ALL, using information from 7,129 genes and 62 patients in\ntheGolub data ( golub.train ).\n64 CHAPTER 1. INTRODUCTION TO DATA\nFigure 1.62: Schematic of the prediction strategy used by the Golub team, repro-\nduced with modiﬁcations from Fig. 1B of the original paper.\nFigure 1.62 illustrates the main ideas behind the strategy developed by the Golub team to\npredict leukemia type from expression data. The vertical orange bars represent the gene expression\nlevels of a patient for each gene, relative to the mean expression for AML patients and ALL patients\nfrom the training dataset (vertical blue bars). A gene will \"vote\" for either AML or ALL, depending\non whether the patient’s expression level is closer to \u0016AML or\u0016ALL. In the example shown, three of\nthe genes are considered to have ALL-like expression, versus the other two that are more AML-like.\nThe votes are also weighted to account for how far an observation is from the midpoint between\nthe two means (horizontal dotted blue line), i.e. the length of the dotted line shows the deviation\nfrom the midpoint. For example, the observed expression value for gene 2 is not as strong an\nindicator of ALL as the expression value for gene 1. The magnitude of the deviations ( v1,v2, ...) are\nsummed to obtain VAML andVALL, and a higher value indicates a prediction of either AML or ALL,\nrespectively.\nThe published analysis chose to use 50 informative genes; a decision about how many genes\nto use in a diagnostic panel typically involves considering factors such as the number of genes\npractical for a clinical setting. For simplicity, a smaller number of genes will be used in the analysis\nshown here.\nSuppose that 10 genes are selected as predictors—the 5 largest outliers and 5 smallest outliers\nfor the di ﬀerence in mean expression between AML and ALL. Figure 1.63 shows expression data for\nthese 10 genes from the 10 patients in golub.test , while Figure 1.64 contains the mean expression\nvalue for each gene among AML and ALL patients in golub.train .\n1.7. EXPLORATORY DATA ANALYSIS 65\nM19507_at M27891_at M11147_at M96326_rna1_at Y00787_s_at M14483_rna1_s_at X82240_rna1_at X58529_at M33680_at U05259_rna1_at\n1 4481 47532 56261 1785 -77 7824 -231 9520 7181 2757\n2 11513 2839 42469 5018 20831 27407 -1116 -221 6978 -187\n3 21294 6439 30239 61951 -187 19692 -540 216 1741 -84\n4 -399 26023 40910 1271 26842 30092 -1247 19033 13117 -188\n5 -147 29609 37606 20053 12745 26985 -1104 -273 8701 -168\n6 -1229 -1206 16932 2250 360 38058 20951 12406 9927 8378\n7 -238 -610 21798 -991 -348 23986 6500 20451 8500 7005\n8 -1021 -792 17732 730 5102 17893 158 9287 7924 9221\n9 432 -1099 9683 -576 -804 14386 7097 5556 9915 5594\n10 -518 -862 26386 -2971 -1032 30100 32706 21007 23932 14841\nFigure 1.63: Expression data from the 10 patients in golub.test , for the 10 genes\nselected as predictors. Each row represents a patient; the ﬁve right-most columns\nare the 5 largest outliers and the ﬁve left-most columns are the 5 smallest outliers.\nProbe AML Mean ALL Mean Midpoint\nM19507_at 20143 322 10232\nM27891_at 17395 -262 8567\nM11147_at 32554 16318 24436\nM96326_rna1_at 16745 830 8787\nY00787_s_at 16847 1002 8924\nM14483_rna1_s_at 22268 33561 27914\nX82240_rna1_at -917 9499 4291\nX58529_at 598 10227 5413\nM33680_at 4151 13447 8799\nU05259_rna1_at 74 8458 4266\nTable 1.64: Mean expression value for each gene among AML patients and ALL\npatients in golub.train , and the midpoint between the means.\nEXAMPLE 1.31\nConsider the expression data for the patient in the ﬁrst row of Figure 1.63. For each gene, identify\nwhether the expression level is more AML-like or more ALL-like.\nFor the gene represented by the M19507_at probe, the patient has a recorded expression level of\n4,481, which is closer to the ALL mean of 322 than the AML mean of 20,143. However, for the gene\nrepresented by the M27891_at probe, the expression level of 47,532 is closer to the AML mean of\n17,395 than the ALL mean of -262.\nExpression at genes represented by M19507_at, M96326_rna1_at, Y00787_s_at, and X58529_at are\nmore ALL-like than AML-like. All other expression levels are closer to \u0016AML.\nEXAMPLE 1.32\nUse the information in Figures 1.63 and 1.64 to calculate the magnitude of the deviations v1and\nv10for the ﬁrst patient.\nFor the gene represented by the M19507_at probe, the magnitude of the deviation is v1=j4;481\u0000\n10;232j= 5;751.\nFor the gene represented by the U05259_rna1_at probe, the magnitude of the deviation is v10=\nj2;757\u00004;266j= 1;509.\n66 CHAPTER 1. INTRODUCTION TO DATA\nM19507_at M27891_at M11147_at M96326_rna1_at Y00787_s_at M14483_rna1_s_at X82240_rna1_at X58529_at M33680_at U05259_rna1_at\n1 5751 38966 31825 7003 9001 20090 4522 4108 1618 1509\n2 1281 5727 18032 3769 11906 507 5408 5634 1821 4453\n3 11061 2128 5803 53164 9111 8222 4831 5196 7058 4350\n4 10632 17457 16474 7516 17918 2178 5538 13621 4318 4454\n5 10379 21042 13169 11265 3820 929 5395 5685 98 4434\n6 11461 9773 7504 6537 8564 10144 16660 6994 1128 4112\n7 10470 9176 2638 9778 9272 3928 2209 15038 300 2739\n8 11254 9358 6704 8057 3823 10021 4133 3875 875 4955\n9 9800 9666 14754 9363 9728 13529 2806 144 1116 1328\n10 10750 9428 1949 11759 9956 2186 28415 15594 15133 10575\nTable 1.65: The magnitude of deviations from the midpoints. Cells for which the\nexpression level is more ALL-like (closer to \u0016ALLthan\u0016AML) are highlighted in\nblue.\nEXAMPLE 1.33\nUsing the information in Figure 1.65, make a prediction for the leukemia status of Patient 1.\nCalculate the total weighted votes for each category:\nVAML = 38;966 + 31;825 + 20;090 + 4;522 + 1;618 + 1;509 = 98;530\nVALL= 5;571 + 7;003 + 9;001 + 4;108 = 25;863\nSinceVAML>VALL, Patient 1 is predicted to have AML.\nGUIDED PRACTICE 1.34\nMake a prediction for the leukemia status of Patient 10.61\nFigure 1.66 shows the comparison between actual leukemia status and predicted leukemia\nstatus based on the described prediction strategy. The prediction matches patient leukemia status\nfor all patients.\nActual Prediction\n1 aml aml\n2 aml aml\n3 aml aml\n4 aml aml\n5 aml aml\n6 allB all\n7 allB all\n8 allB all\n9 allB all\n10 allB all\nFigure 1.66: Actual leukemia status versus predicted leukemia status for the pa-\ntients in golub.test\nThe analysis presented here is meant to illustrate how basic statistical concepts such as the def-\ninition of an outlier can be leveraged to address a relatively complex scientiﬁc question. There are\nentirely di ﬀerent approaches possible for analyzing these data, and many other considerations that\nhave not been discussed. For example, this method of summing the weighted votes for each gene\nassumes that each gene is equally informative; the analysis in the published paper incorporates an\nadditional weighting factor when calculating VAML andVALLthat accounts for how correlated each\ngene is with leukemia type. The published analysis also calculates prediction strength based on\nthe values of VAML andVAML in order to provide a measure of how reliable each prediction is.\n61SinceVAML = 1;949 andVALL= 113;796, Patient 10 is predicted to have ALL.\n1.7. EXPLORATORY DATA ANALYSIS 67\nFinally, it is important to remember that the Golub analysis represented one of the earliest in-\nvestigations into the use of gene expression data for diagnostic purposes. While the overall logical\ngoals remain the same—identifying informative genes and developing a prediction strategy—the\nmeans of accomplishing them have become far more sophisticated. A modern study would have\nthe beneﬁt of referencing established, well-deﬁned techniques for analyzing microarray data.\n1.7.3 Case study: cold-responsive genes in the plant Arabidopsis arenosa\nIn contrast to hybridization-based approaches, RNA sequencing (RNA-Seq) allows for the\nentire transcriptome to be surveyed in a high-throughput, quantitative manner.62Microarrays\nrequire gene-speciﬁc probes, which limits microarray experiments to detecting transcripts that\ncorrespond to known gene sequences. In contrast, RNA-Seq can still be used when genome se-\nquence information is not available, such as for non-model organisms. RNA-Seq is an especially\npowerful tool for researchers interested in studying small-scale genetic variation, such as single\nnucleotide polymorphisms, which microarrays are not capable of detecting.63Compared to mi-\ncroarrays, RNA-Seq technology o ﬀers increased sensitivity for detecting genes expressed at either\nlow or very high levels.\nThis section introduces the concepts behind RNA-Seq technology and discusses a study that\nused RNA-Seq to explore the genetic basis of cold response in the plant Arabidopsis arenosa .\nRNA sequencing (RNA-Seq)\nThe ﬁrst step in an RNA-Seq experiment is to prepare cDNA sequence libraries for each RNA sam-\nple being sequenced. RNA is converted into cDNA and sheared into short fragments; sequencing\nadapters and barcodes are added to each fragment that initiate the sequencing reaction and iden-\ntify sequences that originate from di ﬀerent samples. Once all the cDNA fragments are sequenced,\nthe resulting short sequence reads must be re-constructed to produce the transcriptome. At this\npoint, even the simplest RNA-Seq experiment has generated a relatively large amount of data; the\ncomplexity involved in processing and analyzing RNA-Seq data represents a signiﬁcant challenge\nto widespread adoption of RNA-Seq technology. While a number of programs are available to help\nresearchers process RNA-Seq data, improving computational methods for working with RNA-Seq\ndata remains an active area of research.\nA transcriptome can be assembled from the short sequence reads by either de novo assembly\nor genome mapping. In de novo assembly, sequencing data are run through computer algorithms\nthat identify overlapping regions in the short sequence reads to gradually piece together longer\nstretches of continuous sequence. Alternatively, the reads can be aligned to a reference genome, a\ngenome sequence which functions as a representative template for a given species; in cases where\na species has not been sequenced, the genome of a close relative can also function as a reference\ngenome. By mapping reads against a genome, it is possible to identify the position (and thus, the\ngene) from which a given RNA transcript originated. It is also possible to use a combination of\nthese two strategies, an approach that is especially advantageous when genomes have experienced\nmajor rearrangements, such as in the case of cancer cells.64Once the transcripts have been assem-\nbled, information stored in sequence databases such as those hosted by the National Center for\nBiotechnology (NCBI) can be used to identify gene sequences (i.e., annotate the transcripts).\n62Wang, et al. RNA-Seq: a revolutionary tool for transcriptomics. Nature Genetics 2009; 10: 57-63.\n63A single nucleotide polymorphism (SNP) represents variation at a single position in DNA sequence among individuals.\n64Garber, et al. Computational methods for transcriptome annotation and quantiﬁcation using RNA-seq. Nature Methods\n2011; 8: 469-477.\n68 CHAPTER 1. INTRODUCTION TO DATA\nQuantifying gene expression levels from RNA-Seq data is based on counting the number of\nsequence reads per gene. If a particular gene is highly expressed, there will be a relatively high\nnumber of RNA transcripts originating from that gene; thus, the probability that transcripts from\nthis gene are sequenced multiple times is also relatively high, and the gene will have a high number\nof sequencing reads associated with it. The number of read counts for a given gene provides a\nmeasure of gene expression level, when normalized for transcript length. If a short transcript and\nlong transcript are present in equal amounts, the long transcript will have more sequencing reads\nassociated with it due to the fragmentation step in library construction. Additional normalization\nsteps are necessary when comparing data between samples to account for factors such as di ﬀerences\nin the starting amount of RNA or the total number of sequencing reads generated (sequencing\ndepth, in the language of genomics). A variety of strategies have been developed to carry out such\nnormalization procedures.\nCold-responsive genes in A. arenosa\nArabidopsis arenosa populations exist in di ﬀerent habitats, and exhibit a range of di ﬀerences in ﬂow-\nering time, cold sensitivity, and perenniality. Sensitivity to cold is an important trait for perennials,\nplants that live longer than one year. It is common for perennials to require a period of prolonged\ncold in order to ﬂower. This mechanism, known as vernalization, allows perennials to synchronize\ntheir life cycle with the seasons such that they ﬂower only once winter is over. Plant response to\nlow temperatures is under genetic control, and mediated by a speciﬁc set of cold-responsive genes.\nIn a recent study, researchers used RNA-Seq to investigate how cold responsiveness di ﬀers\nin two populations of A. arenosa : TBG (collected from Triberg, Germany) and KA (collected from\nKasparstein, Austria).65TBG grows in and around railway tracks, while KA is found on shaded\nlimestone outcrops in wooded forests. As an annual, TBG has lost the vernalization response and\ndoes not required extended cold in order to ﬂower; in the wild, TBG plants usually die before the\nonset of winter. In contrast, KA is a perennial plant, in which vernalization is known to greatly\naccelerate the onset of ﬂowering.\nWinter conditions can be simulated by incubating plants at 4\u000eC for several weeks; a plant that\nhas undergone cold treatment is considered vernalized, while plants that have not been exposed\nto cold treatment are non-vernalized. Expression data were collected for 1,088 genes known to be\ncold-responsive in TBG and KA plants that were either vernalized or non-vernalized.\nFigure 1.67 shows the data collected for the KA plants analyzed in the study, while Figure 1.68\nshows the TBG expression data. Each row corresponds to a gene; the ﬁrst column indicates gene\nname, while the rest correspond to expression measured in a plant sample. Three individuals\nof each population were exposed to cold (vernalized, denoted by V), and three were not (non-\nvernalized, denoted by NV). Expression was measured in gene counts (i.e. the number of RNA\ntranscripts present in a sample); the data were then normalized between samples to allow for com-\nparisons between gene counts. For example, a value of 288.20 for the PUX4 gene in KA NV 1\nindicates that in one of the non-vernalized KA individuals, about 288 copies of PUX4 were de-\ntected.\nA high number of transcripts indicates a high level of gene expression. As seen by comparing\nthe expression levels across the ﬁrst rows of Figures 1.67 and 1.68, the expression levels of PUX4\nare higher in vernalized plants than non-vernalized plants.\n65Baduel P, et al. Habitat-Associated Life History and Stress-Tolerance Variation in Arabidopsis arenosa .Plant Physiology\n2016; 171: 437-451.\n1.7. EXPLORATORY DATA ANALYSIS 69\nGene Name KA NV 1 KA NV 2 KA NV 3 KA V 1 KA V 2 KA V 3\n1 PUX4 288.20 322.55 305.35 1429.29 1408.25 1487.08\n2 TZP 79.36 93.34 73.44 1203.40 1230.49 1214.03\n3 GAD2 590.59 492.69 458.02 2639.42 2645.05 2705.32\n4 GAUT6 86.88 99.25 57.98 586.24 590.03 579.71\n5 FB 791.08 912.12 746.94 3430.03 3680.12 3467.06\nFigure 1.67: Five rows and seven columns from the arenosa dataset, showing\nexpression levels in KA plants.\nGene Name TBG NV 1 TBG NV 2 TBG NV 3 TBG V 1 TBG V 2 TBG V 3\n1 PUX4 365.23 288.13 365.01 601.39 800.64 698.73\n2 TZP 493.23 210.27 335.33 939.72 974.36 993.14\n3 GAD2 1429.14 1339.50 2215.27 1630.77 1500.36 1621.28\n4 GAUT6 129.63 76.40 135.02 320.57 298.91 399.27\n5 FB 1472.35 1120.49 1313.14 3092.37 3230.72 3173.00\nFigure 1.68: Five rows and seven columns from the arenosa dataset, showing\nexpression levels in TBG plants.\nThe three measured individuals in a particular group represent biological replicates, individ-\nuals of the same type grown under identical conditions; collecting data from multiple individuals\nof the same group captures the inherent biological variability between organisms. Averaging ex-\npression levels across these replicates provides an estimate of the typical expression level in the\nlarger population. Figure 1.69 shows the mean expression levels for ﬁve genes.\nGene Name KA NV KA V TBG NV TBG V\n1 PUX4 305.36 1441.54 339.46 700.25\n2 TZP 82.05 1215.97 346.28 969.07\n3 GAD2 513.77 2663.26 1661.30 1584.14\n4 GAUT6 81.37 585.33 113.68 339.58\n5 FB 816.71 3525.74 1301.99 3165.36\nFigure 1.69: Mean gene expression levels of ﬁve cold-responsive genes, for non-\nvernalized and vernalized KA and TBG.\nFigure 1.70(a) plots the mean gene expression levels of all 1,088 genes for each group. The\nexpression levels are heavily right-skewed, with many genes present at unusually high levels rela-\ntive to other genes. This is an example of a situation in which a transformation can be useful for\nclarifying the features of a distribution. In Figure 1.70(b), it is easier to see that expression levels of\nvernalized plants are shifted upward relative to non-vernalized plants. Additionally, while median\nexpression is slightly higher in non-vernalized TBG than non-vernalized KA, median expression in\nvernalized KA is higher than in vernalized TBG. Vernalization appears to trigger a stronger change\nin expression of cold-responsive genes in KA plants than in TBG plants.\nFigure 1.70 is only a starting point for exploring how expression of cold-responsive genes dif-\nfers between KA and TBG plants. Consider a gene-level approach, in which the responsiveness of\na gene to vernalization is quantiﬁed as the ratio of expression in a vernalized sample to expression\nin a non-vernalized sample.\n70 CHAPTER 1. INTRODUCTION TO DATA\nExpression (Number of Transcripts)\n02000400060008000\nKA NV KA V TBG NV TBG V\n(a)\nExpression (Log Number of Transcripts)\n−20246810\nKA NV KA V TBG NV TBG V (b)\nFigure 1.70: (a) Mean gene expression levels for non-vernalized KA, vernalized\nKA, non-vernalized TBG, and vernalized TBG plants. (b) Log-transformed mean\ngene expression levels.\nFigure 1.71(a) shows responsiveness for ﬁve genes, calculated separately between V and NV\nTBG and V and NV KA, using the means in Figure 1.69. The ratios provide a measure of how much\nexpression di ﬀers between vernalized and non-vernalized individuals. For example, the gene TZP\nis expressed almost 15 times as much in vernalized KA than it is in non-vernalized KA. In contrast,\nthe geneGAD 2 is expressed slightly less in vernalized TBG than in non-vernalized TBG.\nAs with the mean gene expression levels, it is useful to apply a log transformation (Fig-\nure 1.71(b)). On the log scale, values close to 0 are indicative of low responsiveness, while large val-\nues in either direction correspond to high responsiveness. Figure 1.72 shows the log2-transformed\nexpression ratios as a side-by-side boxplot.66\nGene Name TBG KA\n1 PUX4 2.06 4.72\n2 TZP 2.80 14.82\n3 GAD2 0.95 5.18\n4 GAUT6 2.99 7.19\n5 FB 2.43 4.32\n(a)Gene Name TBG KA\n1 PUX4 1.04 2.24\n2 TZP 1.48 3.89\n3 GAD2 -0.07 2.37\n4 GAUT6 1.58 2.85\n5 FB 1.28 2.11\n(b)\nFigure 1.71: (a) Ratio of mean expression in vernalized individuals to mean ex-\npression in non-vernalized individuals. (b) Log2-transformation of expression\nratios in Figure 1.71(a).\n66One gene is omitted because the expression ratio in KA is 0, and the logarithm of 0 is undeﬁned.\n1.7. EXPLORATORY DATA ANALYSIS 71\nLog2 Expression Ratio\n−10−50510\nTBG KA\nFigure 1.72: Responsiveness for 1,087 genes in arenosa , calculated as the log2\nratio of vernalized over non-vernalized expression levels.\nFigure 1.72 directly illustrates how the magnitude of response to vernalization in TBG is\nsmaller than in KA. The spread of responsiveness in KA is larger than for TBG, as indicated by the\nlarger IQR and range of values; this indicates that more genes in KA are di ﬀerentially expressed\nbetween vernalized and non-vernalized samples. Additionally, the median responsiveness in KA is\nhigher than in TBG.\nThere are several outliers for both KA and TBG, with large outliers representing genes that\nwere much more highly expressed in vernalized plants than non-vernalized plants, and vice versa\nfor low outliers. These highly cold-responsive genes likely play a role in how plants cope with\ncolder temperatures; they could be involved in regulating freezing tolerance, or controlling how\nplants detect cold temperatures. With the help of computing software, it is a simple matter to iden-\ntify the outliers and address questions such as whether particular genes are highly vernalization-\nresponsive in both KA and TBG.\n72 CHAPTER 1. INTRODUCTION TO DATA\nAdvanced data visualization\nThere are many ways to numerically and graphically summarize data that are not explicitly in-\ntroduced in this chapter. Presentation-style graphics in published manuscripts can be especially\ncomplex, and may feature techniques speciﬁc to a certain ﬁeld as well as novel approaches de-\nsigned to highlight particular features of a dataset. This section discusses the ﬁgures generated by\nthe Baduel, et al. research team to visualize the di ﬀerences in vernalization response between KA\nand TBG A. arenosa plants.\nFigure 1.73: Figure 4 from the original manuscript. Plot A compares mean ex-\npression levels between non-vernalized KA and TBG; Plot B compares mean ex-\npression levels between vernalized KA and TBG.\nEach dot in Figure 1.73 represents a gene; each gene is plotted by its mean expression level\nin KA against its mean expression level in TBG. The overall trend can be summarized by a line ﬁt\nto the points.67For the slope of the line to equal 1, each gene would have to be equally expressed\nin KA and TBG. In the upper plot, the slope of the line is less than 1, which indicates that for\nnon-vernalized plants, cold-responsive genes have a higher expression in TBG than in KA. In the\nlower plot, the slope is greater than 1, indicating that the trend is reversed in vernalized plants:\ncold-responsive genes are more highly expressed in KA. This trend is also discernible from the side-\nby-side boxplot in Figure 1.70. Using a scatterplot, however, makes it possible to directly compare\nexpression in KA versus TBG on a gene-by-gene basis, and also locate particular genes of interest\nthat are known from previous research (e.g., the labeled genes in Figure 1.73.)68The colors in the\nplot signify plot density, with warmer colors representing a higher concentration of points.\n67Lines of best ﬁt are discussed in Chapter 6.\n68Only a subset of the 1,088 genes are plotted in Figure 1.73.\n1.7. EXPLORATORY DATA ANALYSIS 73\nFigure 1.74: Figure 3 from the original manuscript. Each gene is plotted based on\nthe values of the log2 expression ratio in KA versus TBG.\nFigure 1.74, like Figure 1.72, compares the cold-responsiveness in KA versus TBG, calculat-\ning responsiveness as the log2 ratio of vernalized over non-vernalized expression levels. As in\nFigure 1.73, each dot represents a single gene. The slope of the best ﬁtting line is greater than 1,\nindicating that the assayed genes typically show greater responsiveness in KA than in TBG.69\nWhile presentation-style graphics may use relatively sophisticated approaches to displaying\ndata that seem far removed from the simple plots discussed in this chapter, the end goal remains\nthe same – to e ﬀectively highlight key features of data.\n69These 608 genes are a subset of the ones plotted in Figure 1.73; genes with expression ratio 0 are not included.\n74 CHAPTER 1. INTRODUCTION TO DATA\n1.8 Notes\nIntroductory treatments of statistics often emphasize the value of formal methods of prob-\nability and inference, topics which are covered in the remaining chapters of this text. However,\nnumerical and graphical summaries are essential for understanding the features of a dataset and\nshould be applied before the process of inference begins. It is inadvisable to begin conducting\ntests or constructing models without a careful understanding of the strengths and weaknesses of a\ndataset. For example, are some measurements out of range, or the result of errors in data recording?\nThe tools of descriptive statistics form the basis of exploratory data analysis; having the in-\ntuition for exploring and interpreting data in the context of a research question is an essential\nstatistical skill. With computing software, it is a relatively simple matter to produce numerical\nand graphical summaries, even with large datasets. The challenge lies instead in understanding\nhow to wade through a dataset, disentangle complex relationships between variables, and piece\ntogether the underlying story.\nIt is important to note that the graphical methods illustrated in the text are relatively simple,\nstatic graphs that, for instance, do not show changes dynamically over time. They will be surpris-\ningly useful in the later chapters. But there has been considerable progress in the visual display\nof data in the last decade, and many wonderful displays exist that show complex, time dependent\ndata. For examples of sophisticated graphical displays, we especially recommend the bubble charts\navailable at the Gapminder web site ( https://www.gapminder.org ) that show international trends\nin public health outcomes and the graphical displays of data in the Upshot section of the New York\nTimes ( https://www.nytimes.com/section/upshot ).\nThere are four labs associated with Chapter 1. The ﬁrst lab introduces basic commands for\nworking with data in R, and shows how to produce the graphical and numerical summaries dis-\ncussed in this chapter. The exercises in Lab 1 rely heavily on the introduction to Rand R Studio in\nLab 00 (Getting Started). The Lab Notes corresponding to Lab 1 provide a systematic introduction\ntoRfunctions useful for getting started with applied data analysis.\nThe remaining three labs explore the data presented in the case studies in Section 1.7, outlin-\ning analyses driven by questions similar to what one might encounter in practice. Does the state of\nCalifornia discriminate in its distribution of funds for developmental disability support (Lab 2)?\nAre particular genes associated with a subtype of pediatric leukemia (Lab 3)? Is there a genetic ba-\nsis to the cold weather response in the plant Arabidopsis arenosa (Lab 4)? Labs 3 and 4 demonstrate\nhow computing is essential for data analysis; even though the two datasets are relatively small\nby modern standards, they are already too large to feasibly analyze without statistical computing\nsoftware. All three labs illustrate how important questions can be examined even with relatively\nsimple statistical concepts.\n1.9. EXERCISES 75\n1.9 Exercises\n1.9.1 Case study\n1.1 Migraine and acupuncture, Part I. A migraine is a particularly painful type of headache, which pa-\ntients sometimes wish to treat with acupuncture. To determine whether acupuncture relieves migraine pain,\nresearchers conducted a randomized controlled study where 89 females diagnosed with migraine headaches\nwere randomly assigned to one of two groups: treatment or control. 43 patients in the treatment group re-\nceived acupuncture that is speciﬁcally designed to treat migraines. 46 patients in the control group received\nplacebo acupuncture (needle insertion at non-acupoint locations). 24 hours after patients received acupunc-\nture, they were asked if they were pain free. Results are summarized in the contingency table below.70\nPain free\nYes No Total\nTreatment 10 33 43GroupControl 2 44 46\nTotal 12 77 89\nidentiﬁed on the antero-internal part of the antitragus, the\nanterior part of the lobe and the upper auricular concha, on\nthe same side of pain. The majority of these points were\neffective very rapidly (within 1 min), while the remaining\npoints produced a slower antalgic response, between 2 and\n5 min. The insertion of a semi-permanent needle in these\nzones allowed stable control of the migraine pain, which\noccurred within 30 min and still persisted 24 h later.\nSince the most active site in controlling migraine pain\nwas the antero-internal part of the antitragus, the aim of\nthis study was to verify the therapeutic value of this elec-\ntive area (appropriate point) and to compare it with an area\nof the ear (representing the sciatic nerve) which is probably\ninappropriate in terms of giving a therapeutic effect on\nmigraine attacks, since it has no somatotopic correlation\nwith head pain.\nMaterials and methods\nThe study enrolled 94 females, diagnosed as migraine\nwithout aura following the International Classiﬁcation of\nHeadache Disorders [ 5], who were subsequently examined\nat the Women’s Headache Centre, Department of Gynae-\ncology and Obstetrics of Turin University. They were all\nincluded in the study during a migraine attack provided that\nit started no more than 4 h previously. According to a\npredetermined computer-made randomization list, the eli-\ngible patients were randomly and blindly assigned to the\nfollowing two groups: group A ( n=46) (average age\n35.93 years, range 15–60), group B ( n=48) (average age\n33.2 years, range 16–58).\nBefore enrollment, each patient was asked to give an\ninformed consent to participation in the study.\nMigraine intensity was measured by means of a VAS\nbefore applying NCT (T0).\nIn group A, a speciﬁc algometer exerting a maximum\npressure of 250 g (SEDATELEC, France) was chosen to\nidentify the tender points with Pain–Pressure Test (PPT).\nEvery tender point located within the identiﬁed area by the\npilot study (Fig. 1, area M) was tested with NCT for 10 s\nstarting from the auricle, that was ipsilateral, to the side of\nprevalent cephalic pain. If the test was positive and the\nreduction was at least 25% in respect to basis, a semi-\npermanent needle (ASP SEDATELEC, France) was\ninserted after 1 min. On the contrary, if pain did not lessen\nafter 1 min, a further tender point was challenged in the\nsame area and so on. When patients became aware of an\ninitial decrease in the pain in all the zones of the head\naffected, they were invited to use a speciﬁc diary card to\nscore the intensity of the pain with a VAS at the following\nintervals: after 10 min (T1), after 30 min (T2), after\n60 min (T3), after 120 min (T4), and after 24 h (T5).In group B, the lower branch of the anthelix was\nrepeatedly tested with the algometer for about 30 s to\nensure it was not sensitive. On both the French and Chinese\nauricular maps, this area corresponds to the representation\nof the sciatic nerve (Fig. 1, area S) and is speciﬁcally used\nto treat sciatic pain. Four needles were inserted in this area,\ntwo for each ear.\nIn all patients, the ear acupuncture was always per-\nformed by an experienced acupuncturist. The analysis of\nthe diaries collecting VAS data was conducted by an\nimpartial operator who did not know the group each patient\nwas in.\nThe average values of VAS in group A and B were\ncalculated at the different times of the study, and a statis-\ntical evaluation of the differences between the values\nobtained in T0, T1, T2, T3 and T4 in the two groups\nstudied was performed using an analysis of variance\n(ANOVA) for repeated measures followed by multiple\nttest of Bonferroni to identify the source of variance.\nMoreover, to evaluate the difference between group B\nand group A, a ttest for unpaired data was always per-\nformed for each level of the variable ‘‘time’’. In the case of\nproportions, a Chi square test was applied. All analyses\nwere performed using the Statistical Package for the Social\nSciences (SPSS) software program. All values given in the\nfollowing text are reported as arithmetic mean ( ±SEM).\nResults\nOnly 89 patients out of the entire group of 94 (43 in group\nA, 46 in group B) completed the experiment. Four patients\nwithdrew from the study, because they experienced an\nunbearable exacerbation of pain in the period preceding the\nlast control at 24 h (two from group A and two from group\nB) and were excluded from the statistical analysis since\nthey requested the removal of the needles. One patient\nfrom group A did not give her consent to the implant of the\nsemi-permanent needles. In group A, the mean number of\nFig. 1 The appropriate area\n(M) versus the inappropriate\narea ( S) used in the treatment\nof migraine attacksS174 Neurol Sci (2011) 32 (Suppl 1):S173–S175\n123Figure from the original pa-\nper displaying the appropri-\nate area (M) versus the inap-\npropriate area (S) used in the\ntreatment of migraine attacks.\n(a) What percent of patients in the treatment group were pain free 24 hours after receiving acupuncture?\n(b) What percent were pain free in the control group?\n(c) In which group did a higher percent of patients become pain free 24 hours after receiving acupuncture?\n(d) Your ﬁndings so far might suggest that acupuncture is an e ﬀective treatment for migraines for all people\nwho su ﬀer from migraines. However this is not the only possible conclusion that can be drawn based\non your ﬁndings so far. What is one other possible explanation for the observed di ﬀerence between the\npercentages of patients that are pain free 24 hours after receiving acupuncture in the two groups?\n1.2 Sinusitis and antibiotics, Part I. Researchers studying the e ﬀect of antibiotic treatment for acute si-\nnusitis compared to symptomatic treatments randomly assigned 166 adults diagnosed with acute sinusitis to\none of two groups: treatment or control. Study participants received either a 10-day course of amoxicillin (an\nantibiotic) or a placebo similar in appearance and taste. The placebo consisted of symptomatic treatments\nsuch as acetaminophen, nasal decongestants, etc. At the end of the 10-day period, patients were asked if they\nexperienced improvement in symptoms. The distribution of responses is summarized below.71\nSelf-reported improvement\nin symptoms\nYes No Total\nTreatment 66 19 85GroupControl 65 16 81\nTotal 131 35 166\n(a) What percent of patients in the treatment group experienced improvement in symptoms?\n(b) What percent experienced improvement in symptoms in the control group?\n(c) In which group did a higher percentage of patients experience improvement in symptoms?\n(d) Your ﬁndings so far might suggest a real di ﬀerence in e ﬀectiveness of antibiotic and placebo treatments\nfor improving symptoms of sinusitis. However, this is not the only possible conclusion that can be drawn\nbased on your ﬁndings so far. What is one other possible explanation for the observed di ﬀerence between\nthe percentages of patients in the antibiotic and placebo treatment groups that experience improvement\nin symptoms of sinusitis?\n70G. Allais et al. “Ear acupuncture in the treatment of migraine attacks: a randomized trial on the e ﬃcacy of appropriate\nversus inappropriate acupoints”. In: Neurological Sci. 32.1 (2011), pp. 173–175.\n71J.M. Garbutt et al. “Amoxicillin for Acute Rhinosinusitis: A Randomized Controlled Trial”. In: JAMA: The Journal of\nthe American Medical Association 307.7 (2012), pp. 685–692.\n76 CHAPTER 1. INTRODUCTION TO DATA\n1.9.2 Data basics\n1.3 Air pollution and birth outcomes, study components. Researchers collected data to examine the\nrelationship between air pollutants and preterm births in Southern California. During the study air pollution\nlevels were measured by air quality monitoring stations. Speciﬁcally, levels of carbon monoxide were recorded\nin parts per million, nitrogen dioxide and ozone in parts per hundred million, and coarse particulate matter\n(PM 10) in\u0016g=m3. Length of gestation data were collected on 143,196 births between the years 1989 and\n1993, and air pollution exposure during gestation was calculated for each birth. The analysis suggested that\nincreased ambient PM 10and, to a lesser degree, CO concentrations may be associated with the occurrence of\npreterm births.72\n(a) Identify the main research question of the study.\n(b) Who are the subjects in this study, and how many are included?\n(c) What are the variables in the study? Identify each variable as numerical or categorical. If numerical, state\nwhether the variable is discrete or continuous. If categorical, state whether the variable is ordinal.\n1.4 Buteyko method, study components. The Buteyko method is a shallow breathing technique developed\nby Konstantin Buteyko, a Russian doctor, in 1952. Anecdotal evidence suggests that the Buteyko method can\nreduce asthma symptoms and improve quality of life. In a scientiﬁc study to determine the e ﬀectiveness\nof this method, researchers recruited 600 asthma patients aged 18-69 who relied on medication for asthma\ntreatment. These patients were randomly split into two research groups: one practiced the Buteyko method\nand the other did not. Patients were scored on quality of life, activity, asthma symptoms, and medication\nreduction on a scale from 0 to 10. On average, the participants in the Buteyko group experienced a signiﬁcant\nreduction in asthma symptoms and an improvement in quality of life.73\n(a) Identify the main research question of the study.\n(b) Who are the subjects in this study, and how many are included?\n(c) What are the variables in the study? Identify each variable as numerical or categorical. If numerical, state\nwhether the variable is discrete or continuous. If categorical, state whether the variable is ordinal.\n1.5 Cheaters, study components. Researchers studying the relationship between honesty, age and self-\ncontrol conducted an experiment on 160 children between the ages of 5 and 15. Participants reported their\nage, sex, and whether they were an only child or not. The researchers asked each child to toss a fair coin in\nprivate and to record the outcome (white or black) on a paper sheet, and said they would only reward children\nwho report white. The study’s ﬁndings can be summarized as follows: “Half the students were explicitly told\nnot to cheat and the others were not given any explicit instructions. In the no instruction group probability\nof cheating was found to be uniform across groups based on child’s characteristics. In the group that was\nexplicitly told to not cheat, girls were less likely to cheat, and while rate of cheating didn’t vary by age for\nboys, it decreased with age for girls.”74\n(a) Identify the main research question of the study.\n(b) Who are the subjects in this study, and how many are included?\n(c) How many variables were recorded for each subject in the study in order to conclude these ﬁndings? State\nthe variables and their types.\n72B. Ritz et al. “E ﬀect of air pollution on preterm birth among children born in Southern California between 1989 and\n1993”. In: Epidemiology 11.5 (2000), pp. 502–511.\n73J. McGowan. “Health Education: Does the Buteyko Institute Method make a di ﬀerence?” In: Thorax 58 (2003).\n74Alessandro Bucciol and Marco Piovesan. “Luck or cheating? A ﬁeld experiment on honesty with children”. In: Journal\nof Economic Psychology 32.1 (2011), pp. 73–78.\n1.9. EXERCISES 77\n1.6 Hummingbird taste behavior, study components. Researchers hypothesized that a particular taste\nreceptor in hummingbirds, T1R1-T1R3, played a primary role in dictating taste behavior; speciﬁcally, in de-\ntermining which compounds hummingbirds detect as sweet. In a series of ﬁeld tests, hummingbirds were\npresented simultaneously with two ﬁlled containers, one containing test stimuli and a second containing su-\ncrose. The test stimuli included aspartame, erythritol, water, and sucrose. Aspartame is an artiﬁcial sweetener\nthat tastes sweet to humans, but is not detected by hummingbird T1R1-T1R3 , while erythritol is an artiﬁcial\nsweetener known to activate T1R1-T1R3.\nData were collected on how long a hummingbird drank from a particular container for a given trial,\nmeasured in seconds. For example, in one ﬁeld test comparing aspartame and sucrose, a hummingbird drank\nfrom the aspartame container for 0.54 seconds and from the sucrose container for 3.21 seconds.\n(a) Which tests are controls? Which tests are treatments?\n(b) Identify the response variable(s) in the study. Are they numerical or categorical?\n(c) Describe the main research question.\n1.7 Egg coloration. The evolutionary signiﬁcance of variation in egg coloration among birds is not fully\nunderstood. One hypothesis suggests that egg coloration may be an indication of female quality, with healthier\nfemales being capable of depositing blue-green pigment into eggshells instead of using it for themselves as an\nantioxidant. In a study conducted on 32 collared ﬂycatchers, half of the females were given supplementary\ndiets before and during egg laying. Eggs were measured for darkness of blue color using spectrophotometry;\nfor example, the mean amount of blue-green chroma was 0.594 absorbance units. Egg mass was also recorded.\n(a) Identify the control and treatment groups.\n(b) Describe the main research question.\n(c) Identify the primary response variable of interest, and whether it is numerical or categorical.\n1.8 Smoking habits of UK residents. A survey was conducted to study the smoking habits of UK residents.\nBelow is a data matrix displaying a portion of the data collected in this survey. Note that “£\" stands for British\nPounds Sterling, “cig\" stands for cigarettes, and “N/A” refers to a missing component of the data.75\nsex age marital grossIncome smoke amtWeekends amtWeekdays\n1 Female 42 Single Under £2,600 Yes 12 cig/day 12 cig/day\n2 Male 44 Single £10,400 to £15,600 No N/A N/A\n3 Male 53 Married Above £36,400 Yes 6 cig/day 6 cig/day\n::::::::::::::::::::::::\n1691 Male 40 Single £2,600 to £5,200 Yes 8 cig/day 8 cig/day\n(a) What does each row of the data matrix represent?\n(b) How many participants were included in the survey?\n(c) For each variable, indicate whether it is numerical or categorical. If numerical, identify the variable as\ncontinuous or discrete. If categorical, indicate if the variable is ordinal.\n75National STEM Centre, Large Datasets from stats4schools.\n78 CHAPTER 1. INTRODUCTION TO DATA\n1.9 The microbiome and colon cancer. A study was conducted to assess whether the abundance of par-\nticular bacterial species in the gastrointestinal system is associated with the development of colon cancer.\nThe following data matrix shows a subset of the data collected in the study. Cancer stage is coded 1-4, with\nlarger values indicating cancer that is more di ﬃcult to treat. The abundance levels are given for ﬁve bacterial\nspecies; abundance is calculated as the frequency of that species divided by the total number of bacteria from\nall species.\nage gender stage bug 1 bug 2 bug 3 bug 4 bug 5\n1 71 Female 2 0.03 0.09 0.52 0.00 0.00\n2 53 Female 4 0.16 0.08 0.08 0.00 0.00\n3 55 Female 2 0.00 0.01 0.31 0.00 0.00\n4 44 Male 2 0.11 0.14 0.00 0.07 0.05\n:::::::::::::::::::::::::::\n73 48 Female 3 0.21 0.05 0.00 0.00 0.04\n(a) What does each row of the data matrix represent?\n(b) Identify explanatory and response variables.\n(c) For each variable, indicate whether it is numerical or categorical.\n1.9.3 Data collection principles\n1.10 Cheaters, scope of inference. Exercise 1.5 introduces a study where researchers studying the rela-\ntionship between honesty, age, and self-control conducted an experiment on 160 children between the ages of\n5 and 15. The researchers asked each child to toss a fair coin in private and to record the outcome (white or\nblack) on a paper sheet, and said they would only reward children who report white. Half the students were\nexplicitly told not to cheat and the others were not given any explicit instructions. Di ﬀerences were observed\nin the cheating rates in the instruction and no instruction groups, as well as some di ﬀerences across children’s\ncharacteristics within each group.\n(a) Identify the population of interest and the sample in this study.\n(b) Comment on whether or not the results of the study can be generalized to the population, and if the\nﬁndings of the study can be used to establish causal relationships.\n1.11 Air pollution and birth outcomes, scope of inference. Exercise 1.3 introduces a study where re-\nsearchers collected data to examine the relationship between air pollutants and preterm births in Southern\nCalifornia. During the study, air pollution levels were measured by air quality monitoring stations. Length of\ngestation data were collected on 143,196 births between the years 1989 and 1993, and air pollution exposure\nduring gestation was calculated for each birth. It can be assumed that the 143,196 births are e ﬀectively the\nentire population of births during this time period.\n(a) Identify the population of interest and the sample in this study.\n(b) Comment on whether or not the results of the study can be generalized to the population, and if the\nﬁndings of the study can be used to establish causal relationships.\n1.12 Herbal remedies. Echinacea has been widely used as an herbal remedy for the common cold, but pre-\nvious studies evaluating its e ﬃcacy as a remedy have produced conﬂicting results. In a new study, researchers\nrandomly assigned 437 volunteers to receive either a placebo or echinacea treatment before being infected\nwith rhinovirus. Healthy young adult volunteers were recruited for the study from the University of Virginia\ncommunity.\n(a) Identify the population of interest and the sample in this study.\n(b) Comment on whether or not the results of the study can be generalized to a larger population.\n(c) Can the ﬁndings of the study be used to establish causal relationships? Justify your answer.\n1.9. EXERCISES 79\n1.13 Buteyko method, scope of inference. Exercise 1.4 introduces a study on using the Buteyko shal-\nlow breathing technique to reduce asthma symptoms and improve quality of life. As part of this study 600\nasthma patients aged 18-69 who relied on medication for asthma treatment were recruited and randomly as-\nsigned to two groups: one practiced the Buteyko method and the other did not. Those in the Buteyko group\nexperienced, on average, a signiﬁcant reduction in asthma symptoms and an improvement in quality of life.\n(a) Identify the population of interest and the sample in this study.\n(b) Comment on whether or not the results of the study can be generalized to the population, and if the\nﬁndings of the study can be used to establish causal relationships.\n1.14 Vitamin supplements. In order to assess the e ﬀectiveness of taking large doses of vitamin C in reducing\nthe duration of the common cold, researchers recruited 400 healthy volunteers from sta ﬀand students at a\nuniversity. A quarter of the patients were randomly assigned a placebo, and the rest were randomly allocated\nbetween 1g Vitamin C, 3g Vitamin C, or 3g Vitamin C plus additives to be taken at onset of a cold for the\nfollowing two days. All tablets had identical appearance and packaging. No signiﬁcant di ﬀerences were\nobserved in any measure of cold duration or severity between the four medication groups, and the placebo\ngroup had the shortest duration of symptoms.76\n(a) Was this an experiment or an observational study? Why?\n(b) What are the explanatory and response variables in this study?\n(c) Participants are ultimately able to choose whether or not to use the pills prescribed to them. We might\nexpect that not all of them will adhere and take their pills. Does this introduce a confounding variable to\nthe study? Explain your reasoning.\n1.15 Chicks and antioxidants. Environmental factors early in life can have long-lasting e ﬀects on an organ-\nism. In one study, researchers examined whether dietary supplementation with vitamins C and E inﬂuences\nbody mass and corticosterone level in yellow-legged gull chicks. Chicks were randomly assigned to either the\nnonsupplemented group or the vitamin supplement experimental group. The initial study group consisted of\n108 nests, with 3 eggs per nest. Chicks were assessed at age 7 days.\n(a) What type of study is this?\n(b) What are the experimental and control treatments in this study?\n(c) Explain why randomization is an important feature of this experiment.\n1.16 Exercise and mental health. A researcher is interested in the e ﬀects of exercise on mental health and\nhe proposes the following study: Use stratiﬁed random sampling to recruit 18-30, 31-40 and 41-55 year olds\nfrom the population. Next, randomly assign half the subjects from each age group to exercise twice a week,\nand instruct the rest not to exercise. Conduct a mental health exam at the beginning and at the end of the\nstudy, and compare the results.\n(a) What type of study is this?\n(b) What are the treatment and control groups in this study?\n(c) Does this study make use of blocking? If so, what is the blocking variable?\n(d) Comment on whether or not the results of the study can be used to establish a causal relationship be-\ntween exercise and mental health, and indicate whether or not the conclusions can be generalized to the\npopulation at large.\n(e) Suppose you are given the task of determining if this proposed study should get funding. Would you have\nany reservations about the study proposal?\n76C. Audera et al. “Mega-dose vitamin C in treatment of the common cold: a randomised controlled trial”. In: Medical\nJournal of Australia 175.7 (2001), pp. 359–362.\n80 CHAPTER 1. INTRODUCTION TO DATA\n1.17 Internet use and life expectancy. The following scatterplot was created as part of a study evaluating\nthe relationship between estimated life expectancy at birth (as of 2014) and percentage of internet users (as of\n2009) in 208 countries for which such data were available.77\n(a) Describe the relationship between life ex-\npectancy and percentage of internet users.\n(b) What type of study is this?\n(c) State a possible confounding variable that\nmight explain this relationship and describe\nits potential e ﬀect.\n0204060801005060708090\n% Internet usersLife expectancy at birth\n1.18 Stressed out. A study that surveyed a random sample of otherwise healthy high school students found\nthat they are more likely to get muscle cramps when they are stressed. The study also noted that students\ndrink more co ﬀee and sleep less when they are stressed.\n(a) What type of study is this?\n(b) Can this study be used to conclude a causal relationship between increased stress and muscle cramps?\n(c) State possible confounding variables that might explain the observed relationship between increased\nstress and muscle cramps.\n1.19 Evaluate sampling methods. A university wants to assess how many hours of sleep students are\ngetting per night. For each proposed method below, discuss whether the method is reasonable or not.\n(a) Survey a simple random sample of 500 students.\n(b) Stratify students by their ﬁeld of study, then sample 10% of students from each stratum.\n(c) Cluster students by their class year (e.g. freshmen in one cluster, sophomores in one cluster, etc.), then\nrandomly sample three clusters and survey all students in those clusters.\n1.20 City council survey. A city council has requested a household survey be conducted in a suburban\narea of their city. The area is broken into many distinct and unique neighborhoods, some including large\nhomes, some with only apartments, and others a diverse mixture of housing structures. Identify the sampling\nmethods described below, and comment on whether or not you think they would be e ﬀective in this setting.\n(a) Randomly sample 50 households from the city.\n(b) Divide the city into neighborhoods, and sample 20 households from each neighborhood.\n(c) Divide the city into neighborhoods, randomly sample 10 neighborhoods, and sample all households from\nthose neighborhoods.\n(d) Divide the city into neighborhoods, randomly sample 10 neighborhoods, and then randomly sample 20\nhouseholds from those neighborhoods.\n(e) Sample the 200 households closest to the city council o ﬃces.\n77CIA Factbook, Country Comparisons, 2014.\n1.9. EXERCISES 81\n1.21 Flawed reasoning. Identify the ﬂaw(s) in reasoning in the following scenarios. Explain what the\nindividuals in the study should have done di ﬀerently if they wanted to make such conclusions.\n(a) Students at an elementary school are given a questionnaire that they are asked to return after their parents\nhave completed it. One of the questions asked is, “Do you ﬁnd that your work schedule makes it di ﬃcult\nfor you to spend time with your kids after school?\" Of the parents who replied, 85% said “no\". Based on\nthese results, the school o ﬃcials conclude that a great majority of the parents have no di ﬃculty spending\ntime with their kids after school.\n(b) A survey is conducted on a simple random sample of 1,000 women who recently gave birth, asking them\nabout whether or not they smoked during pregnancy. A follow-up survey asking if the children have\nrespiratory problems is conducted 3 years later, however, only 567 of these women are reached at the\nsame address. The researcher reports that these 567 women are representative of all mothers.\n(c) An orthopedist administers a questionnaire to 30 of his patients who do not have any joint problems and\nﬁnds that 20 of them regularly go running. He concludes that running decreases the risk of joint problems.\n1.22 Reading the paper. Below are excerpts from two articles published in the NY Times :\n(a) An article titled Risks: Smokers Found More Prone to Dementia states the following:78\n“Researchers analyzed data from 23,123 health plan members who participated in a voluntary exam and health\nbehavior survey from 1978 to 1985, when they were 50-60 years old. 23 years later, about 25% of the group had\ndementia, including 1,136 with Alzheimer’s disease and 416 with vascular dementia. After adjusting for other\nfactors, the researchers concluded that pack-a-day smokers were 37% more likely than nonsmokers to develop\ndementia, and the risks went up with increased smoking; 44% for one to two packs a day; and twice the risk for\nmore than two packs.\"\nBased on this study, can it be concluded that smoking causes dementia later in life? Explain your reason-\ning.\n(b) Another article titled The School Bully Is Sleepy states the following:79\n“The University of Michigan study, collected survey data from parents on each child’s sleep habits and asked\nboth parents and teachers to assess behavioral concerns. About a third of the students studied were identiﬁed by\nparents or teachers as having problems with disruptive behavior or bullying. The researchers found that children\nwho had behavioral issues and those who were identiﬁed as bullies were twice as likely to have shown symptoms\nof sleep disorders.\"\nA friend of yours who read the article says, “The study shows that sleep disorders lead to bullying in\nschool children.\" Is this statement justiﬁed? If not, how best can you describe the conclusion that can be\ndrawn from this study?\n1.23 Alcohol consumption and STIs. An observational study published last year in The American Journal\nof Preventive Medicine investigated the e ﬀects of an increased alcohol sales tax in Maryland on the rates of gon-\norrhea and chlamydia.80After a tax increase from 6% to 9% in 2011, the statewide gonorrhea rate declined\nby 24%, the equivalent of 1,600 cases per year. In a statement to the New York Times , the lead author of the\npaper was quoted saying, \"Policy makers should consider raising liquor taxes if they’re looking for ways to\nprevent sexually transmitted infections. In the year and a half following the alcohol tax rise in Maryland, this\nprevented 2,400 cases of gonorrhea and saved half a million dollars in health care costs.\" Explain whether the\nlead author’s statement is accurate.\n78R.C. Rabin. “Risks: Smokers Found More Prone to Dementia”. In: New York Times (2010).\n79T. Parker-Pope. “The School Bully Is Sleepy”. In: New York Times (2011).\n80S. Staras, et al., 2015. Maryland Alcohol Sales Tax and Sexually Transmitted Infections. The American Journal of\nPreventive Medicine 50: e73-e80.\n82 CHAPTER 1. INTRODUCTION TO DATA\n1.24 Income and education in US counties. The scatterplot below shows the relationship between per\ncapita income (in thousands of dollars) and percent of population with a bachelor’s degree in 3,143 counties\nin the US in 2010.\n(a) What are the explanatory and response vari-\nables?\n(b) Describe the relationship between the two\nvariables. Make sure to discuss unusual ob-\nservations, if any.\n(c) Can we conclude that having a bachelor’s\ndegree increases one’s income?\nPercent with Bachelor's degreePer capita income (in $1,000)\n10 30 50 70204060\n1.25 Eat better, feel better. In a public health study on the e ﬀects of consumption of fruits and vegetables\non psychological well-being in young adults, participants were randomly assigned to three groups: (1) diet-as-\nusual, (2) an ecological momentary intervention involving text message reminders to increase their fruits and\nvegetable consumption plus a voucher to purchase them, or (3) a fruit and vegetable intervention in which\nparticipants were given two additional daily servings of fresh fruits and vegetables to consume on top of their\nnormal diet. Participants were asked to take a nightly survey on their smartphones. Participants were student\nvolunteers at the University of Otago, New Zealand. At the end of the 14-day study, only participants in the\nthird group showed improvements to their psychological well-being across the 14-days relative to the other\ngroups.81\n(a) What type of study is this?\n(b) Identify the explanatory and response variables.\n(c) Comment on whether the results of the study can be generalized to the population.\n(d) Comment on whether the results of the study can be used to establish causal relationships.\n(e) A newspaper article reporting on the study states, “The results of this study provide proof that giving\nyoung adults fresh fruits and vegetables to eat can have psychological beneﬁts, even over a brief period of\ntime.” How would you suggest revising this statement so that it can be supported by the study?\n1.9.4 Numerical data\n1.26 Means and SDs. For each part, compare distributions (1) and (2) based on their means and standard\ndeviations. You do not need to calculate these statistics; simply state how the means and the standard de-\nviations compare. Make sure to explain your reasoning. Hint: It may be useful to sketch dot plots of the\ndistributions.\n(a) (1) 3, 5, 5, 5, 8, 11, 11, 11, 13\n(2) 3, 5, 5, 5, 8, 11, 11, 11, 20\n(b) (1) -20, 0, 0, 0, 15, 25, 30, 30\n(2) -40, 0, 0, 0, 15, 25, 30, 30(c) (1) 0, 2, 4, 6, 8, 10\n(2) 20, 22, 24, 26, 28, 30\n(d) (1) 100, 200, 300, 400, 500\n(2) 0, 50, 300, 550, 600\n81Tamlin S Conner et al. “Let them eat fruit! The e ﬀect of fruit and vegetable consumption on psychological well-being\nin young adults: A randomized controlled trial”. In: PloS one 12.2 (2017), e0171206.\n1.9. EXERCISES 83\n1.27 Medians and IQRs. For each part, compare distributions (1) and (2) based on their medians and IQRs.\nYou do not need to calculate these statistics; simply state how the medians and IQRs compare. Make sure to\nexplain your reasoning.\n(a) (1) 3, 5, 6, 7, 9\n(2) 3, 5, 6, 7, 20\n(b) (1) 3, 5, 6, 7, 9\n(2) 3, 5, 8, 7, 9(c) (1) 1, 2, 3, 4, 5\n(2) 6, 7, 8, 9, 10\n(d) (1) 0, 10, 50, 60, 100\n(2) 0, 100, 500, 600, 1000\n1.28 Mix-and-match. Describe the distribution in the histograms below and match them to the box plots.\n(a)506070\n(b)050100\n(c)0246\n(1)0246\n(2)55606570\n(3)020406080100\n1.29 Air quality. Daily air quality is measured by the air quality index (AQI) reported by the Environmen-\ntal Protection Agency. This index reports the pollution level and what associated health e ﬀects might be a\nconcern. The index is calculated for ﬁve major air pollutants regulated by the Clean Air Act and takes values\nfrom 0 to 300, where a higher value indicates lower air quality. AQI was reported for a sample of 91 days in\n2011 in Durham, NC. The relative frequency histogram below shows the distribution of the AQI values on\nthese days.82\n(a) Based on the histogram, describe the distribution of\ndaily AQI.\n(b) Estimate the median AQI value of this sample.\n(c) Would you expect the mean AQI value of this sample\nto be higher or lower than the median? Explain your\nreasoning.\nDaily AQI10203040506000.050.10.150.2\n82US Environmental Protection Agency, AirData, 2011.\n84 CHAPTER 1. INTRODUCTION TO DATA\n1.30 Nursing home residents. Since states with larger numbers of elderly residents would naturally have\nmore nursing home residents, the number of nursing home residents in a state is often adjusted for the number\nof people 65 years of age or order (65+). That adjustment is usually given as the number of nursing home\nresidents age 65+ per 1,000 members of the population age 65+. For example, a hypothetical state with 200\nnursing home residents age 65+ and 50,000 people age 65+ would have the same adjusted number of residents\nas a state with 400 residents and a total age 65+ population of 100,000 residents: 4 residents per 1,000.\nUse the two plots below to answer the following questions. Both plots show the distribution of the\nnumber of nursing home residents per 1,000 members of the population 65+ (in each state).\nadjusted number of residents102030405060708002468101214\n203040506070\n(a) Is the distribution of adjusted number of nursing home residents symmetric or skewed? Are there any\nstates that could be considered outliers?\n(b) Which plot is more informative: the histogram or the boxplot? Explain your answer.\n(c) What factors might inﬂuence the substantial amount of variability among di ﬀerent states? This question\ncannot be answered from the data; speculate using what you know about the demographics of the United\nStates.\n1.31 Income at the coffee shop. The ﬁrst histogram below shows the distribution of the yearly incomes of\n40 patrons at a college co ﬀee shop. Suppose two new people walk into the co ﬀee shop: one making $225,000\nand the other $250,000. The second histogram shows the new income distribution. Summary statistics are\nalso provided.\n(1)60000 62500 65000 67500 70000\n(2)60000 110000 160000 210000 260000\n(1) (2)\nn 40 42\nMin. 60,680 60,680\n1st Qu. 63,620 63,710\nMedian 65,240 65,350\nMean 65,090 73,300\n3rd Qu. 66,160 66,540\nMax. 69,890 250,000\nSD 2,122 37,321\n(a) Would the mean or the median best represent what we might think of as a typical income for the 42\npatrons at this co ﬀee shop? What does this say about the robustness of the two measures?\n(b) Would the standard deviation or the IQR best represent the amount of variability in the incomes of the 42\npatrons at this co ﬀee shop? What does this say about the robustness of the two measures?\n1.32 Midrange. The midrange of a distribution is deﬁned as the average of the maximum and the minimum\nof that distribution. Is this statistic robust to outliers and extreme skew? Explain your reasoning.\n1.9. EXERCISES 85\n1.9.5 Categorical data\n1.33 Flossing habits. Suppose that an anonymous questionnaire is given to patients at a dentist’s o ﬃce\nonce they arrive for an appointment. One of the questions asks \"How often do you ﬂoss?\", and four answer\noptions are provided: a) at least twice a day, b) at least once a day, c) a few times a week, and d) a few times a\nmonth. At the end of a week, the answers are tabulated: 31 individuals chose answer a), 55 chose b), 39 chose\nc), and 12 chose d).\n(a) Describe how these data could be numerically and graphically summarized.\n(b) Assess whether the results of this survey can be generalized to provide information about ﬂossing habits\nin the general population.\n1.34 Views on immigration. 910 randomly sampled registered voters from Tampa, FL were asked if they\nthought workers who have illegally entered the US should be (i) allowed to keep their jobs and apply for\nUS citizenship, (ii) allowed to keep their jobs as temporary guest workers but not allowed to apply for US\ncitizenship, or (iii) lose their jobs and have to leave the country. The results of the survey by political ideology\nare shown below.83\nPolitical ideology\nConservative Moderate Liberal Total\n(i) Apply for citizenship 57 120 101 278\n(ii) Guest worker 121 113 28 262Response(iii) Leave the country 179 126 45 350\n(iv) Not sure 15 4 1 20\nTotal 372 363 175 910\n(a) What percent of these Tampa, FL voters identify themselves as conservatives?\n(b) What percent of these Tampa, FL voters are in favor of the citizenship option?\n(c) What percent of these Tampa, FL voters identify themselves as conservatives and are in favor of the citi-\nzenship option?\n(d) What percent of these Tampa, FL voters who identify themselves as conservatives are also in favor of the\ncitizenship option? What percent of moderates share this view? What percent of liberals share this view?\n1.9.6 Relationships between two variables\n1.35 Mammal life spans. Data were collected on life spans (in years) and gestation lengths (in days) for 62\nmammals. A scatterplot of life span versus length of gestation is shown below.84\n(a) Does there seem to be an association be-\ntween length of gestation and life span? If\nso, what type of association? Explain your\nreasoning.\n(b) What type of an association would you ex-\npect to see if the axes of the plot were re-\nversed, i.e. if we plotted length of gestation\nversus life span?\n●\n●●●\n●\n●●\n●●\n●●●\n●●\n●●●\n●\n●●●\n●\n●●●\n●\n●\n●●●\n●●●●●\n●●\n●●\n●\n●\n●●●\n●●●●\n●\n●●\n●●\n●●\nGestation (days)Life Span (years)\n0 200 400 6000255075100\n83SurveyUSA, News Poll #18927, data collected Jan 27-29, 2012.\n84T. Allison and D.V. Cicchetti. “Sleep in mammals: ecological and constitutional correlates”. In: Arch. Hydrobiol 75\n(1975), p. 442.\n86 CHAPTER 1. INTRODUCTION TO DATA\n1.36 Associations. Indicate which of the plots show a\n(a) positive association\n(b) negative association\n(c) no association\nAlso determine if the positive and\nnegative associations are linear or\nnonlinear. Each part may refer to\nmore than one plot.\n●●●\n●●\n●●●\n●\n●\n●●●●●\n●●●\n●\n●●●●\n●●\n●\n●●●\n●\n●\n●●●●\n●●\n●●\n●●●●\n●●●\n●●\n●\n●●●\n●●●\n●\n●●\n●●\n●\n●●●\n●●\n●\n●●\n●●●\n●\n●●\n●●\n●●\n●●\n●●\n●\n●●\n●\n●●\n●\n●\n●\n●●\n●●●●●\n●●\n(1)●\n●●●●\n●●●●\n●\n●●\n●●●\n●●\n●\n●●●●\n●●●●●●●\n●●\n●●\n●●\n●●●●\n●\n●\n●●●\n●●\n●●\n●\n●●●\n●●\n●●\n●●●●\n●●●\n●●\n●●\n●\n●●●●●\n●●\n●●\n●●\n●\n●●●\n●\n●●\n●\n●●●●\n●●●●\n●\n●\n●●●●\n(2)\n●●\n●●●\n●●●●\n●●\n●●●●\n●●●\n●●●●●●●●●●\n●●●●●●●●\n●●●●\n●●●●●●●●\n●●●●●\n●●●●\n●●●●●●●●●●●●●\n●●●●●\n●●●●●●●●●●●●●\n●●●●●●\n●●●●●●●\n(3)●\n●●\n●●\n●●\n●●●●●●\n●●\n●●●\n●\n●●●●\n●●\n●●●●\n●\n●●●\n●\n●●\n●●\n●●\n●●●●●●\n●\n●●●\n●●●\n●●\n●\n●●\n●\n●●●\n●●●\n●●\n●\n●●\n●\n●●\n●\n●\n●●\n●\n●●●●\n●●\n●●●●\n●\n●●●●\n●●\n●●●●\n●●\n(4)\n1.37 Adolescent fertility. Data are available on the number of children born to women aged 15-19 from\n189 countries in the world for the years 1997, 2000, 2002, 2005, and 2006. The data are deﬁned using a\nscaling similar to that used for the nursing home data in Exercise 1.30. The values for the annual adolescent\nfertility rates represent the number of live births among women aged 15-19 per 1,000 female members of the\npopulation of that age.\nFertility Rate (live births per 1000 women)050100150200250\n19972000200220052006\n(a) In 2006, the standard deviation of the distribution of adolescent fertility is 75.73. Write a sentence ex-\nplaining the 75thpercentile in the context of this data.\n(b) For the years 2000-2006, data are not available for Iraq. Why might those observations be missing? Would\nthe ﬁve-number summary have been a ﬀected very much if the values had been available?\n(c) From the side-by-side boxplots shown above, describe how the distribution of fertility rates changes over\ntime. Is there a trend?\n1.9. EXERCISES 87\n1.38 Smoking and stenosis. Researchers collected data from an observational study to investigate the\nassociation between smoking status and the presence of aortic stenosis, a narrowing of the aorta that impedes\nblood ﬂow to the body.\nSmoking Status\nNon-smoker Smoker Total\nAbsent 67 43 110Disease StatusPresent 54 51 105\nTotal 121 94 215\n(a) What percentage of the 215 participants were both smokers and had aortic stenosis? This percentage is\none component of the joint distribution of smoking and stenosis; what are the other three numbers of the\njoint distribution?\n(b) Among the smokers, what proportion have aortic stenosis? This number is a component of the conditional\ndistribution of stenosis for the two categories of smokers. What proportion of non-smokers have aortic\nstenosis?\n(c) In this context, relative risk is the ratio of the proportion of smokers with stenosis to the proportion of\nnon-smokers with stenosis. Relative risks greater than 1 indicate that smokers are at a higher risk for\naortic stenosis than non-smokers; relative risks of 1.2 or higher are generally considered cause for alarm.\nCalculate the relative risk for the 215 participants, comparing smokers to non-smokers. Does there seem\nto be evidence that smoking is associated with an increased probability of stenosis?\n1.39 Anger and cardiovascular health. Trait anger is deﬁned as a relatively stable personality trait that\nis manifested in the frequency, intensity, and duration of feelings associated with anger. People with high\ntrait anger have rage and fury more often, more intensely, and with long-laster episodes than people with low\ntrait anger. It is thought that people with high trait anger might be particularly susceptible to coronary heart\ndisease; 12,986 participants were recruited for a study examining this hypothesis. Participants were followed\nfor ﬁve years. The following table shows data for the participants identiﬁed as having normal blood pressure\n(normotensives).\nTrait Anger Score\nLow Moderate High Total\nYes 53 110 27 190CHD EventNo 3057 4704 606 8284\nTotal 3110 4731 633 8474\n(a) What percentage of participants have moderate anger scores?\n(b) What percentage of individuals who experienced a CHD event have moderate anger scores?\n(c) What percentage of participants with high trait anger scores experienced a CHD event (i.e., heart attack)?\n(d) What percentage of participants with low trait anger scores experienced a CHD event?\n(e) Are individuals with high trait anger more likely to experience a CHD event than individuals with low\ntrait anger? Calculate the relative risk of a CHD event for individuals with high trait anger compared to\nlow trait anger.\n(f) Researchers also collected data on various participant traits, such as level of blood cholesterol (measured\nin mg/dL). What graphical summary might be useful for examining how blood cholesterol level di ﬀers\nbetween anger groups?\n1.9.7 Exploratory data analysis\nSince exploratory data analysis relies heavily on the use of computation, refer to the labs for exercises\nrelated to this section, which are free and may be found at openintro.org/book/biostat .\n88\nChapter 2\nProbability\n2.1 Deﬁning probability\n2.2 Conditional probability\n2.3 Extended example\n2.4 Notes\n2.5 Exercises\n89\nWhat are the chances that a woman with an abnormal mammogram has breast\ncancer? What is the probability that a woman with an abnormal mammogram has\nbreast cancer, given that she is in her 40’s? What is the likelihood that out of 100\nwomen who undergo a mammogram and test positive for breast cancer, at least\none of the women has received a false positive result?\nThese questions use the language of probability to express statements about out-\ncomes that may or may not occur. More speciﬁcally, probability is used to quantify\nthe level of uncertainty about each outcome. Like all mathematical tools, proba-\nbility becomes easier to understand and work with once important concepts and\nterminology have been formalized.\nThis chapter introduces that formalization, using two types of examples. One set\nof examples uses settings familiar to most people – rolling dice or picking cards\nfrom a deck. The other set of examples draws from medicine, biology, and pub-\nlic health, reﬂecting the contexts and language speciﬁc to those ﬁelds. The ap-\nproaches to solving these two types of problems are surprisingly similar, and in\nboth cases, seemingly di ﬃcult problems can be solved in a series of reliable steps.\nFor labs, slides, and other resources, please visit\nwww.openintro.org/book/biostat\n90 CHAPTER 2. PROBABILITY\n2.1 Deﬁning probability\n2.1.1 Some examples\nThe rules of probability can easily be modeled with classic scenarios, such as ﬂipping coins or\nrolling dice. When a coin is ﬂipped, there are only two possible outcomes, heads or tails. With a\nfair coin, each outcome is equally likely; thus, the chance of ﬂipping heads is 1/2, and likewise for\ntails. The following examples deal with rolling a die or multiple dice; a die is a cube with six faces\nnumbered 1,2,3,4,5, and 6.\nEXAMPLE 2.1\nWhat is the chance of getting 1when rolling a die?\nIf the die is fair, then there must be an equal chance of rolling a 1as any other possible number.\nSince there are six outcomes, the chance must be 1-in-6 or, equivalently, 1 =6.\nEXAMPLE 2.2\nWhat is the chance of not rolling a 2?\nNot rolling a 2is the same as getting a 1,3,4,5, or6, which makes up ﬁve of the six equally likely\noutcomes and has probability 5 =6.\nEXAMPLE 2.3\nConsider rolling two fair dice. What is the chance of getting two 1s?\nIf 1=6thof the time the ﬁrst die is a 1and 1=6thofthose times the second die is also a 1, then the\nchance that both dice are 1is (1=6)(1=6) or 1=36.\nProbability can also be used to model less artiﬁcial contexts, such as to predict the inheritance\nof genetic disease. Cystic ﬁbrosis (CF) is a life-threatening genetic disorder caused by mutations\nin the CFTR gene located on chromosome 7. Defective copies of CFTR can result in the reduced\nquantity and function of the CFTR protein, which leads to the buildup of thick mucus in the lungs\nand pancreas.1CF is an autosomal recessive disorder; an individual only develops CF if they\nhave inherited two a ﬀected copies of CFTR . Individuals with one normal (wild-type) copy and one\ndefective (mutated) copy are known as carriers; they do not develop CF, but may pass the disease-\ncausing mutation onto their o ﬀspring.\n1The CFTR protein is responsible for transporting sodium and chloride ions across cell membranes.\n2.1. DEFINING PROBABILITY 91\nEXAMPLE 2.4\nSuppose that both members of a couple are CF carriers. What is the probability that a child of this\ncouple will be a ﬀected by CF? Assume that a parent has an equal chance of passing either gene\ncopy (i.e., allele) to a child.\nSolution 1: Enumerate all of the possible outcomes and exploit the fact that the outcomes are equally\nlikely, as in Example 2.1. Figure 2.1 shows the four possible genotypes for a child of these parents.\nThe paternal chromosome is in blue and the maternal chromosome in green, while chromosomes\nwith the wild-type and mutated versions of CFTR are marked with + and \u0000, respectively. The child\nis only a ﬀected if they have genotype ( \u0000/\u0000), with two mutated copies of CFTR . Each of the four\noutcomes occurs with equal likelihood, so the child will be a ﬀected with probability 1-in-4, or 1 =4.\nIt is important to recognize that the child being an una ﬀected carrier (+/\u0000) consists of two distinct\noutcomes, not one.\nSolution 2: Calculate the proportion of outcomes that produce an a ﬀected child, as in Example 2.3.\nDuring reproduction, one parent will pass along an a ﬀected copy half of the time. When the child\nreceives an a ﬀected allele from one parent, half of the those times, they will also receive an a ﬀected\nallele from the other parent. Thus, the proportion of times the child will have two a ﬀected copies\nis (1=2)\u0002(1=2) = 1=4.\nFigure 2.1: Pattern of CF inheritance for a child of two una ﬀected carriers\n92 CHAPTER 2. PROBABILITY\nGUIDED PRACTICE 2.5\nSuppose the father has CF and the mother is an una ﬀected carrier. What is the probability that\ntheir child will be a ﬀected by the disease?2\n2.1.2 Probability\nProbability is used to assign a level of uncertainty to the outcomes of phenomena that either\nhappen randomly (e.g. rolling dice, inheriting of disease alleles), or appear random because of\na lack of understanding about exactly how the phenomenon occurs (e.g. a woman in her 40’s\ndeveloping breast cancer). Modeling these complex phenomena as random can be useful, and in\neither case, the interpretation of probability is the same: the chance that some event will occur.\nMathematicians and philosophers have struggled for centuries to arrive at a clear statement\nof how probability is deﬁned, or what it means. The most common deﬁnition is used in this text.\nPROBABILITY\nThe probability of an outcome is the proportion of times the outcome would occur if the\nrandom phenomenon could be observed an inﬁnite number of times.\nThis deﬁnition of probability can be illustrated by simulation. Suppose a die is rolled many\ntimes. Let ˆpnbe the proportion of outcomes that are 1after the ﬁrst nrolls. As the number of\nrolls increases, ˆpnwill converge to the probability of rolling a 1,p= 1=6. Figure 2.2 shows this\nconvergence for 100,000 die rolls. The tendency of ˆpnto stabilize around pis described by the\nLaw of Large Numbers . The behavior shown in Figure 2.2 matches most people’s intuition about\nprobability, but proving mathematically that the behavior is always true is surprisingly di ﬃcult\nand beyond the level of this text.\nn (number of rolls)1 10 100 1,000 10,000 100,0000.00.10.20.3\np^\nn\nFigure 2.2: The fraction of die rolls that are 1at each stage in a simulation. The\nproportion tends to get closer to the probability 1 =6\u00190:167 as the number of rolls\nincreases.\nOccasionally the proportion veers o ﬀfrom the probability and appear to defy the Law of Large\nNumbers, as ˆpndoes many times in Figure 2.2. However, the likelihood of these large deviations\nbecomes smaller as the number of rolls increases.\n2Since the father has CF, he must have two a ﬀected copies; he will always pass along a defective copy of the gene. Since\nthe mother will pass along a defective copy half of the time, the child will be a ﬀected half of the time, or with probability\n(1)\u0002(1=2) = 1=2.\n2.1. DEFINING PROBABILITY 93\nLAW OF LARGE NUMBERS\nAs more observations are collected, the proportion ˆpnof occurrences with a particular outcome\nconverges to the probability pof that outcome.\nProbability is deﬁned as a proportion, and it always takes values between 0 and 1 (inclusively).\nIt may also be expressed as a percentage between 0% and 100%. The probability of rolling a 1,p,\ncan also be written as P(rolling a 1).\nThis notation can be further abbreviated. For instance, if it is clear that the process is “rolling\na die”,P(rolling a 1) can be written as P(1). There also exists a notation for an event itself; the event\nAof rolling a 1 can be written as A=frolling a 1g, with associated probability P(A).P(A)\nProbability of\noutcomeA\n2.1.3 Disjoint or mutually exclusive outcomes\nTwo outcomes are disjoint ormutually exclusive if they cannot both happen at the same\ntime. When rolling a die, the outcomes 1and 2are disjoint since they cannot both occur. However,\nthe outcomes 1and “rolling an odd number” are not disjoint since both occur if the outcome of the\nroll is a 1.3\nWhat is the probability of rolling a 1or a 2? When rolling a die, the outcomes 1and 2are\ndisjoint. The probability that one of these outcomes will occur is computed by adding their separate\nprobabilities:\nP(1or2) =P(1) +P(2) = 1=6 + 1=6 = 1=3:\nWhat about the probability of rolling a 1,2,3,4,5, or6? Here again, all of the outcomes are disjoint,\nso add the individual probabilities:\nP(1or2or3or4or5or6)\n=P(1) +P(2) +P(3) +P(4) +P(5) +P(6)\n= 1=6 + 1=6 + 1=6 + 1=6 + 1=6 + 1=6 = 1:\nADDITION RULE OF DISJOINT OUTCOMES\nIfA1andA2represent two disjoint outcomes, then the probability that either one of them\noccurs is given by\nP(A1orA2) =P(A1) +P(A2):\nIf there are kdisjoint outcomes A1, ...,Ak, then the probability that either one of these out-\ncomes will occur is\nP(A1) +P(A2) +\u0001\u0001\u0001+P(Ak): (2.6)\n3The terms disjoint and mutually exclusive are equivalent and interchangeable.\n94 CHAPTER 2. PROBABILITY\nGUIDED PRACTICE 2.7\nConsider the CF example. Is the event that two carriers of CF have a child that is also a carrier\nrepresented by mutually exclusive outcomes? Calculate the probability of this event.4\nProbability problems often deal with setsorcollections of outcomes. Let Arepresent the event\nin which a die roll results in 1or2andBrepresent the event that the die roll is a 4or a 6. We write\nAas the set of outcomes f1,2gandB=f4,6g. These sets are commonly called events . BecauseA\nandBhave no elements in common, they are disjoint events. AandBare represented in Figure 2.3.\nFigure 2.3: Three events, A,B, andD, consist of outcomes from rolling a die. A\nandBare disjoint since they do not have any outcomes in common.\nThe Addition Rule applies to both disjoint outcomes and disjoint events. The probability that\none of the disjoint events AorBoccurs is the sum of the separate probabilities:\nP(AorB) =P(A) +P(B) = 1=3 + 1=3 = 2=3:\nGUIDED PRACTICE 2.8\n(a) Verify the probability of event A,P(A), is 1=3 using the Addition Rule. (b) Do the same for\neventB.5\nGUIDED PRACTICE 2.9\n(a) Using Figure 2.3 as a reference, which outcomes are represented by event D? (b) Are events B\nandDdisjoint? (c) Are events AandDdisjoint?6\n4Yes, there are two mutually exclusive outcomes for which a child of two carriers can also be a carrier - a child can either\nreceive an a ﬀected copy of CFTR from the mother and a normal copy from the father, or vice versa (since each parent can\nonly contribute one allele). Thus, the probability that a child will be a carrier is 1/4 + 1/4 = 1/2.\n5(a)P(A) =P(1or2) =P(1) +P(2) =1\n6+1\n6=2\n6=1\n3. (b) Similarly, P(B) = 1=3.\n6(a) Outcomes 2and 3. (b) Yes, events BandDare disjoint because they share no outcomes. (c) The events AandD\nshare an outcome in common, 2, and so are not disjoint.\n2.1. DEFINING PROBABILITY 95\nGUIDED PRACTICE 2.10\nIn Guided Practice 2.9, you conﬁrmed BandDfrom Figure 2.3 are disjoint. Compute the proba-\nbility that event Bor eventDoccurs.7\n2.1.4 Probabilities when events are not disjoint\nVenn diagrams are useful when outcomes can be categorized as “in” or “out” for two or three\nvariables, attributes, or random processes. The Venn diagram in Figure 2.5 uses one oval to repre-\nsent diamonds and another to represent face cards (the cards labeled jacks, queens, and kings); if a\ncard is both a diamond and a face card, it falls into the intersection of the ovals.\n2|3|4|5|6|7|8|9|10|J|Q|K|A|\n2}3}4}5}6}7}8}9}10}J}Q}K}A}\n2~3~4~5~6~7~8~9~10~J~Q~K~A~\n2345678910JQKA\nFigure 2.4: A regular deck of 52 cards is split into four suits: |(club),}(diamond),\n~(heart),(spade). Each suit has 13 labeled cards: 2,3, ...,10,J(jack), Q(queen),\nK(king), and A(ace). Thus, each card is a unique combination of a suit and a label,\ne.g.4~and J|.\nFigure 2.5: A Venn diagram for diamonds and face cards.\nGUIDED PRACTICE 2.11\n(a) What is the probability that a randomly selected card is a diamond? (b) What is the probability\nthat a randomly selected card is a face card?8\n7SinceBandDare disjoint events, use the Addition Rule: P(BorD) =P(B) +P(D) =1\n3+1\n3=2\n3.\n8(a) There are 52 cards and 13 diamonds. If the cards are thoroughly shu ﬄed, each card has an equal chance of being\ndrawn, so the probability that a randomly selected card is a diamond is P(}) =13\n52= 0:250. (b) Likewise, there are 12 face\ncards, soP(face card) =12\n52=3\n13= 0:231.\n96 CHAPTER 2. PROBABILITY\nLetArepresent the event that a randomly selected card is a diamond and Brepresent the\nevent that it is a face card. Events AandBare not disjoint – the cards J},Q}, and K}fall into both\ncategories.\nAs a result, adding the probabilities of the two events together is not su ﬃcient to calculate\nP(AorB):\nP(A) +P(B) =P(}) +P(face card) = 12 =52 + 13=52:\nInstead, a small modiﬁcation is necessary. The three cards that are in both events were counted\ntwice. To correct the double counting, subtract the probability that both events occur:\nP(AorB) =P(face card or})\n=P(face card) + P(})\u0000P(face card and}) (2.12)\n= 13=52 + 12=52\u00003=52\n= 22=52 = 11=26:\nEquation (2.12) is an example of the General Addition Rule .\nGENERAL ADDITION RULE\nIfAandBare any two events, disjoint or not, then the probability that at least one of them\nwill occur is\nP(AorB) =P(A) +P(B)\u0000P(AandB); (2.13)\nwhereP(AandB) is the probability that both events occur.\nNote that in the language of statistics, \"or\" is inclusive such that AorBoccurs means A,B, or\nbothAandBoccur.\nGUIDED PRACTICE 2.14\n(a) IfAandBare disjoint, describe why this implies P(AandB) = 0. (b) Using part (a), verify\nthat the General Addition Rule simpliﬁes to the Addition Rule for disjoint events if AandBare\ndisjoint.9\nGUIDED PRACTICE 2.15\nHuman immunodeﬁciency virus (HIV) and tuberculosis (TB) a ﬀect substantial proportions of\nthe population in certain areas of the developing world. Individuals sometimes are co-infected\n(i.e., have both diseases). Children of HIV-infected mothers may have HIV and TB can spread\nfrom one family member to another. In a mother-child pair, let A=fthe mother has HIV g,\nB=fthe mother has TB g,C=fthe child has HIV g,D=fthe child has TBg. Write out the deﬁnitions\nof the events AorB,AandB,AandC,AorD.10\n9(a) IfAandBare disjoint, AandBcan never occur simultaneously. (b) If AandBare disjoint, then the last term of\nEquation (2.13) is 0 (see part (a)) and we are left with the Addition Rule for disjoint events.\n10EventsAorB: the mother has HIV, the mother has TB, or the mother has both HIV and TB. Events AandB: the mother\nhas both HIV and TB. Events AandC: The mother has HIV and the child has HIV. AorD: The mother has HIV, the child\nhas TB, or the mother has HIV and the child has TB.\n2.1. DEFINING PROBABILITY 97\n2.1.5 Probability distributions\nAprobability distribution consists of all disjoint outcomes and their associated probabilities.\nFigure 2.6 shows the probability distribution for the sum of two dice.\nDice sum 2 3 4 5 6 7 8 9 10 11 12\nProbability1\n362\n363\n364\n365\n366\n365\n364\n363\n362\n361\n36\nFigure 2.6: Probability distribution for the sum of two dice.\nRULES FOR A PROBABILITY DISTRIBUTION\nA probability distribution is a list of all possible outcomes and their associated probabilities\nthat satisﬁes three rules:\n1. The outcomes listed must be disjoint.\n2. Each probability must be between 0 and 1.\n3. The probabilities must total to 1.\nDice sum234567891011120.000.050.100.15Probability\nFigure 2.7: The probability distribution of the sum of two dice.\nProbability distributions can be summarized in a bar plot. The probability distribution for\nthe sum of two dice is shown in Figure 2.7, with the bar heights representing the probabilities of\noutcomes.\nFigure 2.8 shows a bar plot of the birth weight data for 3,999,386 live births in the United\nStates in 2010, for which total counts have been converted to proportions. Since birth weight\ntrends do not change much between years, it is valid to consider the plot as a representation of the\nprobability distribution of birth weights for upcoming years, such as 2017. The data are available\nas part of the US CDC National Vital Statistics System.11\nThe graph shows that while most babies born weighed between 2000 and 5000 grams (2 to\n5 kg), there were both small (less than 1000 grams) and large (greater than 5000 grams) babies.\nPediatricians consider birth weights between 2.5 and 5 kg as normal.12A probability distribution\ngives a sense of which outcomes can be considered unusual (i.e., outcomes with low probability).\n11http://205.207.175.93/vitalstats/ReportFolders/reportFolders.aspx\n12https://www.nlm.nih.gov/medlineplus/birthweight.html\n98 CHAPTER 2. PROBABILITY\n499 or less 500 − 999 1000 − 1499 1500 − 1999 2000 − 2499 2500 − 2999 3000 − 3499 3500 − 3999 4000 − 4499 4500 − 4999 5000 − 8165 Not stated\nBirth Weight (Grams)Probability\n0.00.10.20.30.4\nFigure 2.8: Distribution of birth weights (in grams) of babies born in the US in\n2010.\nContinuous probability distributions\nProbability distributions for events that take on a ﬁnite number of possible outcomes, such as the\nsum of two dice rolls, are referred to as discrete probability distributions .\nConsider how the probability distribution for adult heights in the US might best be repre-\nsented. Unlike the sum of two dice rolls, height can occupy any value over a continuous range.\nThus, height has a continuous probability distribution , which is speciﬁed by a probability den-\nsity function rather than a table; Figure 2.9 shows a histogram of the height for 3 million US adults\nfrom the mid-1990’s, with an overlaid density curve.13\nJust as in the discrete case, the probabilities of all possible outcomes must still sum to 1; the\ntotal area under a probability density function equals 1.\nheight (cm)140 160 180 200\nFigure 2.9: The continuous probability distribution of heights for US adults.\n13This sample can be considered a simple random sample from the US population. It relies on the USDA Food Com-\nmodity Intake Database.\n2.1. DEFINING PROBABILITY 99\nEXAMPLE 2.16\nEstimate the probability that a randomly selected adult from the US population has height between\n180 and 185 centimeters. In Figure 2.10(a), the two bins between 180 and 185 centimeters have\ncounts of 195,307 and 156,239 people.\nFind the proportion of the histogram’s area that falls in the range 180cm and 185: add the heights\nof the bins in the range and divide by the sample size:\n195;307 + 156;239\n3,000,000= 0:1172:\nThe probability can be calculated precisely with the use of computing software, by ﬁnding the area\nof the shaded region under the curve between 180and 185:\nP(height between 180and 185) = area between 180and 185= 0:1157:\nheight (cm)140 160 180 200\n(a)\nheight (cm)140 160 180 200 (b)\nFigure 2.10: (a) A histogram with bin sizes of 2.5 cm, with bars between 180and\n185cm shaded. (b) Density for heights in the US adult population with the area\nbetween 180and 185cm shaded.\nEXAMPLE 2.17\nWhat is the probability that a randomly selected person is exactly 180cm? Assume that height can\nbe measured perfectly.\nThis probability is zero. A person might be close to 180cm, but not exactly 180cm tall. This\nalso coheres with the deﬁnition of probability as an area under the density curve; there is no area\ncaptured between 180cm and 180cm.\nGUIDED PRACTICE 2.18\nSuppose a person’s height is rounded to the nearest centimeter. Is there a chance that a random\nperson’s measured height will be 180cm?14\n14This has positive probability. Anyone between 179.5 cm and 180.5 cm will have a measured height of 180cm. This a\nmore realistic scenario to encounter in practice versus Example 2.17.\n100 CHAPTER 2. PROBABILITY\n2.1.6 Complement of an event\nRolling a die produces a value in the set f1,2,3,4,5,6g. This set of all possible outcomes is\ncalled the sample space (S) for rolling a die.S\nSample space\nLetD=f2,3grepresent the event that the outcome of a die roll is 2or3. The complement ofAc\nComplement\nof outcome ADrepresents all outcomes in the sample space that are not in D, which is denoted by Dc=f1,4,5,\n6g. That is,Dcis the set of all possible outcomes not already included in D. Figure 2.11 shows the\nrelationship between D,Dc, and the sample space S.\nFigure 2.11: Event D=f2,3gand its complement, Dc=f1,4,5,6g.Srepresents\nthe sample space, which is the set of all possible events.\nGUIDED PRACTICE 2.19\n(a) Compute P(Dc) =P(rolling a 1,4,5, or6). (b) What is P(D) +P(Dc)?15\nGUIDED PRACTICE 2.20\nEventsA=f1,2gandB=f4,6gare shown in Figure 2.3 on page 94. (a) Write out what AcandBc\nrepresent. (b) Compute P(Ac) andP(Bc). (c) Compute P(A) +P(Ac) andP(B) +P(Bc).16\nA complement of an event Ais constructed to have two very important properties: every\npossible outcome not in Ais inAc, andAandAcare disjoint. If every possible outcome not in Ais\ninAc, this implies that\nP(AorAc) = 1: (2.21)\nThen, by Addition Rule for disjoint events,\nP(AorAc) =P(A) +P(Ac): (2.22)\nCombining Equations (2.21) and (2.22) yields a useful relationship between the probability of an\nevent and its complement.\n15(a) The outcomes are disjoint and each has probability 1 =6, so the total probability is 4 =6 = 2=3. (b) We can also see that\nP(D) =1\n6+1\n6= 1=3. SinceDandDcare disjoint, P(D) +P(Dc) = 1.\n16Brief solutions: (a) Ac=f3,4,5,6gandBc=f1,2,3,5g. (b) Noting that each outcome is disjoint, add the individual\noutcome probabilities to get P(Ac) = 2=3 andP(Bc) = 2=3. (c)AandAcare disjoint, and the same is true of BandBc.\nTherefore,P(A) +P(Ac) = 1 andP(B) +P(Bc) = 1.\n2.1. DEFINING PROBABILITY 101\nCOMPLEMENT\nThe complement of event Ais denotedAc, andAcrepresents all outcomes not in A.AandAc\nare mathematically related:\nP(A) +P(Ac) = 1;i.e.P(A) = 1\u0000P(Ac): (2.23)\nIn simple examples, computing either AorAcis feasible in a few steps. However, as problems\ngrow in complexity, using the relationship between an event and its complement can be a useful\nstrategy.\nGUIDED PRACTICE 2.24\nLetArepresent the event of selecting an adult from the US population with height between 180\nand 185 cm, as calculated in Example 2.16. What is P(Ac)?17\nGUIDED PRACTICE 2.25\nLetArepresent the event in which two dice are rolled and their total is less than 12. (a) What does\nthe eventAcrepresent? (b) Determine P(Ac) from Figure 2.6 on page 97. (c) Determine P(A).18\nGUIDED PRACTICE 2.26\nConsider again the probabilities from Figure 2.6 and rolling two dice. Find the following probabil-\nities: (a) The sum of the dice is not6. (b) The sum is at least 4. That is, determine the probability of\nthe eventB=f4,5, ...,12g. (c) The sum is no more than 10. That is, determine the probability of the\neventD=f2,3, ...,10g.19\n2.1.7 Independence\nJust as variables and observations can be independent, random phenomena can also be inde-\npendent. Two processes are independent if knowing the outcome of one provides no information\nabout the outcome of the other. For instance, ﬂipping a coin and rolling a die are two indepen-\ndent processes – knowing that the coin lands heads up does not help determine the outcome of\nthe die roll. On the other hand, stock prices usually move up or down together, so they are not\nindependent.\n17P(Ac) = 1\u0000P(A) = 1\u00000:1157 = 0:8843.\n18(a) The complement of A: when the total is equal to 12. (b)P(Ac) = 1=36. (c) Use the probability of the complement\nfrom part (b), P(Ac) = 1=36, and Equation (2.23): P(less than 12) = 1\u0000P(12) = 1\u00001=36 = 35=36.\n19(a) First ﬁnd P(6) = 5=36, then use the complement: P(not 6) = 1\u0000P(6) = 31=36.\n(b) First ﬁnd the complement, which requires much less e ﬀort:P(2or3) = 1=36 + 2=36 = 1=12. Then calculate P(B) =\n1\u0000P(Bc) = 1\u00001=12 = 11=12.\n(c) As before, ﬁnding the complement is the more direct way to determine P(D). First ﬁnd P(Dc) =P(11or12) = 2=36 +\n1=36 = 1=12. Then calculate P(D) = 1\u0000P(Dc) = 11=12.\n102 CHAPTER 2. PROBABILITY\nExample 2.3 provides a basic example of two independent processes: rolling two dice. What\nis the probability that both will be 1? Suppose one of the dice is blue and the other green. If the\noutcome of the blue die is a 1, it provides no information about the outcome of the green die. This\nquestion was ﬁrst encountered in Example 2.3: 1 =6thof the time the blue die is a 1, and 1=6thof\nthose times the green die will also be 1. This is illustrated in Figure 2.12. Because the rolls are\nindependent, the probabilities of the corresponding outcomes can be multiplied to obtain the ﬁnal\nanswer: (1=6)(1=6) = 1=36. This can be generalized to many independent processes.\nFigure 2.12: 1 =6thof the time, the ﬁrst roll is a 1. Then 1=6thofthose times, the\nsecond roll will also be a 1.\nComplicated probability problems, such as those that arise in biology or medicine, are often\nsolved with the simple ideas used in the dice example. For instance, independence was used im-\nplicitly in the second solution to Example 2.4, when calculating the probability that two carriers\nwill have an a ﬀected child with cystic ﬁbrosis. Genes are typically passed along from the mother\nand father independently. This allows for the assumption that, on average, half of the o ﬀspring\nwho receive a mutated gene copy from the mother will also receive a mutated copy from the father.\nGUIDED PRACTICE 2.27\nWhat if there were also a red die independent of the other two? What is the probability of rolling\nthe three dice and getting all 1s?20\nGUIDED PRACTICE 2.28\nThree US adults are randomly selected. The probability the height of a single adult is between 180\nand 185 cm is 0.1157.21\n(a) What is the probability that all three are between 180 and 185 cm tall?\n(b) What is the probability that none are between 180 and 185 cm tall?\n20The same logic applies from Example 2.3. If 1 =36thof the time the blue and green dice are both 1, then 1=6thofthose\ntimes the red die will also be 1, so multiply:\nP(blue =1andgreen =1andred=1) =P(blue =1)P(green =1)P(red=1)\n= (1=6)(1=6)(1=6) = 1=216:\n21Brief answers: (a) 0 :1157\u00020:1157\u00020:1157 = 0:0015. (b) (1\u00000:1157)3= 0:692.\n2.1. DEFINING PROBABILITY 103\nMULTIPLICATION RULE FOR INDEPENDENT PROCESSES\nIfAandBrepresent events from two di ﬀerent and independent processes, then the probability\nthat bothAandBoccur is given by:\nP(AandB) =P(A)P(B): (2.29)\nSimilarly, if there are keventsA1, ...,Akfromkindependent processes, then the probability\nthey all occur is\nP(A1)P(A2)\u0001\u0001\u0001P(Ak):\nEXAMPLE 2.30\nMandatory drug testing. Mandatory drug testing in the workplace is common practice for certain\nprofessions, such as air tra ﬃc controllers and transportation workers. A false positive in a drug\nscreening test occurs when the test incorrectly indicates that a screened person is an illegal drug\nuser. Suppose a mandatory drug test has a false positive rate of 1.2% (i.e., has probability 0.012 of\nindicating that an employee is using illegal drugs when that is not the case). Given 150 employees\nwho are in reality drug free, what is the probability that at least one will (falsely) test positive?\nAssume that the outcome of one drug test has no e ﬀect on the others.\nFirst, note that the complement of at least 1 person testing positive is that no one tests positive (i.e.,\nall employees test negative). The multiplication rule can then be used to calculate the probability\nof 150 negative tests.\nP(At least 1 \"+\") = P(1 or 2 or 3 . . . or 150 are \"+\")\n= 1\u0000P(None are \"+\")\n= 1\u0000P(150 are \"-\")\n= 1\u0000P(\"-\")150\n= 1\u0000(0:988)150= 1\u00000:16 = 0:84:\nEven when using a test with a small probability of a false positive, the company is more than 80%\nlikely to incorrectly claim at least one employee is an illegal drug user!\nGUIDED PRACTICE 2.31\nBecause of the high likelihood of at least one false positive in company wide drug screening pro-\ngrams, an individual with a positive test is almost always re-tested with a di ﬀerent screening test:\none that is more expensive than the ﬁrst, but has a lower false positive probability. Suppose the\nsecond test has a false positive rate of 0.8%. What is the probability that an employee who is not\nusing illegal drugs will test positive on both tests?22\n22The outcomes of the two tests are independent of one another; P(AandB) =P(A)\u0002P(B), where events AandBare the\nresults of the two tests. The probability of a false positive with the ﬁrst test is 0.012 and 0.008 with the second. Thus, the\nprobability of an employee who is not using illegal drugs testing positive on both tests is 0 :012\u00020:008 = 9:6\u000210\u00005\n104 CHAPTER 2. PROBABILITY\nFigure 2.13: Inheritance of ABO blood groups.\nEXAMPLE 2.32\nABO blood groups. There are four di ﬀerent common blood types (A, B, AB, and O), which are\ndetermined by the presence of certain antigens located on cell surfaces. Antigens are substances\nused by the immune system to recognize self versus non-self; if the immune system encounters\nantigens not normally found on the body’s own cells, it will attack the foreign cells. When patients\nreceive blood transfusions, it is critical that the antigens of transfused cells match those of the\npatient’s, or else an immune system response will be triggered.\nThe ABO blood group system consists of four di ﬀerent blood groups, which describe whether an\nindividual’s red blood cells carry the A antigen, B antigen, both, or neither. The ABO gene has three\nalleles:IA,IB, and i. The iallele is recessive to both IAandIB, and does not produce antigens;\nthus, an individual with genotype IAiis blood group A and an individual with genotype IBiis\nblood group B. The IAandIBalleles are codominant, such that individuals of IAIBgenotype are\nAB. Individuals homozygous for the iallele are known as blood group O, with neither A nor B\nantigens.\nSuppose that both members of a couple have Group AB blood.\na) What is the probability that a child of this couple will have Group A blood?\nb) What is the probability that they have two children with Group A blood?\na) An individual with Group AB blood is genotype IAIB. TwoIAIBparents can produce children\nwith genotypes IAIB,IAIA, orIBIB. Of these possibilities, only children with genotype IAIA\nhave Group A blood. Each parent has 0.5 probability of passing down their IAallele. Thus, the\nprobability that a child of this couple will have Group A blood is P(parent 1 passes down IA\nallele)\u0002P(parent 2 passes down IAallele) = 0:5\u00020:5 = 0:25.\nb) Inheritance of alleles is independent between children. Thus, the probability of two children\nhaving Group A blood equals P(child 1 has Group A blood) \u0002P(child 2 has group A blood). The\nprobability of a child of this couple having Group A blood was previously calculated as 0.25.\nThe answer is given by 0 :25\u00020:25 = 0:0625.\n2.1. DEFINING PROBABILITY 105\nThe previous examples in this section have used independence to solve probability problems.\nThe deﬁnition of independence can also be used to check whether two events are independent –\ntwo eventsAandBare independent if they satisfy Equation (2.29).\nEXAMPLE 2.33\nIs the event of drawing a heart from a deck of cards independent of drawing an ace?\nThe probability the card is a heart is 1 =4 (13=52 = 1=4) and the probability that it is an ace is\n1=13 (4=52 = 1=13). The probability that the card is the ace of hearts ( A~) is 1=52. Check whether\nEquation 2.29 is satisﬁed:\nP(~)P(A) =\u00121\n4\u0013\u00121\n13\u0013\n=1\n52=P(~and A):\nSince the equation holds, the event that the card is a heart and the event that the card is an ace are\nindependent events.\nEXAMPLE 2.34\nIn the general population, about 15% of adults between 25 and 40 years of age are hypertensive.\nSuppose that among males of this age, hypertension occurs about 18% of the time. Is hypertension\nindependent of sex?\nAssume that the population is 50% male, 50% female; it is given in the problem that hypertension\noccurs about 15% of the time in adults between ages 25 and 40.\nP(hypertension)\u0002P(male) = (0:15)(0:50) = 0:075,0:18:\nEquation 2.29 is not satisﬁed, therefore hypertension is not independent of sex. In other words,\nknowing whether an individual is male or female is informative as to whether they are hyperten-\nsive. If hypertension and sex were independent, then we would expect hypertension to occur at an\nequal rate in males as in females.\n106 CHAPTER 2. PROBABILITY\n2.2 Conditional probability\nWhile it is di ﬃcult to obtain precise estimates, the US CDC estimated that in 2012, approxi-\nmately 29.1 million Americans had type 2 diabetes – about 9.3% of the population.23A health care\npractitioner seeing a new patient would expect a 9.3% chance that the patient might have diabetes.\nHowever, this is only the case if nothing is known about the patient. The prevalence of type\n2 diabetes varies with age. Between the ages of 20 and 44, only about 4% of the population have\ndiabetes, but almost 27% of people age 65 and older have the disease. Knowing the age of a patient\nprovides information about the chance of diabetes; age and diabetes status are not independent.\nWhile the probability of diabetes in a randomly chosen member of the population is 0.093, the\nconditional probability of diabetes in a person known to be 65 or older is 0.27.\nConditional probability is used to characterize how the probability of an outcome varies with\nthe knowledge of another factor or condition, and is closely related to the concepts of marginal and\njoint probabilities.\n2.2.1 Marginal and joint probabilities\nFigures 2.14 and 2.15 provide additional information about the relationship between diabetes\nprevalence and age.24Figure 2.14 is a contingency table for the entire US population in 2012; the\nvalues in the table are in thousands (to make the table more readable).\nDiabetes No Diabetes Sum\nLess than 20 years 200 86,664 86,864\n20 to 44 years 4,300 98,724 103,024\n45 to 64 years 13,400 68,526 81,926\nGreater than 64 years 11,200 30,306 41,506\nSum 29,100 284,220 313,320\nFigure 2.14: Contingency table showing type 2 diabetes status and age group, in\nthousands.\nIn the ﬁrst row, for instance, Figure 2.14 shows that in the entire population of approximately\n313,320,000 people, approximately 200,000 individuals were in the less than 20 years age group\nand diagnosed with diabetes – about 0.1%. The table also indicates that among the approximately\n86,864,000 individuals less than 20 years of age, only 200,000 su ﬀered from type 2 diabetes, ap-\nproximately 0.2%. The distinction between these two statements is small but important. The ﬁrst\nprovides information about the size of the group with type 2 diabetes population that is less than\n20 years of age, relative to the entire population. In contrast, the second statement is about the size\nof the diabetes population within the less than 20 years of age group, relative to the size of that age\ngroup.\n2321 million of these cases are diagnosed, while the CDC predicts that 8.1 million cases are undiagnosed; that is, ap-\nproximately 8.1 million people are living with diabetes, but they (and their physicians) are unaware that they have the\ncondition.\n24Because the CDC provides only approximate numbers for diabetes prevalence, the numbers in the table are approxi-\nmations of actual population counts.\n2.2. CONDITIONAL PROBABILITY 107\nGUIDED PRACTICE 2.35\nWhat fraction of the US population are 45 to 64 years of age and have diabetes? What fraction of\nthe population age 45 to 64 have diabetes?25\nThe entries in Figure 2.15 show the proportions of the population in each of the eight cate-\ngories deﬁned by diabetes status and age, obtained by dividing each value in the cells of Figure 2.14\nby the total population size.\nDiabetes No Diabetes Sum\nLess than 20 years 0.001 0.277 0.277\n20 to 44 years 0.014 0.315 0.329\n45 to 64 years 0.043 0.219 0.261\nGreater than 64 years 0.036 0.097 0.132\nSum 0.093 0.907 1.000\nFigure 2.15: Probability table summarizing diabetes status and age group.\nIf these proportions are interpreted as probabilities for randomly chosen individuals from the\npopulation, the value 0.014 in the ﬁrst column of the second row implies that the probability of\nselecting someone at random who has diabetes and whose age is between 20 and 44 is 0.014, or\n1.4%. The entries in the eight main table cells (i.e., excluding the values in the margins) are joint\nprobabilities , which specify the probability of two events happening at the same time – in this\ncase, diabetes and a particular age group. In probability notation, this joint probability can be\nexpressed as 0 :014 =P(diabetes and age 20 to 44).26\nThe values in the last row and column of the table are the sums of the corresponding rows or\ncolumns. The sum of the of the probabilities of the disjoint events (diabetes, age 20 to 44) and (no\ndiabetes, age 20 to 44), 0.329, is the probability of being in the age group 20 to 44. The row and\ncolumn sums are marginal probabilities ; they are probabilities about only one type of event, such\nas age. For example, the sum of the ﬁrst column (0.093) is the marginal probability of a member of\nthe population having diabetes.\nMARGINAL AND JOINT PROBABILITIES\nAmarginal probability is a probability only related to a single event or process, such as P(A).\nAjoint probability is the probability that two or more events or processes occur jointly, such as\nP(AandB).\nGUIDED PRACTICE 2.36\nWhat is the interpretation of the value 0.907 in the last row of the table? And of the value 0.097\ndirectly above it?27\n25The ﬁrst value is given by the intersection of \"45 - 64 years of age\" and \"diabetes\", divided by the total population\nnumber: 13;400;000=313;320;000 = 0:043. The second value is given by dividing 13,400,000 by 81,926,000, the number of\nindividuals in that age group: 13 ;400;000=81;926;000 = 0:164.\n26Alternatively, this is commonly written as as P(diabetes, age 20 to 44), with a comma replacing “and”.\n27The value 0.907 in the last row indicates the total proportion of individuals in the population who do not have diabetes.\nThe value 0.097 indicates the joint probability of not having diabetes and being in the greater than 64 years age group.\n108 CHAPTER 2. PROBABILITY\n2.2.2 Deﬁning conditional probability\nThe probability that a randomly selected individual from the US has diabetes is 0.093, the\nsum of the ﬁrst column in Figure 2.15. How does that probability change if it is known that the\nindividual’s age is 64 or greater?\nThe conditional probability can be calculated from Figure 2.14, which shows that 11,200,000\nof the 41,506,000 people in that age group have diabetes, so the likelihood that someone from that\nage group has diabetes is:\n11;200;000\n41;506;000= 0:27;\nor 27%. The additional information about a patient’s age allows for a more accurate estimate of the\nprobability of diabetes.\nSimilarly, the conditional probability can be calculated from the joint and marginal propor-\ntions in Figure 2.15. Consider the main di ﬀerence between the conditional probability versus the\njoint and marginal probabilities. Both the joint probability and marginal probabilities are proba-\nbilities relative to the entire population. However, the conditional probability is the probability of\nhaving diabetes, relative only to the segment of the population greater than the age of 64.\nIntuitively, the denominator in the calculation of a conditional probability must account for\nthe fact that only a segment of the population is being considered, rather than the entire popula-\ntion. The conditional probability of diabetes given age 64 or older is simply the joint probability\nof having diabetes and being greater than 64 years of age divided by the marginal probability of\nbeing in that age group:\nprop. of population with diabetes, age 64 or greater\nprop. of population greater than age 64=11;200;000=313;320;000\n41;506;000=313;320;000\n=0:036\n0:132\n= 0:270:\nThis leads to the mathematical deﬁnition of conditional probability.\nCONDITIONAL PROBABILITY\nThe conditional probability of an event Agiven an event or condition Bis:\nP(AjB) =P(AandB)\nP(B): (2.37)\nGUIDED PRACTICE 2.38\nCalculate the probability that a randomly selected person has diabetes, given that their age is be-\ntween 45 and 64.28\n28LetAbe the event a person has diabetes, and Bthe event that their age is between 45 and 64. Use the information in\nFigure 2.15 to calculate P(AjB).P(AjB) =P(AandB)\nP(B)=0:043\n0:261= 0:165:\n2.2. CONDITIONAL PROBABILITY 109\nGUIDED PRACTICE 2.39\nCalculate the probability that a randomly selected person is between 45 and 64 years old, given\nthat the person has diabetes.29\nConditional probabilities have similar properties to regular (unconditional) probabilities.\nSUM OF CONDITIONAL PROBABILITIES\nLetA1, ...,Akrepresent all the disjoint outcomes for a variable or process. Then if Bis an event,\npossibly for another variable or process, we have:\nP(A1jB) +\u0001\u0001\u0001+P(AkjB) = 1:\nThe rule for complements also holds when an event and its complement are conditioned on\nthe same information:\nP(AjB) = 1\u0000P(AcjB):\nGUIDED PRACTICE 2.40\nCalculate the probability a randomly selected person is older than 20 years of age, given that the\nperson has diabetes.30\n29Again, letAbe the event a person has diabetes, and Bthe event that their age is between 45 and 64. Find P(BjA).\nP(BjA) =P(AandB)\nP(A)=0:043\n0:093= 0:462:\n30LetAbe the event that a person has diabetes, and Bbe the event that their age is less than 20 years. The desired\nprobability is P(BcjA) = 1\u0000P(BjA) = 1\u00000:001\n0:093= 0:989.\n110 CHAPTER 2. PROBABILITY\n2.2.3 General multiplication rule\nSection 2.1.7 introduced the Multiplication Rule for independent processes. Here, the General\nMultiplication Rule is introduced for events that might not be independent.\nGENERAL MULTIPLICATION RULE\nIfAandBrepresent two outcomes or events, then\nP(AandB) =P(AjB)P(B):\nIt is useful to think of Aas the outcome of interest and Bas the condition.\nThis General Multiplication Rule is simply a rearrangement of the deﬁnition for conditional\nprobability in Equation (2.37) on page 108.\nEXAMPLE 2.41\nSuppose that among male adults between 25 and 40 years of age, hypertension occurs about 18%\nof the time. Assume that the population is 50% male, 50% female. What is the probability of\nrandomly selecting a male with hypertension from the population of individuals 25-40 years of\nage?\nLetAbe the event that a person has hypertension, and Bthe event that they are a male adult\nbetween 25 and 40 years of age. P(AjB), the probability of hypertension given male sex, is 0.18.\nThus,P(AandB) = (0:18)(0:50) = 0:09.\n2.2.4 Independence and conditional probability\nIf two events are independent, knowing the outcome of one should provide no information\nabout the other.\nEXAMPLE 2.42\nLetXandYrepresent the outcomes of rolling two dice. Use the formula for conditional probability\nto compute P(Y=1jX=1). What isP(Y= 1)? Is this di ﬀerent fromP(Y=1jX=1)?\nP(Y=1andX=1)\nP(X=1)=1=36\n1=6= 1=6:\nThe probability P(Y= 1) = 1=6 is the same as the conditional probability. The probability that\nY= 1 was unchanged by knowledge about X, since the events XandYare independent.\n2.2. CONDITIONAL PROBABILITY 111\nUsing the Multiplication Rule for independent events allows for a mathematical illustration\nof why the condition information has no inﬂuence in Example 2.42:\nP(Y=1jX=1) =P(Y=1andX=1)\nP(X=1)\n=P(Y=1)P(X=1)\nP(X=1)\n=P(Y=1):\nThis is a speciﬁc instance of the more general result that if two events AandBare independent,\nP(AjB) =P(A) as long asP(B)>0:\nP(AjB) =P(AandB)\nP(B)\n=P(A)P(B)\nP(B)\n=P(A):\nGUIDED PRACTICE 2.43\nIn the US population, about 45% of people are blood group O. Suppose that 40% of Asian people\nliving in the US are blood group O, and that the Asian population in the United States is approxi-\nmately 4%. Do these data suggest that blood group is independent of ethnicity?31\n2.2.5 Bayes’ Theorem\nThis chapter began with a straightforward question – what are the chances that a woman with\nan abnormal (i.e., positive) mammogram has breast cancer? For a clinician, this question can be\nrephrased as the conditional probability that a woman has breast cancer, given that her mammo-\ngram is abnormal. This conditional probability is called the positive predictive value (PPV) of a\nmammogram. More concisely, if A= {a woman has breast cancer}, and B= {a mammogram is positive},\nthe PPV of a mammogram is P(AjB).\nThe characteristics of a mammogram (and other diagnostic tests) are given with the reverse\nconditional probabilities—the probability that the mammogram correctly returns a positive result\nif a woman has breast cancer, as well as the probability that the mammogram correctly returns\na negative result if a woman does not have breast cancer. These are the probabilities P(BjA) and\nP(BcjAc), respectively.\nGiven the probabilities P(BjA) andP(BcjAc), as well as the marginal probability of disease\nP(A), how can the positive predictive value P(AjB) be calculated?\nThere are several possible strategies for approaching this type of problem—1) constructing\ntree diagrams, 2) using a purely algebraic approach using Bayes’ Theorem, and 3) creating contin-\ngency tables based on calculating conditional probabilities from a large, hypothetical population.\n31LetArepresent blood group O, and Brepresent Asian ethnicity. Since P(AjB) = 0:40 does not equal P(A) = 0:45, the\ntwo events are not independent. Blood group does not seem to be independent of ethnicity.\n112 CHAPTER 2. PROBABILITY\nEXAMPLE 2.44\nIn Canada, about 0.35% of women over 40 will develop breast cancer in any given year. A common\nscreening test for cancer is the mammogram, but it is not perfect. In about 11% of patients with\nbreast cancer, the test gives a false negative : it indicates a woman does not have breast cancer when\nshe does have breast cancer. Similarly, the test gives a false positive in 7% of patients who do not\nhave breast cancer: it indicates these patients have breast cancer when they actually do not.32If a\nrandomly selected woman over 40 is tested for breast cancer using a mammogram and the test is\npositive – that is, the test suggests the woman has cancer – what is the probability she has breast\ncancer?\nRead on in the text for three solutions to this example.\nExample 2.44 Solution 1. Tree Diagram.\nCancer Status Mammogram\ncancer,  0.0035positive,  0.890.0035*0.89 = 0.00312\nnegative,  0.110.0035*0.11 = 0.00038\nno cancer,  0.9965positive,  0.070.9965*0.07 = 0.06976\nnegative,  0.930.9965*0.93 = 0.92675\nFigure 2.16: A tree diagram for breast cancer screening.\nAtree diagram is a tool to organize outcomes and probabilities around the structure of data,\nand is especially useful when two or more processes occur in a sequence, with each process condi-\ntioned on its predecessors.\nIn Figure 2.16, the primary branches split the population by cancer status, and show the\nmarginal probabilities 0.0035 and 0.9965 of having cancer or not, respectively. The secondary\nbranches are conditioned on the primary branch and show conditional probabilities; for example,\nthe top branch is the probability that a mammogram is positive given that an individual has cancer.\nThe problem provides enough information to compute the probability of testing positive if breast\ncancer is present, since this probability is the complement of the probability of a false negative:\n1\u00000:11 = 0:89.\nJoint probabilities can be constructed at the end of each branch by multiplying the numbers\nfrom right to left, such as the probability that a woman tests positive given that she has breast\ncancer (abbreviated as BC), times the probability she has breast cancer:\nP(BC and mammogram+) =P(mammogram+jBC)\u0002P(BC)\n= (0:89)(0:0035) = 0:00312:\n32The probabilities reported here were obtained using studies reported at www.breastcancer.org and\nwww.ncbi.nlm.nih.gov/pmc/articles/PMC1173421.\n2.2. CONDITIONAL PROBABILITY 113\nUsing the tree diagram allows for the information in the problem to be mapped out in a way\nthat makes it easier to calculate the desired conditional probability. In this case, the diagram makes\nit clear that there are two scenarios in which someone can test positive: either testing positive when\nhaving breast cancer or by testing positive in the absence of breast cancer. To ﬁnd the probability\nthat a woman has breast cancer given that she tests positive, apply the conditional probability\nformula: divide the probability of testing positive when having breast cancer by the probability of\ntesting positive.\nThe probability of a positive test result is the sum of the two corresponding scenarios:\nP(mammogram+) =P(mammogram+and has BC) + P(mammogram+and no BC)\n=[P(mammogram+jhas BC)\u0002P(has BC)] + [ P(mammogram+jno BC)\u0002P(no BC)]\n=(0:0035)(0:89) + (0:9965)(0:07) = 0:07288:\nThus, if the mammogram screening is positive for a patient, the probability that the patient has\nbreast cancer is given by:\nP(has BCjmammogram+) =P(has BC and mammogram+)\nP(mammogram+)\n=0:00312\n0:07288\u00190:0428:\nEven with a positive mammogram, there is still only a 4% chance of breast cancer! It may seem\nsurprising that even when the false negative and false positive probabilities of the test are small\n(0.11 and 0.07, respectively), the conditional probability of disease given a positive test could also\nbe so small. In this population, the probability that a woman does not have breast cancer is high\n(1 - 0.0035 = 0.9965), which results in a relatively high number of false positives in comparison to\ntrue positives.\nCalculating probabilities for diagnostic tests is done so often in medicine that the topic has\nsome specialized terminology. The sensitivity of a test is the probability of a positive test re-\nsult when disease is present, such as a positive mammogram when a patient has breast cancer.\nThe speciﬁcity of a test is the probability of a negative test result when disease is absent.33The\nprobability of disease in a population is referred to as the prevalence . With speciﬁcity and sen-\nsitivity information for a particular test, along with disease prevalence, the positive predictive\nvalue (PPV) can be calculated: the probability that disease is present when a test result is positive.\nSimilarly, the negative predictive value is the probability that disease is absent when test results\nare negative. These terms are used for nearly all diagnostic tests used to screen for diseases.\nGUIDED PRACTICE 2.45\nIdentify the prevalence, sensitivity, speciﬁcity, and PPV from the scenario in Example 2.44.34\n33The sensitivity and speciﬁcity are, respectively, the probability of a true positive test result and the probability of a\ntrue negative test result.\n34The prevalence of breast cancer is 0.0035. The sensitivity is the probability of a positive test result when disease is\npresent, which is the complement of a false negative: 1 \u00000:11 = 0:89. The speciﬁcity is the probability of a negative test\nresult when disease is absent, which is the complement of a false positive: 1 \u00000:07 = 0:93. The PPV is 0.04, the probability\nof breast cancer given a positive mammogram.\n114 CHAPTER 2. PROBABILITY\nExample 2.44 Solution 2. Bayes’ Rule.\nThe process used to solve the problem via the tree diagram can be condensed into a single algebraic\nexpression by substituting the original probability expressions into the numerator and denomina-\ntor:\nP(has BCjmammogram+) =P(has BC and mammogram+)\nP(mammogram+)\n=P(mammogram+jhas BC)\u0002P(has BC)\n[P(mammogram+jhas BC)\u0002P(has BC)] + [ P(mammogram+jno BC)\u0002P(no BC)]:\nThe expression can also be written in terms of diagnostic testing language, where D= {has disease},\nDc= {does not have disease}, T+= {positive test result}, and T\u0000= {negative test result}.\nP(DjT+) =P(DandT+)\nP(T+)\n=P(T+jD)\u0002P(D)\n[P(T+jD)\u0002P(D)] + [P(T+jDc)\u0002P(Dc)]\nPPV =sensitivity\u0002prevalence\n[sensitivity\u0002prevalence] + [(1 - speciﬁcity) \u0002(1 - prevalence)]:\nThe generalization of this formula is known as Bayes’ Theorem or Bayes’ Rule.\nBAYES’ THEOREM\nConsider the following conditional probability for variable 1 and variable 2:\nP(outcomeA1of variable 1joutcomeBof variable 2) :\nBayes’ Theorem states that this conditional probability can be identiﬁed as the following frac-\ntion:\nP(BjA1)P(A1)\nP(BjA1)P(A1) +P(BjA2)P(A2) +\u0001\u0001\u0001+P(BjAk)P(Ak); (2.46)\nwhereA2,A3, ..., andAkrepresent all other possible outcomes of the ﬁrst variable.\nThe numerator identiﬁes the probability of getting both A1andB. The denominator is the\nmarginal probability of getting B. This bottom component of the fraction describes the adding of\nprobabilities from the di ﬀerent ways to get B.\nTo apply Bayes’ Theorem correctly, there are two preparatory steps:\n(1) First identify the marginal probabilities of each possible outcome of the ﬁrst variable: P(A1),\nP(A2), ...,P(Ak).\n(2) Then identify the probability of the outcome B, conditioned on each possible scenario for the\nﬁrst variable: P(BjA1),P(BjA2), ...,P(BjAk).\nOnce these probabilities are identiﬁed, they can be applied directly within the formula.\n2.2. CONDITIONAL PROBABILITY 115\nExample 2.44 Solution 3. Contingency Table.\nThe positive predictive value (PPV) of a diagnostic test can be calculated by constructing a two-way\ncontingency table for a large, hypothetical population and calculating conditional probabilities by\nconditioning on rows or columns. Using a large enough hypothetical population results in an\nempirical estimate of PPV that is very close to the exact value obtained via using the previously\ndiscussed approaches.\nBegin by constructing an empty 2 \u00022 table, with the possible outcomes of the diagnostic test\nas the rows, and the possible disease statuses as the columns (Figure 2.17). Include cells for the\nrow and column sums.\nChoose a large number N, for the hypothetical population size. Typically, Nof 100,000 is\nsuﬃcient for an accurate estimate.\nBreast Cancer Present Breast Cancer Absent Sum\nMammogram Positive – – –\nMammogram Negative – – –\nSum – – 100,000\nFigure 2.17: A 2\u00022 table for the mammogram example, with hypothetical popu-\nlation sizeNof 100,000.\nContinue populating the table, using the provided information about the prevalence of breast\ncancer in this population (0.35%), the chance of a false negative mammogram (11%), and the\nchance of a false positive (7%):\n1. Calculate the two column totals (the number of women with and without breast cancer) from\nP(BC), the disease prevalence:\nN\u0002P(BC) = 100;000\u0002:0035 = 350 women with BC\nN\u0002[1\u0000P(BC)] = 100;000\u0002[1\u0000:0035] = 99;650 women without BC\nAlternatively, the number of women without breast cancer can be calculated by subtracting\nthe number of women with breast cancer from N.\n2. Calculate the two numbers in the ﬁrst column: the number of women who have breast cancer\nand tested either negative (false negative) or positive (true positive).\nwomen with BC\u0002P(false \"-\") = 350\u0002:11 = 38:5 false \"-\" results\nwomen with BC\u0002[1\u0000P(false \"-\")] = 350\u0002[1\u0000:11] = 311:5 true \"+\" results\n3. Calculate the two numbers in the second column: the number of women who do not have\nbreast cancer and tested either positive (false positive) or negative (true negative).\nwomen without BC \u0002P(false \"+\") = 99 ;650\u0002:07 = 6;975:5 false \"+\" results\nwomen without BC \u0002[1\u0000P(false \"+\")] = 99 ;650\u0002[1\u0000:07] = 92;674:5 true \"-\" results\n4. Complete the table by calculating the two row totals: the number of positive and negative\nmammograms out of 100,000.\n(true \"+\" results) + (false \"+\" results) = 311 :5 + 6;975:5 = 7;287 \"+\" mammograms\n(true \"-\" results) + (false \"-\" results) = 38 :5 + 92;674:5 = 92;713 \"-\" mammograms\n116 CHAPTER 2. PROBABILITY\n5. Finally, calculate the PPV of the mammogram by using the ratio of the number of true pos-\nitives to the total number of positive mammograms. This estimate is more than accurate\nenough, with the calculated value di ﬀering only in the third decimal place from the exact\ncalculation,\ntrue \"+\" results\n\"+\" mammograms=311:5\n7;287= 0:0427:\nBreast Cancer Present Breast Cancer Absent Sum\nMammogram Positive 311.5 6,975.5 7,287\nMammogram Negative 38.5 92,674.5 92,713\nSum 350 99,650 100,000\nFigure 2.18: Completed table for the mammogram example. The table shows\nagain why the PPV of the mammogram is low: almost 7,300 women will have\na positive mammogram result in this hypothetical population, but only ~312 of\nthose women actually have breast cancer.\nGUIDED PRACTICE 2.47\nSome congenital disorders are caused by errors that occur during cell division, resulting in the\npresence of additional chromosome copies. Trisomy 21 occurs in approximately 1 out of 800 births.\nCell-free fetal DNA (cfDNA) testing is one commonly used way to screen fetuses for trisomy 21.\nThe test sensitivity is 0.98 and the speciﬁcity is 0.995. Calculate the PPV and NPV of the test.35\n35PPV =P(T+jD)\u0002P(D)\n[P(T+jD)\u0002P(D)] + [P(T+jDc)\u0002P(Dc)]=(0:98)(1=800)\n(0:98)(1=800) + (1\u00000:995)(799=800)= 0:197.\nNPV =P(T\u0000jDc)\u0002P(Dc)\n[P(T\u0000jD)\u0002P(D)] + [P(T\u0000jDc)\u0002P(Dc)]=(0:995)(799=800)\n(1\u00000:98)(1=800) + (0:995)(799=800)= 0:999975.\n2.3. EXTENDED EXAMPLE 117\n2.3 Extended example: cat genetics\nSo far, the principles of probability have only been illustrated with short examples. In a more\ncomplex setting, it can be surprisingly di ﬃcult to accurately translate a problem scenario into the\nlanguage of probability. This section demonstrates how the rules of probability can be applied to\nwork through a relatively sophisticated conditioning problem.\nProblem statement\nThe gene that controls white coat color in cats, KIT , is known to be responsible for multiple phe-\nnotypes such as deafness and blue eye color. A dominant allele Wat one location in the gene has\ncomplete penetrance for white coat color; all cats with the Wallele have white coats. There is in-\ncomplete penetrance for blue eyes and deafness; not all white cats will have blue eyes and not all\nwhite cats will be deaf. However, deafness and blue eye color are strongly linked, such that white\ncats with blue eyes are much more likely to be deaf. The variation in penetrance for eye color and\ndeafness may be due to other genes as well as environmental factors.\nSuppose that 30% of white cats have one blue eye, while 10% of white cats have two blue eyes.\nAbout 73% of white cats with two blue eyes are deaf and 40% of white cats with one blue eye are\ndeaf. Only 19% of white cats with other eye colors are deaf.\na) Calculate the prevalence of deafness among white cats.\nb) Given that a white cat is deaf, what is the probability that it has two blue eyes?\nc) Suppose that deaf, white cats have an increased chance of being blind, but that the prevalence\nof blindness di ﬀers according to eye color. While deaf, white cats with two blue eyes or two\nnon-blue eyes have probability 0.20 of developing blindness, deaf and white cats with one blue\neye have probability 0.40 of developing blindness. White cats that are not deaf have probability\n0.10 of developing blindness, regardless of their eye color.\ni. What is the prevalence of blindness among deaf, white cats?\nii. What is the prevalence of blindness among white cats?\niii. Given that a cat is white and blind, what is the probability that it has two blue eyes?\nDeﬁning notation\nBefore beginning any calculations, it is essential to clearly deﬁne any notation that will be used.\nFor this problem, there are several events of interest: deafness, number of blue eyes (either 0, 1, or\n2), and blindness.\n– LetDrepresent the event that a white cat is deaf.\n– LetB0= {zero blue eyes}, B1= {one blue eye}, and B2= {two blue eyes}.\n– LetLrepresent the event that a white cat is blind.\nNote that since all cats mentioned in the problem are white, it is not necessary to deﬁne\nwhiteness as an event; white cats represent the sample space.\n118 CHAPTER 2. PROBABILITY\nPart a) Deafness\nThe prevalence of deafness among white cats is the proportion of white cats that are deaf; i.e., the\nprobability of deafness among white cats. In the notation of probability, this question asks for the\nvalue ofP(D).\nEXAMPLE 2.48\nThe following information has been given in the problem. Re-write the information using the\nnotation deﬁned earlier.\nSuppose that 30% of white cats have one blue eye, while 10% of white cats have two\nblue eyes. About 73% of white cats with two blue eyes are deaf and 40% of white cats\nwith one blue eye are deaf. Only 19% of white cats with other eye colors are deaf.\nThe ﬁrst sentence provides information about the prevalence of white cats with one blue eye and\nwhite cats with two blue eyes: P(B1) = 0:30 andP(B2) = 0:10. The only other possible eye color\ncombination is zero blue eyes (i.e., two non-blue eyes); i.e., since P(B0) +P(B1) +P(B2) = 1,P(B0) =\n1\u0000P(B1)\u0000P(B2) = 0:60. 60% of white cats have two non-blue eyes.\nWhile it is not di ﬃcult to recognize that the second and third sentences provide information about\ndeafness in relation to eye color, it can be easy to miss that these probabilities are conditional\nprobabilities. A close reading should focus on the language—\"About 73% ofwhite cats with two\nblue eyes are deaf...\": i.e., out of the white cats that have two blue eyes, 73% are deaf. Thus,\nthese are probabilities of deafness conditioned on eye color. From these sentences, P(DjB2) = 0:73,\nP(DjB1) = 0:40, andP(DjB0) = 0:19.\nConsider that there are three possible ways to partition the event D, that a white cat is deaf: a\ncat could be deaf and have two blue eyes, be deaf and have one blue eye (and one non-blue eye), or\nbe deaf and have two non-blue eyes. Thus, by the addition rule of disjoint outcomes:\nP(D) =P(DandB2) +P(DandB1) +P(DandB0):\nAlthough the joint probabilities of being deaf and having particular eye colors are not given in\nthe problem, these can be solved for based on the given information. The deﬁnition of conditional\nprobability P(AjB) relates the joint probability P(AandB) with the marginal probability P(B).36\nP(AjB) =P(AandB)\nP(B)P(AandB) =P(AjB)P(B):\nThus, the probability P(D) is given by:\nP(D) =P(DandB2) +P(DandB1) +P(DandB0)\n=P(DjB2)P(B2) +P(DjB1)P(B1) +P(DjB0)P(B0)\n=(0:73)(0:10) + (0:40)(0:30) + (0:19)(0:60)\n=0:307:\nThe prevalence of deafness among white cats is 0.307.\n36This rearrangement of the deﬁnition of conditional probability, P(AandB) =P(AjB)P(B), is also known as the general\nmultiplication rule.\n2.3. EXTENDED EXAMPLE 119\nPart b) Deafness and eye color\nThe probability that a white cat has two blue eyes, given that it is deaf, can be expressed as P(B2jD).\nEXAMPLE 2.49\nUsing the deﬁnition of conditional probability, solve for P(B2jD).\nP(B2jD) =P(DandB2)\nP(D)=P(DjB2)P(B2)\nP(D)=(0:73)(0:10)\n0:307= 0:238:\nThe probability that a white cat has two blue eyes, given that it is deaf, is 0.238.\nIt is also possible to think of this as a Bayes’ Rule problem, where there are three possible\npartitions of the event of deafness, D. In this problem, it is possible to directly solve from the\ndeﬁnition of conditional probability since P(D) was solved for in part a); note that the expanded\ndenominator below matches the earlier work to calculate P(D).\nP(B2jD) =P(DandB2)\nP(D)=P(DjB2)P(B2)\nP(DjB2)P(B2) +P(DjB1)P(B1) +P(DjB0)P(B0):\nPart c) Blindness, deafness, and eye color\nEXAMPLE 2.50\nThe following information has been given in the problem. Re-write the information using the\nnotation deﬁned earlier.\nSuppose that deaf, white cats have an increased chance of being blind, but that the\nprevalence of blindness di ﬀers according to eye color. While deaf, white cats with two\nblue eyes or two non-blue eyes have probability 0.20 of developing blindness, deaf and\nwhite cats with one blue eye have probability 0.40 of developing blindness. White cats\nthat are not deaf have probability 0.10 of developing blindness, regardless of their eye\ncolor.\nThe second sentence gives probabilities of blindness, conditional on eye color and being deaf:\nP(LjB2;D) =P(LjB0;D) = 0:20, andP(LjB1;D) = 0:40. The third sentence gives the probability that\na white cat is blind, given that it is not deaf: P(LjDC) = 0:10.\nPart i. asks for the prevalence of blindness among deaf, white cats: P(LjD). As in part a), the\nevent of blindness given deafness can be partitioned by eye color:\nP(LjD) =P(LandB0jD) +P(LandB1jD) +P(LandjD):\n120 CHAPTER 2. PROBABILITY\nEXAMPLE 2.51\nExpand the previous expression using the general multiplication rule, P(AandB) =P(AjB)P(B).\nThe general multiplication rule may seem di ﬃcult to apply when conditioning is present, but the\nprinciple remains the same. Think of the conditioning as a way to restrict the sample space; in\nthis context, conditioning on deafness implies that for this part of the problem, all the cats being\nconsidered are deaf (and white).\nFor instance, consider the ﬁrst term, P(LandB0jD), the probability of being blind and having\ntwo non-blue eyes, given deafness. How could this be rewritten if the probability were simply\nP(LandB0)?\nP(LandB0) =P(LjB0)P(B0)\nNow, recall that for this part of the problem, the sample space is restricted to deaf (and white) cats.\nThus, all of the terms in the expansion should include conditioning on deafness:\nP(LandB0jD) =P(LjD;B 0)P(B0jD):\nThus,\nP(LjD) =P(LjD;B 0)P(B0jD) +P(LjD;B 1)P(B1jD) +P(LjD;B 2)P(B2jD):\nAlthoughP(LjD;B 0),P(LjD;B 1), andP(LjD;B 2) are given from the problem statement, P(B0jD,\nP(B1jD), andP(B2jD) are not. However, note that the probability that a white cat has two blue eyes\ngiven that it is deaf, P(B2jD), was calculated in part b).\nGUIDED PRACTICE 2.52\nCalculateP(B0jD) andP(B1jD).37\nThere is now su ﬃcient information to calculate P(LjD):\nP(LjD) =P(LandB0jD) +P(LandB1jD) +P(LandB2jD)\n=P(LjD;B 0)P(B0jD) +P(LjD;B 1)P(B1jD) +P(LjD;B 2)P(B2jD)\n=(0:20)(0:371) + (0:40)(0:391) + (0:20)(0:238)\n=0:278:\nThe prevalence of blindness among deaf, white cats is 0.278.\nPart ii. asks for the prevalence of blindness among white cats, P(L). Again, partitioning is an\neﬀective strategy. Instead of partitioning by eye color, however, partition by deafness.\n37\nP(B0jD) =P(DandB0)\nP(D)=P(DjB0)P(B0)\nP(D)=(0:19)(0:60)\n0:307= 0:371:\nP(B1jD) =P(DandB1)\nP(D)=P(DjB1)P(B1)\nP(D)=(0:40)(0:30)\n0:307= 0:391:\n2.3. EXTENDED EXAMPLE 121\nEXAMPLE 2.53\nCalculate the prevalence of blindness among white cats, P(L).\nP(L) =P(LandD) +P(LandDC)\n=P(LjD)P(D) +P(LjDC)P(DC)\n=(0:278)(0:307) + (0:10)(1\u00000:307)\n=0:155:\nP(D) was calculated in part a), while P(LjD) was calculated in part c, i. The conditioning proba-\nbility of blindness given a white cat is not deaf is 0.10, as given in the question statement. By the\ndeﬁnition of the complement, P(DC) = 1\u0000P(D).\nThe prevalence of blindness among white cats is 0.155.\nPart iii. asks for the probability that a cat has two blue eyes, given that it is white and blind.\nThis probability can be expressed as P(B2jL). Recall that since all cats being discussed in the prob-\nlem are white, it is not necessary to condition on coat color.\nStart out with the deﬁnition of conditional probability:\nP(B2jL) =P(B2andL)\nP(L):\nThe key to calculating P(B2jL) relies on recognizing that the event a cat is blind and has two\nblue eyes can be partitioned by whether or not the cat is also deaf:\nP(B2jL) =P(B2andLandD) +P(B2andLandDC)\nP(L): (2.54)\nEXAMPLE 2.55\nDraw a tree diagram to organize the events involved in this problem. Identify the branches that\nrepresent the possible paths for a white cat to both have two blue eyes and be blind.\nWhen drawing a tree diagram, remember that each branch is conditioned on the previous branches.\nWhile there are various possible trees, the goal is to construct a tree for which as many of the\nbranches as possible have known probabilities.\nThe tree for this problem will have three branch points, corresponding to either deafness, blind-\nness, or eye color. The ﬁrst set of branches contain unconditional probabilities, the second set\ncontains conditional probabilities given one event, and the third set contains conditional probabil-\nities given two events.\nRecall that the probabilities P(LjD;B 0),P(LjD;B 1), andP(LjD;B 2) were provided in the problem\nstatement. These are the only probabilities conditioned on two events that have previously ap-\npeared in the problem, so blindness is the most convenient choice of third branch point.\nIt is not immediately obvious whether it will be more e ﬃcient to start with deafness or eye color,\nsince unconditional and conditional probabilities related to both have appeared in the problem.\nFigure 2.19 shows two trees, one starting with deafness and the other starting with eye color. The\ntwo possible paths for a white cat to both have two blue eyes and be blind are shown in green.\n122 CHAPTER 2. PROBABILITY\n(a)\n (b)\nFigure 2.19: In (a), the ﬁrst branch is based on deafness, while in (b), the ﬁrst\nbranch is based on eye color.\nEXAMPLE 2.56\nExpand Equation 2.54 according to the tree shown in Figure 2.19(a), and solve for P(B2jL).\nP(B2jL) =P(B2andLandD) +P(B2andLandDC)\nP(L)\n=P(LjB2;D)P(B2jD)P(D) +P(LjB2;DC)P(B2jDC)P(DC)\nP(L)\n=(0:20)(0:238)(0:307) + (0:10)P(B2jDC)P(DC)\n0:155:\nTwo of the probabilities have not been calculated previously: P(B2jDC) andP(DC). From the def-\ninition of the complement, P(DC) = 1\u0000P(D) = 0:693;P(D) was calculated in part a). To calculate\nP(B2jDC), apply the deﬁnition of conditional probability as in part b), where P(B2jD) was calcu-\nlated:\nP(B2jDC) =P(DCandB2)\nP(DC)=P(DCjB2)(P(B2)\nP(DC)=(1\u00000:73)(0:10)\n0:693= 0:0390:\nP(B2jL) =(0:20)(0:238)(0:307) + (0:10)(0:0390)(0:693)\n0:155\n= 0:112\nThe probability that a white cat has two blue eyes, given that it is blind, is 0.112.\n2.3. EXTENDED EXAMPLE 123\nGUIDED PRACTICE 2.57\nExpand Equation 2.54 according to the tree shown in Figure 2.19(b), and solve for P(B2jL).38\nA tree diagram is useful for visualizing the di ﬀerent possible ways that a certain set of out-\ncomes can occur. Although conditional probabilities can certainly be calculated without the help\nof tree diagrams, it is often easy to make errors with a strictly algebraic approach. Once a tree\nis constructed, it can be used to solve for several probabilities of interest. The following example\nshows how one of the previous trees can be applied to answer a di ﬀerent question than the one\nposed in part c), iii.\n38\nP(B2jL) =P(LjB2;D)P(DjB2)P(B2) +P(LjB2;DC)P(DCjB2)P(B2)\nP(L)=(0:20)(0:73)(0:10) + (0:10)(1\u00000:73)(0:10)\n0:155= 0:112:\n124 CHAPTER 2. PROBABILITY\nEXAMPLE 2.58\nWhat is the probability that a white cat has one blue eye and one non-blue eye, given that it is\nnot blind?\nCalculateP(B1jLC). Start with the deﬁnition of conditional probability, then expand.\nP(B1jLC) =P(B1andLC)\nP(LC)=P(B1andLCandD) +P(B1andLCandDC)\nP(LC):\nFigure 2.20 is a reproduction of the earlier tree diagram (Figure 2.19(b)), with yellow arrows show-\ning the two paths of interest.\nAs before, expand the numerator and ﬁll in the known values.\nP(B1jLC) =P(B1andLCandD) +P(B1andLCandDC)\nP(LC)\n=P(LCjD;B 1)P(DjB1)P(B1) +P(LCjDC;B1)P(DCjB1)P(B1)\nP(LC)\n=P(LCjD;B1)(0:40)(0:30) + P(LCjDC;B1)P(DCjB1)(0:30)\nP(LC):\nThe probabilities in bold are not known. Apply the deﬁnition of the complement; recall that\nthe rule for complements holds when an event and its complement are conditioned on the same\ninformation: P(AjB) = 1\u0000P(ACjB).\n–P(LC) = 1\u0000P(L) = 1\u00000:155 = 0:845\n–P(DCjB1) = 1\u0000P(DjB1) = 1\u00000:40 = 0:60\n–P(LCjD;B 1) = 1\u0000P(LjD;B 1) = 1\u00000:40 = 0:60\nThe deﬁnition of the complement can also be applied to calculate P(LCjDC;B1). The problem state-\nment originally speciﬁed that white cats that are not deaf have probability 0.10 of developing\nblindness regardless of eye color: P(LjDC) = 0:10. Thus,P(LCjDC;B1) =P(LCjDC). By the deﬁnition\nof the complement, P(LCjDC) = 1\u0000P(LjDC) = 1\u00000:10 = 0:90.\nP(B1jLC) =P(B1andLCandD) +P(B1andLCandDC)\nP(LC)\n=P(LCjD;B 1)P(DjB1)P(B1) +P(LCjDC;B1)P(DCjB1)P(B1)\nP(LC)\n=(0:60)(0:40)(0:30) + (0:90)(0:60)(0:30)\n0:845\n=0:277:\nThe probability that a white cat has one blue eye and one non-blue eye, given that it is not blind,\nis 0.277.\n2.3. EXTENDED EXAMPLE 125\nFigure 2.20: The two possible paths for a white cat to both have one blue eye (and\none non-blue eye) and to not be blind are shown in yellow.\n126 CHAPTER 2. PROBABILITY\n2.4 Notes\nProbability is a powerful framework for quantifying uncertainty and randomness. In partic-\nular, conditional probability represents a way to update the uncertainty associated with an event\ngiven that speciﬁc information has been observed. For example, the probability that a person has\na particular disease can be adjusted based on observed information, such as age, sex, or the results\nof a diagnostic test.\nAs discussed in the text, there are several possible approaches to solving conditional probabil-\nity problems, including the use of tree diagrams or contingency tables. It can also be intuitive to use\na simulation approach in computing software; refer to the labs for details about this method. Re-\ngardless of the speciﬁc approach that will be used for calculation, it is always advisable to start any\nproblem by understanding the problem context (i.e., the sample space, given information, proba-\nbilities of interest) and reading the problem carefully, in order to avoid mistakes when translating\nbetween words and probability notation. A common mistake is to confuse joint and conditional\nprobabilities.\nProbability distributions were brieﬂy introduced in Section 2.1.5. This topic will be discussed\nin greater detail in the next chapter.\nProbability forms the foundation for data analysis and statistical inference, since nearly every\nconclusion to a study should be accompanied by a measure of uncertainty. For example, the pub-\nlication reporting the results of the LEAP study discussed in Chapter 1 included the probability\nthat the observed results could have been due to chance variation. This aspect of probability will\nbe discussed in later chapters.\nThe four labs for Chapter 2 cover basic principles of probability, conditional probability, pos-\nitive predictive value of a diagnostic test (via Bayes’ Theorem), and the calculation of probabilities\nconditional on several events in the context of genetic inheritance. Probabilities can be calculated\nalgebraically, using formulas given in this and other texts, but can also be calculated with simple\nsimulations, since a probability represents a proportion of times an event happens when an exper-\niment is repeated many times. Computers are particularly good at keeping track of events during\nmany replications of an experiment. The labs for this chapter use both algebraic and simulation\nmethods, and are particularly useful for building programming skills with the Rlanguage.\nIn medicine, the positive predictive value of a diagnostic test may be one of the most important\napplications of probability theory. It is certainly the most common. The positive predictive value of\na test is the conditional probability of the presence of a disease or condition, given a positive test for\nthe condition, and is often used when counseling patients about their risk for being diagnosed with\na disease in the future. The lab on positive predictive value examines the conditional probability of\na trisomy 21 genetic mutation (Down syndrome) given that a test based on cell-free DNA suggests\nits presence.\n2.5. EXERCISES 127\n2.5 Exercises\n2.5.1 Deﬁning probability\n2.1 True or false. Determine if the statements below are true or false, and explain your reasoning.\n(a) Assume that a couple has an equal chance of having a boy or a girl. If a couple’s previous three children\nhave all been boys, then the chance that their next child is a boy is somewhat less than 50%.\n(b) Drawing a face card (jack, queen, or king) and drawing a red card from a full deck of playing cards are\nmutually exclusive events.\n(c) Drawing a face card and drawing an ace from a full deck of playing cards are mutually exclusive events.\n2.2 Dice rolls. If you roll a pair of fair dice, what is the probability of\n(a) getting a sum of 1?\n(b) getting a sum of 5?\n(c) getting a sum of 12?\n2.3 Colorblindness. Red-green colorblindness is a commonly inherited form of colorblindness; the gene\ninvolved is transmitted on the X chromosome in a recessive manner. If a male inherits an a ﬀected X chromo-\nsome, he is necessarily colorblind (genotype X\u0000Y). However, a female can only be colorblind if she inherits\ntwo defective copies (genotype X\u0000X\u0000); heterozygous females are not colorblind. Suppose that a couple con-\nsists of a genotype X+Ymale and a genotype X+X\u0000female.\n(a) What is the probability of the couple producing a colorblind male?\n(b) True or false: Among the couple’s o ﬀspring, colorblindness and female sex are mutually exclusive events.\n2.4 Diabetes and hypertension. Diabetes and hypertension are two of the most common diseases in West-\nern, industrialized nations. In the United States, approximately 9% of the population have diabetes, while\nabout 30% of adults have high blood pressure. The two diseases frequently occur together: an estimated 6%\nof the population have both diabetes and hypertension.\n(a) Are having diabetes and having hypertension disjoint?\n(b) Draw a Venn diagram summarizing the variables and their associated probabilities.\n(c) LetArepresent the event of having diabetes, and Bthe event of having hypertension. Calculate P(AorB).\n(d) What percent of Americans have neither hypertension nor diabetes?\n(e) Is the event of someone being hypertensive independent of the event that someone has diabetes?\n128 CHAPTER 2. PROBABILITY\n2.5 Educational attainment by gender. The table below shows the distribution of education level attained\nby US residents by gender based on data collected during the 2010 American Community Survey.39\nGender\nMale Female\nLess than 9th grade 0.07 0.13\n9th to 12th grade, no diploma 0.10 0.09\nHighest HS graduate (or equivalent) 0.30 0.20\neducation Some college, no degree 0.22 0.24\nattained Associate’s degree 0.06 0.08\nBachelor’s degree 0.16 0.17\nGraduate or professional degree 0.09 0.09\nTotal 1.00 1.00\n(a) What is the probability that a randomly chosen individual is a high school graduate? Assume that there\nis an equal proportion of males and females in the population.\n(b) Deﬁne Event Aas having a graduate or professional degree. Calculate the probability of the complement,\nAc.\n(c) What is the probability that a randomly chosen man has at least a Bachelor’s degree?\n(d) What is the probability that a randomly chosen woman has at least a Bachelor’s degree?\n(e) What is the probability that a man and a woman getting married both have at least a Bachelor’s degree?\nNote any assumptions made – are they reasonable?\n2.6 Poverty and language. The American Community Survey is an ongoing survey that provides data\nevery year to give communities the current information they need to plan investments and services. The 2010\nAmerican Community Survey estimates that 14.6% of Americans live below the poverty line, 20.7% speak a\nlanguage other than English (foreign language) at home, and 4.2% fall into both categories.40\n(a) Are living below the poverty line and speaking a foreign language at home disjoint?\n(b) Draw a Venn diagram summarizing the variables and their associated probabilities.\n(c) What percent of Americans live below the poverty line and only speak English at home?\n(d) What percent of Americans live below the poverty line or speak a foreign language at home?\n(e) What percent of Americans live above the poverty line and only speak English at home?\n(f) Is the event that someone lives below the poverty line independent of the event that the person speaks a\nforeign language at home?\n2.7 Urgent care visits. Urgent care centers are open beyond typical o ﬃce hours and provide a broader range\nof services than that of many primary care o ﬃces. A study conducted to collect information about urgent care\ncenters in the United States reported that in one week, 15.8% of centers saw 0-149 patients, 33.7% saw 150-\n299 patients, 28.8% saw 300-449 patients, and 21.7% saw 450 or more patients. Assume that the data can be\ntreated as a probability distribution of patient visits for any given week.\n(a) What is the probability that three random urgent care centers in a county all see between 300-449 patients\nin a week? Note any assumptions made. Are the assumptions reasonable?\n(b) What is the probability that ten random urgent care centers throughout a state all see 450 or more patients\nin a week? Note any assumptions made. Are the assumptions reasonable?\n(c) With the information provided, is it possible to compute the probability that one urgent care center sees\nbetween 150-299 patients in one week and 300-449 patients in the next week? Explain why or why not.\n39U.S. Census Bureau, 2010 American Community Survey 1-Year Estimates, Educational Attainment.\n40U.S. Census Bureau, 2010 American Community Survey 1-Year Estimates, Characteristics of People by Language Spo-\nken at Home.\n2.5. EXERCISES 129\n2.8 School absences. Data collected at elementary schools in DeKalb County, GA suggest that each year\nroughly 25% of students miss exactly one day of school, 15% miss 2 days, and 28% miss 3 or more days due\nto sickness.41\n(a) What is the probability that a student chosen at random doesn’t miss any days of school due to sickness\nthis year?\n(b) What is the probability that a student chosen at random misses no more than one day?\n(c) What is the probability that a student chosen at random misses at least one day?\n(d) If a parent has two kids at a DeKalb County elementary school, what is the probability that neither kid\nwill miss any school? Note any assumptions made and evaluate how reasonable they are.\n(e) If a parent has two kids at a DeKalb County elementary school, what is the probability that both kids will\nmiss some school, i.e. at least one day? Note any assumptions made and evaluate how reasonable they\nare.\n2.9 Disjoint vs. independent. In parts (a) and (b), identify whether the events are disjoint, independent, or\nneither (events cannot be both disjoint and independent).\n(a) You and a randomly selected student from your class both earn A’s in this course.\n(b) You and your class study partner both earn A’s in this course.\n(c) If two events can occur at the same time, must they be dependent?\n2.10 Health coverage, frequencies. The Behavioral Risk Factor Surveillance System (BRFSS) is an annual\ntelephone survey designed to identify risk factors in the adult population and report emerging health trends.\nThe following table summarizes two variables for the respondents: health status and health coverage, which\ndescribes whether each respondent had health insurance.42\nHealth Status\nExcellent Very good Good Fair Poor Total\nHealth No 459 727 854 385 99 2,524\nCoverage Yes 4,198 6,245 4,821 1,634 578 17,476\nTotal 4,657 6,972 5,675 2,019 677 20,000\n(a) If one individual is drawn at random, what is the probability that the respondent has excellent health and\ndoesn’t have health coverage?\n(b) If one individual is drawn at random, what is the probability that the respondent has excellent health or\ndoesn’t have health coverage?\n41S.S. Mizan et al. “Absence, Extended Absence, and Repeat Tardiness Related to Asthma Status among Elementary\nSchool Children”. In: Journal of Asthma 48.3 (2011), pp. 228–234.\n42Oﬃce of Surveillance, Epidemiology, and Laboratory Services Behavioral Risk Factor Surveillance System, BRFSS 2010\nSurvey Data.\n130 CHAPTER 2. PROBABILITY\n2.5.2 Conditional probability\n2.11 Global warming. A Pew Research poll asked 1,306 Americans “From what you’ve read and heard, is\nthere solid evidence that the average temperature on earth has been getting warmer over the past few decades,\nor not?\". The table below shows the distribution of responses by party and ideology, where the counts have\nbeen replaced with relative frequencies.43\nResponse\nEarth is Not Don’t Know\nwarming warming Refuse Total\nConservative Republican 0.11 0.20 0.02 0.33\nParty and Mod/Lib Republican 0.06 0.06 0.01 0.13\nIdeology Mod/Cons Democrat 0.25 0.07 0.02 0.34\nLiberal Democrat 0.18 0.01 0.01 0.20\nTotal 0.60 0.34 0.06 1.00\n(a) Are believing that the earth is warming and being a liberal Democrat mutually exclusive?\n(b) What is the probability that a randomly chosen respondent believes the earth is warming or is a liberal\nDemocrat?\n(c) What is the probability that a randomly chosen respondent believes the earth is warming given that he is\na liberal Democrat?\n(d) What is the probability that a randomly chosen respondent believes the earth is warming given that he is\na conservative Republican?\n(e) Does it appear that whether or not a respondent believes the earth is warming is independent of their\nparty and ideology? Explain your reasoning.\n(f) What is the probability that a randomly chosen respondent is a moderate/liberal Republican given that\nhe does not believe that the earth is warming?\n2.12 ABO blood groups. The ABO blood group system consists of four di ﬀerent blood groups, which\ndescribe whether an individual’s red blood cells carry the A antigen, B antigen, both, or neither. The ABO\ngene has three alleles: IA,IB, and i. The iallele is recessive to both IAandIB, while the IAandIBallels\nare codominant. Individuals homozygous for the iallele are known as blood group O, with neither A nor B\nantigens.\nAlleles inherited Blood type\nIAandIAA\nIAandIBAB\nIAandi A\nIBandIBB\nIBandi B\niandi O\nBlood group follows the rules of Mendelian single-gene inheritance – alleles are inherited independently\nfrom either parent, with probability 0.5.\n(a) Suppose that both members of a couple have Group AB blood. What is the probability that a child of this\ncouple will have Group A blood?\n(b) Suppose that one member of a couple is genotype IBiand the other is IAi. What is the probability that\ntheir ﬁrst child has Type O blood and the next two do not?\n(c) Suppose that one member of a couple is genotype IBiand the other is IAi. Given that one child has Type\nO blood and two do not, what is the probability of the ﬁrst child having Type O blood?\n43Pew Research Center, Majority of Republicans No Longer See Evidence of Global Warming, data collected on October\n27, 2010.\n2.5. EXERCISES 131\n2.13 Seat belts. Seat belt use is the most e ﬀective way to save lives and reduce injuries in motor vehicle\ncrashes. In a 2014 survey, respondents were asked, \"How often do you use seat belts when you drive or ride\nin a car?\". The following table shows the distribution of seat belt usage by sex.\nSeat Belt Usage\nAlways Nearly always Sometimes Seldom Never Total\nSexMale 146,018 19,492 7,614 3,145 4,719 180,988\nFemale 229,246 16,695 5,549 1,815 2,675 255,980\nTotal 375,264 36,187 13,163 4,960 7,394 436,968\n(a) Calculate the marginal probability that a randomly chosen individual always wears seatbelts.\n(b) What is the probability that a randomly chosen female always wears seatbelts?\n(c) What is the conditional probability of a randomly chosen individual always wearing seatbelts, given that\nthey are female?\n(d) What is the conditional probability of a randomly chosen individual always wearing seatbelts, given that\nthey are male?\n(e) Calculate the probability that an individual who never wears seatbelts is male.\n(f) Does gender seem independent of seat belt usage?\n2.14 Health coverage, relative frequencies. The Behavioral Risk Factor Surveillance System (BRFSS) is an\nannual telephone survey designed to identify risk factors in the adult population and report emerging health\ntrends. The following table displays the distribution of health status of respondents to this survey (excellent,\nvery good, good, fair, poor) conditional on whether or not they have health insurance.\nHealth Status\nExcellent Very good Good Fair Poor Total\nHealth No 0.0230 0.0364 0.0427 0.0192 0.0050 0.1262\nCoverage Yes 0.2099 0.3123 0.2410 0.0817 0.0289 0.8738\nTotal 0.2329 0.3486 0.2838 0.1009 0.0338 1.0000\n(a) Are being in excellent health and having health coverage mutually exclusive?\n(b) What is the probability that a randomly chosen individual has excellent health?\n(c) What is the probability that a randomly chosen individual has excellent health given that he has health\ncoverage?\n(d) What is the probability that a randomly chosen individual has excellent health given that he doesn’t have\nhealth coverage?\n(e) Do having excellent health and having health coverage appear to be independent?\n2.15 HIV in Swaziland. Swaziland has the highest HIV prevalence in the world: 25.9% of this country’s\npopulation is infected with HIV.44The ELISA test is one of the ﬁrst and most accurate tests for HIV. For those\nwho carry HIV, the ELISA test is 99.7% accurate. For those who do not carry HIV, the test is 92.6% accurate.\nCalculate the PPV and NPV of the test.\n44Source: CIA Factbook, Country Comparison: HIV/AIDS - Adult Prevalence Rate.\n132 CHAPTER 2. PROBABILITY\n2.16 Assortative mating. Assortative mating is a nonrandom mating pattern where individuals with similar\ngenotypes and/or phenotypes mate with one another more frequently than what would be expected under a\nrandom mating pattern. Researchers studying this topic collected data on eye colors of 204 Scandinavian\nmen and their female partners. The table below summarizes the results. For simplicity, we only include\nheterosexual relationships in this exercise.45\nPartner (female)\nBlue Brown Green Total\nBlue 78 23 13 114\nSelf (male)Brown 19 23 12 54\nGreen 11 9 16 36\nTotal 108 55 41 204\n(a) What is the probability that a randomly chosen male respondent or his partner has blue eyes?\n(b) What is the probability that a randomly chosen male respondent with blue eyes has a partner with blue\neyes?\n(c) What is the probability that a randomly chosen male respondent with brown eyes has a partner with blue\neyes? What about the probability of a randomly chosen male respondent with green eyes having a partner\nwith blue eyes?\n(d) Does it appear that the eye colors of male respondents and their partners are independent? Explain your\nreasoning.\n2.17 It’s never lupus. Lupus is a medical phenomenon where antibodies that are supposed to attack for-\neign cells to prevent infections instead see plasma proteins as foreign bodies, leading to a high risk of blood\nclotting. It is believed that 2% of the population su ﬀer from this disease. The test is 98% accurate if a person\nactually has the disease. The test is 74% accurate if a person does not have the disease. There is a line from\nthe Fox television show House that is often used after a patient tests positive for lupus: “It’s never lupus.\" Do\nyou think there is truth to this statement? Use appropriate probabilities to support your answer.\n2.18 Predisposition for thrombosis. A genetic test is used to determine if people have a predisposition for\nthrombosis , which is the formation of a blood clot inside a blood vessel that obstructs the ﬂow of blood through\nthe circulatory system. It is believed that 3% of people actually have this predisposition. The genetic test is\n99% accurate if a person actually has the predisposition, meaning that the probability of a positive test result\nwhen a person actually has the predisposition is 0.99. The test is 98% accurate if a person does not have the\npredisposition.\n(a) What is the probability that a randomly selected person who tests positive for the predisposition by the\ntest actually has the predisposition?\n(b) What is the probability that a randomly selected person who tests negative for the predisposition by the\ntest actually does not have the predisposition?\n45B. Laeng et al. “Why do blue-eyed men prefer women with the same eye color?” In: Behavioral Ecology and Sociobiology\n61.3 (2007), pp. 371–384.\n2.5. EXERCISES 133\n2.19 Views on evolution. A 2013 analysis conducted by the Pew Research Center found that 60% of survey\nrespondents agree with the statement \"humans and other living things have evolved over time\" while 33%\nsay that \"humans and other living things have existed in their present form since the beginning of time\"\n(7% responded \"don’t know\"). They also found that there are di ﬀerences among partisan groups in beliefs\nabout evolution. While roughly two-thirds of Democrats (67%) and independents (65%) say that humans and\nother living things have evolved over time, 48% of Republicans reject the idea of evolution. Suppose that\n45% of respondents identiﬁed as Democrats, 40% identiﬁed as Republicans, and 15% identiﬁed as political\nindependents. The survey was conducted among a national sample of 1,983 adults.\n(a) Suppose that a person is randomly selected from the population and found to identify as a Democrat.\nWhat is the probability that this person does not agree with the idea of evolution?\n(b) Suppose that a political independent is randomly selected from the population. What is the probability\nthat this person does not agree with the idea of evolution?\n(c) Suppose that a person is randomly selected from the population and found to identify as a Republican.\nWhat is the probability that this person agrees with the idea of evolution?\n(d) Suppose that a person is randomly selected from the population and found to support the idea of evolu-\ntion. What is the probability that this person identiﬁes as a Republican?\n2.20 Cystic ﬁbrosis testing. The prevalence of cystic ﬁbrosis in the United States is approximately 1 in\n3,500 births. Various screening strategies for CF exist. One strategy uses dried blood samples to check the\nlevels of immunoreactive trypsogen (IRT); IRT levels are commonly elevated in newborns with CF. The sensi-\ntivity of the IRT screen is 87% and the speciﬁcity is 99%.\n(a) In a hypothetical population of 100,000, how many individuals would be expected to test positive? Of\nthose who test positive, how many would be true positives? Calculate the PPV of IRT.\n(b) In order to account for lab error or physiological ﬂuctuations in IRT levels, infants who tested positive on\nthe initial IRT screen are asked to return for another IRT screen at a later time, usually two weeks after\nthe ﬁrst test. This is referred to as an IRT/IRT screening strategy. Calculate the PPV of IRT/IRT.\n2.21 Mumps. Mumps is a highly contagious viral infection that most often occurs in children, but can\naﬀect adults, particularly if they are living in shared living spaces such as college dormitories. It is most\nrecognizable by the swelling of salivary glands at the side of the face under the ears, but earlier symptoms\ninclude headaches, fever, and joint pain. Suppose a college student at a university presents to a physician with\nsymptoms of headaches, fever, and joint pain. Let A= {headaches, fever, and joint pain}, and suppose that the\npossible disease state of the patient can be partitioned into: B1= normal,B2= common cold, B3= mumps.\nFrom clinical experience, the physician estimates P(AjBi):P(AjB1) = 0:001,P(AjB2) = 0:70,P(AjB3) = 0:95.\nThe physician, aware that some students have contracted the mumps, then estimates that for students at this\nuniversity,P(B1) = 0:95,P(B2) = 0:025, andP(B3) = 0:025. Given the previous symptoms, which of the disease\nstates is most likely?\n2.22 Twins. About 30% of human twins are identical, and the rest are fraternal. Identical twins are necessar-\nily the same sex – half are males and the other half are females. One-quarter of fraternal twins are both male,\none-quarter both female, and one-half are mixes: one male, one female. You have just become a parent of\ntwins and are told they are both girls. Given this information, what is the probability that they are identical?\n134 CHAPTER 2. PROBABILITY\n2.23 IQ testing. A psychologist conducts a study on intelligence in which participants are asked to take an\nIQ test consisting of nquestions, each with mchoices.\n(a) One thing the psychologist must be careful about when analyzing the results is accounting for lucky\nguesses. Suppose that for a given question a particular participant either knows the answer or guesses.\nThe participant knows the correct answer with probability p, and does not know the answer (and therefore\nwill have to guess) with probability 1 \u0000p. The participant guesses completely randomly. What is the\nconditional probability that the participant knew the answer to a question, given that they answered it\ncorrectly?\n(b) About 1 in 1,100 people have IQs over 150. If a subject receives a score of greater than some speciﬁed\namount, they are considered by the psychologist to have an IQ over 150. But the psychologist’s test is not\nperfect. Although all individuals with IQ over 150 will deﬁnitely receive such a score, individuals with\nIQs less than 150 can also receive such scores about 0.1% of the time due to lucky guessing. Given that a\nsubject in the study is labeled as having an IQ over 150, what is the probability that they actually have an\nIQ below 150?\n2.24 Breast cancer and age. The strongest risk factor for breast cancer is age; as a woman gets older, her risk\nof developing breast cancer increases. The following table shows the average percentage of American women\nin each age group who develop breast cancer, according to statistics from the National Cancer Institute. For\nexample, approximately 3.56% of women in their 60’s get breast cancer.\nAge Group Prevalence\n30 - 40 0.0044\n40 - 50 0.0147\n50 - 60 0.0238\n60 - 70 0.0356\n70 - 80 0.0382\nA mammogram typically identiﬁes a breast cancer about 85% of the time, and is correct 95% of the time\nwhen a woman does not have breast cancer.\n(a) Calculate the PPV for each age group. Describe any trend(s) you see in the PPV values as prevalence\nchanges. Explain the reason for the trend(s) in language that someone who has not taken a statistics\ncourse would understand.\n(b) Suppose that two new mammogram imaging technologies have been developed which can improve the\nPPV associated with mammograms; one improves sensitivity to 99% (but speciﬁcity remains at 95%),\nwhile the other improves speciﬁcity to 99% (while sensitivity remains at 85%). Which technology o ﬀers a\nhigher increase in PPV? Explain why.\n2.5. EXERCISES 135\n2.25 Prostate-speciﬁc antigen. Prostate-speciﬁc antigen (PSA) is a protein produced by the cells of the\nprostate gland. Blood PSA level is often elevated in men with prostate cancer, but a number of benign (not\ncancerous) conditions can also cause a man’s PSA level to rise. The PSA test for prostate cancer is a laboratory\ntest that measures PSA levels from a blood sample. The test measures the amount of PSA in ng/ml (nanograms\nper milliliter of blood).\nThe sensitivity and speciﬁcity of the PSA test depend on the cuto ﬀvalue used to label a PSA level as\nabnormally high. In the last decade, 4.0 ng/ml has been considered the upper limit of normal, and values 4.1\nand higher were used to classify a PSA test as positive. Using this value, the sensitivity of the PSA test is 20%\nand the speciﬁcity is 94%.\nThe likelihood that a man has undetected prostate cancer depends on his age. This likelihood is also\ncalled the prevalence of undetected cancer in the male population. The following table shows the prevalence\nof undetected prostate cancer by age group.\nAge Group Prevalence PPV NPV\n<50 years 0.001\n50 - 60 years 0.020\n61 - 70 years 0.060\n71 - 80 years 0.100\n(a) Calculate the missing PPV and NPV values.\n(b) Describe any trends you see in the PPV and NPV values.\n(c) Explain the reason for the trends in part b), in language that someone who has not taken a statistics course\nwould understand.\n(d) The cuto ﬀfor a positive test is somewhat controversial. Explain, in your own words, how lowering the\ncutoﬀfor a positive test from 4.1 ng/ml to 2.5 ng/ml would a ﬀect sensitivity and speciﬁcity.\n136 CHAPTER 2. PROBABILITY\n2.5.3 Extended example\n2.26 Eye color. One of the earliest models for the genetics of eye color was developed in 1907, and proposed\na single-gene inheritance model, for which brown eye color is always dominant over blue eye color. Suppose\nthat in the population, 25% of individuals are homozygous dominant ( BB), 50% are heterozygous ( Bb), and\n25% are homozygous recessive ( bb).\n(a) Suppose that two parents have brown eyes. What is the probability that their ﬁrst child has blue eyes?\n(b) Does the probability change if it is now known that the paternal grandfather had blue eyes? Justify your\nanswer.\n(c) Given that their ﬁrst child has brown eyes, what is the probability that their second child has blue eyes?\nIgnore the condition given in part (b).\n2.27 Colorblindness. The most common form of colorblindness is a recessive, sex-linked hereditary con-\ndition caused by a defect on the X chromosome. Females are XX, while males are XY. Individuals inherit\none chromosome from each parent, with equal probability; for example, an individual has a 50% chance of\ninheriting their father’s X chromosome, and a 50% chance of inheriting their father’s Y chromosome. If a male\nhas an X chromosome with the defect, he is colorblind. However, a female with only one defective X chromo-\nsome will not be colorblind. Thus, colorblindness is more common in males than females; 7% of males are\ncolorblind but only 0.5% of females are colorblind.\n(a) Assume that the X chromosome with the wild-type allele is X+and the one with the disease allele is X\u0000.\nWhat is the expected frequency of each possible female genotype: X+X+,X+X\u0000, andX\u0000X\u0000? What is the\nexpected frequency of each possible male genotype: X+YandX\u0000Y?\n(b) Suppose that two parents are not colorblind. What is the probability that they have a colorblind child?\n2.28 Rapid feathering. Sex linkage refers to the inheritance pattern that results from a mutation occurring\non a gene located on a sex chromosome. A classic example of a sex-linked trait in humans is red-green color\nblindness; females can only be red-green colorblind if they have two copies of the mutation (one on each X\nchromosome), while a single copy of the mutation is su ﬃcient to confer colorblindness in males (since males\nonly have one X chromosome).\nIn birds, females are the heterogametic sex (with sex chromosomes ZW) and males are the homogametic\nsex (with sex chromosomes ZZ). A commonly known sex-linked trait in domestic chickens is the rapid feath-\nering trait, which is carried on the Z chromosome. Chickens with the rapid feathering trait grow feathers at\na faster rate; this di ﬀerence is especially pronounced within the ﬁrst few days from hatching. The wild-type\nalleleK\u0000is dominant to the mutant alelle K+; presence of the K\u0000allele produces slow feathering. Females\ncan be either genotype ZK+WorZK\u0000W. Males can be either heterozygous ( ZK+ZK\u0000), homozygous for slow\nfeathering (ZK\u0000ZK\u0000), or homozygous for rapid feathering ( ZK+ZK+).\nIn a population of chickens, 9% of males are rapid feathering and 16% of females are rapid feathering.\nSuppose that slow feathering chickens are mated. What is the probability that out of their 12 o ﬀspring, at\nleast two are rapid feathering?\n2.5. EXERCISES 137\n2.29 Genetics of Australian cattle dogs. Australian cattle dogs are known to have a high prevalence of\ncongenital deafness. Deafness in both ears is referred to as bilateral deafness, while deafness in one ear is\nreferred to as unilateral deafness.\nDeafness in dogs is associated with the white spotting gene Sthat controls the expression of coat and\neye pigmentation. The dominant allele Sproduces solid color, while the three recessive alleles contribute to\nincreasing amounts of white in coat pigmentation: Irish spotting ( si), piebald (sp), and extreme white piebald\n(sw). Thespandswalleles are responsible for the distinctive Australian cattle dog coat pattern of white hair\nevenly speckled throughout either a predominantly red or black coat. The dogs are born with white coats,\nand the speckled pattern develops as they age.\nWhile all Australian cattle dogs have some combination of the spandswalleles, the gene displays incom-\nplete penetrance such that individuals show some variation in phenotype despite having the same genotype.\nIndividuals with low penetrance of the alleles tend to have additional patterns on their coat, such as a dark\n\"mask\" around one or both eyes (in other words, a unilateral mask or a bilateral mask). High penetrance of\nthe piebald alleles is associated with deafness.\nSuppose that 40% of Australian cattle dogs have black coats; these individuals are commonly referred to\nas \"Blue Heelers\" as opposed to \"Red Heelers\". Among Blue Heelers, 35% of individuals have bilateral masks\nand 25% have unilateral masks. About 50% of Red Heelers exhibit no eye masking and 10% have bilateral\nmasks.\nLetMrepresent the event that an Australian cattle dog has a facial mask, where M2represents a bilateral\nmask,M1represents a unilateral mask, and M0indicates lack of a mask.\n(a) Calculate the probability an Australian cattle dog has a facial mask and a black coat.\n(b) Calculate the prevalence of bilateral masks in Australian cattle dogs.\n(c) Among Australian cattle dogs with bilateral facial masks, what is the probability of being a Red Heeler?\n(d) Unilateral deafness occurs in Red Heelers with probability 0.15, in both dogs that either lack facial mask-\ning or exhibit a unilateral mask; for both unmasked and unilaterally masked Red Heelers, 60% of dogs\nare not deaf. The overall prevalence of bilaterally masked Australian cattle dogs with bilateral deafness\nand red coats is 1.2% and the overall prevalence of bilaterally masked Australian cattle dogs with unilat-\neral deafness and red coats is 4.5%; these prevalences are the same for Australian cattle dogs with black\ncoats. Among Blue Heelers with either no facial masking or a unilateral mask, the probability of unilateral\ndeafness is 0.05 and the probability of bilateral deafness is 0.01.\nLetDrepresent the event that an Australian cattle dog is deaf (i.e., deaf in at least one ear), where D2\nrepresents bilateral deafness and D1represents unilateral deafness.\ni. What is the probability that an Australian cattle dog has a bilateral mask, no hearing deﬁcits, and a\nred coat?\nii. Calculate the proportion of bilaterally masked Blue Heelers without hearing deﬁcits.\niii. Compare the prevalence of deafness between Red Heelers and Blue Heelers.\niv. If a dog is known to have no hearing deﬁcits, what is the probability it is a Blue Heeler?\n138\nChapter 3\nDistributions of\nrandom variables\n3.1 Random variables\n3.2 Binomial distribution\n3.3 Normal distribution\n3.4 Poisson distribution\n3.5 Distributions related to Bernoulli trials\n3.6 Distributions for pairs of random variables\n3.7 Notes\n3.8 Exercises\n139\nWhen planning clinical research studies, investigators try to anticipate the results\nthey might see under certain hypotheses. The treatments for some forms of can-\ncer, such as advanced lung cancer, are only e ﬀective in a small percentage of pa-\ntients: typically 20% or less. Suppose that a study testing a new treatment will be\nconducted on 20 participants, where the working assumption is that 20% of the\npatients will respond to the treatment. How might the possible outcomes of the\nstudy be represented, along with their probabilities? It is possible to express vari-\nous outcomes using the probability notation in the previous chapter, e.g. if Awere\nthe event that one patient responds to treatment, but this would quickly become\nunwieldy.\nInstead, the anticipated outcome in the study can be represented as a random\nvariable , which numerically summarizes the possible outcomes of a random ex-\nperiment. For example, let Xrepresent the number of patients who respond to\ntreatment; a numerical value xcan be assigned to each possible outcome, and\nthe probabilities of 1 ;2;:::;x patients having a good response can be expressed as\nP(X= 1);P(X= 2);:::;P (X=x). The distribution of a random variable speciﬁes the\nprobability of each possible outcome associated with the random variable.\nThis chapter will begin by outlining general properties of random variables and\ntheir distributions. The rest of the chapter discusses speciﬁc named distributions\nthat are commonly used throughout probability and statistics.\nFor labs, slides, and other resources, please visit\nwww.openintro.org/book/biostat\n140 CHAPTER 3. DISTRIBUTIONS OF RANDOM V ARIABLES\n3.1 Random variables\n3.1.1 Distributions of random variables\nFormally, a random variable assigns numerical values to the outcome of a random phenomenon,\nand is usually written with a capital letter such as X,Y, orZ.\nIf a coin is tossed three times, the outcome is the sequence of observed heads and tails. One\nsuch outcome might be TTH: tails on the ﬁrst two tosses, heads on the third. If the random variable\nXis the number of heads for the three tosses, X= 1; ifYis the number of tails, then Y= 2. For\nthe sequence THT, only the order has changed, but the values of XandYremain the same. For the\nsequence HHH, however, X= 3 andY= 0. Even in this simple setting, is possible to deﬁne other\nrandom variables; for example, if Zis the toss when the ﬁrst H occurs, then Z= 3 for the ﬁrst set\nof tosses (TTH) and 1 for the third set (HHH).\nFigure 3.1: Possible outcomes for number of heads in three tosses of a coin.\nIf probabilities can be assigned to the outcomes in a random phenomenon or study, then\nthose can be used to assign probabilities to values of a random variable. Using independence,\nP(HHH) = (1=2)3= 1=8. SinceXin the above example can only be three if the three tosses are all\nheads,P(X= 3) = 1=8. The distribution of a random variable is the collection of probabilities for all\nof the variable’s unique values. Figure 3.1 shows the eight possible outcomes when a coin is cossed\nthree times: TTT, HTT, THT, TTH, HHT, HTH, THH, HHH. For the ﬁrst set of tosses, X= 0; for the\nnext three,X= 1, thenX= 2 for the following three tosses and X= 3 for the last set (HHH).\nUsing independence again, each of the 8 outcomes have probability 1/8, so P(X= 0) =P(X=\n3) = 1=8 andP(X= 1) =P(X= 2) = 3=8. Figure 3.2 shows the probability distribution for X.\nProbability distributions for random variables follow the rules for probability; for instance, the\nsum of the probabilities must be 1.00. The possible outcomes of Xare labeled with a corresponding\nlower case letter xand subscripts. The values of X are x1= 0,x2= 1,x3= 2, andx4= 3; these occur\nwith probabilities 1 =8, 3=8, 3=8 and 1=8.\ni 1 2 3 4 Total\nxi 0 1 2 3 –\nP(X=xi) 1/8 3/8 3/8 1/8 8/8 = 1.00\nFigure 3.2: Tabular form for the distribution of the number of heads in three coin\ntosses.\n3.1. RANDOM V ARIABLES 141\n0 1 2 3Probabilities\n0.00.10.20.30.4\nFigure 3.3: Bar plot of the distribution of the number of heads in three coin tosses.\nBar graphs can be used to show the distribution of a random variable. Figure 3.3 is a bar\ngraph of the distribution of Xin the coin tossing example. When bar graphs are used to show the\ndistribution of a dataset, the heights of the bars show the frequency of observations; in contrast,\nbar heights for a probability distribution show the probabilities of possible values of a random\nvariable.\nXis an example of a discrete random variable since it takes on a ﬁnite number of values.1A\ncontinuous random variable can take on any real value in an interval.\nIn the hypothetical clinical study described at the beginning of this section, how unlikely\nwould it be for 12 or more patients to respond to the treatment, given that only 20% of patients\nare expected to respond? Suppose Xis a random variable that will denote the possible number\nof responding patients, out of a total of 20. Xwill have the same probability distribution as the\nnumber of heads in a 20 tosses of a weighted coin, where the probability of landing heads is 0.20.\nThe graph of the probability distribution for Xin Figure 3.4 can be used to approximate this prob-\nability. The event of 12 or more consists of nine values (12, 13, . . . , 20); the graph shows that the\nprobabilities for each value is extremely small, so the chance of 12 or more responses must be less\nthan 0.01.2\n0 5 10 15 200.000.050.100.150.20\nXProbability\nFigure 3.4: Bar plot of the distribution of the number of responses in a study with\n20 participants and response probability 0.20\n1Some discrete random variables have an inﬁnite number of possible values, such as all the non-negative integers.\n2Formulas in Section 3.2 can be used to show that the exact probability is slightly larger than 0.0001.\n142 CHAPTER 3. DISTRIBUTIONS OF RANDOM V ARIABLES\n3.1.2 Expectation\nJust like distributions of data, distributions of random variables also have means, variances,\nstandard deviations, medians, etc.; these characteristics are computed a bit di ﬀerently for random\nvariables. The mean of a random variable is called its expected value and written E(X). To calcu-\nlate the mean of a random variable, multiply each possible value by its corresponding probability\nand add these products.\nEXPECTED VALUE OF A DISCRETE RANDOM VARIABLE\nIfXtakes on outcomes x1, ...,xkwith probabilities P(X=x1), ...,P(X=xk), the expected value\nofXis the sum of each outcome multiplied by its corresponding probability:\nE(X) =x1P(X=x1) +\u0001\u0001\u0001+xkP(X=xk)\n=kX\ni=1xiP(X=xi): (3.1)\nThe Greek letter \u0016may be used in place of the notation E(X).\nEXAMPLE 3.2\nCalculate the expected value of X, whereXrepresents the number of heads in three tosses of a fair\ncoin.\nXcan take on values 0, 1, 2, and 3. The probability of each xkis given in Figure 3.2.\nE(X) =x1P(X=x1) +\u0001\u0001\u0001+xkP(X=xk)\n= (0)(P(X= 0)) + (1)(P(X= 1)) + (2)(P(X= 2)) + (3)(P(X= 3))\n= (0)(1=8) + (1)(3=8) + (2)(3=8) + (3)(1=8) = 12=8\n= 1:5:\nThe expected value of Xis 1.5.\nThe expected value for a random variable represents the average outcome. For example,\nE(X) = 1:5 represents the average number of heads in three tosses of a coin, if the three tosses\nwere repeated many times.3It often happens with discrete random variables that the expected\nvalue is not precisely one of the possible outcomes of the variable.E(X)\nExpected Value\nofX\nGUIDED PRACTICE 3.3\nCalculate the expected value of Y, whereYrepresents the number of heads in three tosses of an\nunfair coin, where the probability of heads is 0.70.4\n3The expected value E(X) can also be expressed as \u0016, e.g.\u0016= 1:5\n4First, calculate the probability distribution. P(Y= 0) = (1\u00000:70)3= 0:027 andP(Y= 3) = (0:70)3= 0:343:Note that\nthere are three ways to obtain 1 head (HTT, THT, TTH), thus, P(Y= 1) = (3)(0 :70)(1\u00000:70)2= 0:189. By the same logic,\nP(Y= 2) = (3)(0:70)2(1\u00000:70) = 0:441. Thus,E(Y) = (0)(0:027)+(1)(0:189)+(2)(0:441)+(3)(0:343) = 2:1. The expected value\nofYis 2.1.\n3.1. RANDOM V ARIABLES 143\n3.1.3 Variability of random variables\nThe variability of a random variable can be described with variance and standard deviation.\nFor data, the variance is computed by squaring deviations from the mean ( xi\u0000\u0016) and then averaging\nover the number of values in the dataset (Section 1.4.2).\nIn the case of a random variable, the squared deviations from the mean of the random variable\nare used instead, and their sum is weighted by the corresponding probabilities. This weighted sum\nof squared deviations equals the variance; the standard deviation is the square root of the variance.\nVARIANCE OF A DISCRETE RANDOM VARIABLE\nIfXtakes on outcomes x1, ...,xkwith probabilities P(X=x1), . . . ,P(X=xk) and expected value\n\u0016=E(X), then the variance of X, denoted by Var( X) or\u001b2, is\nVar(X) = (x1\u0000\u0016)2P(X=x1) +\u0001\u0001\u0001+ (xk\u0000\u0016)2P(X=xk)\n=kX\ni=1(xi\u0000\u0016)2P(X=xi): (3.4)\nThe standard deviation of X, labeledSD(X) or\u001b, is the square root of the variance.Var(X)\nVariance\nofX\nThe variance of a random variable can be interpreted as the expectation of the terms ( xi\u0000\u0016)2;\ni.e.,\u001b2=E(X\u0000\u0016)2. While this compact form is not useful for direct computation, it can be helpful\nfor understanding the concept of variability in the context of a random variable; variance is simply\nthe average of the deviations from the mean.\nEXAMPLE 3.5\nCompute the variance and standard deviation of X, the number of heads in three tosses of a\nfair coin.\nIn the formula for the variance, k= 4 and\u0016X=E(X) = 1:5.\n\u001b2\nX= (x1\u0000\u0016X)2P(X=x1) +\u0001\u0001\u0001+ (x4\u0000\u0016)2P(X=x4)\n= (0\u00001:5)2(1=8) + (1\u00001:5)2(3=8) + (2\u00001:5)2(3=8) + (3\u00001:5)2(1=8)\n= 3=4:\nThe variance is 3 =4 = 0:75 and the standard deviation isp\n3=4 = 0:866.\nThe coin tossing scenario provides a simple illustration of the mean and variance of a ran-\ndom variable. For the rest of this section, a more realistic example will be discussed—calculating\nexpected health care costs.\nIn most typical health insurance plans in the United States, members of the plan pay annually\nin three categories: a monthly premium, a deductible amount that members pay each year before\nthe insurance covers service, and “out-of-pocket” costs which include co-payments for each physi-\ncian visit or prescription.5Picking a new health plan involves estimating costs for the next year\nbased on a person’s best guess at the type and number of services that will be needed.\n5The deductible also includes care and supplies that are not covered by insurance.\n144 CHAPTER 3. DISTRIBUTIONS OF RANDOM V ARIABLES\nIn 2015, Harvard University o ﬀered several alternative plans to its employees. In the Health\nMaintenance Organization (HMO) plan for employees earning less than $70,000 per year, the\nmonthly premium was $79, and the co-payment for each o ﬃce visit or physical therapy session\nwas $20. After a new employee examined her health records for the last 10 years, she noticed that\nin three of the 10 years, she visited the o ﬃce of her primary care physician only once, for one an-\nnual physical. In four of the 10 years, she visited her physician three times: once for a physical,\nand twice for cases of the ﬂu. In two of the years, she had four visits. In one of the 10 years, she\nexperienced a knee injury that required 3 o ﬃce visits and 5 physical therapy sessions.\nEXAMPLE 3.6\nIgnoring the cost of prescription drugs, over-the-counter medications, and the annual deductible\namount, calculate the expectation and the standard deviation of the expected annual health care\ncost for this employee.\nLet the random variable Xdenote annual health care costs, where xirepresents the costs in a year\nforinumber of visits. If the last ten years are an accurate picture of annual costs for this employee,\nXwill have four possible values.\nThe total cost of the monthly premiums in a single year is 12 \u0002$79 = $948. The cost of each visit is\n$20, so the total visit cost for a year is $20 times the number of visits.\nFor example, the ﬁrst column in the table contains information about the years in which the em-\nployee had one o ﬃce visit. Adding the $948 for the annual premium and $20 for one visit results\ninx1= $968;P(X=xi) = 3=10 = 0:30.\ni 1 2 3 4 Sum\nNumber of visits 1 3 4 8\nxi 968 1008 1028 1108\nP(X=xi) 0.30 0.40 0.20 0.10 1.00\nxiP(X=xi) 290.40 403.20 205.60 110.80 1010.00\nThe expected cost of health care for a year,P\nixiP(X=xi), is\u0016= $1010:00.\ni 1 2 3 4 Sum\nNumber of visits 1 3 4 8\nxi 968 1008 1028 1108\nP(X=xi) 0.30 0.40 0.20 0.10 1.00\n(xi)P(X=xi) 290.40 403.20 205.60 110.80 1010.00\nxi\u0000\u0016 -42.00 -2.00 18.00 98.00\n(xi\u0000\u0016)21764.00 4.00 324.00 9604\n(xi\u0000\u0016)2P(X=xi) 529.20 1.60 64.80 960.40 1556.00\nThe variance of X,P\ni(xi\u0000\u0016)2P(X=xi), is\u001b2= 1556:00, and the standard deviation is \u001b= $39:45.6\n6Note that the standard deviation always has the same units as the original measurements.\n3.1. RANDOM V ARIABLES 145\n3.1.4 Linear combinations of random variables\nSums of random variables arise naturally in many problems. In the health insurance example,\nthe amount spent by the employee during her next ﬁve years of employment can be represented as\nX1+X2+X3+X4+X5, whereX1is the cost of the ﬁrst year, X2the second year, etc. If the employee’s\ndomestic partner has health insurance with another employer, the total annual cost to the couple\nwould be the sum of the costs for the employee ( X) and for her partner ( Y), orX+Y. In each\nof these examples, it is intuitively clear that the average cost would be the sum of the average of\neach term.\nSums of random variables represent a special case of linear combinations of variables.\nLINEAR COMBINATIONS OF RANDOM VARIABLES AND THEIR EXPECTED VALUES\nIfXandYare random variables, then a linear combination of the random variables is given\nby\naX+bY;\nwhereaandbare constants. The mean of a linear combination of random variables is\nE(aX+bY) =aE(X) +bE(Y) =a\u0016X+b\u0016Y:\nThe formula easily generalizes to a sum of any number of random variables. For example, the\naverage health care cost for 5 years, given that the cost for services remains the same, is\nE(X1+X2+X3+X4+X5) =E(5X1) = 5E(X1) = (5)(1010) = $5 ;050:\nThe formula implies that for a random variable Z,E(a+Z) =a+E(Z). This could have been\nused when calculating the average health costs for the employee by deﬁning aas the ﬁxed cost of\nthe premium ( a= $948) and Zas the cost of the physician visits. Thus, the total annual cost for a\nyear could be calculated as: E(a+Z) =a+E(Z) = $948 +E(Z) = $948 +:30(1\u0002$20) +:40(3\u0002$20) +\n:20(4\u0002$20) + 0:10(8\u0002$20) = $1;010:00.\nGUIDED PRACTICE 3.7\nSuppose the employee will begin a domestic partnership in the next year. Although she and her\ncompanion will begin living together and sharing expenses, they will each keep their existing\nhealth insurance plans; both, in fact, have the same plan from the same employer. In the last\nﬁve years, her partner visited a physician only once in four of the ten years, and twice in the other\nsix years. Calculate the expected total cost of health insurance to the couple in the next year.7\nCalculating the variance and standard deviation of a linear combination of random variables\nrequires more care. The formula given here requires that the random variables in the linear com-\nbination be independent, such that an observation on one of the variables provides no information\nabout the value of the other variable.\n7LetXrepresent the costs for the employee and Yrepresent the costs for her partner. E(X) = $1;010:00, as previously\ncalculated.E(Y) = 948+0:4(1\u0002$20)+0:6(2\u0002$20) = $980 :00. Thus,E(X+Y) =E(X)+E(Y) = $1;010:00+$980:00 = $1;990:00.\n146 CHAPTER 3. DISTRIBUTIONS OF RANDOM V ARIABLES\nVARIABILITY OF LINEAR COMBINATIONS OF RANDOM VARIABLES\nVar(aX+bY) =a2Var(X) +b2Var(Y):\nThis equation is valid only if the random variables are independent of each other.\nFor the transformation a+bZ, the variance is b2Var(Z), since a constant ahas variance 0.\nWhenb= 1, variance of a+Zis Var(Z)—adding a constant to a random variable has no e ﬀect on\nthe variability of the random variable.\nEXAMPLE 3.8\nCalculate the variance and standard deviation for the combined cost of next year’s health care for\nthe two partners, assuming that the costs for each person are independent.\nLetXrepresent the sum of costs for the employee and Ythe sum of costs for her partner.\nFirst, calculate the variance of health care costs for the partner. The partner’s costs are the sum of\nthe annual ﬁxed cost and the variable annual costs, so the variance will simply be the variance of the\nvariable costs. If Zrepresents the component of the variable costs, E(Z) = 0:4(1\u0002$20)+0:6(2\u0002$20) =\n$8 + $24 = $32. Thus, the variance of Zequals\nVar(Z) = 0:4(20\u000032)2+ 0:6(40\u000032)2= 96:\nUnder the assumption of independence, Var( X+Y) = Var(X) + Var(Y) = 1556 + 96 = 1652, and the\nstandard deviation isp\n1652 = $40:64.\nThe example of health insurance costs has been simpliﬁed to make the calculations clearer. It\nignores the fact that many plans have a deductible amount, and that plan members pay for services\nat diﬀerent rates before and after the deductible has been reached. Often, insured individuals no\nlonger need to pay for services at all once a maximum amount has been reached in a year. The\nexample also assumes that the proportions of number of physician visits per year, estimated from\nthe last 10 years, can be treated as probabilities measured without error. Had a di ﬀerent timespan\nbeen chosen, the proportions might well have been di ﬀerent.\nIt also relies on the assumption that health care costs for the two partners are independent.\nTwo individuals living together may pass on infectious diseases like the ﬂu, or may participate to-\ngether in activities that lead to similar injuries, such as skiing or long distance running. Section 3.6\nshows how to adjust a variance calculation when independence is unrealistic.\n3.2. BINOMIAL DISTRIBUTION 147\n3.2 Binomial distribution\nThe hypothetical clinical study and coin tossing example discussed earlier in this chapter are\nboth examples of experiments that can be modeled with a binomial distribution. The binomial\ndistribution is a more general case of another named distribution, the Bernoulli distribution.\n3.2.1 Bernoulli distribution\nPsychologist Stanley Milgram began a series of experiments in 1963 to study the e ﬀect of\nauthority on obedience. In a typical experiment, a participant would be ordered by an authority\nﬁgure to give a series of increasingly severe shocks to a stranger. Milgram found that only about\n35% of people would resist the authority and stop giving shocks before the maximum voltage was\nreached. Over the years, additional research suggested this number is approximately consistent\nacross communities and time.8\nEach person in Milgram’s experiment can be thought of as a trial . Suppose that a trial is\nlabeled a success if the person refuses to administer the worst shock. If the person does administer\nthe worst shock, the trial is a failure . The probability of a success can be written as p= 0:35. The\nprobability of a failure is sometimes denoted with q= 1\u0000p.\nWhen an individual trial only has two possible outcomes, it is called a Bernoulli random\nvariable . It is arbitrary as to which outcome is labeled success.\nBernoulli random variables are often denoted as 1for a success and 0for a failure. Suppose\nthat ten trials are observed, of which 6 are successes and 4 are failures:\n0 1 1 1 1 0 1 1 0 0 .\nThe sample proportion ,ˆp, is the sample mean of these observations:\nˆp=# of successes\n# of trials=0 + 1 + 1 + 1 + 1 + 0 + 1 + 1 + 0 + 0\n10= 0:6:\nSince 0and 1are numerical outcomes, the mean and standard deviation of a Bernoulli random\nvariable can be deﬁned. If pis the true probability of a success, then the mean of a Bernoulli\nrandom variable Xis given by\n\u0016=E[X] =P(X= 0)\u00020 +P(X= 1)\u00021\n= (1\u0000p)\u00020 +p\u00021 = 0 +p=p:\nSimilarly, the variance of Xcan be computed:\n\u001b2=P(X= 0)(0\u0000p)2+P(X= 1)(1\u0000p)2\n= (1\u0000p)p2+p(1\u0000p)2=p(1\u0000p):\nThe standard deviation is \u001b=p\np(1\u0000p).\n8Find further information on Milgram’s experiment at\nwww.cnr.berkeley.edu/ucce50/ag-labor/7article/article35.htm.\n148 CHAPTER 3. DISTRIBUTIONS OF RANDOM V ARIABLES\nBERNOULLI RANDOM VARIABLE\nIfXis a random variable that takes value 1 with probability of success pand 0 with probability\n1\u0000p, thenXis a Bernoulli random variable with mean pand standard deviationp\np(1\u0000p).\nSupposeXrepresents the outcome of a single toss of a fair coin, where heads is labeled success.\nXis a Bernoulli random variable with probability of success p= 0:50; this can be expressed as\nX\u0018Bern(p), or speciﬁcally, X\u0018Bern(0:50). It is essential to specify the probability of success\nwhen characterizing a Bernoulli random variable. For example, although the outcome of a single\ntoss of an unfair coin can also be represented by a Bernoulli, it will have a di ﬀerent probability\ndistribution since pdoes not equal 0.50 for an unfair coin.\nThe success probability pis the parameter of the distribution, and identiﬁes a speciﬁc Bernoulli\ndistribution out of the entire family of Bernoulli distributions where pcan be any value between 0\nand 1 (inclusive).Bern(p)\nBernoulli dist.\nwithpprob. of\nsuccess\nEXAMPLE 3.9\nSuppose that four individuals are randomly selected to participate in Milgram’s experiment. What\nis the chance that there will be exactly one successful trial, assuming independence between trials?\nSuppose that the probability of success remains 0.35.\nConsider a scenario in which there is one success (i.e., one person refuses to give the strongest\nshock). Label the individuals as A,B,C, andD:\nP(A=refuse; B=shock; C=shock; D=shock )\n=P(A=refuse )P(B=shock )P(C=shock )P(D=shock )\n= (0:35)(0:65)(0:65)(0:65) = (0:35)1(0:65)3= 0:096:\nHowever, there are three other possible scenarios: either B,C, orDcould have been the one to\nrefuse. In each of these cases, the probability is also (0 :35)1(0:65)3. These four scenarios exhaust all\nthe possible ways that exactly one of these four people could refuse to administer the most severe\nshock, so the total probability of one success is (4)(0 :35)1(0:65)3= 0:38.\n3.2. BINOMIAL DISTRIBUTION 149\n3.2.2 The binomial distribution\nThe Bernoulli distribution is unrealistic in all but the simplest of settings. However, it is a\nuseful building block for other distributions. The binomial distribution describes the probability\nof having exactly ksuccesses in nindependent Bernoulli trials with probability of a success p. In\nExample 3.9, the goal was to calculate the probability of 1 success out of 4 trials, with probability\nof success 0.35 ( n= 4,k= 1,p= 0:35).\nLike the Bernoulli distribution, the binomial is a discrete distribution, and can take on only a\nﬁnite number of values. A binomial variable has values 0, 1, 2, . . . , n.\nA general formula for the binomial distribution can be developed from re-examining Exam-\nple 3.9. There were four individuals who could have been the one to refuse, and each of these four\nscenarios had the same probability. Thus, the ﬁnal probability can be written as:\n[# of scenarios]\u0002P(single scenario :) (3.10)\nThe ﬁrst component of this equation is the number of ways to arrange the k= 1 successes among\nthen= 4 trials. The second component is the probability of any of the four (equally probable)\nscenarios.\nConsiderP(single scenario) under the general case of ksuccesses and n\u0000kfailures in the n\ntrials. In any such scenario, the Multiplication Rule for independent events can be applied:\npk(1\u0000p)n\u0000k:\nSecondly, there is a general formula for the number of ways to choose ksuccesses in ntrials,\ni.e. arrange ksuccesses and n\u0000kfailures:\n n\nk!\n=n!\nk!(n\u0000k)!:\nThe quantity\u0000n\nk\u0001is read n choose k .9The exclamation point notation (e.g. k!) denotes a factorial\nexpression.10\nUsing the formula, the number of ways to choose k= 1 successes in n= 4 trials can be com-\nputed as:\n 4\n1!\n=4!\n1!(4\u00001)!=4!\n1!3!=4\u00023\u00022\u00021\n(1)(3\u00022\u00021)= 4:\nSubstituting nchoosekfor the number of scenarios and pk(1\u0000p)n\u0000kfor the single scenario\nprobability in Equation (3.10) yields the general binomial formula.\n9Other notation for nchoosekincludesnCk,Ckn, andC(n;k).\n100! = 1 , 1! = 1, 2! = 2 \u00021 = 2,:::,n! =n\u0002(n\u00001)\u0002:::2\u00021.\n150 CHAPTER 3. DISTRIBUTIONS OF RANDOM V ARIABLES\nBINOMIAL DISTRIBUTION\nSuppose the probability of a single trial being a success is p. The probability of observing\nexactlyksuccesses in nindependent trials is given by\nP(X=k) = n\nk!\npk(1\u0000p)n\u0000k=n!\nk!(n\u0000k)!pk(1\u0000p)n\u0000k: (3.11)\nAdditionally, the mean, variance, and standard deviation of the number of observed successes\nare, respectively\n\u0016=np \u001b2=np(1\u0000p) \u001b=p\nnp(1\u0000p): (3.12)\nA binomial random variable Xcan be expressed as X\u0018Bin(n;p).Bin(n;p)\nBinomial dist.\nwithntrials\n&pprob. of\nsuccess\nIS IT BINOMIAL? FOUR CONDITIONS TO CHECK.\n(1) The trials are independent.\n(2) The number of trials, n, is ﬁxed.\n(3) Each trial outcome can be classiﬁed as a success orfailure .\n(4) The probability of a success, p, is the same for each trial.\nEXAMPLE 3.13\nWhat is the probability that 3 of 8 randomly selected participants will refuse to administer the\nworst shock?\nFirst, check the conditions for applying the binomial model. The number of trials is ﬁxed ( n= 8)\nand each trial outcome can be classiﬁed as either success or failure. The sample is random, so the\ntrials are independent, and the probability of success is the same for each trial.\nFor the outcome of interest, k= 3 successes occur in n= 8 trials, and the probability of a success is\np= 0:35. Thus, the probability that 3 of 8 will refuse is given by\nP(X= 3) = 8\n3!\n(0:35)3(1\u00000:35)8\u00003=8!\n3!(8\u00003)!(0:35)3(1\u00000:35)8\u00003\n= (56)(0:35)3(0:65)5\n= 0:28:\n3.2. BINOMIAL DISTRIBUTION 151\nEXAMPLE 3.14\nWhat is the probability that at most 3 of 8 randomly selected participants will refuse to administer\nthe worst shock?\nThe event of at most 3 out of 8 successes can be thought of as the combined probability of 0, 1, 2,\nand 3 successes. Thus, the probability that at most 3 of 8 will refuse is given by:\nP(X\u00143) =P(X= 0) +P(X= 1) +P(X= 2) +P(X= 3)\n= 8\n0!\n(0:35)0(1\u00000:35)8\u00000+ 8\n1!\n(0:35)1(1\u00000:35)8\u00001\n+ 8\n2!\n(0:35)2(1\u00000:35)8\u00002+ 8\n3!\n(0:35)3(1\u00000:35)8\u00003\n= (1)(0:35)0(1\u00000:35)8+ (8)(0:35)1(1\u00000:35)7\n+ (28)(0:35)2(1\u00000:35)6+ (56)(0:35)3(1\u00000:35)5\n= 0:706:\nEXAMPLE 3.15\nIf 40 individuals were randomly selected to participate in the experiment, how many individuals\nwould be expected to refuse to administer the worst shock? What is the standard deviation of the\nnumber of people expected to refuse?\nBoth quantities can directly be computed from the formulas in Equation (3.12). The expected\nvalue (mean) is given by: \u0016=np= 40\u00020:35 = 14. The standard deviation is: \u001b=p\nnp(1\u0000p) =p\n40\u00020:35\u00020:65 = 3:02.\nGUIDED PRACTICE 3.16\nThe probability that a smoker will develop a severe lung condition in their lifetime is about 0.30.\nSuppose that 5 smokers are randomly selected from the population. What is the probability that\n(a) one will develop a severe lung condition? (b) that no more than one will develop a severe lung\ncondition? (c) that at least one will develop a severe lung condition?11\n11Letp= 0:30;X\u0018Bin(5;0:30). (a)P(X= 1) =\u00005\n1\u0001(0:30)1(1\u00000:30)5\u00001= 0:36 (b)P(X\u00141) =P(X= 0) +P(X= 1) =\u00005\n0\u0001(0:30)0(1\u00000:30)5\u00000+ 0:36 = 0:53 (c)P(X\u00151) = 1\u0000P(X= 0) = 1\u00000:36 = 0:83\n152 CHAPTER 3. DISTRIBUTIONS OF RANDOM V ARIABLES\n3.3 Normal distribution\nAmong the many distributions seen in practice, one is by far the most common: the normal\ndistribution , which has the shape of a symmetric, unimodal bell curve. Many variables are nearly\nnormal, which makes the normal distribution useful for a variety of problems. For example, char-\nacteristics such as human height closely follow the normal distribution.\n3.3.1 Normal distribution model\nThe normal distribution model always describes a symmetric, unimodal, bell-shaped curve.\nHowever, the curves can di ﬀer in center and spread; the model can be adjusted using mean and\nstandard deviation. Changing the mean shifts the bell curve to the left or the right, while changing\nthe standard deviation stretches or constricts the curve. Figure 3.5 shows the normal distribution\nwith mean 0 and standard deviation 1 in the left panel and the normal distribution with mean 19\nand standard deviation 4 in the right panel. Figure 3.6 shows these distributions on the same axis.\n−3−2−10123Y\n7111519232731\nFigure 3.5: Both curves represent the normal distribution; however, they di ﬀer\nin their center and spread. The normal distribution with mean 0 and standard\ndeviation 1 is called the standard normal distribution .\n0 10 20 30\nFigure 3.6: The normal models shown in Figure 3.5 but plotted together and on\nthe same scale.\nFor any given normal distribution with mean \u0016and standard deviation \u001b, the distribution can\nbe written as N(\u0016;\u001b);\u0016and\u001bare the parameters of the normal distribution. For example, N(0;1)N(\u0016;\u001b)\nNormal dist.\nwith mean\u0016\n& st. dev.\u001brefers to the standard normal distribution, as shown in Figure 3.5.\nUnlike the Bernoulli and binomial distributions, the normal distribution is a continuous dis-\ntribution.\n3.3. NORMAL DISTRIBUTION 153\n3.3.2 Standardizing with Z-scores\nThe Z-score of an observation quantiﬁes how far the observation is from the mean, in units of Z\nZ-score, the\nstandardized\nobservationstandard deviation(s). If xis an observation from a distribution N(\u0016;\u001b), the Z-score is mathemati-\ncally deﬁned as:\nZ=x\u0000\u0016\n\u001b:\nAn observation equal to the mean has a Z-score of 0. Observations above the mean have\npositive Z-scores, while observations below the mean have negative Z-scores. For example, if an\nobservation is one standard deviation above the mean, it has a Z-score of 1; if it is 1.5 standard\ndeviations below the mean, its Z-score is -1.5.\nZ-scores can be used to identify which observations are more extreme than others, and are\nespecially useful when comparing observations from di ﬀerent normal distributions. One observa-\ntionx1is said to be more unusual than another observation x2if the absolute value of its Z-score\nis larger than the absolute value of the other observation’s Z-score: jZ1j>jZ2j. In other words, the\nfurther an observation is from the mean in either direction, the more extreme it is.\nEXAMPLE 3.17\nThe SAT and the ACT are two standardized tests commonly used for college admissions in the\nUnited States. The distribution of test scores are both nearly normal. For the SAT, N(1500;300); for\nthe ACT,N(21;5). While some colleges request that students submit scores from both tests, others\nallow students the choice of either the ACT or the SAT. Suppose that one student scores an 1800 on\nthe SAT (Student A) and another scores a 24 on the ACT (Student B). A college admissions o ﬃcer\nwould like to compare the scores of the two students to determine which student performed better.\nCalculate a Z-score for each student; i.e., convert xto Z.\nUsing\u0016SAT= 1500,\u001bSAT= 300, andxA= 1800, ﬁnd Student A’s Z-score:\nZA=xA\u0000\u0016SAT\n\u001bSAT=1800\u00001500\n300= 1:\nFor Student B:\nZB=xB\u0000\u0016ACT\n\u001bACT=24\u000021\n5= 0:6:\nStudent A’s score is 1 standard deviation above average on the SAT, while Student B’s score is 0.6\nstandard deviations above the mean on the ACT. As illustrated in Figure 3.7, Student A’s score\nis more extreme, indicating that Student A has scored higher with respect to other scores than\nStudent B.\nTHE Z-SCORE\nThe Z-score of an observation quantiﬁes how far the observation is from the mean, in units of\nstandard deviation(s). The Z-score for an observation xthat follows a distribution with mean\n\u0016and standard deviation \u001bcan be calculated using\nZ=x\u0000\u0016\n\u001b:\n154 CHAPTER 3. DISTRIBUTIONS OF RANDOM V ARIABLES\nX9001200150018002100Student A\n1116212631Student B\nFigure 3.7: Scores of Students A and B plotted on the distributions of SAT and\nACT scores.\nEXAMPLE 3.18\nHow high would a student need to score on the ACT to have a score equivalent to Student A’s score\nof 1800 on the SAT?\nAs shown in Example 3.7, a score of 1800 on the SAT is 1 standard deviation above the mean. ACT\nscores are normally distributed with mean 21 and standard deviation 5. To convert a value from\nthe standard normal curve (Z) to one on a normal distribution N(\u0016;\u001b):\nx=\u0016+Z\u001b:\nThus, a student would need a score of 21 + 1(5) = 26 on the ACT to have a score equivalent to 1800\non the SAT.\nGUIDED PRACTICE 3.19\nSystolic blood pressure (SBP) for adults in the United States aged 18-39 follow an approximate\nnormal distribution, N(115;17:5). As age increases, systolic blood pressure also tends to increase.\nMean systolic blood pressure for adults 60 years of age and older is 136 mm Hg, with standard\ndeviation 40 mm Hg. Systolic blood pressure of 140 mm Hg or higher is indicative of hypertension\n(high blood pressure). (a) How many standard deviations away from the mean is a 30-year-old with\nsystolic blood pressure of 125 mm Hg? (b) Compare how unusual a systolic blood pressure of 140\nmm Hg is for a 65-year-old, versus a 30-year-old.12\n12(a) Calculate the Z-score:x\u0000\u0016\n\u001b=125\u0000115\n17:5= 0:571. A 30-year-old with systolic blood pressure of 125 mm Hg is about\n0.6 standard deviations above the mean. (b) For x1= 140 mm Hg: Z1=x1\u0000\u0016\n\u001b=140\u0000115\n17:5= 1:43. Forx2= 140 mm Hg:\nZ2=x2\u0000\u0016\n\u001b=140\u0000136\n40= 0:1. While an SBP of 140 mm Hg is almost 1.5 standard deviations above the mean for a 30-year-old,\nit is only 0.1 standard deviations above the mean for a 65-year-old.\n3.3. NORMAL DISTRIBUTION 155\n3.3.3 The empirical rule\nThe empirical rule (also known as the 68-95-99.7 rule) states that for a normal distribution,\nalmost all observations will fall within three standard deviations of the mean. Speciﬁcally, 68% of\nobservations are within one standard deviation of the mean, 95% are within two SD’s, and 99.7%\nare within three SD’s.\nµ−3σµ−2σµ−σµµ+σµ+2σµ+3σ99.7%95%68%\nFigure 3.8: Probabilities for falling within 1, 2, and 3 standard deviations of the\nmean in a normal distribution.\nWhile it is possible for a normal random variable to take on values 4, 5, or even more standard\ndeviations from the mean, these occurrences are extremely rare if the data are nearly normal. For\nexample, the probability of being further than 4 standard deviations from the mean is about 1-in-\n30,000.\n3.3.4 Calculating normal probabilities\nThe normal distribution is a continuous probability distribution. Recall from Section 2.1.5\nthat the total area under the density curve is always equal to 1, and the probability that a variable\nhas a value within a speciﬁed interval is the area under the curve over that interval. By using\neither statistical software or normal probability tables, the normal model can be used to identify a\nprobability or percentile based on the corresponding Z-score (and vice versa).\nnegative Z positive Z\nFigure 3.9: The area to the left of Zrepresents the percentile of the observation.\n156 CHAPTER 3. DISTRIBUTIONS OF RANDOM V ARIABLES\nSecond decimal place of Z\nZ 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09\n0.0 0.5000 0.5040 0.5080 0.5120 0.5160 0.5199 0.5239 0.5279 0.5319 0.5359\n0.1 0.5398 0.5438 0.5478 0.5517 0.5557 0.5596 0.5636 0.5675 0.5714 0.5753\n0.2 0.5793 0.5832 0.5871 0.5910 0.5948 0.5987 0.6026 0.6064 0.6103 0.6141\n0.3 0.6179 0.6217 0.6255 0.6293 0.6331 0.6368 0.6406 0.6443 0.6480 0.6517\n0.4 0.6554 0.6591 0.6628 0.6664 0.6700 0.6736 0.6772 0.6808 0.6844 0.6879\n0.5 0.6915 0.6950 0.6985 0.7019 0.7054 0.7088 0.7123 0.7157 0.7190 0.7224\n0.6 0.7257 0.7291 0.7324 0.7357 0.7389 0.7422 0.7454 0.7486 0.7517 0.7549\n0.7 0.7580 0.7611 0.7642 0.7673 0.7704 0.7734 0.7764 0.7794 0.7823 0.7852\n0.8 0.7881 0.7910 0.7939 0.7967 0.7995 0.8023 0.8051 0.8078 0.8106 0.8133\n0.9 0.8159 0.8186 0.8212 0.8238 0.8264 0.8289 0.8315 0.8340 0.8365 0.8389\n1.0 0.8413 0.8438 0.8461 0.8485 0.8508 0.8531 0.8554 0.8577 0.8599 0.8621\n1.1 0.8643 0.8665 0.8686 0.8708 0.8729 0.8749 0.8770 0.8790 0.8810 0.8830\n:::::::::::::::::::::::::::::::::\nFigure 3.10: A section of the normal probability table. The percentile for a normal\nrandom variable with Z= 0:43 has been highlighted , and the percentile closest to\n0.8000 has also been highlighted .\nAnormal probability table is given in Appendix B.1 on page 463 and abbreviated in Fig-\nure 3.10. This table can be used to identify the percentile corresponding to any particular Z-\nscore; for instance, the percentile of Z= 0:43 is shown in row 0 :4 and column 0 :03 in Figure 3.10:\n0.6664, or the 66 :64thpercentile. First, ﬁnd the proper row in the normal probability table up\nthrough the ﬁrst decimal, and then determine the column representing the second decimal value.\nThe intersection of this row and column is the percentile of the observation. This value also rep-\nresents the probability that the standard normal variable Z takes on a value of 0.43 or less; i.e.\nP(Z\u00140:43) = 0:6664.\nThe table can also be used to ﬁnd the Z-score associated with a percentile. For example, to\nidentify Z for the 80thpercentile, look for the value closest to 0.8000 in the middle portion of the\ntable: 0.7995. The Z-score for the 80thpercentile is given by combining the row and column Z\nvalues: 0.84.\nEXAMPLE 3.20\nStudent A from Example 3.17 earned a score of 1800 on the SAT, which corresponds to Z= 1. What\npercentile is this score associated with?\nIn this context, the percentile is the percentage of people who earned a lower SAT score than\nStudent A. From the normal table, Zof 1.00 is 0.8413. Thus, the student is in the 84thpercentile\nof test takers. This area is shaded in Figure 3.11.\nGUIDED PRACTICE 3.21\nDetermine the proportion of SAT test takers who scored better than Student A on the SAT.13\n13If 84% had lower scores than Student A, the number of people who had better scores must be 16%.\n3.3. NORMAL DISTRIBUTION 157\n60090012001500180021002400\nFigure 3.11: The normal model for SAT scores, with shaded area representing\nscores below 1800.\n3.3.5 Normal probability examples\nThere are two main types of problems that involve the normal distribution: calculating prob-\nabilities from a given value (whether XorZ), or identifying the observation that corresponds to a\nparticular probability.\nEXAMPLE 3.22\nCumulative SAT scores are well-approximated by a normal model, N(1500;300). What is the prob-\nability that a randomly selected test taker scores at least 1630 on the SAT?\nFor any normal probability problem, it can be helpful to start out by drawing the normal curve and\nshading the area of interest.\n1630\nTo ﬁnd the shaded area under the curve, convert 1630 to a Z-score:\nZ=x\u0000\u0016\n\u001b=1630\u00001500\n300=130\n300= 0:43:\nLook up the percentile of Z= 0:43 in the normal probability table shown in Figure 3.10 or in\nAppendix B.1 on page 463: 0.6664. However, note that the percentile describes those who had a\nZ-score lower than 0.43, or in other words, the area below 0.43. To ﬁnd the area aboveZ= 0:43,\nsubtract the area of the lower tail from the total area under the curve, 1:\n1.0000 0.6664 0.3336 = \nThe probability that a student scores at least 1630 on the SAT is 0.3336.\n158 CHAPTER 3. DISTRIBUTIONS OF RANDOM V ARIABLES\nDISCRETE VERSUS CONTINUOUS PROBABILITIES\nRecall that the probability of a continuous random variable equaling some exact value\nis always 0. As a result, for a continuous random variable X,P(X\u0014x) =P(X < x ) and\nP(X\u0015x) =P(X>x ). It is valid to state that P(X\u0015x) = 1\u0000P(X\u0014x) = 1\u0000P(X<x ).\nThis is notthe case for discrete random variables. For example, for a discrete random variable\nY,P(Y\u00152) = 1\u0000P(Y <2) = 1\u0000P(Y\u00141). It would be incorrect to claim that P(Y\u00152) = 1\u0000P(Y\u0014\n2).\nGUIDED PRACTICE 3.23\nWhat is the probability of a student scoring at most 1630 on the SAT?14\nGUIDED PRACTICE 3.24\nSystolic blood pressure for adults 60 years of age and older in the United States is approximately\nnormally distributed: N(136;40). What is the probability of an adult in this age group having\nsystolic blood pressure of 140 mm Hg or greater?15\n14This probability was calculated as part of Example 3.22: 0.6664. A picture for this exercise is represented by the\nshaded area below “0.6664” in Example 3.22.\n15The Z-score for this observation was calculated in Exercise 3.19 as 0.1. From the table, the P(Z\u00150:1) = 1\u00000:54 = 0:46.\n3.3. NORMAL DISTRIBUTION 159\nEXAMPLE 3.25\nThe height of adult males in the United States between the ages of 20 and 62 is nearly normal, with\nmean 70 inches and standard deviation 3.3 inches.16What is the probability that a random adult\nmale is between 5’9” and 6’2”?\nThese heights correspond to 69 inches and 74 inches. First, draw the ﬁgure. The area of interest is\nan interval, rather than a tail area.\n6974\nTo ﬁnd the middle area, ﬁnd the area to the left of 74; from that area, subtract the area to the left\nof 69.\nFirst, convert to Z-scores:\nZ74=x\u0000\u0016\n\u001b=74\u000070\n3:3= 1:21; Z 62=x\u0000\u0016\n\u001b=69\u000070\n3:3=\u00000:30:\nFrom the normal probability table, the areas are respectively, 0 :8868 and 0:3821. The middle area\nis 0:8868\u00000:3821 = 0:5048. The probability of being between heights 5’9” and 6’2” is 0.5048.\n0.8868 0.3821 0.5048 = \nGUIDED PRACTICE 3.26\nWhat percentage of adults in the United States ages 60 and older have blood pressure between 145\nand 130 mm Hg?17\n16As based on a sample of 100 men, from the USDA Food Commodity Intake Database.\n17First calculate Z-scores, then ﬁnd the percent below 145 mm Hg and below 130 mm Hg: Z145= 0:23!0:5890,\nZ130=\u00000:15!0:4404 (area above). Final answer: 0 :5890\u00000:4404 = 0:1486.\n160 CHAPTER 3. DISTRIBUTIONS OF RANDOM V ARIABLES\nEXAMPLE 3.27\nHow tall is a man with height in the 40thpercentile?\nFirst, draw a picture. The lower tail probability is 0.40, so the shaded area must start before the\nmean.\n70  40%\n(0.40)\nDetermine the Z-score associated with the 40thpercentile. Because the percentile is below 50%, Z\nwill be negative. Look for the probability inside the negative part of table that is closest to 0.40:\n0.40 falls in row \u00000:2 and between columns 0 :05 and 0:06. Since it falls closer to 0 :05, choose\nZ=\u00000:25.\nConvert the Z-score to X, whereX\u0018N(70;3:3).\nX=\u0016+\u001bZ= 70 + (\u00000:25)(3:3) = 69:18:\nA man with height in the 40thpercentile is 69.18 inches tall, or about 5’ 9”.\nGUIDED PRACTICE 3.28\n(a) What is the 95thpercentile for SAT scores? (b) What is the 97 :5thpercentile of the male\nheights?18\n18(a) Look for 0.95 in the probability portion (middle part) of the normal probability table: row 1.6 and (about) column\n0.05, i.e.Z95= 1:65. Knowing Z95= 1:65,\u0016= 1500, and \u001b= 300, convert Z to x: 1500 + (1:65)(300) = 1995. (b) Similarly,\nﬁndZ97:5= 1:96, and convert to x:x97:5= 76:5 inches.\n3.3. NORMAL DISTRIBUTION 161\n3.3.6 Normal approximation to the binomial distribution\nThe normal distribution can be used to approximate other distributions, such as the bino-\nmial distribution. The binomial formula is cumbersome when sample size is large, particularly\nwhen calculating probabilities for a large number of observations. Under certain conditions, the\nnormal distribution can be used to approximate binomial probabilities. This method was widely\nused when calculating binomial probabilities by hand was the only option. Nowadays, modern\nstatistical software is capable of calculating exact binomial probabilities even for very large n. The\nnormal approximation to the binomial is discussed here since it is an important result that will be\nrevisited in later chapters.\nConsider the binomial model when probability of success is p= 0:10. Figure 3.12 shows four\nhollow histograms for simulated samples from the binomial distribution using four di ﬀerent sam-\nple sizes:n= 10;30;100;300. As the sample size increases from n= 10 ton= 300, the distribution\nis transformed from a blocky and skewed distribution into one resembling the normal curve.\nn  =  100 2 4 6\nn  =  300246810\nn  =  10005101520\nn  =  3001020304050\nFigure 3.12: Hollow histograms of samples from the binomial model when p=\n0:10. The sample sizes for the four plots are n= 10, 30, 100, and 300, respectively.\nNORMAL APPROXIMATION OF THE BINOMIAL DISTRIBUTION\nThe binomial distribution with probability of success pis nearly normal when the sample size\nnis suﬃciently large such that npandn(1\u0000p) are both at least 10. The approximate normal\ndistribution has parameters corresponding to the mean and standard deviation of the binomial\ndistribution:\n\u0016=np \u001b =p\nnp(1\u0000p)\n162 CHAPTER 3. DISTRIBUTIONS OF RANDOM V ARIABLES\nEXAMPLE 3.29\nApproximately 20% of the US population smokes cigarettes. A local government commissioned a\nsurvey of 400 randomly selected individuals to investigate whether their community might have a\nlower smoker rate than 20%. The survey found that 59 of the 400 participants smoke cigarettes. If\nthe true proportion of smokers in the community is 20%, what is the probability of observing 59\nor fewer smokers in a sample of 400 people?\nThe desired probability is equivalent to the sum of the individual probabilities of observing k= 0,\n1, ..., 58, or 59 smokers in a sample of n= 400:P(X\u001459). Conﬁrm that the normal approximation\nis valid:np= 400\u00020:20 = 80,n(1\u0000p) = 400\u00020:8 = 320. To use the normal approximation, calculate\nthe mean and standard deviation from the binomial model:\n\u0016=np= 80 \u001b=p\nnp(1\u0000p) = 8:\nConvert 59 to a Z-score: Z=59\u000080\n8=\u00002:63. Use the normal probability table to identify the left\ntail area, which is 0.0043.\nThis estimate is very close to the answer derived from the exact binomial calculation:\nP(k= 0 ork= 1 or\u0001\u0001\u0001ork= 59) =P(k= 0) +P(k= 1) +\u0001\u0001\u0001+P(k= 59) = 0:0041:\nHowever, even when the conditions for using the approximation are met, the normal approxi-\nmation to the binomial tends to perform poorly when estimating the probability of a small range of\ncounts. Suppose the normal approximation is used to compute the probability of observing 69, 70,\nor 71 smokers in 400 when p= 0:20. In this setting, the exact binomial and normal approximation\nresult in notably di ﬀerent answers: the approximation gives 0.0476, while the binomial returns\n0.0703.\nThe cause of this discrepancy is illustrated in Figure 3.13, which shows the areas representing\nthe binomial probability (outlined) and normal approximation (shaded). Notice that the width of\nthe area under the normal distribution is 0.5 units too slim on both sides of the interval.\n60 70 80 90 100\nFigure 3.13: A normal curve with the area between 69 and 71 shaded. The out-\nlined area represents the exact binomial probability.\nThe normal approximation can be improved if the cuto ﬀvalues for the range of observations\nis modiﬁed slightly: the lower value should be reduced by 0.5 and the upper value increased by 0.5.\nThe normal approximation with continuity correction gives 0.0687 for the probability of observing\n69, 70, or 71 smokers in 400 when p= 0:20, which is closer to the exact binomial result of 0.0703.\nThis adjustment method is known as a continuity correction, which allows for increased ac-\ncuracy when a continuous distribution is used to approximate a discrete one. The modiﬁcation is\ntypically not necessary when computing a tail area, since the total interval in that case tends to be\nquite wide.\n3.3. NORMAL DISTRIBUTION 163\n3.3.7 Evaluating the normal approximation\nThe normal model can also be used to approximate data distributions. While using a normal\nmodel can be convenient, it is important to remember that normality is always an approximation.\nTesting the appropriateness of the normal assumption is a key step in many data analyses.\nMale heights (inches)6065707580●\n●●●\n●\n●●●\n●●\n●●●\n●\n●\n●●\n●\n●●●\n●●\n●\n●●\n●\n●●\n●●\n●●\n●●\n●●●\n●●\n●●●\n●●●\n●\n●●\n●●\n●●●●\n●●\n●\n●\n●●\n●●●\n●●\n●●●\n●\n●●\n●\n●\n●\n●●\n●●●●\n●●\n●\n●●●●●\n●●\n●●\n●\n●●●\n●●●\nTheoretical QuantilesMale Heights (inches)\n−2−1012657075\nFigure 3.14: A sample of 100 male heights. Since the observations are rounded to\nthe nearest whole inch, the points in the normal probability plot appear to jump\nin increments.\nExample 3.27 suggests the distribution of heights of US males is well approximated by the\nnormal model. There are two visual methods used to assess the assumption of normality. The ﬁrst\nis a simple histogram with the best ﬁtting normal curve overlaid on the plot, as shown in the left\npanel of Figure 3.14. The sample mean ¯xand standard deviation sare used as the parameters of the\nbest ﬁtting normal curve. The closer this curve ﬁts the histogram, the more reasonable the normal\nmodel assumption. More commonly, a normal probability plot is used, such as the one shown in\nthe right panel of Figure 3.14.19If the points fall on or near the line, the data closely follow the\nnormal model.\n19Also called a quantile-quantile plot , or Q-Q plot.\n164 CHAPTER 3. DISTRIBUTIONS OF RANDOM V ARIABLES\nEXAMPLE 3.30\nThree datasets were simulated from a normal distribution, with sample sizes n= 40,n= 100, and\nn= 400; the histograms and normal probability plots of the datasets are shown in Figure 3.15.\nWhat happens as sample size increases?\nAs sample size increases, the data more closely follows the normal distribution; the histograms\nbecome more smooth, and the points on the Q-Q plots show fewer deviations from the line.\nIt is important to remember that when evaluating normality in a small dataset, apparent deviations\nfrom normality may simply be due to small sample size. Remember that all three of these simulated\ndatasets are drawn from a normal distribution.\nWhen assessing the normal approximation in real data, it will be rare to observe a Q-Q plot as\nclean as the one shown for n= 400. Typically, the normal approximation is reasonable even if there\nare some small observed departures from normality in the tails, such as in the plot for n= 100.\n−3−2−10123−3−2−10123−3−2−10123\n●●●\n●\n●\n●●\n●●●●\n●\n●●\n●\n●●●\n●●\n●\n●●\n●\n●●●\n●\n●●\n●\n●●\n●●\n●●\n●\n●●\nTheoretical Quantilesobserved\n−2−1012−2−101\n●●●●\n●●\n●●\n●●\n●●\n●●\n●\n●●\n●\n●\n●\n●●\n●●●\n●\n●\n●●\n●●\n●●●●\n●\n●\n●●\n●\n●●\n●●\n●●\n●●●\n●\n●●●●\n●\n●●●\n●●\n●●\n●●●\n●●\n●●\n●●\n●●\n●●●\n●\n●\n●●●●\n●\n●●\n●●●●\n●\n●●●●\n●●●\n●●●\nTheoretical Quantilesobserved\n−2−1012−2−1012\n●\n●●●\n●●●●\n●●\n●●●\n●●●\n●\n●\n●●●\n●●\n●●\n●●\n●●●\n●\n●●\n●●●●●\n●●●\n●●\n●●\n●\n●●\n●\n●●●●\n●●●\n●\n●●\n●●●\n●\n●●●●\n●●●●●\n●\n●\n●\n●●\n●●●\n●●\n●●\n●●\n●●\n●\n●●●●\n●\n●●\n●●\n●●\n●\n●●●\n●●\n●\n●●\n●\n●●●\n●\n●●●●\n●●\n●\n●●●\n●●●●\n●●\n●\n●\n●●\n●\n●●\n●●\n●●●●●\n●\n●●\n●●\n●●●\n●●●\n●●\n●●\n●●●●\n●●●●\n●●\n●●●\n●●●●●●●\n●●●\n●●\n●●●●●\n●\n●●●●\n●●●●\n●●\n●\n●\n●●\n●\n●●●●\n●●●●\n●●\n●●\n●●\n●●\n●\n●●\n●\n●●●\n●●\n●●\n●●\n●●●\n●\n●●\n●●\n●●●\n●\n●●\n●●●●\n●\n●●●●\n●\n●●●●\n●\n●●●\n●●\n●\n●●●●\n●●\n●\n●●●●●●\n●●\n●\n●●●●●\n●●\n●\n●●\n●●●●\n●●\n●\n●●●●●\n●●●●\n●●\n●●●●●\n●●\n●●\n●●●●●\n●●\n●\n●●●\n●\n●●●●●\n●●●\n●\n●●\n●●●\n●\n●●●\n●\n●●\n●●\n●●\n●●\n●\n●\n●\n●\n●●●\n●●\n●●●\n●●\n●●●\n●●\n●\n●●\n●●\n●●\n●●\n●●\n●●●●●●\n●●●\nTheoretical Quantilesobserved\n−3−10123−3−2−10123\nFigure 3.15: Histograms and normal probability plots for three simulated normal\ndata sets;n= 40 (left),n= 100 (middle), n= 400 (right).\n3.3. NORMAL DISTRIBUTION 165\nEXAMPLE 3.31\nWould it be reasonable to use the normal distribution to accurately calculate percentiles of heights\nof NBA players? Consider all 435 NBA players from the 2008-9 season presented in Figure 3.16.20\nThe histogram in the left panel is slightly left skewed, and the points in the normal probability\nplot do not closely follow a straight line, particularly in the upper quantiles. The normal model is\nnot an accurate approximation of NBA player heights.\nHeight (inches)7075808590●\n●●●●\n●\n●●\n●\n●●\n●●●●\n●●\n●●●\n●●\n●●●●\n●●\n●●\n●●●●●\n●●\n●●\n●\n●●\n●●\n●●●●\n●●\n●●●\n●●\n●●●●\n●\n●●\n●\n●●●\n●●\n●●●●\n●●●\n●\n●\n●\n●●\n●\n●●\n●\n●●●\n●●\n●●\n●●●●●●●\n●●\n●●●\n●●\n●\n●\n●●●\n●●●\n●●\n●●\n●●\n●●●\n●\n●●\n●●\n●●●\n●\n●\n●●\n●●\n●●●\n●\n●\n●●●\n●\n●●\n●●●●●\n●●●\n●\n●●●\n●●\n●●●\n●●●\n●●●\n●●\n●●●\n●●●●\n●\n●●●\n●●\n●\n●●\n●\n●●\n●●\n●●\n●\n●●\n●\n●●\n●●\n●\n●●\n●●●\n●●\n●●\n●●\n●\n●●●●●\n●\n●●●\n●\n●●●\n●●\n●\n●●●●\n●●●\n●\n●●●\n●●●\n●●●\n●●\n●\n●●●●\n●●\n●●●\n●●●●\n●●\n●●\n●●\n●●\n●●\n●\n●\n●●\n●●\n●●\n●●●\n●●\n●●●●\n●●\n●●\n●●●\n●●●\n●\n●\n●●●\n●\n●●\n●\n●●●●\n●●\n●●\n●●●●\n●●\n●\n●●●\n●\n●●\n●●\n●●●\n●●●\n●●●●\n●●\n●●\n●●\n●●\n●●●\n●\n●●\n●\n●\n●\n●●●\n●\n●●●\n●●●\n●\n●\n●\n●●●\n●●●\n●●\n●\n●●\n●●●\n●\n●●●\n●●\n●\n●●●\n●●\n●●\n●●\n●\n●●●\n●●\n●●\n●●●\n●\n●●●\n●●●\n●●\n●●\n●●\n●\n●●●\n●\n●●\nTheoretical Quantilesobserved\n−3−2−101237075808590\nFigure 3.16: Histogram and normal probability plot for the NBA heights from the\n2008-9 season.\nEXAMPLE 3.32\nConsider the poker winnings of an individual over 50 days. A histogram and normal probability\nplot of these data are shown in Figure 3.17 Evaluate whether a normal approximation is appropri-\nate.\nThe data are very strongly right skewed in the histogram, which corresponds to the very strong\ndeviations on the upper right component of the normal probability plot. These data show very\nstrong deviations from the normal model; the normal approximation should not be applied to\nthese data.\nPoker earnings (US$)−2000−1000 01000200030004000●●●●\n●●●\n●●●●\n●\n●●●\n●●●●●●\n●\n●\n●●●\n●●\n●●●●●\n●●\n●●●\n●●\n●●\n●\n●\n●●\n●●●\n●\nTheoretical quantiles−2−1 012−10000100020003000\nFigure 3.17: A histogram of poker data with the best ﬁtting normal plot and a\nnormal probability plot.\n20These data were collected from www.nba.com.\n166 CHAPTER 3. DISTRIBUTIONS OF RANDOM V ARIABLES\nGUIDED PRACTICE 3.33\nDetermine which data sets represented in Figure 3.18 plausibly come from a nearly normal distri-\nbution.21\n●\n●\n●●●●●\n●●●\n●\n●●●●\n●\n●●\n●\n●●●\n●●●\n●\n●●\n●\n●●\n●\n●●\n●\n●\n●●●\n●●●\n●\n●●\n●\n●●\n●●\n●\n●●\n●●●\n●●●\n●●\n●●\n●●\n●●\n●\n●●\n●●●\n●●\n●●\n●●\n●●●\n●●●\n●\n●●●●●●\n●●\n●●\n●●●●Observed\nTheoretical quantiles−2−101204080120\n●\n●●\n●\n●●\n●●●\n●●\n●●\n●●\n●\n●●\n●●\n●\n●\n●●●\n●\n●●●●●\n●●\n●●●●\n●\n●●\n●●\n●●●\n●●\n●●●Observed\nTheoretical quantiles−2−101268101214\n●●\n●●\n●●\n●●\n●●●●●\n●\n●●●\n●●●\n●●\n●●\n●●\n●\n●●●\n●●●\n●\n●●\n●●●\n●\n●●●\n●●●●●\n●\n●●\n●●●\n●●●\n●●●\n●●\n●●\n●●●●\n●\n●●●\n●\n●●●\n●\n●\n●●●\n●\n●●\n●\n●●●\n●\n●●\n●\n●●●\n●●\n●●\n●●●●\n●\n●●\n●●●●●●\n●●●\n●\n●●●\n●●●\n●●●\n●\n●●\n●●\n●●\n●●\n●●\n●●\n●●\n●●●\n●●\n●\n●●\n●●\n●●\n●●●\n●\n●●\n●\n●\n●●\n●●\n●\n●●\n●●●●●\n●●●\n●●●●\n●●\n●\n●●●\n●\n●●\n●\n●●\n●●●\n●●●\n●●●\n●\n●\n●\n●●\n●●●\n●\n●●●\n●●●●\n●●\n●●●\n●●\n●●\n●●\n●\n●●●\n●●●\n●●\n●●\n●●\n●●\n●●●\n●●\n●●\n●\n●●●\n●●\n●●●\n●●\n●●\n●●\n●●●\n●\n●●●●\n●●●\n●●\n●●\n●●\n●●●●●\n●●\n●●\n●\n●●\n●●\n●●●●\n●●●\n●●●●\n●●●\n●●●\n●●●\n●●●\n●●\n●●\n●●●\n●●\n●●●\n●●\n●●●●\n●\n●●●\n●●\n●\n●\n●●\n●●●●\n●●\n●\n●●\n●●●\n●●●\n●●\n●\n●●●●\n●\n●●\n●●●\n●●●\n●\n●●\n●●●\n●●●●\n●●\n●\n●●●\n●\n●●\n●●\n●●\n●●\n●\n●\n●●●\n●●\n●●\n●●●●●\n●●●\n●\n●\n●●\n●●●\n●●●●●\n●●●\n●●\n●●●\n●\n●●●\n●●\n●●\n●●\n●●\n●●●\n●●●●\n●●\n●●●●\n●●\n●●●●●\n●●●\n●●\n●●\n●\n●●●\n●●\n●●●\n●●\n●\n●●●●\n●●●●●\n●Observed\nTheoretical quantiles−3−2−10123−3−2−1\n●\n●●\n●\n●●●●\n●\n●●●\n●●\n●Observed\nTheoretical quantiles−1010204060\nFigure 3.18: Four normal probability plots for Guided Practice 3.33.\n21Answers may vary. The top-left plot shows some deviations in the smallest values in the dataset; speciﬁcally, the left\ntail shows some large outliers. The top-right and bottom-left plots do not show any obvious or extreme deviations from\nthe lines for their respective sample sizes, so a normal model would be reasonable. The bottom-right plot has a consistent\ncurvature that suggests it is not from the normal distribution. From examining the vertical coordinates of the observations,\nmost of the data are between -20 and 0, then there are about ﬁve observations scattered between 0 and 70; this distribution\nhas strong right skew.\n3.3. NORMAL DISTRIBUTION 167\nWhen observations spike downwards on the left side of a normal probability plot, this indi-\ncates that the data have more outliers in the left tail expected under a normal distribution. When\nobservations spike upwards on the right side, the data have more outliers in the right tail than\nexpected under the normal distribution.\nGUIDED PRACTICE 3.34\nFigure 3.19 shows normal probability plots for two distributions that are skewed. One distribution\nis skewed to the low end (left skewed) and the other to the high end (right skewed). Which is\nwhich?22\n●●\n●\n●●\n●●●\n●●●●\n●\n●●●●●\n●\n●●●\n●●●Observed\nTheoretical quantiles−2−1012012●\n●●\n●\n●●●●●\n●\n●●●\n●●\n●●●\n●●\n●●\n●●●●\n●\n●●●\n●\n●\n●●\n●●●●●\n●\n●●\n●\n●●\n●●●●\n●Observed\nTheoretical quantiles−2−101251015\nFigure 3.19: Normal probability plots for Guided Practice 3.34.\n22Examine where the points fall along the vertical axis. In the ﬁrst plot, most points are near the low end with fewer\nobservations scattered along the high end; this describes a distribution that is skewed to the high end. The second plot\nshows the opposite features, and this distribution is skewed to the low end.\n168 CHAPTER 3. DISTRIBUTIONS OF RANDOM V ARIABLES\n3.4 Poisson distribution\nThe Poisson distribution is a discrete distribution used to calculate probabilities for the num-\nber of occurrences of a rare event. In technical terms, it is used as a model for count data. For\nexample, historical records of hospitalizations in New York City indicate that among a population\nof approximately 8 million people, 4.4 people are hospitalized each day for an acute myocardial\ninfarction (AMI), on average. A histogram of showing the distribution of the number of AMIs per\nday on 365 days for NYC is shown in Figure 3.20.23\nfrequency\n0 5 10020406080\nFigure 3.20: A histogram of the number of people hospitalized for an AMI on 365\ndays for NYC, as simulated from a Poisson distribution with mean 4.4.\nPOISSON DISTRIBUTION\nThe Poisson distribution is a probability model for the number of events that occur in a popu-\nlation. The probability that exactly kevents occur is given by\nP(X=k) =e\u0000\u0015(\u0015)k\nk!;\nwherekmay take a value 0, 1, 2, . . . The mean and standard deviation of this distribution are\n\u0015andp\n\u0015, respectively. A Poisson random variable Xcan be expressed as X\u0018Pois(\u0015).Pois(\u0015)\nPoisson dist.\nwith rate\u0015\nWhen events accumulate over time in such a way that the probability an event occurs in an in-\nterval is proportional to the length of an interval and that the number of events in non-overlapping\nintervals are independent, the parameter \u0015(the Greek letter lambda ) represents the average num-\u0015\nRate for the\nPoisson dist. ber of events per unit time; i.e., the rate per unit time.\nIn this setting, the number of events in tunits of time has probability\nP(X=k) =e\u0000\u0015t(\u0015t)k\nk!;\nwherektakes on values 0, 1, 2, . . . . When used this way, the mean and standard deviation are \u0015t\nandp\n\u0015t, respectively. The rate parameter \u0015represents the expected number of events per unit\ntime, while the quantity \u0015trepresents the expected number events over a time period of tunits.\nThe histogram in Figure 3.20 approximates a Poisson distribution with rate equal to 4.4 events\nper day, for a population of 8 million.\n23These data are simulated. In practice, it would be important to check for an association between successive days.\n3.4. POISSON DISTRIBUTION 169\nEXAMPLE 3.35\nIn New York City, what is the probability that 2 individuals are hospitalized for AMI in seven days,\nif the rate is known to be 4.4 deaths per day?\nFrom the given information, \u0015= 4:4,k= 2, andt= 7.\nP(X=k) =e\u0000\u0015t(\u0015t)k\nk!\nP(X= 2) =e\u00004:4\u00027(4:4\u00027)2\n2!= 1:99\u000210\u000011:\nGUIDED PRACTICE 3.36\nIn New York City, what is the probability that (a) at most 2 individuals are hospitalized for AMI in\nseven days, (b) at least 3 individuals are hospitalized for AMI in seven days?24\nA rigorous set of conditions for the Poisson distribution is not discussed here. Generally, the\nPoisson distribution is used to calculate probabilities for rare events that accumulate over time,\nsuch as the occurrence of a disease in a population.\nEXAMPLE 3.37\nFor children ages 0 - 14, the incidence rate of acute lymphocytic leukemia (ALL) was approximately\n30 diagnosed cases per million children per year in 2010. Approximately 20% of the US population\nof 319,055,000 are in this age range. What is the expected number of cases of ALL in the US over\nﬁve years?\nThe incidence rate for one year can be expressed as 30 =1;000;000 = 0:00003; for ﬁve years,\nthe rate is (5)(0 :00003) = 0 :00015. The number of children age 0-14 in the population is\n(0:20)(319;055;000)\u001963;811;000.\n\u0015= (relevant population size)(rate per child)\n= 63;811;000\u00020:00015\n= 9;571:5\nThe expected number of cases over ﬁve years is 9,571.5 cases.\n24(a)P(X\u00142) =P(X= 0) +P(X= 1) +P(X= 2) =e\u00004:4\u00027(4:4\u00027)0\n0!+e\u00004:4\u00027(4:4\u00027)1\n1!+e\u00004:4\u00027(4:4\u00027)2\n2!= 2:12\u000210\u000011(b)\nP(X\u00153) = 1\u0000P(X<3) = 1\u0000P(X\u00142) = 1\u00002:12\u000210\u000011\u00191\n170 CHAPTER 3. DISTRIBUTIONS OF RANDOM V ARIABLES\n3.5 Distributions related to Bernoulli trials\nThe binomial distribution is not the only distribution that can be built from a series of re-\npeated Bernoulli trials. This section discusses the geometric, negative binomial, and hypergeomet-\nric distributions.\n3.5.1 Geometric distribution\nThe geometric distribution describes the waiting time until one success for a series of inde-\npendent Bernoulli random variables, in which the probability of success premains constant.\nEXAMPLE 3.38\nRecall that in the Milgram shock experiments, the probability of a person refusing to give the most\nsevere shock is p= 0:35. Suppose that participants are tested one at a time until one person refuses;\ni.e., until the ﬁrst occurrence of a successful trial. What are the chances that the ﬁrst occurrence\nhappens with the ﬁrst trial? The second trial? The third?\nThe probability that the ﬁrst trial is successful is simply p= 0:35.\nIf the second trial is the ﬁrst successful one, then the ﬁrst one must have been unsuccessful. Thus,\nthe probability is given by (0 :65)(0:35) = 0:228.\nSimilarly, the probability that the ﬁrst success is the third trial: (0 :65)(0:65)(0:35) = 0:148.\nThis can be stated generally. If the ﬁrst success is on the nthtrial, then there are n\u00001 failures and\nﬁnally 1 success, which corresponds to the probability (0 :65)n\u00001(0:35).\nThe geometric distribution from Example 3.38 is shown in Figure 3.21. In general, the proba-\nbilities for a geometric distribution decrease exponentially .\nProbability\nNumber of trials24681012140.00.10.20.3\n...\nFigure 3.21: The geometric distribution when the probability of success is\np= 0:35.\n3.5. DISTRIBUTIONS RELATED TO BERNOULLI TRIALS 171\nGEOMETRIC DISTRIBUTION\nIf the probability of a success in one trial is pand the probability of a failure is 1 \u0000p, then the\nprobability of ﬁnding the ﬁrst success in the kthtrial is given by\nP(X=k) = (1\u0000p)k\u00001p:\nThe mean (i.e. expected value), variance, and standard deviation of this wait time are given by\n\u0016=1\np\u001b2=1\u0000p\np2\u001b=s\n1\u0000p\np2\nA geometric random variable Xcan be expressed as X\u0018Geom(p).Geom(p)\nGeometric dist.\nwithpprob. of\nsuccess\nGUIDED PRACTICE 3.39\nIf individuals were examined until one did not administer the most severe shock, how many might\nneed to be tested before the ﬁrst success?25\nEXAMPLE 3.40\nWhat is the probability of the ﬁrst success occurring within the ﬁrst 4 people?\nThis is the probability it is the ﬁrst ( k= 1), second ( k= 2), third ( k= 3), or fourth ( k= 4) trial that\nis the ﬁrst success, which represent four disjoint outcomes. Compute the probability of each case\nand add the separate results:\nP(X= 1;2;3;or 4)\n=P(X= 1) +P(X= 2) +P(X= 3) +P(X= 4)\n= (0:65)1\u00001(0:35) + (0:65)2\u00001(0:35) + (0:65)3\u00001(0:35) + (0:65)4\u00001(0:35)\n= 0:82:\nAlternatively, ﬁnd the complement of P(X = 0), since the described event is the complement of no\nsuccess in 4 trials: 1 \u0000(0:65)4(0:35)0= 0:82.\nThere is a 0.82 probability that the ﬁrst success occurs within 4 trials.\nNote that there are di ﬀering conventions for deﬁning the geometric distribution; while this\ntext uses the deﬁnition that the distribution describes the total number of trials including the suc-\ncess, others deﬁne the distribution as the number of trials required before the success is obtained.\nInR, the latter deﬁnition is used.\n25About 1=p= 1=0:35 = 2:86 individuals.\n172 CHAPTER 3. DISTRIBUTIONS OF RANDOM V ARIABLES\n3.5.2 Negative binomial distribution\nThe geometric distribution describes the probability of observing the ﬁrst success on the kth\ntrial. The negative binomial distribution is more general: it describes the probability of observing\ntherthsuccess on the kthtrial.\nSuppose a research assistant needs to successfully extract RNA from four plant samples before\nleaving the lab for the day. Yesterday, it took 6 attempts to attain the fourth successful extraction.\nThe last extraction must have been a success; that leaves three successful extractions and two un-\nsuccessful ones that make up the ﬁrst ﬁve attempts. There are ten possible sequences, which are\nshown in 3.22.\nExtraction Attempt\n1 2 3 4 5 6\n1F F1\nS2\nS3\nS4\nS\n2F1\nSF2\nS3\nS4\nS\n3F1\nS2\nSF3\nS4\nS\n4F1\nS2\nS3\nSF4\nS\n51\nSF F2\nS3\nS4\nS\n61\nSF2\nSF3\nS4\nS\n71\nSF2\nS3\nSF4\nS\n81\nS2\nSF F3\nS4\nS\n91\nS2\nSF3\nSF4\nS\n101\nS2\nS3\nSF F4\nS\nFigure 3.22: The ten possible sequences when the fourth successful extraction is\non the sixth attempt.\nGUIDED PRACTICE 3.41\nEach sequence in Figure 3.22 has exactly two failures and four successes with the last attempt\nalways being a success. If the probability of a success is p= 0:8, ﬁnd the probability of the ﬁrst\nsequence.26\nIf the probability of a successful extraction is p= 0:8, what is the probability that it takes\nexactly six attempts to reach the fourth successful extraction? As expressed by 3.41, there are 10\ndiﬀerent ways that this event can occur. The probability of the ﬁrst sequence was identiﬁed in\nGuided Practice 3.41 as 0.0164, and each of the other sequences have the same probability. Thus,\nthe total probability is (10)(0 :0164) = 0:164.\n26The ﬁrst sequence: 0 :2\u00020:2\u00020:8\u00020:8\u00020:8\u00020:8 = 0:0164.\n3.5. DISTRIBUTIONS RELATED TO BERNOULLI TRIALS 173\nA general formula for computing a negative binomial probability can be generated using sim-\nilar logic as for binomial probability. The probability is comprised of two pieces: the probability\nof a single sequence of events, and then the number of possible sequences. The probability of ob-\nservingrsuccesses out of kattempts can be expressed as (1 \u0000p)k\u0000rpr. Next, identify the number of\npossible sequences. In the above example, 10 sequences were identiﬁed by ﬁxing the last observa-\ntion as a success and looking for ways to arrange the other observations. In other words, the goal is\nto arranger\u00001 successes in k\u00001 trials. This can be expressed as:\n k\u00001\nr\u00001!\n=(k\u00001)!\n(r\u00001)!((k\u00001)\u0000(r\u00001))!=(k\u00001)!\n(r\u00001)!(k\u0000r)!:\nNEGATIVE BINOMIAL DISTRIBUTION\nThe negative binomial distribution describes the probability of observing the rthsuccess on\nthekthtrial, for independent trials:\nP(X=k) = k\u00001\nr\u00001!\npr(1\u0000p)k\u0000r; (3.42)\nwherepis the probability an individual trial is a success.\nThe mean and variance are given by\n\u0016=r\np\u001b2=r(1\u0000p)\np2\nA negative binomial random variable Xcan be expressed as X\u0018NB(r;p).NB(r;p)\nNeg. Bin. dist.\nwithk\nsuccesses\n&pprob. of\nsuccess\nIS IT NEGATIVE BINOMIAL? FOUR CONDITIONS TO CHECK.\n(1) The trials are independent.\n(2) Each trial outcome can be classiﬁed as a success or failure.\n(3) The probability of a success ( p) is the same for each trial.\n(4) The last trial must be a success.\nEXAMPLE 3.43\nCalculate the probability of a fourth successful extraction on the ﬁfth attempt.\nThe probability of a single success is p= 0:8, the number of successes is r= 4, and the number of\nnecessary attempts under this scenario is k= 5.\n k\u00001\nr\u00001!\npr(1\u0000p)k\u0000r=4!\n3!1!(0:8)4(0:2) = 4\u00020:08192 = 0:328:\n174 CHAPTER 3. DISTRIBUTIONS OF RANDOM V ARIABLES\nGUIDED PRACTICE 3.44\nAssume that each extraction attempt is independent. What is the probability that the fourth success\noccurs within 5 attempts?27\nBINOMIAL VERSUS NEGATIVE BINOMIAL\nThe binomial distribution is used when considering the number of successes for a ﬁxed num-\nber of trials. For negative binomial problems, there is a ﬁxed number of successes and the goal\nis to identify the number of trials necessary for a certain number of successes (note that the\nlast observation must be a success).\nGUIDED PRACTICE 3.45\nOn 70% of days, a hospital admits at least one heart attack patient. On 30% of the days, no heart\nattack patients are admitted. Identify each case below as a binomial or negative binomial case, and\ncompute the probability. (a) What is the probability the hospital will admit a heart attack patient\non exactly three days this week? (b) What is the probability the second day with a heart attack\npatient will be the fourth day of the week? (c) What is the probability the ﬁfth day of next month\nwill be the ﬁrst day with a heart attack patient?28\nInR, the negative binomial distribution is deﬁned as the number of failures that occur before\na target number of successes is reached; i.e., k\u0000r. In this text, the distribution is deﬁned in terms\nof the total number of trials required to observe rsuccesses, where the last trial is necessarily a\nsuccess.\n27If the fourth success ( r= 4) is within ﬁve attempts, it either took four or ﬁve tries ( k= 4 ork= 5):\nP(k= 4 ORk= 5) =P(k= 4) +P(k= 5)\n= 4\u00001\n4\u00001!\n0:84+ 5\u00001\n4\u00001!\n(0:8)4(1\u00000:8) = 1\u00020:41 + 4\u00020:082 = 0:41 + 0:33 = 0:74:\n28In each part, p= 0:7. (a) The number of days is ﬁxed, so this is binomial. The parameters are k= 3 andn= 7: 0.097. (b)\nThe last \"success\" (admitting a patient) is ﬁxed to the last day, so apply the negative binomial distribution. The parameters\narer= 2,k= 4: 0.132. (c) This problem is negative binomial with r= 1 andk= 5: 0.006. Note that the negative binomial\ncase whenr= 1 is the same as using the geometric distribution.\n3.5. DISTRIBUTIONS RELATED TO BERNOULLI TRIALS 175\n3.5.3 Hypergeometric distribution\nSuppose that a large number of deer live in a forest. Researchers are interested in using the\ncapture-recapture method to estimate total population size. A number of deer are captured in an\ninitial sample and marked, then released; at a later time, another sample of deer are captured, and\nthe number of marked and unmarked deer are recorded.29An estimate of the total population can\nbe calculated based on the assumption that the proportion of marked deer in the second sample\nshould equal the proportion of marked deer in the entire population. For example, if 50 deer were\ninitially captured and marked, and then 5 out of 40 deer (12.5%) in a second sample are found to\nbe marked, then the population estimate is 400 deer, since 50 out of 400 is 12.5%.\nThe capture-recapture method sets up an interesting scenario that requires a new probability\ndistribution. Let Nrepresent the total number of deer in the forest, mthe number of marked deer\ncaptured in the original sample, and nthe number of deer in the second sample. What are the\nprobabilities of obtaining 0 ;1;:::;m marked deer in the second sample, if Nandmare known?\nIt is helpful to think in terms of a series of Bernoulli trials, where each capture in the second\nsample represents a trial; consider the trial a success if a marked deer is captured, and a failure\nif an unmarked deer is captured. If the deer were sampled with replacement , such that one deer\nwas sampled, checked if it were marked versus unmarked, then released before another deer was\nsampled, then the probability of obtaining some number of marked deer in the second sample\nwould be binomially distributed with probability of success m=N (out ofntrials). The trials are\nindependent, and the probability of success remains constant across trials.\nHowever, in capture-recapture, the goal is to collect a representative sample such that the pro-\nportion of marked deer in the sample can be used to estimate the total population—the sampling\nis done without replacement . Once a trial occurs and a deer is sampled, it is not returned to the pop-\nulation before the next trial. The probability of success is not constant from trial to trial; i.e., these\ntrials are dependent. For example, if a marked deer has just been sampled, then the probability of\nsampling a marked deer in the next trial decreases, since there is one fewer marked deer available.\nSuppose that out of 9 deer, 4 are marked. What is the probability of observing 1 marked deer\nin a sample of size 3, if the deer are sampled without replacement? First, consider the total number\nof ways to draw 3 deer from the population; As shown in Figure 3.23, samples may consist of 3,\n2, 1, or 0 marked deer. There are\u00004\n3\u0001ways to obtain a sample consisting of 3 marked deer out\nof the 4 total marked deer. By independence, there are\u00004\n2\u0001\u00005\n1\u0001ways to obtain a sample consisting\nof exactly 2 marked deer and 1 unmarked deer. In total, there are 84 possible combinations; this\nquantity is equivalent to\u00009\n3\u0001. Only\u00004\n1\u0001\u00005\n2\u0001= 40 of those combinations represent the desired event\nof exactly 1 marked deer. Thus, the probability of observing 1 marked deer in a sample of size 3,\nunder sampling without replacement, equals 40 =84 = 0:476.\nFigure 3.23: Possible samples of marked and unmarked deer in a sample n= 3,\nwherem= 4 andN\u0000m= 5. Striped circles represent marked deer, and empty\ncircles represent unmarked deer.\n29It is assumed that enough time has passed so that the marked deer redistribute themselves in the population, and that\nmarked and unmarked deer have equal probability of being captured in the second sample.\n176 CHAPTER 3. DISTRIBUTIONS OF RANDOM V ARIABLES\nGUIDED PRACTICE 3.46\nSuppose that out of 9 deer, 4 are marked. What is the probability of observing 1 marked deer in a\nsample of size 3, if the deer are sampled with replacement?30\nHYPERGEOMETRIC DISTRIBUTION\nThe hypergeometric distribution describes the probability of observing ksuccesses in a sam-\nple of sizen, from a population of size N, where there are msuccesses, and individuals are\nsampled without replacement:\nP(X=k) =\u0000m\nk\u0001\u0000N\u0000m\nn\u0000k\u0001\n\u0000N\nn\u0001:\nLetp=m=N , the probability of success. The mean and variance are given by\n\u0016=np \u001b2=np(1\u0000p)N\u0000n\nN\u00001\nA hypergeometric random variable Xcan be written as X\u0018HGeom(m;N\u0000m;n).\nIS IT HYPERGEOMETRIC? THREE CONDITIONS TO CHECK.\n(1) The trials are dependent.\n(2) Each trial outcome can be classiﬁed as a success or failure.\n(3) The probability of a success is di ﬀerent for each trial.\nGUIDED PRACTICE 3.47\nA small clinic would like to draw a random sample of 10 individuals from their patient list of 120,\nof which 30 patients are smokers. (a) What is the probability of 6 individuals in the sample being\nsmokers? (b) What is the probability that at least 2 individuals in the sample smoke?31\n30LetXrepresent the number of marked deer in the sample of size 3. If the deer are sampled with replacement, X\u0018\nBin(3;4=9), andP(X= 1) =\u00003\n1\u0001(4=9)1(5=9)2= 0:412.\n31(a) LetXrepresent the number of smokers in the sample. P(X= 6) =\u000030\n6\u0001\u000090\n4\u0001\n\u0000120\n10\u0001= 0:013. (b)P(X\u00152) = 1\u0000P(X\u00141) =\n1\u0000P(X= 0)\u0000P(X= 1) = 1\u0000\u000030\n0\u0001\u000090\n10\u0001\n\u0000120\n10\u0001\u0000\u000030\n1\u0001\u000090\n9\u0001\n\u0000120\n10\u0001= 0:768.\n3.6. DISTRIBUTIONS FOR PAIRS OF RANDOM V ARIABLES 177\n3.6 Distributions for pairs of random variables\nExample 3.8 calculated the variability in health care costs for an employee and her partner\nrelying on the assumption that the number of health episodes between the two are not related.\nIt could be reasonable to assume that the health status of one person gives no information about\nthe other’s health, given that the two are not physically related and were not previously living\ntogether. However, associations between random variables can be subtle. For example, couples are\noften attracted to each other because of common interests or lifestyles, which suggests that health\nstatus may actually be related.\nThe relationship between a pair of discrete random variables is a feature of the joint distri-\nbution of the pair. In this example the joint distribution of annual costs is a table of all possible\ncombinations of costs for the employee and her partner, using the probabilities and costs from the\nlast 10 years (these costs were previously calculated in Example 3.6 and Guided Practice 3.7). En-\ntries in the table are probabilities of pairs of annual costs. For example, the entry 0.25 in the second\nrow and second column of Figure 3.24 indicates that in approximately 25% of the last 10 years, the\nemployee paid $1,008 in costs while her partner paid $988.\nPartner costs, Y\nEmployee costs, X $968 $988\n$968 0.18 0.12\n$1,008 0.15 0.25\n$1,028 0.04 0.16\n$1,108 0.03 0.07\nFigure 3.24: Joint distribution of health care costs.\nMore generally, the deﬁnition of a joint distribution for a pair of random variables XandY\nuses the notion of joint probabilities discussed in Section 2.2.1.\nJOINT DISTRIBUTION\nThe joint distribution pX;Y(x;y) for a pair of random variables ( X;Y) is the collection of prob-\nabilities\np(xi;yj) =P(X=xiandY=yj)\nfor all pairs of values ( xi;yj) that the random variables XandYtake on.\n178 CHAPTER 3. DISTRIBUTIONS OF RANDOM V ARIABLES\nJoint distributions are often displayed in tabular form as in Figure 3.24. If XandYhavek1\nandk2possible values respectively, there will be ( k1)(k2) possible (x;y) pairs. This is unlike pairs\nof values (x;y) observed in a dataset, where each observed value of xis usually paired with only\none value of y. A joint distribution is often best displayed as a table of probabilities, with ( k1)(k2)\nentries. Figure 3.25 shows the general form of the table for the joint distribution of two discrete\ndistributions.\nValues ofY\nValues ofX y 1y2\u0001\u0001\u0001yk2\nx1p(x1;y1)p(x1;y2)\u0001\u0001\u0001p(x1;yk2)\nx2p(x2;y1)p(x2;y2)\u0001\u0001\u0001p(x2;yk2)\n:::\u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001\nxk1p(xk1;y1)p(xk1;y2)\u0001\u0001\u0001p(xk1;yk2)\nFigure 3.25: Table for a joint distribution. Entries are probabilities for pairs\n(xi;yj). These probabilities can be written as p(xi;yj) or more speciﬁcally,\npX;Y(xi;yj).\nWhen two variables XandYhave a joint distribution, the marginal distribution ofXis\nthe collection of probabilities for XwhenYis ignored.32IfXrepresents employee costs and Y\nrepresents partner costs, the event ( X= $968) consists of the two disjoint events ( X= $968;Y=\n$968) and (X= $968;Y= $988), soP(X= $968) = 0:18 + 0:12 = 0:30, the sum of the ﬁrst row of the\ntable. The row sums are the values of the marginal distribution of X, while the column sums are\nthe values of the marginal distributions of Y. The marginal distributions of XandYare shown in\nFigure 3.26, along with the joint distribution of XandY. The term marginal distribution is apt in\nthis setting—the marginal probabilities appear in the table margins.\nPartner Costs, Y\nEmployee costs, X $968 $988 Marg. Dist., X\n$968 0.18 0.12 0.30\n$1,008 0.15 0.25 0.40\n$1,028 0.04 0.16 0.20\n$1,108 0.03 0.07 0.10\nMarg. Dist., Y 0.40 0.60 1.00\nFigure 3.26: Joint and marginal distributions of health care costs\nFor a pair of random variables XandY, the conditional distribution ofYgiven a value xof\nthe variable Xis the probability distribution of Ywhen its values are restricted to the value xfor\nX. Just as marginal and joint probabilities are used to calculate conditional probabilities, joint and\nmarginal distributions can be used to obtain conditional distributions. If information is observed\nabout the value of one of the correlated random variables, such as X, then this information can be\nused to obtain an updated distribution for Y; unlike the marginal distribution of Y, the conditional\ndistribution of YgivenXaccounts for information from X.\n32The marginal distribution of Xcan be written as pX(x), and a speciﬁc value in the marginal distribution written as\npX(xi).\n3.6. DISTRIBUTIONS FOR PAIRS OF RANDOM V ARIABLES 179\nCONDITIONAL DISTRIBUTION\nThe conditional distribution pYjX(yjx) for a pair of random variables ( X;Y) is the collection\nof probabilities\nP(Y=yjjX=xi) =P(Y=yjandX=xi)\nP(X=xj)\nfor all pairs of values ( xi;yj) that the random variables XandYtake on.\nEXAMPLE 3.48\nIf it is known that the employee’s annual health care cost is $968, what is the conditional distribu-\ntion of the partner’s annual health care cost?\nNote that there is a di ﬀerent conditional distribution of Yfor every possible value of X; this prob-\nlem speciﬁcally asks for the conditional distribution of Ygiven thatX= $968.\npYjX($968j$968) =P(Y= $968jX= $968) =P(Y= $968 and X= $968)\nP(X= $968)=0:18\n0:30= 0:60\npYjX($988j$968) =P(Y= $988jX= $968) =P(Y= $988 and X= $968)\nP(X= $968)=0:12\n0:30= 0:40\nWith the knowledge that the employee’s annual health care cost is $968, there is a probability of\n0.60 that the partner’s cost is $968 and 0.40 that the partner’s cost is $988.\nGUIDED PRACTICE 3.49\nConsider two random variables, XandY, with the joint distribution shown in Figure 3.27.\n(a) Compute the marginal distributions of XandY.\n(b) Identify the joint probability pX;Y(1;2).\n(c) What is the value of pX;Y(2;1)?\n(d) Compute the conditional distribution of Xgiven thatY= 2.33\nY= 1Y= 2\nX= 1 0.20 0.40\nX= 4 0.30 0.10\nFigure 3.27: Joint distribution of XandY\nThe variance calculation in Example 3.8 relied on the assumption that the patterns of health\ncare expenses for the two partners were unrelated. In Example 3.48, 0.40 is the conditional prob-\nability that the partner’s health care costs will be $988, given that the employee’s cost is $968.\nThe marginal probability that the partner’s health care cost is $988 is 0.60, which is di ﬀerent from\n0.40. The patterns of health care costs are related in that knowing the value of the employee’s costs\nchanges the probabilities associated with partner’s costs. The marginal and conditional distribu-\ntions of the partner’s costs are not the same.\nThe notion of independence of two events discussed in Chapter 2 can be applied to the setting\nof random variables. Recall that two events AandBare independent if the conditional proba-\n33(a) The marginal distribution of X:pX(1) = 0:60,pX(4) = 0:40. The marginal distribution of Y:pY(1) = 0:50,pY(2) =\n0:50 (b)pX;Y(1;2) =P(X= 1;Y= 2) = 0:40 (c) Since Xcannot take on value 2, pX;Y(2;1) = 0. (d) The conditional distribution\nofXgiven thatY= 2:pXjY(1j2) =pX;Y(1;2)\npY(2)=0:40\n0:50= 0:80,pXjY(4j2) =pX;Y(4;2)\npY(2)=0:10\n0:50= 0:20.\n180 CHAPTER 3. DISTRIBUTIONS OF RANDOM V ARIABLES\nbilityP(AjB) equals the marginal probability P(A) or equivalently, if the product of the marginal\nprobabilities P(A) andP(B) equals the joint probability P(AandB).\nA pair (X;Y) of random variables are called independent random variables if the conditional\ndistribution for Y, given any value of X, is the same as the marginal distribution of Y. Addi-\ntionally, if all joint probabilities P(X=xi;Y=yj) that comprise the joint distribution of XandY\ncan be computed from the product of the marginal probabilities, P(X=xi)P(Y=yj),XandYare\nindependent.\nINDEPENDENT RANDOM VARIABLES\nTwo random variables XandYare independent if the probabilities\nP(Y=yjjX=xi) =P(Y=yj)\nfor all pairs of values ( xi;yj).\nEquivalently, XandYare independent if the probabilities\nP(Y=yjandX=xi) =P(Y=yj)P(X=xi)\nfor all pairs of values ( xi;yj).\nEXAMPLE 3.50\nDemonstrate that the employee’s health care costs and the partner’s health care costs are not inde-\npendent random variables.\nAs shown in Example 3.48, the conditional distribution of the partner’s annual health care cost\ngiven that the employee’s annual cost is $968 is P(Y= $968jX= $968) = 0 :60,P(Y= $988jX=\n$968) = 0:40. However, the marginal distribution of the partner’s annual health care cost is P(Y=\n$968) = 0:40,P(Y= $968) = 0:60. Thus,XandYare not independent.\nThis can also be demonstrated from examining the joint distribution, as shown in Figure 3.26.\nThe probability that the employee’s cost and partner’s cost are both $968 is 0.18. The marginal\nprobabilities P(X= $968) and P(Y= $968), respectively, are 0.30 and 0.40. Since (0 :40)(0:30),\n0:18,XandYare dependent random variables.\nNote that demonstrating P(Y=yjjX=xi) =P(Y=yj) orP(Y=yjandX=xi) =P(Y=yj)P(X=xi)\ndoes not hold for any one ( xi;yj) pair is su ﬃcient to prove that XandYare not independent, since\nindependence requires these conditions to hold over allpairs of values ( xi;yj).\nGUIDED PRACTICE 3.51\nBased on Figure 3.27, check whether XandYare independent.34\nTwo random variables that are not independent are called correlated random variables . The\ncorrelation between two random variables is a measure of the strength of the relationship between\nthem, just as it was for pairs of data points explored in Section 1.6.1. There are many examples\nof correlated random variables, such as height and weight in a population of individuals, or the\ngestational age and birth weight of newborns.\nWhen two random variables are positively correlated, they tend to increase or decrease to-\ngether. If one of the variables increases while the other decreases (or vice versa) they are negatively\n34XandYare not independent. One way to demonstrate this is to compare pX(1) withpXjY(1j2). IfXwere independent\nofY, then conditioning on Y= 2 should not provide any information about X, andpX(1) should equal pXjY(1j2). However,\npX(1) = 0:60 andpXjY(1j2) = 0:80. Thus,XandYare not independent.\n3.6. DISTRIBUTIONS FOR PAIRS OF RANDOM V ARIABLES 181\ncorrelated. Correlation is easy to identify in a scatterplot, but is more di ﬃcult to identify in a table\nof a joint distribution. Fortunately, there is a formula to calculate correlation for a joint distribution\nspeciﬁed in a table.\nCorrelation between random variables is similar to correlation between pairs of observations\nin a dataset, with some important di ﬀerences. Calculating a correlation rin a dataset was intro-\nduced in Section 1.6.1 and uses the formula:\nr=1\nn\u00001nX\ni=1 xi\u0000x\nsx! yi\u0000y\nsy!\n: (3.52)\nThe correlation coe ﬃcientris an average of products, with each term in the product measuring the\ndistance between xand its mean xand the distance between yand its mean y, after the distances\nhave been scaled by respective standard deviations.\nThe compact formula for the correlation between two random variables XandYuses the same\nidea:\n\u001aX;Y=E X\u0000\u0016x\n\u001bX! Y\u0000\u0016Y\n\u001bY!\n; (3.53)\nwhere\u001aX;Yis the correlation between the two variables, and \u0016X;\u0016Y,\u001bX;\u001bYare the respective means\nand standard deviations for XandY. Just as with the mean of a random variable, the expectation\nin the formula for correlation is a weighted sum of products, with each term weighted by the prob-\nability of values for the pair ( X;Y). Equation 3.53 is useful for understanding the analogy between\ncorrelation of random variables and correlation of observations in a dataset, but it cannot be used\nto calculate \u001aX;Ywithout the probability weights. The weights come from the joint distribution of\nthe pair of variables ( X;Y).\nEquation 3.54 is an expansion of Equation 3.53. The double summation adds up terms over\nall combinations of the indices iandj.\n\u001aX;Y=X\niX\njp(i;j)(xi\u0000\u0016X)\nsd(X)(yj\u0000\u0016Y)\nsd(Y): (3.54)\nEXAMPLE 3.55\nCompute the correlation between annual health care costs for the employee and her partner.\nAs calculated previously, E(X) = $1010, Var( X) = 1556,E(Y) = $980, and Var( Y) = 96. Thus,\nSD(X) = $39:45 andSD(Y) = $9:80.\n\u001aX;Y=p(x1;y1)(x1\u0000\u0016X)\nsd(X)(y1\u0000\u0016Y)\nsd(Y)+p(x1;y2)(x1\u0000\u0016X)\nsd(X)(y2\u0000\u0016Y)\nsd(Y)\n+\u0001\u0001\u0001+p(x4;y1)(x4\u0000\u0016X)\nsd(X)(y1\u0000\u0016Y)\nsd(Y)+p(x4;y2)(x4\u0000\u0016X)\nsd(X)(y2\u0000\u0016Y)\nsd(Y)\n= (0:18)(968\u00001010)\n39:45(968\u0000980)\n9:8+ (0:12)(968\u00001010)\n39:45(988\u0000980)\n9:8\n+\u0001\u0001\u0001+ (0:03)(1108\u00001010)\n39:45(968\u0000980)\n9:8+ (0:07)(1108\u00001010)\n39:45(988\u0000980)\n9:8\n= 0:22:\nThe correlation between annual health care costs for these two individuals is positive. It is reason-\nable to expect that there might be a positive correlation in health care costs for two individuals in a\nrelationship; for example, if one person contracts the ﬂu, then it is likely the other person will also\ncontract the ﬂu, and both may need to see a doctor.\n182 CHAPTER 3. DISTRIBUTIONS OF RANDOM V ARIABLES\nGUIDED PRACTICE 3.56\nBased on Figure 3.27, compute the correlation between XandY. For your convenience, the follow-\ning values are provided: E(X) = 2:2, Var(X) = 2:16,E(Y) = 1:5, Var(Y) = 0:25.35\nWhen two random variables XandYare correlated:\nVariance(X+Y) = Variance( X) + Variance( Y) + 2\u001bX\u001bYCorrelation( X;Y) (3.57)\nVariance(X\u0000Y) = Variance( X) + Variance( Y)\u00002\u001bX\u001bYCorrelation( X;Y): (3.58)\nWhen random variables are positively correlated the variance of the sum or the di ﬀerence of\ntwo variables will be larger than the sum of the two variances. When they are negatively correlated\nthe variance of the sum or di ﬀerence will be smaller than the sum of the two variances.\nThe standard deviation for the sum or di ﬀerence will always be the square root of the variance.\nEXAMPLE 3.59\nCalculate the standard deviation of the sum of the health care costs for the couple.\nThis calculation uses Equation 3.57 to calculate the variance of the sum. The standard deviation\nwill be the square root of the variance.\nVar(X+Y) = Var(X) + Var(Y) + 2\u001bX\u001bY\u001aX;Y\n= (1556 + 96) + (2)(39 :45)(9:80)(0:22)\n= 1822:10:\nThe standard deviation isp\n1822:10 = $42:69. Because the health care costs are correlated, the\nstandard deviation of the total cost is larger than the value calculated in Example 3.8 under the\nassumption that the annual costs were independent.\nGUIDED PRACTICE 3.60\nCompute the standard deviation of X\u0000Yfor the pair of random variables shown in Figure 3.27.36\n35The correlation between XandYis\u001aX;Y= (0:20)(1\u00002:2)p\n2:16(1\u00001:5)p\n0:25+\u0001\u0001\u0001+ (0:10)(4\u00002:2)p\n2:16(2\u00001:5)p\n0:25=\u00000:0208:\n36Var(X\u0000Y) = Var(X) + Var(Y)\u00002\u001bX\u001bY\u001aX;Y= 2:16 + 0:25\u00002(p\n2:16)(p\n0:25)(\u00000:0208) = 2:44. Thus,SD(X\u0000Y) =p\n2:44 =\n1:56:\n3.6. DISTRIBUTIONS FOR PAIRS OF RANDOM V ARIABLES 183\nEXAMPLE 3.61\nThe Association of American Medical Colleges (AAMC) introduced a new version of the Medical\nCollege Admission Test (MCAT) in the spring of 2015. Data from the scores were recently released\nby AAMC.37The test consists of 4 components: chemical and physical foundations of biological\nsystems; critical analysis and reasoning skills; biological and biochemical foundations of living\nsystems; psychological, social and biological foundations of behavior. The overall score is the sum\nof the individual component scores. The grading for each of the four components is scaled so that\nthe mean score is 125. The means and standard deviations for the four components and the total\nscores for the population taking the exam in May 2015 exam are shown in Figure 3.28.\nShow that the standard deviation in the table for the total score does not agree with that obtained\nunder the assumption of independence.\nThe variance of each component of the score is the square of each standard deviation. Under the\nassumption of independence, the variance of the total score would be\nVar(Total Score) = 3 :02+ 3:02+ 3:02+ 3:12\n= 36:61;\nso the standard deviation is 6.05, which is less than 10.6.\nSince the observed standard deviation is larger than that calculated under independence, this sug-\ngests the component scores are positively correlated.\nIt would not be reasonable to expect that the component scores are independent. Think about a\nstudent taking the MCAT exam: someone who scores well on one component of the exam is likely\nto score well on the other parts.\nComponent Mean Standard Deviation\nChem. Phys. Found. 125 3.0\nCrit. Analysis 125 3.0\nLiving Systems 125 3.0\nFound. Behavior 125 3.1\nTotal Score 500 10.6\nFigure 3.28: Means and Standard Deviations for MCAT Scores\n37https://www.aamc.org/students/download/434504/data/percentilenewmcat.pdf\n184 CHAPTER 3. DISTRIBUTIONS OF RANDOM V ARIABLES\n3.7 Notes\nThinking in terms of random variables and distributions of probabilities makes it easier to\ndescribe all possible outcomes of an experiment or process of interest, versus only considering\nprobabilities on the scale of individual outcomes or sets of outcomes. Several of the fundamental\nconcepts of probability can naturally be extended to probability distributions. For example, the\nprocess of obtaining a conditional distribution is analogous to the one for calculating a conditional\nprobability.\nMany processes can be modeled using a speciﬁc named distribution. The statistical techniques\ndiscussed in later chapters, such as hypothesis testing and regression, are often based on particular\ndistributional assumptions. In particular, many methods rely on the assumption that data are\nnormally distributed.\nThe discussion of random variables and their distribution provided in this chapter only rep-\nresents an introduction to the topic. In this text, properties of random variables such as expected\nvalue or correlation are presented in the context of discrete random variables; these concepts are\nalso applicable to continuous random variables. A course in probability theory will cover addi-\ntional named distributions as well as more advanced methods for working with distributions.\nLab 1 introduces the general notion of a random variable and its distribution using a simula-\ntion, then discusses the binomial distribution. Lab 2 discusses the normal distribution and working\nwith normal probabilities, as well as the Poisson distribution. Lab 3 covers the geometric, negative\nbinomial, and hypergeometric distributions. All three labs include practice problems that illus-\ntrate the use of Rfunctions for probability distributions and introduce additional features of the\nRprogramming language. Lab 4 discusses distributions for pairs of random variables and some R\nfunctions useful for matrix calculations.\n3.8. EXERCISES 185\n3.8 Exercises\n3.8.1 Random variables\n3.1 College smokers. At a university, 13% of students smoke.\n(a) Calculate the expected number of smokers in a random sample of 100 students from this university.\n(b) The university gym opens at 9 am on Saturday mornings. One Saturday morning at 8:55 am there are\n27 students outside the gym waiting for it to open. Should you use the same approach from part (a) to\ncalculate the expected number of smokers among these 27 students?\n3.2 Ace of clubs wins. Consider the following card game with a well-shu ﬄed deck of cards. If you draw a\nred card, you win nothing. If you get a spade, you win $5. For any club, you win $10 plus an extra $20 for the\nace of clubs.\n(a) Create a probability model for the amount you win at this game. Also, ﬁnd the expected winnings for a\nsingle game and the standard deviation of the winnings.\n(b) What is the maximum amount you would be willing to pay to play this game? Explain your reasoning.\n3.3 Hearts win. In a new card game, you start with a well-shu ﬄed full deck and draw 3 cards without\nreplacement. If you draw 3 hearts, you win $50. If you draw 3 black cards, you win $25. For any other draws,\nyou win nothing.\n(a) Create a probability model for the amount you win at this game, and ﬁnd the expected winnings. Also\ncompute the standard deviation of this distribution.\n(b) If the game costs $5 to play, what would be the expected value and standard deviation of the net proﬁt (or\nloss)?\n(c) If the game costs $5 to play, should you play this game? Explain.\n3.4 Baggage fees. An airline charges the following baggage fees: $25 for the ﬁrst bag and $35 for the\nsecond. Suppose 54% of passengers have no checked luggage, 34% have one piece of checked luggage and\n12% have two pieces. We suppose a negligible portion of people check more than two bags.\n(a) Build a probability model, compute the average revenue per passenger, and compute the corresponding\nstandard deviation.\n(b) About how much revenue should the airline expect for a ﬂight of 120 passengers? With what standard\ndeviation? Note any assumptions you make and if you think they are justiﬁed.\n3.5 Gull clutch size. Large black-tailed gulls usually lay one to three eggs, and rarely have a fourth egg\nclutch. It is thought that clutch sizes are e ﬀectively limited by how e ﬀectively parents can incubate their eggs.\nSuppose that on average, gulls have a 25% of laying 1 egg, 40% of laying 2 eggs, 30% chance of laying 3 eggs,\nand 5% chance of laying 4 eggs.\n(a) Calculate the expected number of eggs laid by a random sample of 100 gulls.\n(b) Calculate the standard deviation of the number of eggs laid by a random sample of 100 gulls.\n186 CHAPTER 3. DISTRIBUTIONS OF RANDOM V ARIABLES\n3.6 Scooping ice cream. Ice cream usually comes in 1.5 quart boxes (48 ﬂuid ounces), and ice cream\nscoops hold about 2 ounces. However, there is some variability in the amount of ice cream in a box as well\nas the amount of ice cream scooped out. We represent the amount of ice cream in the box as Xand the\namount scooped out as Y. Suppose these random variables have the following means, standard deviations,\nand variances:\nmean SD variance\nX 48 1 1\nY 2 0.25 0.0625\n(a) An entire box of ice cream, plus 3 scoops from a second box is served at a party. How much ice cream do\nyou expect to have been served at this party? What is the standard deviation of the amount of ice cream\nserved?\n(b) How much ice cream would you expect to be left in the box after scooping out one scoop of ice cream?\nThat is, ﬁnd the expected value of X\u0000Y. What is the standard deviation of the amount left in the box?\n(c) Using the context of this exercise, explain why we add variances when we subtract one random variable\nfrom another.\n3.8.2 Binomial distribution\n3.7 Underage drinking, Part I. Data collected by the Substance Abuse and Mental Health Services Adminis-\ntration (SAMSHA) suggests that 69.7% of 18-20 year olds consumed alcoholic beverages in any given year.38\n(a) Suppose a random sample of ten 18-20 year olds is taken. Is the use of the binomial distribution appro-\npriate for calculating the probability that exactly six consumed alcoholic beverages? Explain.\n(b) Calculate the probability that exactly 6 out of 10 randomly sampled 18- 20 year olds consumed an alco-\nholic drink.\n(c) What is the probability that exactly four out of ten 18-20 year olds have notconsumed an alcoholic bever-\nage?\n(d) What is the probability that at most 2 out of 5 randomly sampled 18-20 year olds have consumed alcoholic\nbeverages?\n(e) What is the probability that at least 1 out of 5 randomly sampled 18-20 year olds have consumed alcoholic\nbeverages?\n3.8 Chickenpox, Part I. The US CDC estimates that 90% of Americans have had chickenpox by the time\nthey reach adulthood.\n(a) Suppose we take a random sample of 100 American adults. Is the use of the binomial distribution appro-\npriate for calculating the probability that exactly 97 out of 100 randomly sampled American adults had\nchickenpox during childhood? Explain.\n(b) Calculate the probability that exactly 97 out of 100 randomly sampled American adults had chickenpox\nduring childhood.\n(c) What is the probability that exactly 3 out of a new sample of 100 American adults have nothad chickenpox\nin their childhood?\n(d) What is the probability that at least 1 out of 10 randomly sampled American adults have had chickenpox?\n(e) What is the probability that at most 3 out of 10 randomly sampled American adults have nothad chick-\nenpox?\n38SAMHSA, O ﬃce of Applied Studies, National Survey on Drug Use and Health, 2007 and 2008.\n3.8. EXERCISES 187\n3.9 Underage drinking, Part II. We learned in Exercise 3.7 that about 70% of 18-20 year olds consumed\nalcoholic beverages in any given year. We now consider a random sample of ﬁfty 18-20 year olds.\n(a) How many people would you expect to have consumed alcoholic beverages? And with what standard\ndeviation?\n(b) Would you be surprised if there were 45 or more people who have consumed alcoholic beverages?\n(c) What is the probability that 45 or more people in this sample have consumed alcoholic beverages? How\ndoes this probability relate to your answer to part (b)?\n3.10 Chickenpox, Part II. We learned in Exercise 3.8 that about 90% of American adults had chickenpox\nbefore adulthood. We now consider a random sample of 120 American adults.\n(a) How many people in this sample would you expect to have had chickenpox in their childhood? And with\nwhat standard deviation?\n(b) Would you be surprised if there were 105 people who have had chickenpox in their childhood?\n(c) What is the probability that 105 or fewer people in this sample have had chickenpox in their childhood?\nHow does this probability relate to your answer to part (b)?\n3.11 Donating blood. When patients receive blood transfusions, it is critical that the blood type of the donor\nis compatible with the patients, or else an immune system response will be triggered. For example, a patient\nwith Type O- blood can only receive Type O- blood, but a patient with Type O+ blood can receive either Type\nO+ or Type O-. Furthermore, if a blood donor and recipient are of the same ethnic background, the chance\nof an adverse reaction may be reduced. According to a 10-year donor database, 0.37 of white, non-Hispanic\ndonors are O+ and 0.08 are O-.\n(a) Consider a random sample of 15 white, non-Hispanic donors. Calculate the expected value of individuals\nwho could be a donor to a patient with Type O+ blood. With what standard deviation?\n(b) What is the probability that 3 or more of the people in this sample could donate blood to a patient with\nType O- blood?\n3.12 Sickle cell anemia. Sickle cell anemia is a genetic blood disorder where red blood cells lose their\nﬂexibility and assume an abnormal, rigid, “sickle\" shape, which results in a risk of various complications.\nIf both parents are carriers of the disease, then a child has a 25% chance of having the disease, 50% chance\nof being a carrier, and 25% chance of neither having the disease nor being a carrier. If two parents who are\ncarriers of the disease have 3 children, what is the probability that\n(a) two will have the disease?\n(b) none will have the disease?\n(c) at least one will neither have the disease nor be a carrier?\n(d) the ﬁrst child with the disease will the be 3rdchild?\n3.13 Hepatitis C. Hepatitis C is spread primarily through contact with the blood of an infected person,\nand is nearly always transmitted through needle sharing among intravenous drug users. Suppose that in a\nmonth’s time, an IV drug user has a 30% chance of contracting hepatitis C through needle sharing. What is\nthe probability that 3 out of 5 IV drug users contract hepatitis C in a month? Assume that the drug users live\nin diﬀerent parts of the country.\n3.14 Arachnophobia. A Gallup Poll found that 7% of teenagers (ages 13 to 17) su ﬀer from arachnophobia\nand are extremely afraid of spiders. At a summer camp there are 10 teenagers sleeping in each tent. Assume\nthat these 10 teenagers are independent of each other.39\n(a) Calculate the probability that at least one of them su ﬀers from arachnophobia.\n(b) Calculate the probability that exactly 2 of them su ﬀer from arachnophobia.\n(c) Calculate the probability that at most 1 of them su ﬀers from arachnophobia.\n(d) If the camp counselor wants to make sure no more than 1 teenager in each tent is afraid of spiders, does\nit seem reasonable for him to randomly assign teenagers to tents?\n39Gallup Poll, What Frightens America’s Youth?, March 29, 2005.\n188 CHAPTER 3. DISTRIBUTIONS OF RANDOM V ARIABLES\n3.15 Wolbachia infection. Approximately 12,500 stocks of Drosophila melanogaster ﬂies are kept at The\nBloomington Drosophila Stock Center for research purposes. A 2006 study examined how many stocks were\ninfected with Wolbachia, an intracellular microbe that can manipulate host reproduction for its own beneﬁt.\nAbout 30% of stocks were identiﬁed as infected. Researchers working with infected stocks should be cautious\nof the potential confounding e ﬀects that Wolbachia infection may have on experiments. Consider a random\nsample of 250 stocks.\n(a) Calculate the probability that exactly 60 stocks are infected.\n(b) Calculate the probability that at most 60 stocks are infected.\n(c) Calculate the probability that at least 80 stocks are infected.\n(d) If a researcher wants to make sure that no more than 40% of the stocks used for an experiment are infected,\ndoes it seem reasonable to take a random sample of 250?\n3.16 Male children. While it is often assumed that the probabilities of having a boy or a girl are the same,\nthe actual probability of having a boy is slightly higher at 0.51. Suppose a couple plans to have 3 kids.\n(a) Use the binomial model to calculate the probability that two of them will be boys.\n(b) Write out all possible orderings of 3 children, 2 of whom are boys. Use these scenarios to calculate the\nsame probability from part (a) but using the addition rule for disjoint outcomes. Conﬁrm that your\nanswers from parts (a) and (b) match.\n(c) If we wanted to calculate the probability that a couple who plans to have 8 kids will have 3 boys, brieﬂy\ndescribe why the approach from part (b) would be more tedious than the approach from part (a).\n3.17 Hyponatremia. Hyponatremia (low sodium levels) occurs in a certain proportion of marathon runners\nduring a race. Suppose that historically, the proportion of runners who develop hyponatremia is 0.12. In a\ncertain marathon, there are 200 runners participating.\n(a) How many cases of hyponatremia are expected during the marathon?\n(b) What is the probability of more than 30 cases of hyponatremia occurring?\n3.18 Sleep deprivation. Consider a senior Statistics concentrator with a packed extracurricular schedule,\ntaking ﬁve classes, and writing a thesis. Each time she takes an exam, she either scores very well (at least two\nstandard deviations above the mean) or does not. Her performance on any given exam depends on whether\nshe is operating on a reasonable amount of sleep the night before (more than 7 hours), relatively little sleep\n(between 4 - 7 hours, inclusive), or practically no sleep (less than 4 hours).\nWhen she has had practically no sleep, she scores very well about 30% of the time. When she has had\nrelatively little sleep, she scores very well 40% of the time. When she has had a reasonable amount of sleep,\nshe scores very well 42% of the time. Over the course of a semester, she has a reasonable amount of sleep 50%\nof nights, and practically no sleep 30% of nights.\n(a) What is her overall probability of scoring very well on an exam?\n(b) What is the probability she had practically no sleep the night before an exam where she scored very well?\n(c) Suppose that one day she has three exams scheduled. What is the probability that she scores very well on\nexactly two of the exams, under the assumption that her performance on each exam is independent of her\nperformance on another exam?\n(d) What is the probability that she had practically no sleep the night prior to a day when she scored very\nwell on exactly two out of three exams?\n3.8. EXERCISES 189\n3.8.3 Normal distribution\n3.19 Area under the curve, Part I. What percent of a standard normal distribution N(\u0016= 0;\u001b= 1) is found\nin each region? Be sure to draw a graph.\n(a)Z<\u00001:35 (b) Z>1:48 (c) \u00000:4<Z< 1:5 (d)jZj>2\n3.20 Area under the curve, Part II. What percent of a standard normal distribution N(\u0016= 0;\u001b= 1) is found\nin each region? Be sure to draw a graph.\n(a)Z>\u00001:13 (b) Z<0:18 (c) Z>8 (d) jZj<0:5\n3.21 The standard normal distribution. Consider the standard normal distribution with mean \u0016= 0 and\nstandard deviation \u001b= 1.\n(a) What is the probability that an outcome Zis greater than 2.60?\n(b) What is the probability that Zis less than 1.35?\n(c) What is the probability that Zis between -1.70 and 3.10?\n(d) What value of Zcuts oﬀthe upper 15% of the distribution?\n(e) What value of Zmarks o ﬀthe lower 20% of the distribution?\n3.22 Triathlon times. In triathlons, it is common for racers to be placed into age and gender groups. The\nﬁnishing times of men ages 30-34 has mean of 4,313 seconds with a standard deviation of 583 seconds. The\nﬁnishing times of the women ages 25-29 has a mean of 5,261 seconds with a standard deviation of 807 seconds.\nThe distribution of ﬁnishing times for both groups is approximately normal. Note that a better performance\ncorresponds to a faster ﬁnish.\n(a) If a man of the 30-34 age group ﬁnishes the race in 4,948 seconds, what percent of the triathletes in the\ngroup did he ﬁnish faster than?\n(b) If a woman of the 25-29 age group ﬁnishes the race in 5,513 seconds, what percent of the triathletes in\nthe group did she ﬁnish faster than?\n(c) Calculate the cuto ﬀtime for the fastest 5% of athletes in the men’s group.\n(d) Calculate the cuto ﬀtime for the slowest 10% of athletes in the women’s group.\n3.23 GRE scores. The Graduate Record Examination (GRE) is a standardized test commonly taken by grad-\nuate school applicants in the United States. The total score is comprised of three components: Quantitative\nReasoning, Verbal Reasoning, and Analytical Writing. The ﬁrst two components are scored from 130 - 170.\nThe mean score for Verbal Reasoning section for all test takers was 151 with a standard deviation of 7, and\nthe mean score for the Quantitative Reasoning was 153 with a standard deviation of 7.67. Suppose that both\ndistributions are nearly normal.\n(a) A student scores 160 on the Verbal Reasoning section and 157 on the Quantitative Reasoning section.\nRelative to the scores of other students, which section did the student perform better on?\n(b) Calculate the student’s percentile scores for the two sections. What percent of test takers performed better\non the Verbal Reasoning section?\n(c) Compute the score of a student who scored in the 80thpercentile on the Quantitative Reasoning section.\n(d) Compute the score of a student who scored worse than 70% of the test takers on the Verbal Reasoning\nsection.\n3.24 Osteoporosis. The World Health Organization deﬁnes osteoporosis in young adults as a measured\nbone mineral density 2.5 or more standard deviations below the mean for young adults. Assume that bone\nmineral density follows a normal distribution in young adults. What percentage of young adults su ﬀer from\nosteoporosis according to this criterion?\n190 CHAPTER 3. DISTRIBUTIONS OF RANDOM V ARIABLES\n3.25 LA weather. The average daily high temperature in June in LA is 77\u000eF with a standard deviation of\n5\u000eF. Suppose that the temperatures in June closely follow a normal distribution.\n(a) What is the probability of observing an 83\u000eF temperature or higher in LA during a randomly chosen day\nin June?\n(b) How cold are the coldest 10% of the days during June in LA?\n3.26 Clutch volume. A study investigating maternal investment in a frog species found on the Tibetan\nPlateau reported data on the volume of egg clutches measured across 11 study sites. The distribution is\nroughly normal, with approximate distribution N(882:5;380)mm3.\n(a) What is the probability of observing an egg clutch between volume 700-800 mm3?\n(b) How large are the largest 5% of egg clutches?\n3.27 Glucose levels. Fasting blood glucose levels for normal non-diabetic individuals are normally dis-\ntributed in the population, with mean \u0016= 85 mg/dL and standard deviation \u001b= 7:5 mg/dL.\n(a) What is the probability that a randomly chosen member of the population has a fasting glucose level\nhigher than 100 mg/dL?\n(b) What value of fasting glucose level deﬁnes the lower 5thpercentile of the distribution?\n3.28 Arsenic poisoning. Arsenic blood concentration is normally distributed with mean \u0016= 3:2\u0016g/dl and\nstandard deviation \u001b= 1:5\u0016g/dl. What range of arsenic blood concentration deﬁnes the middle 95% of this\ndistribution?\n3.29 Age at childbirth. In the last decade, the average age of a mother at childbirth is 26.4 years, with\nstandard deviation 5.8 years. The distribution of age at childbirth is approximately normal.\n(a) What proportion of women who give birth are 21 years of age or older?\n(b) Giving birth at what age puts a woman in the upper 2.5% of the age distribution?\n3.30 Find the SD. Find the standard deviation of the distribution in the following situations.\n(a) MENSA is an organization whose members have IQs in the top 2% of the population. IQs are normally\ndistributed with mean 100, and the minimum IQ score required for admission to MENSA is 132.\n(b) Cholesterol levels for women aged 20 to 34 follow an approximately normal distribution with mean 185\nmilligrams per deciliter (mg/dl). Women with cholesterol levels above 220 mg/dl are considered to have\nhigh cholesterol and about 18.5% of women fall into this category.\n3.31 Underage drinking, Part III. As ﬁrst referenced in Exercise 3.7, about 70% of 18-20 year olds consumed\nalcoholic beverages in 2008. Consider a random sample of ﬁfty 18-20 year olds.\n(a) Of these ﬁfty people, how many would be expected to have consumed alcoholic beverages? With what\nstandard deviation?\n(b) Evaluate the conditions for using the normal approximation to the binomial. What is the probability that\n45 or more people in this sample have consumed alcoholic beverages?\n3.32 Chickenpox, Part III. As ﬁrst referenced in Exercise 3.8, about 90% of American adults had chickenpox\nbefore adulthood. Consider a random sample of 120 American adults.\n(a) How many people in this sample would be expected to have had chickenpox in their childhood? With\nwhat standard deviation?\n(b) Evaluate the conditions for using the normal approximation to the binomial. What is the probability that\n105 or fewer people in this sample have had chickenpox in their childhood?\n3.8. EXERCISES 191\n3.33 University admissions. Suppose a university announced that it admitted 2,500 students for the fol-\nlowing year’s freshman class. However, the university has dorm room spots for only 1,786 freshman students.\nIf there is a 70% chance that an admitted student will decide to accept the o ﬀer and attend this university,\nwhat is the approximate probability that the university will not have enough dormitory room spots for the\nfreshman class?\n3.34 SAT scores. SAT scores (out of 2400) are distributed normally with a mean of 1500 and a standard\ndeviation of 300. Suppose a school council awards a certiﬁcate of excellence to all students who score at least\n1900 on the SAT, and suppose we pick one of the recognized students at random. What is the probability this\nstudent’s score will be at least 2100? (The material covered in Section 2.2 would be useful for this question.)\n3.35 Scores on stats ﬁnal. The ﬁnal exam scores of 20 introductory statistics students are plotted below.\nDo these data appear to follow a normal distribution? Explain your reasoning.\nScores60 70 80 90\n●●\n●●●\n●\n●●●\n●\n●●\n●●\n●●●\n●●●\nTheoretical QuantilesSample Quantiles\n−2 −1 0 1 260708090\n3.36 Heights of female college students. The heights of 25 female college students are plotted below. Do\nthese data appear to follow a normal distribution? Explain your reasoning.\nHeights505560657075\n●●●●●●●●●●●●●●●●●●●●●●●●●\nTheoretical QuantilesSample Quantiles\n−2 −1 0 1 255606570\n3.8.4 Poisson distribution\n3.37 Computing Poisson probabilities. This is a simple exercise in computing probabilities for a Poisson\nrandom variable. Suppose that Xis a Poisson random variable with rate parameter \u0015= 2. Calculate P(X= 2),\nP(X\u00142), andP(X\u00153).\n3.38 Stenographer’s typos. A very skilled court stenographer makes one typographical error (typo) per\nhour on average.\n(a) What are the mean and the standard deviation of the number of typos this stenographer makes in an\nhour?\n(b) Calculate the probability that this stenographer makes at most 3 typos in a given hour.\n(c) Calculate the probability that this stenographer makes at least 5 typos over 3 hours.\n192 CHAPTER 3. DISTRIBUTIONS OF RANDOM V ARIABLES\n3.39 Customers at a coffee shop. A coﬀee shop serves an average of 75 customers per hour during the\nmorning rush.\n(a) What are the mean and the standard deviation of the number of customers this co ﬀee shop serves in one\nhour during this time of day?\n(b) Would it be considered unusually low if only 60 customers showed up to this co ﬀee shop in one hour\nduring this time of day?\n(c) Calculate the probability that this co ﬀee shop serves 70 customers in one hour during this time of day.\n3.40 Osteosarcoma in NYC. Osteosarcoma is a relatively rare type of bone cancer. It occurs most often\nin young adults, age 10 - 19; it is diagnosed in approximately 8 per 1,000,000 individuals per year in that\nage group. In New York City (including all ﬁve boroughs), the number of young adults in this age range is\napproximately 1,400,000.\n(a) What is the expected number of cases of osteosarcoma in NYC in a given year?\n(b) What is the probability that 15 or more cases will be diagnosed in a given year?\n(c) The largest concentration of young adults in NYC is in the borough of Brooklyn, where the population\nin that age range is approximately 450,000. What is the probability of 10 or more cases in Brooklyn in a\ngiven year?\n(d) Suppose that in a given year, 10 cases of osteosarcoma were observed in NYC, with all 10 cases occurring\namong young adults living in Brooklyn. An o ﬃcial from the NYC Public Health Department claims that\nthe probability of this event (that is, the probability of 10 or more cases being observed, and all of them\noccurring in Brooklyn) is what was calculated in part c). Is the o ﬃcial correct? Explain your answer.\nYou may assume that your answer to part c) is correct. This question can be answered without doing any\ncalculations.\n(e) Suppose that over ﬁve years, there was one year in which 10 or more cases of osteosarcoma were observed\nin Brooklyn. Is the probability of this event equal to the probability calculated in part c)? Explain your\nanswer.\n3.41 How many cars show up? For Monday through Thursday when there isn’t a holiday, the average\nnumber of vehicles that visit a particular retailer between 2pm and 3pm each afternoon is 6.5, and the number\nof cars that show up on any given day follows a Poisson distribution.\n(a) What is the probability that exactly 5 cars will show up next Monday?\n(b) What is the probability that 0, 1, or 2 cars will show up next Monday between 2pm and 3pm?\n(c) There is an average of 11.7 people who visit during those same hours from vehicles. Is it likely that the\nnumber of people visiting by car during this hour is also Poisson? Explain.\n3.42 Lost baggage. Occasionally an airline will lose a bag. Suppose a small airline has found it can reason-\nably model the number of bags lost each weekday using a Poisson model with a mean of 2.2 bags.\n(a) What is the probability that the airline will lose no bags next Monday?\n(b) What is the probability that the airline will lose 0, 1, or 2 bags on next Monday?\n(c) Suppose the airline expands over the course of the next 3 years, doubling the number of ﬂights it makes,\nand the CEO asks you if it’s reasonable for them to continue using the Poisson model with a mean of 2.2.\nWhat is an appropriate recommendation? Explain.\n3.8. EXERCISES 193\n3.43 Hemophilia. Hemophilia is a sex-linked bleeding disorder that slows the blood clotting process. In\nsevere cases of hemophilia, continued bleeding occurs after minor trauma or even in the absence of injury.\nHemophilia a ﬀects 1 in 5,000 male births. In the United States, about 400 males are born with hemophilia\neach year; there are approximately 4,000,000 births per year. Note: this problem is best done using statistical\nsoftware.\n(a) What is the probability that at most 380 newborns in a year are born with hemophilia?\n(b) What is the probability that 450 or more newborns in a year are born with hemophilia?\n(c) Consider a hypothetical country in which there are approximately 1.5 million births per year. If the inci-\ndence rate of hemophilia is equal to that in the US, how many newborns are expected to have hemophilia\nin a year, with what standard deviation?\n3.44 Opioid overdose. The US Centers for Disease Control (CDC) has been monitoring the rate of deaths\nfrom opioid overdoses for at least the last 15 years. In 2013, the rate of opioid-related deaths has risen to 6.8\ndeaths per year per 100,000 non-Hispanic white members. In 2014-2015, the population of Essex County,\nMA, was approximately 769,000, of whom 73% are non-Hispanic white. Assume that incidence rate of opioid\ndeaths in Essex County is the same as the 2013 national rate. Note: this problem is best done using statistical\nsoftware.\n(a) In 2014, Essex County reported 146 overdose fatalities from opioids. Assume that all of these deaths\noccurred in the non-Hispanic white members of the population. What is the probability of 146 or more\nsuch events a year?\n(b) What was the observed rate of opioid-related deaths in Essex County in 2014, stated in terms of deaths\nper 100,000 non-Hispanic white members of the population?\n(c) In 2015, Essex County reported 165 opioid-related deaths in its non-Hispanic white population. Using\nthe rate from part (b), calculate the probability of 165 or more such events.\n3.8.5 Distributions related to Bernoulli trials\n3.45 Married women. The 2010 American Community Survey estimates that 47.1% of women ages 15 years\nand over are married. Suppose that a random sample of women in this age group are selected for a research\nstudy.40\n(a) On average, how many women would need to be sampled in order to select a married woman? What is\nthe standard deviation?\n(b) If the proportion of married women were actually 30%, what would be the new mean and standard\ndeviation?\n(c) Based on the answers to parts (a) and (b), how does decreasing the probability of an event a ﬀect the mean\nand standard deviation of the wait time until success?\n40U.S. Census Bureau, 2010 American Community Survey, Marital Status.\n194 CHAPTER 3. DISTRIBUTIONS OF RANDOM V ARIABLES\n3.46 Donating blood, Part II. Recall from Problem 3.11 that a patient with Type O+ blood can receive either\nType O+ or Type O- blood, while a patient with Type O- blood can only receive Type O- blood. According to\ndata collected from blood donors, 0.37 of white, non-Hispanic donors are Type O+ and 0.08 are Type O-. For\nthe following questions, assume that only white, non-Hispanic donors are being tested.\n(a) On average, how many donors would need to be randomly sampled for a Type O+ donor to be identiﬁed?\nWith what standard deviation?\n(b) What is the probability that 4 donors must be sampled to identify a Type O+ donor?\n(c) What is the probability that more than 4 donors must be sampled to identify a Type O+ donor?\n(d) What is the probability of the ﬁrst Type O- donor being found within the ﬁrst 4 people?\n(e) On average, how many donors would need to be randomly sampled for a Type O- donor to be identiﬁed?\nWith what standard deviation?\n(f) What is the probability that fewer than 4 donors must be tested before a Type O- donor is found?\n3.47 Wolbachia infection, Part II. Recall from Problem 3.15 that 30% of the Drosophila stocks at the BDSC\nare infected with Wolbachia. Suppose a research assistant randomly samples a stock one at a time until\nidentifying an infected stock.\n(a) Calculate the probability that an infected stock is found within the ﬁrst 5 stocks sampled.\n(b) What is the probability that no more than 5 stocks must be tested before an infected one is found?\n(c) Calculate the probability that at least 3 stocks must be tested for an infected one to be found.\n3.48 With and without replacement. In the following situations assume that half of the speciﬁed population\nis male and the other half is female.\n(a) Suppose you’re sampling from a room with 10 people. What is the probability of sampling two females\nin a row when sampling with replacement? What is the probability when sampling without replacement?\n(b) Now suppose you’re sampling from a stadium with 10,000 people. What is the probability of sampling\ntwo females in a row when sampling with replacement? What is the probability when sampling without\nreplacement?\n(c) We often treat individuals who are sampled from a large population as independent. Using your ﬁndings\nfrom parts (a) and (b), explain whether or not this assumption is reasonable.\n3.49 Eye color. A husband and wife both have brown eyes but carry genes that make it possible for their\nchildren to have brown eyes (probability 0.75), blue eyes (0.125), or green eyes (0.125).\n(a) What is the probability the ﬁrst blue-eyed child they have is their third child? Assume that the eye colors\nof the children are independent of each other.\n(b) On average, how many children would such a pair of parents have before having a blue-eyed child? What\nis the standard deviation of the number of children they would expect to have until the ﬁrst blue-eyed\nchild?\n3.8. EXERCISES 195\n3.50 Defective rate. A machine that produces a special type of transistor (a component of computers) has\na 2% defective rate. The production is considered a random process where each transistor is independent of\nthe others.\n(a) What is the probability that the 10thtransistor produced is the ﬁrst with a defect?\n(b) What is the probability that the machine produces no defective transistors in a batch of 100?\n(c) On average, how many transistors would you expect to be produced before the ﬁrst with a defect? What\nis the standard deviation?\n(d) Another machine that also produces transistors has a 5% defective rate where each transistor is produced\nindependent of the others. On average how many transistors would you expect to be produced with this\nmachine before the ﬁrst with a defect? What is the standard deviation?\n(e) Based on your answers to parts (c) and (d), how does increasing the probability of an event a ﬀect the mean\nand standard deviation of the wait time until success?\n3.51 Rolling a die. Calculate the following probabilities and indicate which probability distribution model\nis appropriate in each case. You roll a fair die 5 times. What is the probability of rolling\n(a) the ﬁrst 6 on the ﬁfth roll?\n(b) exactly three 6s?\n(c) the third 6 on the ﬁfth roll?\n3.52 Playing darts. Calculate the following probabilities and indicate which probability distribution model\nis appropriate in each case. A very good darts player can hit the direct center of the board 65% of the time.\nWhat is the probability that a player:\n(a) hits the bullseye for the 10thtime on the 15thtry?\n(b) hits the bullseye 10 times in 15 tries?\n(c) hits the ﬁrst bullseye on the third try?\n3.53 Cilantro preference. Cilantro leaves are widely used in many world cuisines. While some people enjoy\nit, others claim that it has a soapy, pungent aroma. A recent study conducted on participants of European\nancestry identiﬁed a genetic variant that is associated with soapy-taste detection. In the initial questionnaire,\n1,994 respondents out of 14,604 reported that they thought cilantro tasted like soap. Suppose that partici-\npants are randomly selected one by one.\n(a) What is the probability that the ﬁrst soapy-taste detector is the third person selected?\n(b) What is the probability that in a sample of ten people, no more than two are soapy-taste detectors?\n(c) What is the probability that three soapy-taste detectors are identiﬁed from sampling ten people?\n(d) What is the mean and standard deviation of the number of people that must be sampled if the goal is to\nidentify four soapy-taste detectors?\n3.54 Serving in volleyball. A not-so-skilled volleyball player has a 15% chance of making the serve, which\ninvolves hitting the ball so it passes over the net on a trajectory such that it will land in the opposing team’s\ncourt. Suppose that serves are independent of each other.\n(a) What is the probability that on the 10thtry, the player makes their 3rdsuccessful serve?\n(b) Suppose that the player has made two successful serves in nine attempts. What is the probability that\ntheir 10thserve will be successful?\n(c) Even though parts (a) and (b) discuss the same scenario, explain the reason for the discrepancy in proba-\nbilities.\n196 CHAPTER 3. DISTRIBUTIONS OF RANDOM V ARIABLES\n3.55 Cilantro preference, Part II. Recall from Problem 3.53 that in a questionnaire, 1,994 respondents out of\n14,604 reported that they thought cilantro tasted like soap. Suppose that a random sample of 15 individuals\nare selected for further study.\n(a) What is the mean and variance of the number of people sampled that are soapy-taste detectors?\n(b) What is the probability that 4 of the people sampled are soapy-taste detectors?\n(c) What is the probability that at most 2 of the people sampled are soapy-taste detectors?\n(d) Suppose that the 15 individuals were sampled with replacement. What is the probability of selecting 4\nsoapy-taste detectors?\n(e) Compare the answers from parts (b) and (d). Explain why the answers are essentially the same.\n3.56 Dental caries. A study to examine oral health of schoolchildren in Belgium found that of the 4,351\nchildren examined, 44% were caries free (i.e., free of decay, restorations, and missing teeth). Suppose that\nchildren are sampled one by one.\n(a) What is the probability that at least three caries free children are identiﬁed from sampling seven children?\n(b) What is the probability that the ﬁrst caries free child is the second one selected?\n(c) Suppose that in a single school of 350 children, the incidence rate of caries equals the national rate. If 10\nschoolchildren are selected at random, what is the probability that at most 2 have caries?\n(d) What is the probability that in a sample of 50 children, no more than 15 are caries free?\n3.8.6 Distributions for pairs of random variables\n3.57 Joint distributions, Part I. SupposeXandYhave the following joint distribution.\nY = -1 Y = 1\nX = 0 0.20 0.40\nX = 1 0.30 0.10\n(a) Calculate the marginal distributions of XandY.\n(b) Calculate the mean, variance, and standard deviation of X.\n(c) What are the standardized values of X?\n(d) The mean and standard deviation of Yare 0 and 1, respectively, and the two standardized values for Y\nare -1 and 1. Calculate \u001aX;Y, the correlation coe ﬃcient ofXandY.\n(e) AreXandYindependent? Explain your answer.\n3.58 Joint distributions, Part II. SupposeXandYhave the following joint distribution.\nY = -1 Y = 1\nX = 0 0.25 0.25\nX = 1 0.25 0.25\n(a) AreXandYindependent?\n(b) Calculate the correlation between XandY.\n(c) Are your answers to parts (a) and (b) consistent? Explain your answer.\n3.8. EXERCISES 197\n3.59 Joint distributions, Part III. Consider the following joint probability distribution:\nY\nX -1 0 1\n-1 0.10 0 0.35\n0 0 0.10 0.10\n1 0.15 0.10 0.10\n(a) Calculate the marginal distributions.\n(b) Compute \u0016X.\n(c) Compute \u001b2\nY.\n(d) Calculate the conditional distribution of X, givenY= 0.\n3.60 Dice rolls and coin tosses. LetXrepresent the outcome from a roll of a fair six-sided die. Then, toss\na fair coinXtimes and let Ydenote the number of tails observed.\n(a) Consider the joint probability table of XandY. How many entries are in the table for the joint distribution\nofXandY? How many entries equal 0?\n(b) Compute the joint probability P(X= 1;Y= 0).\n(c) Compute the joint probability P(X= 1;Y= 2).\n(d) Compute the joint probability P(X= 6;Y= 3).\n3.61 Health insurance claims. In the health insurance example introduced in Example 3.6, the largest an-\nnual expense for the annual employee ($1,108) was caused by 8 visits to a provider for a knee injury requiring\nphysical therapy. The couple has conﬁdence that this or a similar injury will not happen again in the coming\nyear, and wonders about the e ﬀect of reduced visits on expected total health care costs and its variability.\nA new joint distribution of health care cost for the couple is shown in the following table:\nPartner costs, Y\nEmployee costs, X $968 $988\n$968 0.18 0.12\n$1,008 0.15 0.25\n$1,028 0.07 0.23\n(a) For the partner, will there be a change to the expected cost and its standard deviation?\n(b) Calculated the expected value and standard deviation for the employee’s costs.\n(c) Calculate the expected total cost for the couple.\n(d) Calculate the new correlation for employee and partner costs.\n(e) Calculate the standard deviation of the total cost.\n198\nChapter 4\nFoundations for inference\n4.1 Variability in estimates\n4.2 Conﬁdence intervals\n4.3 Hypothesis testing\n4.4 Notes\n4.5 Exercises\n199\nNot surprisingly, many studies are now demonstrating the adverse e ﬀect of obe-\nsity on health outcomes. A 2017 study conducted by the consortium studying the\nglobal burden of disease estimates that high body mass index (a measure of body\nfat that adjusts for height and weight) may account for as many as 4.0 million\ndeaths globally.1In addition to the physiologic e ﬀects of being overweight, other\nstudies have shown that perceived weight status (feeling that one is overweight or\nunderweight) may have a signiﬁcant e ﬀect on self-esteem.2,3\nAs stated in its mission statement, the United States Centers for Disease Con-\ntrol and Prevention (US CDC) \"serves as the national focus for developing and\napplying disease prevention and control, environmental health, and health pro-\nmotion and health education activities designed to improve the health of the peo-\nple of the United States\".4Since it is not feasible to measure the health status and\noutcome of every single US resident, the CDC estimates features of health from\nsamples taken from the population, via large surveys that are repeated period-\nically. These surveys include the National Health Interview Survey (NHIS), the\nNational Health and Nutrition Examination Survey (NHANES), the Youth Risk\nBehavior Surveillance System (YRBSS) and the Behavior Risk Factor Surveillance\nSystem (BRFSS). In the language of statistics, the average weight of all US adults is\napopulation parameter ; the mean weight in a sample or survey is an estimate of\npopulation average weight. The principles of statistical inference provide not only\nestimates of population parameters, but also measures of uncertainty that account\nfor the fact that di ﬀerent random samples will produce di ﬀerent estimates because\nof the variability of random sampling; i.e., two di ﬀerent random samples will not\ninclude exactly the same people.\nThis chapter introduces the important ideas in drawing estimates from sam-\nples by discussing methods of inference for a population mean, \u0016, including three\nwidely used tools: point estimates for a population mean, interval estimates that\ninclude both a point estimate and a margin of error, and a method for testing scien-\ntiﬁc hypotheses about \u0016. The concepts used in this chapter will appear throughout\nthe rest of the book, which discusses inference for other settings. While particular\nequations or formulas may change to reﬂect the details of a problem at hand, the\nfundamental ideas will not.\n1DOI: 10.1056/NEJMoa1614362\n2J Ment Health Policy Econ. 2010 Jun;13(2):53-63\n3DOI: 10.1186/1471-2458-7-80\n4https://www.cdc.gov/maso/pdf/cdcmiss.pdf\n200\nThe BRFSS was established in 1984 in 15 states to collect data using telephone\ninterviews about health-related risk behaviors, chronic health conditions, and the\nuse of preventive services. It now collects data in all 50 states and the District\nof Columbia from more than 400,000 interviews conducted each year. The data\nsetcdccontains a small number of variables from a random sample of 20,000 re-\nsponses from the 264,684 interviews from the BRFSS conducted in the year 2000.\nPart of this dataset is shown in Figure 4.1, with the variables described in Fig-\nure 4.2.5\ncase age gender weight wtdesire height genhlth\n1 1 77 m 175 175 70 good\n2 2 33 f 125 115 64 good\n3 3 49 f 105 105 60 good\n20000 20000 83 m 170 165 69 good\nFigure 4.1: Four cases from the cdcdataset.\nVariable Variable deﬁnition.\ncase Case number in the dataset, ranging from 1 to 20,000.\nage Age in years.\ngender A factor variable, with levels mfor male, ffor female.\nweight Weight in pounds.\nwtdesire Weight that the respondent wishes to be, in pounds.\nheight Height in inches.\ngenhlth A factor variable describing general health status, with levels excellent ,very\ngood ,good ,fair ,poor .\nFigure 4.2: Some variables and their descriptions for the cdcdataset.\nFew studies are as large as the original BRFSS dataset (more than 250,000\ncases); in fact, few are as large as the 20,000 cases in the dataset cdc. The dataset\ncdcis large enough that estimates calculated from cdccan be thought of as essen-\ntially equivalent to the population characteristics of the entire US adult popula-\ntion. This chapter uses a random sample of 60 cases from cdc, stored as cdc.samp ,\nto illustrate the e ﬀect of sampling variability and the ideas behind inference. In\nother words, suppose that cdcrepresents the population, and that cdc.samp is a\nsample from the population; the goal is to estimate characteristics of the popula-\ntion of 20,000 using only the data from the 60 individuals in the sample.\nFor labs, slides, and other resources, please visit\nwww.openintro.org/book/biostat\n5With small modiﬁcations (character strings re-coded as factors), the data appears in this text as it does in an OpenIntro\nlab. https://www.openintro.org/go?id=statlab_r_core_intro_to_data\n4.1. V ARIABILITY IN ESTIMATES 201\n4.1 Variability in estimates\nA natural way to estimate features of the population, such as the population mean weight,\nis to use the corresponding summary statistic calculated from the sample.6The mean weight in\nthe sample of 60 adults in cdc.samp isxweight = 173:3 lbs; this sample mean is a point estimate of\nthe population mean, \u0016weight . If a di ﬀerent random sample of 60 individuals were taken from cdc,\nthe new sample mean would likely be di ﬀerent as a result of sampling variation . While estimates\ngenerally vary from one sample to another, the population mean is a ﬁxed value.\nGUIDED PRACTICE 4.1\nHow would one estimate the di ﬀerence in average weight between men and women? Given that\nxmen= 185:1 lbs andxwomen = 162:3 lbs, what is a good point estimate for the population di ﬀer-\nence?7\nPoint estimates become more accurate with increasing sample size. Figure 4.3 shows the sam-\nple mean weight calculated for random samples drawn from cdc, where sample size increases by 1\nfor each draw until sample size equals 500. The red dashed horizontal line in the ﬁgure is drawn at\nthe average weight of all adults in cdc, 169.7 lbs, which represents the population mean weight.8\nMean weight\n0100 200 300 400 500130140150160170180190200\nSample size\nFigure 4.3: The mean weight computed for a random sample from cdc, increas-\ning sample size one at a time until n= 500. The sample mean approaches the\npopulation mean (i.e., mean weight in cdc) as sample size increases.\nNote how a sample size around 50 may produce a sample mean that is as much as 10 lbs\nhigher or lower than the population mean. As sample size increases, the ﬂuctuations around the\npopulation mean decrease; in other words, as sample size increases, the sample mean becomes less\nvariable and provides a more reliable estimate of the population mean.\n6Other population parameters, such as population median or population standard deviation, can also be estimated\nusing sample versions.\n7Given thatxmen = 185:1 lbs andxwomen = 162:3 lbs, the di ﬀerence of the two sample means, 185 :1\u0000162:3 = 22:8lbs,\nis a point estimate of the di ﬀerence. The data in the random sample suggests that adult males are, on average, about 23 lbs\nheavier than adult females.\n8It is not exactly the mean weight of all US adults, but will be very close since cdcis so large.\n202 CHAPTER 4. FOUNDATIONS FOR INFERENCE\n4.1.1 The sampling distribution for the mean\nThe sample mean weight calculated from cdc.samp is 173.3 lbs. Another random sample of\n60 participants might produce a di ﬀerent value of x, such as 169.5 lbs; repeated random sampling\ncould result in additional di ﬀerent values, perhaps 172.1 lbs, 168.5 lbs, and so on. Each sample\nmeanxcan be thought of as a single observation from a random variable X. The distribution of\nXis called the sampling distribution of the sample mean , and has its own mean and standard\ndeviation like the random variables discussed in Chapter 3. The concept of a sampling distribution\ncan be illustrated by taking repeated random samples from cdc. Figure 4.4 shows a histogram\nof sample means from 1,000 random samples of size 60 from cdc. The histogram provides an\napproximation of the theoretical sampling distribution of Xfor samples of size 60.\nSample meanFrequency\n150155160165170175180185190 150155160165170175180185190020406080100120140\nFigure 4.4: A histogram of 1000 sample means for weight among US adults, where\nthe samples are of size n= 60.\nSAMPLING DISTRIBUTION\nThe sampling distribution is the distribution of the point estimates based on samples of a\nﬁxed size from a certain population. It is useful to think of a particular point estimate as\nbeing drawn from a sampling distribution.\nSince the complete sampling distribution consists of means for all possible samples of size 60,\ndrawing a much larger number of samples provides a more accurate view of the distribution; the\nleft panel of Figure 4.5 shows the distribution calculated from 100,000 sample means.\nA normal probability plot of these sample means is shown in the right panel of Figure 4.5. All\nof the points closely fall around a straight line, implying that the distribution of sample means is\nnearly normal (see Section 3.3). This result follows from the Central Limit Theorem.\nCENTRAL LIMIT THEOREM, INFORMAL DESCRIPTION\nIf a sample consists of at least 30 independent observations and the data are not strongly\nskewed, then the distribution of the sample mean is well approximated by a normal model.\n4.1. V ARIABILITY IN ESTIMATES 203\nSample MeanFrequency\n150155160165170175180185190020004000\n●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●Sample means\nTheoretical quantiles−4−2024150160170180190\nFigure 4.5: The left panel shows a histogram of the sample means for 100,000\nrandom samples. The right panel shows a normal probability plot of those sample\nmeans.\nThe sampling distribution for the mean is unimodal and symmetric around the mean of the\nrandom variable X. Statistical theory can be used to show that the mean of the sampling distribu-\ntion forXis exactly equal to the population mean \u0016.\nHowever, in almost any study, conclusions about a population parameter must be drawn from\nthe data collected from a single sample. The sampling distribution of Xis a theoretical concept,\nsince obtaining repeated samples by conducting a study many times is not possible. In other words,\nit is not feasible to calculate the population mean \u0016by ﬁnding the mean of the sampling distribu-\ntion forX.\n4.1.2 Standard error of the mean\nThe standard error (SE) of the sample mean measures the sample-to-sample variability of X,SE\nstandard\nerrorthe extent to which values of the repeated sample means oscillate around the population mean. The\ntheoretical standard error of the sample mean is calculated by dividing the population standard\ndeviation (\u001bx) by the square root of the sample size n. Since the population standard deviation \u001b\nis typically unknown, the sample standard deviation sis often used in the deﬁnition of a standard\nerror;sis a reasonably good estimate of \u001b. IfXrepresents the sample mean weight, its standard\nerror (denoted by SE) is\nSEX=sxpn=49:04p\n60= 6:33:\nThis estimate tends to be su ﬃciently good when the sample size is at least 30 and the population\ndistribution is not strongly skewed. In the case of skewed distributions, a larger sample size is\nnecessary.\n204 CHAPTER 4. FOUNDATIONS FOR INFERENCE\nThe probability tools of Section 3.1 can be used to derive the formula \u001bX=\u001bx=pn, but the\nderivation is not shown here. Larger sample sizes produce sampling distributions that have lower\nvariability. Increasing the sample size causes the distribution of Xto be clustered more tightly\naround the population mean \u0016, allowing for more accurate estimates of \u0016from a single sample, as\nshown in Figure 4.6. When sample size is large, it is more likely that any particular sample will\nhave a mean close to the population mean.\nSample meanFrequency\n150155160165170175180185190 150155160165170175180185190020406080100120140\n(a)\nSample meanFrequency\n150155160165170175180185190 150155160165170175180185190020406080100120140 (b)\nFigure 4.6: (a) Reproduced from Figure 4.4, an approximation of the sampling\ndistribution of Xwithn= 60. (b) An approximation of the sampling distribution\nofXwithn= 200.\nTHE STANDARD ERROR (SE) OF THE SAMPLE MEAN\nGivennindependent observations from a population with standard deviation \u001b, the standard\nerror of the sample mean is equal to\nSEX=sxpn:\nThis is an accurate estimate of the theoretical standard deviation of Xwhen the sample size is\nat least 30 and the population distribution is not strongly skewed.\nSUMMARY: POINT ESTIMATE TERMINOLOGY\n– The population mean and standard deviation are denoted by \u0016and\u001b.\n– The sample mean and standard deviation are denoted by xands.\n– The distribution of the random variable Xrefers to the collection of sample means if\nmultiple samples of the same size were repeatedly drawn from a population.\n– The mean of the random variable Xequals the population mean \u0016. In the notation of\nChapter 3,\u0016X=E(X) =\u0016.\n– The standard deviation of X(\u001bX) is called the standard error (SE) of the sample mean.\n– The theoretical standard error of the sample mean, as calculated from a single sample of\nsizen, is equal to\u001bpn. The standard error is abbreviated by SE and is usually estimated\nby usings, the sample standard deviation, such that SE=spn.\n4.2. CONFIDENCE INTERV ALS 205\n4.2 Conﬁdence intervals\n4.2.1 Interval estimates for a population parameter\nWhile a point estimate consists of a single value, an interval estimate provides a plausible\nrange of values for a parameter. When estimating a population mean \u0016, aconﬁdence interval for\n\u0016has the general form\n(x\u0000m;x+m) =x\u0006m;\nwheremis the margin of error . Intervals that have this form are called two-sided conﬁdence\nintervals because they provide both lower and upper bounds, x\u0000mandx+m, respectively. One-\nsided sided intervals are discussed in Section 4.2.3.\nThe standard error of the sample mean is the standard deviation of its distribution; addition-\nally, the distribution of sample means is nearly normal and centered at \u0016. Under the normal model,\nthe sample mean xwill be within 1.96 standard errors (i.e., standard deviations) of the population\nmean\u0016approximately 95% of the time.9Thus, if an interval is constructed that spans 1.96 stan-\ndard errors from the point estimate in either direction, a data analyst can be 95% conﬁdent that\nthe interval\nx\u00061:96\u0002SE (4.2)\ncontains the population mean. The value 95% is an approximation, accurate when the sampling\ndistribution for the sample mean is close to a normal distribution. This assumption holds when\nthe sample size is su ﬃciently large (guidelines for ‘su ﬃciently large’ are given in Section 4.4).\nµ = 169.7●●●●●●●●●●●●●●●●●●●●●●●●●●\nFigure 4.7: Twenty-ﬁve samples of size n= 60 were taken from cdc. For each\nsample, a 95% conﬁdence interval was calculated for the population average adult\nweight. Only 1 of these 25 intervals did not contain the population mean, \u0016=\n169:7 lbs.\nThe phrase \"95% conﬁdent\" has a subtle interpretation: if many samples were drawn from a\npopulation, and a conﬁdence interval is calculated from each one using Equation 4.2, about 95%\nof those intervals would contain the population mean \u0016. Figure 4.7 illustrates this process with 25\nsamples taken from cdc. Of the 25 samples, 24 contain the mean weight in cdcof 169.7 lbs, while\none does not.\n9In other words, the Z-score of 1.96 is associated with 2.5% area to the right (and Z= -1.96 has 2.5% area to the left);\nthis can be found on normal probability tables or from using statistical software.\n206 CHAPTER 4. FOUNDATIONS FOR INFERENCE\nJust as with the sampling distribution of the sample mean, the interpretation of a conﬁdence\ninterval relies on the abstract construct of repeated sampling. A data analyst, who can only observe\none sample, does not know whether the population mean lies within the single interval calculated.\nThe uncertainty is due to random sampling—by chance, it is possible to select a sample from the\npopulation that has unusually high (or low) values, resulting in a sample mean xthat is relatively\nfar from\u0016, and by extension, a conﬁdence interval that does not contain \u0016.\nEXAMPLE 4.3\nThe sample mean adult weight from the 60 observations in cdc.samp isxweight = 173:3 lbs, and\nthe standard deviation is sweight = 49:04 lbs. Use Equation 4.2 to calculate an approximate 95%\nconﬁdence interval for the average adult weight in the US population.\nThe standard error for the sample mean is SE x=49:04p\n60= 6:33 lbs. The 95% conﬁdence interval is\nxweight\u00061:96SEx= 173:3\u0006(1:96)(6:33) = (160:89;185:71) lbs.\nThe data support the conclusion that, with 95% conﬁdence, the average weight of US adults is\nbetween approximately 161 and 186 lbs.\nFigure 4.5 visually shows that the sampling distribution is nearly normal. To assess normality of\nthe sampling distribution without repeated sampling, it is necessary to check whether the data\nare skewed. Although Figure 4.8 shows some skewing, the sample size is large enough that the\nconﬁdence interval should be reasonably accurate.\nWeight (lbs)Frequency\n50 100 150 200 250 300051015202530\nFigure 4.8: Histogram of weight incdc.samp\n4.2. CONFIDENCE INTERV ALS 207\nGUIDED PRACTICE 4.4\nThere are 31 females in the sample of 60 US adults, and the average and standard deviation of\nweight for these individuals are 162.3 lbs and 57.74 lbs, respectively. A histogram of weight for\nthe 31 females is shown in Figure 4.9. Calculate an approximate 95% conﬁdence interval for the\naverage weight of US females. Is the interval likely to be accurate?10\nWeight of FemalesFrequency\n100 150 200 250 300 350 40005101520\nFigure 4.9: Histogram of weight for the 31 females in cdc.samp .\n4.2.2 Changing the conﬁdence level\nNinety-ﬁve percent conﬁdence intervals are the most commonly used interval estimates, but\nintervals with conﬁdence levels other than 95% can also be constructed. The general formula for a\nconﬁdence interval (for the population mean \u0016) is given by\nx\u0006z?\u0002SE; (4.5)\nwherez?is chosen according to the conﬁdence level. When calculating a 95% conﬁdence level, z?\nis 1.96, since the area within 1.96 standard deviations of the mean captures 95% of the distribution.\nTo construct a 99% conﬁdence interval, z?must be chosen such that 99% of the normal curve\nis captured between - z?andz?.\nEXAMPLE 4.6\nLetYbe a normally distributed random variable. Ninety-nine percent of the time, Ywill be within\nhow many standard deviations of the mean?\nThis is equivalent to the z-score with 0.005 area to the right of zand 0.005 to the left of \u0000z. In the\nnormal probability table, this is the z-value that with 0.005 area to its right and 0.995 area to its\nleft. The closest two values are 2.57 and 2.58; for convenience, round up to 2.58. The unobserved\nrandom variable Ywill be within 2.58 standard deviations of \u001699% of the time, as shown in\nFigure 4.10.\n10Applying Equation 4.2: 162 :3\u0006(1:96)(57:73=p\n31)!(149:85;174:67). The usual interpretation would be that a data\nanalyst can be about 95% conﬁdent the average weight of US females is between approximately 150 and 175 lbs. However,\nthe histogram of female weights shows substantial right skewing, and several females with recorded weights larger than\n200 lbs. The conﬁdence interval is probably not accurate; a larger sample should be collected in order for the sampling\ndistribution of the mean to be approximately normal. Chapter 5 will introduce the t-distribution, which is more reliable\nwith small sample sizes than the z-distribution.\n208 CHAPTER 4. FOUNDATIONS FOR INFERENCE\nstandard deviations from the mean−3 −2 −1 0 1 2 395%, extends −1.96 to 1.9699%, extends −2.58 to 2.58\nFigure 4.10: The area between - z?andz?increases asjz?jbecomes larger. If the\nconﬁdence level is 99%, z?is chosen such that 99% of the normal curve is between\n-z?andz?, which corresponds to 0.5% in the lower tail and 0.5% in the upper tail:\nz?= 2:58.\nA 99% conﬁdence interval will have the form\nx\u00062:58\u0002SE; (4.7)\nand will consequently be wider than a 95% interval for \u0016calculated from the same data, since the\nmargin of error mis larger.\nEXAMPLE 4.8\nCreate a 99% conﬁdence interval for the average adult weight in the US population using the data\nincdc.samp . The point estimate is xweight = 173:3 and the standard error is SEx= 6:33.\nApply the 99% conﬁdence interval formula: xweight\u00062:58\u0002SEx!(156:97;189:63). A data analyst\ncan be 99% conﬁdent that the average adult weight is between 156.97 and 189.63 lbs.\nThe 95% conﬁdence interval for the average adult weight is (160.89, 185.71) lbs. Increasing\nthe conﬁdence level to 99% results in the interval (156.97, 189.63) lbs; this wider interval is more\nlikely to contain the population mean \u0016. However, increasing the conﬁdence level comes at a\ncost: a wider interval is less informative in providing a precise estimate of the population mean.\nConsider the extreme: to be \"100% conﬁdent\" that an interval contains \u0016, the interval must span\nall possible values of \u0016. For example, with 100% conﬁdence the average weight is between 0 and\n1000 lbs; while this interval necessarily contains \u0016, it has no interpretive value and is completely\nuninformative.11\nDecreasing the conﬁdence level produces a narrower interval; the estimate is more precise, but\nalso more prone to inaccuracy. For example, consider a 50% conﬁdence interval for average adult\nweight using cdc.samp : thez?value is 0.67, and the conﬁdence interval is (169.06, 177.54) lbs. This\ninterval provides a more precise estimate of the population average weight \u0016than the 99% or 95%\nconﬁdence intervals, but the increased precision comes with less conﬁdence about whether the\n11Strictly speaking, to be 100% conﬁdent requires an interval spanning all positive numbers; 1000 lbs has been arbitrar-\nily chosen as an upper limit for human weight.\n4.2. CONFIDENCE INTERV ALS 209\ninterval contains \u0016. In a theoretical setting of repeated sampling, if 100 50% conﬁdence intervals\nwere computed, only half could be expected to contain \u0016.\nThe choice of conﬁdence level is a trade-o ﬀbetween obtaining a precise estimate and calculat-\ning an interval that can be reasonably expected to contain the population parameter. In published\nliterature, the most used conﬁdence intervals are the 90%, 95%, and 99%.\n4.2.3 One-sided conﬁdence intervals\nOne-sided conﬁdence intervals for a population mean provide either a lower bound or an\nupper bound, but not both. One-sided conﬁdence intervals have the form\n(x\u0000m;1) or (\u00001;x+m):\nWhile the margin of error mfor a one-sided interval is still calculated from the standard error\nofxand az?value, the choice of z?is a diﬀerent than for a two-sided interval. For example, the\nintent of a 95% one-sided upper conﬁdence interval is to provide an upper bound msuch that a\ndata analyst can be 95% conﬁdent that a population mean \u0016is less than x+m. Thez?value must\ncorrespond to the point on the normal distribution that has 0.05 area in the right tail, z?= 1:645.12\nA one-sided upper 95% conﬁdence interval will have the form\n(\u00001;x+ 1:645\u0002SE):\nEXAMPLE 4.9\nCalculate a lower 95% conﬁdence interval for the population average adult weight in the United\nStates. In the sample of 60 adults in cdc.samp , the mean and standard error are x= 173:3 and\nSE= 6:33 days.\nThe lower bound is 173 :3\u0000(1:645\u00026:33) = 163:89. The lower 95% interval (163 :89;1) suggests\nthat one can be 95% conﬁdent that the population average adult weight is at least 163.9 lbs.\nGUIDED PRACTICE 4.10\nCalculate an upper 99% conﬁdence interval for the population average adult weight in the United\nStates. The mean and standard error for weight in cdc.samp arex= 173:3 andSE= 6:33 days.13\n4.2.4 Interpreting conﬁdence intervals\nThe correct interpretation of an XX% conﬁdence interval is, \"We are XX% conﬁdent that the\npopulation parameter is between . . . \" While it may be tempting to say that a conﬁdence interval\ncaptures the population parameter with a certain probability, this is a common error. The conﬁ-\ndence level only quantiﬁes how plausible it is that the parameter is within the interval; there is no\nprobability associated with whether a parameter is contained in a speciﬁc conﬁdence interval. The\nconﬁdence coe ﬃcient reﬂects the nature of a procedure that is correct XX% of the time, given that\nthe assumptions behind the calculations are true.\n12Previously, with a two-sided interval, 1.96 was chosen in order to have a total area of 0.05 from both the right and left\ntails.\n13For a one-sided 99% conﬁdence interval, the z?value corresponds to the point with 0.01 area in the right tail, z?=\n2:326. Thus, the upper bound for the interval is 173 :3 + (2:326\u00026:33) = 188:024:The upper 99% interval ( \u00001;188:024)\nsuggests that one can be 99% conﬁdent that the population average adult weight is at most 188.0 lbs.\n210 CHAPTER 4. FOUNDATIONS FOR INFERENCE\nThe conditions regarding the validity of the normal approximation can be checked using the\nnumerical and graphical summaries discussed in Chapter 1. However, the condition that data\nshould be from a random sample is sometimes overlooked. If the data are not from a random\nsample, then the conﬁdence interval no longer has interpretive value, since there is no population\nmean to which the conﬁdence interval applies. For example, while only simple arithmetic is needed\nto calculate a conﬁdence interval for BMI from the famuss dataset in Chapter 1, the participants\nin the study are almost certainly not a random sample from some population; thus, a conﬁdence\ninterval should not be calculated in this setting.\nEXAMPLE 4.11\nBody mass index (BMI) is one measure of body weight that adjusts for height. The National\nHealth and Nutrition Examination Survey (NHANES) consists of a set of surveys and measure-\nments conducted by the US CDC to assess the health and nutritional status of adults and children\nin the United States. The dataset nhanes.samp contains 76 variables and is a random sample of\n200 individuals from the measurements collected in the years 2009-2010 and 2012-2013.14Use\nnhanes.samp to calculate a 95% conﬁdence interval for adult BMI in the US population, and assess\nwhether the data suggest Americans tend to be overweight.\nIn the random sample of 200 participants, BMI is available for all 135 of the participants that are\n21 years of age or older. As shown in the histogram (Figure 4.11), the data are right-skewed, with\none large outlier. The outlier corresponds to an implausibly extreme BMI value of 69.0; since it\nseems likely that the value represents an error from when the data was recorded, this data point is\nexcluded from the following analysis.\nThe mean and standard deviation in this sample of 134 are 28.8 and 6.7 kg =meter2, respectively.\nThe sample size is large enough to justify using the normal approximation when computing the\nconﬁdence interval. The standard error of the mean is SE = 6 :7=p\n134 = 0:58, so the 95% conﬁdence\ninterval is given by\nxBMI\u0006(1:96)(SE) = 28 :8\u0006(1:96)(0:58)\n= (27:7;29:9):\nBased on this sample, a data analyst can be 95% conﬁdent that the average BMI of US adults is\nbetween 27.7 and 29.9 kg =m2.\nThe World Health Organization (WHO) and other agencies use BMI to set normative guidelines for\nbody weight. The current guidelines are shown in Figure 4.12.\nThe conﬁdence interval (27.7, 29.9) kg =m2certainly suggests that the average BMI in the US pop-\nulation is higher than 21.7, the middle of the range for normal BMIs, and even higher than 24.99,\nthe upper limit of the normal weight category. These data indicate that Americans tend to be\noverweight.\n14The sample was drawn from a larger sample of 20,293 participants in the NHANES package, available from The\nComprehensive R Archive Network (CRAN). The CDC uses a complex sampling design that samples some demographic\nsubgroups with larger probabilities, but nhanes.samp has been adjusted so that it can be viewed as a random sample of the\nUS population.\n4.2. CONFIDENCE INTERV ALS 211\nBMIFrequency\n20 30 40 50 60 7001020304050\nFigure 4.11: The distribution of BMIfor the 135 adults in nhanes.samp .\nCategory BMI range\nUnderweight <18:50\nNormal (healthy weight) 18.5-24.99\nOverweight \u001525\nObese \u001530\nFigure 4.12: WHO body weight categories based on BMI.\n212 CHAPTER 4. FOUNDATIONS FOR INFERENCE\n4.3 Hypothesis testing\nImportant decisions in science, such as whether a new treatment for a disease should be ap-\nproved for the market, are primarily data-driven. For example, does a clinical study of a new\ncholesterol-lowering drug provide robust evidence of a beneﬁcial e ﬀect in patients at risk for heart\ndisease? A conﬁdence interval can be calculated from the study data to provide a plausible range\nof values for a population parameter, such as the population average decrease in cholesterol levels.\nA drug is considered to have a beneﬁcial e ﬀect on a population of patients if the population av-\nerage e ﬀect is large enough to be clinically important. It is also necessary to evaluate the strength\nof the evidence that a drug is e ﬀective; in other words, is the observed e ﬀect larger than would be\nexpected from chance variation alone?\nHypothesis testing is a method for calculating the probability of making a speciﬁc observation\nunder a working hypothesis, called the null hypothesis. By assuming that the data come from a\ndistribution speciﬁed by the null hypothesis, it is possible to calculate the likelihood of observing\na value as extreme as the one represented by the sample. If the chances of such an extreme obser-\nvation are small, there is enough evidence to reject the null hypothesis in favor of an alternative\nhypothesis.\nNULL AND ALTERNATIVE HYPOTHESES\nThe null hypothesis ( H0)often represents either a skeptical perspective or a claim to be tested.\nThe alternative hypothesis ( HA)is an alternative claim and is often represented by a range of\npossible parameter values.\nGenerally, an investigator suspects that the null hypothesis is not true and performs a hypoth-\nesis test in order to evaluate the strength of the evidence against the null hypothesis. The logic\nbehind rejecting or failing to reject the null hypothesis is similar to the principle of presumption\nof innocence in many legal systems. In the United States, a defendant is assumed innocent until\nproven guilty; a verdict of guilty is only returned if it has been established beyond a reasonable\ndoubt that the defendant is not innocent. In the formal approach to hypothesis testing, the null\nhypothesis ( H0) is not rejected unless the evidence contradicting it is so strong that the only rea-\nsonable conclusion is to reject H0in favor ofHA.\nThe next section presents the steps in formal hypothesis testing, which is applied when data\nare analyzed to support a decision or make a scientiﬁc claim.\n4.3.1 The Formal Approach to Hypothesis Testing\nIn this section, hypothesis testing will be used to address the question of whether Americans\ngenerally wish to be heavier or lighter than their current weight. In the cdcdata, the two variables\nweight and wtdesire are, respectively, the recorded actual and desired weights for each respondent,\nmeasured in pounds.\nSuppose that \u0016is the population average of the di ﬀerence weight\u0000wtdesire . Using the ob-\nservations from cdc.samp , assess the strength of the claim that, on average, there is no systematic\npreference to be heavier or lighter.\n4.3. HYPOTHESIS TESTING 213\nStep 1: Formulating null and alternative hypotheses\nThe claim to be tested is that the population average of the di ﬀerence between actual and desired\nweight for US adults is equal to 0.\nH0:\u0016= 0:\nIn the absence of prior evidence that people typically wish to be lighter (or heavier), it is\nreasonable to begin with an alternative hypothesis that allows for di ﬀerences in either direction.\nHA:\u0016,0:\nThe alternative hypothesis HA:\u0016,0 is called a two-sided alternative . A one-sided alternative\ncould be used if, for example, an investigator felt there was prior evidence that people typically\nwish to weigh less than they currently do: HA:\u0016>0.\nMore generally, when testing a hypothesis about a population mean \u0016, the null and alternative\nhypotheses are written as follows\n– For a two-sided alternative:\nH0:\u0016=\u00160; HA:\u0016,\u00160:\n– For a one-sided alternative:\nH0:\u0016=\u00160; HA:\u0016<\u0016 0 orH0:\u0016=\u00160; HA:\u0016>\u0016 0:\nThe symbol \u0016denotes a population mean, while \u00160refers to the numeric value speciﬁed by the null\nhypothesis; in this example, \u00160= 0. Note that null and alternative hypotheses are statements about\nthe underlying population, not the observed values from a sample.\nStep 2: Specifying a signiﬁcance level, \u000b\nIt is important to specify how rare or unlikely an event must be in order to represent su ﬃcient\nevidence against the null hypothesis. This should be done during the design phase of a study, to\nprevent any bias that could result from deﬁning ’rare’ only after analyzing the results.\nWhen testing a statistical hypothesis, an investigator speciﬁes a signiﬁcance level ,\u000b, that de-\nﬁnes a ’rare’ event. Typically, \u000bis chosen to be 0 :05, though it may be larger or smaller, depending\non context; this is discussed in more detail in Section 4.3.4. An \u000blevel of 0:05 implies that an event\noccurring with probability lower than 5% will be considered su ﬃcient evidence against H0.\nStep 3: Calculating the test statistic\nCalculating the test statistic tis analogous to standardizing observations with Z-scores as discussed\nin Chapter 3. The test statistic quantiﬁes the number of standard deviations between the sample\nmeanxand the population mean \u0016:\nt=x\u0000\u00160\ns=pn;\nwheresis the sample standard deviation and nis the number of observations in the sample. If x=\nweight\u0000wtdesire , then for the 60 recorded di ﬀerences in cdc.samp ,x= 18:2 ands= 33:46. In this\nsample, respondents weigh on average about 18 lbs more than they wish. The test statistic is\nt=18:2\u00000\n33:46=p\n60= 4:22:\nThe observed sample mean is 4.22 standard deviations to the right of \u00160= 0.\n214 CHAPTER 4. FOUNDATIONS FOR INFERENCE\nStep 4: Calculating the ppp-value\nTheppp-value is the probability of observing a sample mean as or more extreme than the observed\nvalue, under the assumption that the null hypothesis is true. In samples of size 40 or more, the\nt-statistic will have a standard normal distribution unless the data are strongly skewed or extreme\noutliers are present. Recall that a standard normal distribution has mean 0 and standard deviation\n1.\nFor two-sided tests, with HA:\u0016,\u00160, thep-value is the sum of the area of the two tails deﬁned\nby thet-statistic: 2P(Z\u0015jtj) =P(Z\u0014\u0000jtj) +P(Z\u0015jtj) (Figure 4.13).\nt−statistic µ = 0  \nFigure 4.13: A two-sided p-value forHA:\u0016,\u00160on a standard normal distribu-\ntion. The shaded regions represent observations as or more extreme than xin\neither direction.\nFor one-sided tests with HA:\u0016>\u0016 0, thep-value is given by P(Z\u0015t), as shown in Figure 4.14.\nIfHA:\u0016<\u0016 0, thep-value is the area to the left of the t-statistic,P(Z\u0014t).\nµ = 0  t−statistic\nFigure 4.14: A one-sided p-value forHA:\u0016>\u0016 0on a standard normal distribution\nis represented by the shaded area to the right of the t-statistic. This area equals\nthe probability of making an observation as or more extreme than x, if the null\nhypothesis is true.\nThep-value can either be calculated from software or from the normal probability tables. For\nthe weight-di ﬀerence example, the p-value is vanishingly small: p=P(Z\u0014\u00004:22) +P(Z>4:22)<\n0:001.\n4.3. HYPOTHESIS TESTING 215\nStep 5: Drawing a conclusion\nTo reach a conclusion about the null hypothesis, directly compare pand\u000b. Note that for a conclu-\nsion to be informative, it must be presented in the context of the original question; it is not useful\nto only state whether or not H0is rejected.\nIfp>\u000b , the observed sample mean is not extreme enough to warrant rejecting H0; more for-\nmally stated, there is insu ﬃcient evidence to reject H0. A highp-value suggests that the di ﬀerence\nbetween the observed sample mean and \u00160can reasonably be attributed to random chance.\nIfp\u0014\u000b, there is su ﬃcient evidence to reject H0and accept HA. In the cdc.samp weight-\ndiﬀerence data, the p-value is very small, with the t-statistic lying to the right of the population\nmean. The chance of drawing a sample with mean as large or larger than 18.2 if the distribution\nwere centered at 0 is less than 0.001. Thus, the data support the conclusion that on average, the\ndiﬀerence between actual and desired weight is not 0 and is positive; people generally seem to feel\nthey are overweight.\nGUIDED PRACTICE 4.12\nSuppose that the mean weight di ﬀerence in the sampled group of 60 adults had been 7 pounds\ninstead of 18.2 pounds, but with the same standard deviation of 33.46 pounds. Would there still\nbe enough evidence at the \u000b= 0:05 level to reject H0:\u0016= 0 in favor of HA:\u0016,0?15\n15Re-calculate the t-statistic: (7\u00000)=(33:46=p\n60) = 1:62. Thep-valueP(Z\u0014\u00001:62) +P(Z\u00151:62) = 0:105. Sincep>\u000b,\nthere is insu ﬃcient evidence to reject H0. In this case, a sample average di ﬀerence of 7 is not large enough to discount\nthe possibility that the observed di ﬀerence is due to sampling variation, and that the observations are from a distribution\ncentered at 0.\n216 CHAPTER 4. FOUNDATIONS FOR INFERENCE\n4.3.2 Two examples\nEXAMPLE 4.13\nWhile ﬁsh and other types of seafood are important for a healthy diet, nearly all ﬁsh and shellﬁsh\ncontain traces of mercury. Dietary exposure to mercury can be particularly dangerous for young\nchildren and unborn babies. Regulatory organizations such as the US Food and Drug Administra-\ntion (FDA) provide guidelines as to which types of ﬁsh have particularly high levels of mercury\nand should be completely avoided by pregnant women and young children; additionally, certain\nspecies known to have low mercury levels are recommended for consumption. While there is no\ninternational standard that deﬁnes excessive mercury levels in saltwater ﬁsh species, general con-\nsensus is that ﬁsh with levels above 0.50 parts per million (ppm) should not be consumed. A study\nconducted to assess mercury levels for saltwater ﬁsh caught o ﬀthe coast of New Jersey found that\na sample of 23 blueﬁn tuna had mean mercury level of 0.52 ppm, with standard deviation 0.16\nppm.16Based on these data, should the FDA add blueﬁn tuna from New Jersey to the list of\nspecies recommended for consumption, or should a warning be issued about their mercury levels?\nLet\u0016be the population average mercury content for blueﬁn tuna caught o ﬀthe coast of New Jersey.\nConduct a two-sided test of the hypothesis \u0016= 0:50 ppm in order to assess the evidence for either\ndeﬁnitive safety or potential danger.\nFormulate the null and alternative hypotheses .H0:\u0016= 0:50 ppm vs. HA:\u0016,0:50 ppm\nSpecify the signiﬁcance level, \u000b. A signiﬁcance level of \u000b= 0:05 seems reasonable.\nCalculate the test statistic . Thet-statistic has value\nt=x\u0000\u00160\ns=pn=0:52\u00000:50\n0:16=p\n23= 0:599:\nCalculate the p-value . For this two-sided alternative HA:\u0016,0:50, thep-value is\nP(Z\u0014\u0000jtj) +P(Z\u0015jtj) = 2\u0002P(Z\u00150:599) = 0:549:\nDraw a conclusion . Thep-value is larger than the speciﬁed signiﬁcance level \u000b, as shown in Fig-\nure 4.15.17The data do not show that the mercury content of blueﬁn tuna caught o ﬀthe coast of\nNew Jersey di ﬀers signiﬁcantly from 0.50 ppm. Since p>\u000b , there is insu ﬃcient evidence to reject\nthe null hypothesis that the mean mercury level for the New Jersey coastal population of blueﬁn\ntuna is 0.50 ppm.\nNote that \"failure to reject\" is not equivalent to \"accepting\" the null hypothesis. Recall the earlier\nanalogy related to the principle of \"innocent until proven guilty\". If there is not enough evidence\nto prove that the defendant is guilty, the o ﬃcial decision must be \"not guilty\", since the defendant\nmay not necessarily be innocent. Similarly, while there is not enough evidence to suggest that \u0016is\nnot equal to 0.5 ppm, it would be incorrect to claim that the evidence states that \u0016is0.5 ppm.\nFrom these data, there is not statistically signiﬁcant evidence to either recommend these ﬁsh as\nclearly safe for consumption or to warn consumers against eating them. Based on these data, the\nFood and Drug Administration might decide to monitor this species more closely and conduct\nfurther studies.\n16J. Burger, M. Gochfeld, Science of the Total Environment 409 (2011) 1418–1429\n17The grey shaded regions are bounded by -1.96 and 1.96, since the area within 1.96 standard deviations of the mean\ncaptures 95% of the distribution.\n4.3. HYPOTHESIS TESTING 217\nt = 0.599µ = 0  \nFigure 4.15: The large blue shaded regions represent the p-value, the area to the\nright oft= 0:599 and to the left of \u0000t=\u00000:599. The smaller grey shaded regions\nrepresents the rejection region as deﬁned by \u000b; in this case, an area of 0.025 in\neach tail. The t-statistic calculated from xwould have to lie within either of the\nextreme tail areas to constitute su ﬃcient evidence against the null hypothesis.\nEXAMPLE 4.14\nIn 2015, the National Sleep Foundation published new guidelines for the amount of sleep rec-\nommended for adults: 7-9 hours of sleep per night.18The NHANES survey includes a question\nasking respondents about how many hours per night they sleep; the responses are available in\nnhanes.samp . In the sample of 134 adults used in the BMI example, the average reported hours of\nsleep is 6.90, with standard deviation 1.39. Is there evidence that American adults sleep less than\n7 hours per night?\nLet\u0016be the population average of hours of sleep per night for US adults. Conduct a one-sided test,\nsince the question asks whether the average amount of sleep per night might be less than 7 hours.\nFormulate the null and alternative hypotheses .H0:\u0016= 7 hours vs. HA:\u0016<7 hours.\nSpecify the signiﬁcance level, \u000b. Let\u000b= 0:05, since the question does not reference a di ﬀerent value.\nCalculate the test statistic . Thet-statistic has value\nt=x\u0000\u00160\ns=pn=6:90\u00007:00\n1:33=p\n134=\u00000:864:\nCalculate the p-value .\nFor this one-sided alternative HA:\u0016<7, thep-value is\nP(Z\u0014t) =P(Z<\u00000:864) = 0:19:\nSince the alternative states that \u00160is less than 7, the p-value is represented by the area to the left\noft=\u00000:864, as shown in Figure 4.16.\nDraw a conclusion . Thep-value is larger than the speciﬁed signiﬁcance level \u000b. The null hypoth-\nesis is not rejected since the data do not represent su ﬃcient evidence to support the claim that\nAmerican adults sleep less than 7 hours per night.\n18Sleep Health: Journal of the National Sleep Foundation, Vol. 1, Issue 1, pp. 40 - 43\n218 CHAPTER 4. FOUNDATIONS FOR INFERENCE\nt = −0.864 µ = 0  \nFigure 4.16: The large blue shaded region represents the p-value, the area to the\nleft oft=\u00000:864. The smaller grey shaded region represents the rejection region\nof area 0.05 in the left tail.\nGUIDED PRACTICE 4.15\nFrom these data, is there su ﬃcient evidence at the \u000b= 0:10 signiﬁcance level to support the claim\nthat American adults sleep more than 7 hours per night?19\n4.3.3 Hypothesis testing and conﬁdence intervals\nThe relationship between a hypothesis test and the corresponding conﬁdence interval is de-\nﬁned by the signiﬁcance level \u000b; the two approaches are based on the same inferential logic, and\ndiﬀer only in perspective. The hypothesis testing approach asks whether xis far enough away\nfrom\u00160to be considered extreme, while the conﬁdence interval approach asks whether \u00160is close\nenough toxto be plausible. In both cases, \"far enough\" and \"close enough\" are deﬁned by \u000b, which\ndetermines the z?used to calculate the margin of error m=z?(s=pn).\nHypothesis Test . For a two-sided test, xneeds to be at least munits away from \u00160in either\ndirection to be considered extreme. The t-points marking o ﬀthe rejection region are equal to\nthez?value used in the conﬁdence interval, with the positive and negative t-points account-\ning for the\u0006structure in the conﬁdence interval.\nConﬁdence Interval . The plausible range of values for \u00160aroundxis deﬁned as ( x\u0000m;x+m).\nIf\u00160is plausible, it can at most be munits away in either direction from x. If the interval\ndoes not contain \u00160, then\u00160is implausible according to \u000band there is su ﬃcient evidence to\nrejectH0.\n19Thet-statistic does not change from 1.65. Re-calculate the p-value since the alternative hypothesis is now HA:\u0016>7:\nP(Z\u0015\u00000:864) = 0:81. Sincep>\u000b, there is insu ﬃcient evidence to reject H0at\u000b= 0:10. A common error when conducting\none-sided tests is to assume that the p-value will always be the area in the smaller of the two tails to the right or left of the\nobserved value. It is important to remember that the area corresponding to the p-value is in the direction speciﬁed by the\nalternative hypothesis.\nt = −0.864\n4.3. HYPOTHESIS TESTING 219\nSuppose that a two-sided test is conducted at signiﬁcance level \u000b; the conﬁdence level of\nthe matching interval is (1 \u0000\u000b)%. For example, a two-sided hypothesis test with \u000b= 0:05 can\nbe compared to a 95% conﬁdence interval. A hypothesis test will reject at \u000b= 0:05 if the 95%\nconﬁdence interval does not contain the null hypothesis value of the population mean ( \u00160).\nTHE RELATIONSHIP BETWEEN TWO-SIDED HYPOTHESIS TESTS AND CONFIDENCE INTERVALS\nWhen testing the null hypothesis H0:\u0016=\u00160against the two-sided alternative HA:\u0016,\u00160,H0\nwill be rejected at signiﬁcance level \u000bwhen the 100(1\u0000\u000b)% conﬁdence interval for \u0016does not\ncontain\u00160.\nEXAMPLE 4.16\nCalculate the conﬁdence interval for the average mercury level for blueﬁn tuna caught o ﬀthe coast\nof New Jersey. The summary statistics for the sample of 21 ﬁsh are x= 0:53 ppm and s= 0:16 ppm.\nDoes the interval agree with the results of Example 4.13?\nThe 95% conﬁdence interval is:\nx\u00061:96spn= 0:53\u00061:960:16p\n21= (0:462;0:598) ppm:\nThe conﬁdence interval is relatively wide, containing values below 0.50 ppm that might be re-\ngarded as safe, in addition to values that might be regarded as potentially dangerous. This interval\nsupports the conclusion reached from hypothesis testing; the sample data does not suggest that the\nmercury level di ﬀers signiﬁcantly from 0.50 ppm in either direction.\nThe same relationship applies for one-sided hypothesis tests. For example, a one-sided hy-\npothesis test with \u000b= 0:05 andHA:\u0016>\u0016 0corresponds to a one-sided 95% conﬁdence interval that\nhas a lower bound, but no upper bound (i.e., ( x\u0000m;1)).\nTHE RELATIONSHIP BETWEEN ONE-SIDED HYPOTHESIS TESTS AND CONFIDENCE INTERVALS\n– When testing the null hypothesis H0:\u0016=\u00160against the one-sided alternative HA:\u0016>\n\u00160,H0will be rejected at signiﬁcance level \u000bwhen\u00160is smaller than the lower bound of\nthe 100(1\u0000\u000b)% conﬁdence interval for \u0016. This is equivalent to \u00160having a value outside\nthe lower one-sided conﬁdence interval ( x\u0000m;1).\n– When testing the null hypothesis H0:\u0016=\u00160against the one-sided alternative HA:\u0016<\n\u00160,H0will be rejected at signiﬁcance level \u000bwhenever\u00160is larger than the upper bound\nof the 100(1\u0000\u000b)% conﬁdence interval for \u0016. This is equivalent to \u00160having a value\noutside the upper one-sided conﬁdence interval ( \u00001;x+m).\n220 CHAPTER 4. FOUNDATIONS FOR INFERENCE\nEXAMPLE 4.17\nPreviously, a hypothesis test was conducted at \u000b= 0:05 to test the null hypothesis H0:\u0016= 7\nhours against the alternative HA:\u0016<7 hours, for the average sleep per night US adults. Calculate\nthe corresponding one-sided conﬁdence interval and compare the information obtained from a\nconﬁdence interval versus a hypothesis test. The summary statistics for the sample of 134 adults\narex= 6:9 ands= 1:39.\nIn theory, a one-sided upper conﬁdence interval extends to 1on the left side, but since it is im-\npossible to get negative sleep, it is more sensible to bound this conﬁdence interval by 0. The upper\none-sided 95% conﬁdence interval is\n(0;x+ 1:645spn) = (0;6:9 + 1:6451:39p\n134) = (0;7:1) hours.\nFrom these data, we can be 95% conﬁdent that the average sleep per night among US adults is\nat most 7.1 hours per night. The \u00160value of 7 hours is inside the one-sided interval; thus, there\nis not su ﬃcient evidence to reject the null hypothesis H0:\u0016= 7 against the one-sided alternative\nH0:\u0016<7 hours at\u000b= 0:05.\nThe interval provides a range of plausible values for a parameter based on the observed sample;\nin this case, the data suggest that the population average sleep per night for US adults is no larger\nthan 7.1 hours. The p-value from a hypothesis test represents a measure of the strength of the\nevidence against the null hypothesis, indicating how unusual the observed sample would be under\nH0; the hypothesis test indicated that the data do not seem extreme enough ( p= 0:19) to contradict\nthe hypothesis that the population average sleep hours per night is 7.\nIn practice, both a p-value and a conﬁdence interval are computed when using a sample to make\ninferences about a population parameter.\n4.3.4 Decision errors\nHypothesis tests can potentially result in incorrect decisions, such as rejecting the null hypoth-\nesis when the null is actually true. Figure 4.17 shows the four possible ways that the conclusion of\na test can be right or wrong.\nTest conclusion\nFail to reject H0RejectH0in favor ofHA\nH0True Correct Decision Type 1 Error\nRealityHATrue Type 2 Error Correct Decision\nFigure 4.17: Four di ﬀerent scenarios for hypothesis tests.\nRejecting the null hypothesis when the null is true represents a Type I error , while a Type II\nerror refers to failing to reject the null hypothesis when the alternative is true.\n4.3. HYPOTHESIS TESTING 221\nEXAMPLE 4.18\nIn a trial, the defendant is either innocent ( H0) or guilty (HA). After hearing evidence from both\nthe prosecution and the defense, the court must reach a verdict. What does a Type I Error represent\nin this context? What does a Type II Error represent?\nIf the court makes a Type I error, this means the defendant is innocent, but wrongly convicted\n(rejectingH0whenH0is true). A Type II error means the court failed to convict a defendant that\nwas guilty (failing to reject H0whenH0is false).\nThe probability of making a Type I error is the same as the signiﬁcance level \u000b, since\u000bdeter-\nmines the cuto ﬀpoint for rejecting the null hypothesis. For example, if \u000bis chosen to be 0.05, then\nthere is a 5% chance of incorrectly rejecting H0.\nThe rate of Type I error can be reduced by lowering \u000b(e.g., to 0.01 instead of 0.05); doing\nso requires an observation to be more extreme to qualify as su ﬃcient evidence against the null\nhypothesis. However, this inevitably raises the rate of Type II errors, since the test will now have a\nhigher chance of failing to reject the null hypothesis when the alternative is true.\nEXAMPLE 4.19\nIn a courtroom setting, how might the rate of Type I errors be reduced? What e ﬀect would this\nhave on the rate of Type II errors?\nLowering the rate of Type I error is equivalent to raising the standards for conviction such that\nfewer people are wrongly convicted. This increases Type II error, since higher standards for con-\nviction leads to fewer convictions for people who are actually guilty.\nGUIDED PRACTICE 4.20\nIn a courtroom setting, how might the rate of Type II errors be reduced? What e ﬀect would this\nhave on the rate of Type I errors?20\nChoosing a signiﬁcance level\nReducing the error probability of one type of error increases the chance of making the other type.\nAs a result, the signiﬁcance level is often adjusted based on the consequences of any decisions that\nmight follow from the result of a signiﬁcance test.\nBy convention, most scientiﬁc studies use a signiﬁcance level of \u000b= 0:05; small enough such\nthat the chance of a Type I error is relatively rare (occurring on average 5 out of 100 times), but\nalso large enough to prevent the null hypothesis from almost never being rejected. If a Type I error\nis especially dangerous or costly, a smaller value of \u000bis chosen (e.g., 0.01). Under this scenario,\nit is better to be cautious about rejecting the null hypothesis, so very strong evidence against H0\nis required in order to reject the null and accept the alternative. Conversely, if a Type II error is\nrelatively dangerous, then a larger value of \u000bis chosen (e.g., 0.10). Hypothesis tests with larger\nvalues of\u000bwill rejectH0more often.\nFor example, in the early stages of assessing a drug therapy, it may be important to continue\nfurther testing even if there is not very strong initial evidence for a beneﬁcial e ﬀect. If the scientists\nconducting the research know that any initial positive results will eventually be more rigorously\ntested in a larger study, they might choose to use \u000b= 0:10 to reduce the chances of making a Type\nII error: prematurely ending research on what might turn out to be a promising drug.\n20To lower the rate of Type II error, the court could lower the standards for conviction, or in other words, lower the bar\nfor what constitutes su ﬃcient evidence of guilt (increase \u000b, e.g. to 0.10 instead of 0.05). This will result in more guilty\npeople being convicted, but also increase the rate of wrongful convictions, increasing the Type I error.\n222 CHAPTER 4. FOUNDATIONS FOR INFERENCE\nA government agency responsible for approving drugs to be marketed to the general popu-\nlation, however, would likely be biased towards minimizing the chances of making a Type I er-\nror—approving a drug that turns out to be unsafe or ine ﬀective. As a result, they might conduct\ntests at signiﬁcance level 0.01 in order to reduce the chances of concluding that a drug works when\nit is in fact ine ﬀective. The US FDA and the European Medical Agency (EMA) customarily require\nthat two independent studies show the e ﬃcacy of a new drug or regimen using \u000b= 0:05, though\nother values are sometimes used.\n4.3.5 Choosing between one-sided and two-sided tests\nIn some cases, the choice of a one-sided or two-sided test can inﬂuence whether the null\nhypothesis is rejected. For example, consider a sample for which the t-statistic is 1.80. If a two-\nsided test is conducted at \u000b= 0:05, thep-value is\nP(Z\u0014\u0000jtj) +P(Z\u0015jtj) = 2P(Z\u00151:80) = 0:072:\nThere is insu ﬃcient evidence to reject H0, sincep>\u000b . However, what if a one-sided test is\nconducted at \u000b= 0:05, withHA:\u0016>\u0016 0? In this case, the p-value is\nP(Z\u0015t) =P(Z\u00151:80) = 0:036:\nThe conclusion of the test is di ﬀerent: since p<\u000b , there is su ﬃcient evidence to reject H0in\nfavor of the alternative hypothesis. Figure 4.18 illustrates the di ﬀerent outcomes from the tests.\nµ = 0α2= 0.025t = 1.80\nFigure 4.18: Under a one-sided test at signiﬁcance level \u000b= 0.05, at-statistic of\n1.80 is within the rejection region (shaded light blue). However, it would not be\nwithin the rejection region under a two-sided test with \u000b= 0.05 (darker blue).\nTwo-sided tests are more \"conservative\" than one-sided tests; it is more di ﬃcult to reject the\nnull hypothesis with a two-sided test. The p-value for a one-sided test is exactly half the p-value\nfor a two-sided test conducted at the same signiﬁcance level; as a result, it is easier for the p-value\nfrom a one-sided test to be smaller than \u000b. Additionally, since the rejection region for a two-sided\ntest is divided between two tails, a test statistic needs to be more extreme in order to fall within\na rejection region. While the t-statistic of 1.80 is not within the two-sided rejection region, it is\nwithin the one-sided rejection region.21\n21The two-sided rejection regions are bounded by -1.96 and 1.96, while the one-sided rejection region begins at 1.65.\n4.3. HYPOTHESIS TESTING 223\nFor a ﬁxed sample size, a one-tailed test will have a smaller probability of Type II error in\ncomparison to a two-tailed test conducted at the same \u000blevel. In other words, with a one-sided\ntest, it is easier to reject the null hypothesis if the alternative is actually true.\nThe choice of test should be driven by context, although it is not always clear which test is\nappropriate. Since it is easier to reject H0with the one-tailed test, it might be tempting to always\nuse a one-tailed test when a signiﬁcant result in a particular direction would be interesting or\ndesirable.\nHowever, it is important to consider the potential consequences of missing a signiﬁcant dif-\nference in the untested direction. Generally, a two-sided test is the safest option, since it does not\nincorporate any existing biases about the direction of the results and can detect a di ﬀerence at ei-\nther the upper or lower tail. In the 1980s, researchers were interested in assessing a new set of\ndrugs expected to be more e ﬀective at reducing heart arrhythmias than previously available ther-\napies. They designed a one-sided clinical trial, convinced that the newer therapy would reduce\nmortality. The trial was quickly terminated due to an unanticipated e ﬀect of the drug; an indepen-\ndent review board found that the newer therapy was almost 4 times as likely to kill patients as a\nplacebo! In a clinical research setting, it can be dangerous and even unethical to conduct a one-\nsided test under the belief that there is no possibility of patient harm from the drug intervention\nbeing tested.\nOne-sided tests are appropriate if the consequences of missing an e ﬀect in the untested di-\nrection are negligible, or if a large observed di ﬀerence in the untested direction and a conclusion\nof \"no di ﬀerence\" lead to the same decision. For example, suppose that a company has developed\na drug to reduce blood pressure that is cheaper to produce than current options available on the\nmarket. If the drug is shown to be equally e ﬀective or more e ﬀective than an existing drug, the\ncompany will continue investing in it. Thus, they are only interested in testing the alternative hy-\npothesis that the new drug is less e ﬀective than the existing drug, in which case, they will stop the\nproject. It is acceptable to conduct a one-sided test in this situation since missing an e ﬀect in the\nother direction causes no harm.\nThe decision as to whether to use a one-sided or two-sided test must be made before data\nanalysis begins, in order to avoid biasing conclusions based on the results of a hypothesis test. In\nparticular, changing to a one-sided test after discovering that the results are \"almost\" signiﬁcant for\nthe two-sided test is unacceptable. Manipulating analyses in order to achieve low p-values leads to\ninvalid results that are often not replicable. Unfortunately, this kind of \"signiﬁcance-chasing\" has\nbecome widespread in published science, leading to concern that most current published research\nﬁndings are false.\n224 CHAPTER 4. FOUNDATIONS FOR INFERENCE\n4.3.6 The informal use of ppp-values\nFormal hypothesis tests are designed for settings where a decision or a claim about a hypoth-\nesis follows a test, such as in scientiﬁc publications where an investigator wishes to claim that an\nintervention changes an outcome. However, progress in science is usually based on a collection of\nstudies or experiments, and it is often the case that the results of one study are used as a guide for\nthe next study or experiment.\nSir Ronald Fisher was the ﬁrst to propose using p-values as one of the statistical tools for\nevaluating an experiment. In his view, an outcome from an experiment that would only happen 1 in\n20 times (p= 0.05) was worth investigating further. The use of p-values for formal decision making\ncame later. While valuable, formal hypothesis testing can often be overused; not all signiﬁcant\nresults should lead to a deﬁnitive claim, but instead prompt further analysis.\nThe formal use of p-values is emphasized here because of its prominence in the scientiﬁc\nliterature, and because the steps outlined are fundamental to the scientiﬁc method for empirical\nresearch: specify hypotheses, state in advance how strong the evidence should be to constitute\nsuﬃcient evidence against the null, specify the method of analysis and compute the test statistic,\ndraw a conclusion. These steps are designed to avoid the pitfall of choosing a hypothesis or method\nof analysis that is biased by the data and hence reaches a conclusion that may not be reproducible.\n4.4. NOTES 225\n4.4 Notes\nConﬁdence intervals and hypothesis testing are two of the central concepts in inference for\na population based on a sample. The conﬁdence interval shows a range of population parameter\nvalues consistent with the observed sample, and is often used to design additional studies. Hypoth-\nesis testing is a useful tool for evaluating the strength of the evidence against a working hypothesis\naccording to a pre-speciﬁed standard for accepting or rejecting hypotheses.\nThe calculation of p-values and conﬁdence intervals is relatively straightforward; given the\nnecessary summary statistics, \u000b, and conﬁdence coe ﬃcients, ﬁnding any p-value or conﬁdence in-\nterval simply involves a set of formulaic steps. However, the more di ﬃcult parts of any inference\nproblem are the steps that do not involve any calculations. Specifying appropriate null and alter-\nnative hypotheses for a test relies on an understanding of the problem context and the scientiﬁc\nsetting of the investigation. Similarly, a choice about a conﬁdence coe ﬃcient for an interval relies\non judgment as to balancing precision against the chance of possible error. It is also not necessarily\nobvious when a signiﬁcance level other than \u000b= 0:05 should be applied. These choices represent\nthe largest distinction between a true statistics problem as compared to a purely mathematical\nexercise.\nFurthermore, in order to rely on the conclusions drawn from making inferences, it is necessary\nto consider factors such as study design, measurement quality, and the validity of any assumptions\nmade. For example, is it valid to use the normal approximation to calculate p-values? In small to\nmoderate sample sizes (30 \u0014n\u001450), it may not be clear that the normal model is accurate. It is even\nnecessary to be cautious about the use and interpretation of the p-value. For example, an article\npublished in Nature about the mis-use of p-values references a published study that showed people\nwho meet their spouses online are more likely to have marital satisfaction, with p-value less than\n0.001. However, statistical signiﬁcance does not measure the importance or practical relevance of\na result; in this case, the change in happiness moved from 5.48 to 5.64 on a 7-point scale. A p-value\nreported without context or other evidence is uninformative and potentially deceptive.\nThese nuanced issues cannot be adequately covered in any introduction to statistics. It is\nunrealistic to encourage students to use their own judgment with aspects of inference that even ex-\nperienced investigators ﬁnd challenging. At the same time, it would also be misleading to suggest\nthat the choices are always clear-cut in practice. It seems best to o ﬀer some practical guidance for\ngetting started:\n– The default choice of \u000bis 0.05; similarly, the default conﬁdence coe ﬃcient for a conﬁdence\ninterval is 95%.\n– Unless it is clear from the context of a problem that change in only one direction from the\nnull hypothesis is of interest, the alternative hypothesis should be two-sided.\n– The use of a standard normal distribution to calculate p-values is reasonable for sample sizes\nof 30 or more if the distribution of data are not strongly skewed and there are no large out-\nliers. If there is skew or a few large outliers, sample sizes of 50 or more are usually su ﬃcient.\n– Pay attention to the context of a problem, particularly when formulating hypotheses and\ndrawing conclusions.\nThe next chapters will discuss methods of inference in speciﬁc settings, such as comparing\ntwo groups. These settings expand on the concepts discussed in this chapter and o ﬀer additional\nopportunities to practice calculating tests and intervals, reading problems for context, and check-\ning underlying assumptions behind methods of inference.\n226 CHAPTER 4. FOUNDATIONS FOR INFERENCE\nThe labs for the chapter reinforce conceptual understanding of conﬁdence intervals and hy-\npothesis tests, and their link to sampling variability using the data from the YRBSS and NHANES.\nBoth datasets are large enough to be viewed in an instructional setting as populations from which\nrepeated samples can be drawn. They are useful platforms for illustrating the conceptual role of\nhypothetical repeated sampling in the properties of tests and intervals, a topic which many stu-\ndents ﬁnd di ﬃcult. Students may ﬁnd the last lab for this chapter (Lab 4) particularly helpful\nfor understanding conceptual details of inference, such as the distinction between the signiﬁcance\nlevel\u000band thep-value, and the deﬁnition of \u000bas the Type I error rate.\n4.5. EXERCISES 227\n4.5 Exercises\n4.5.1 Variability in estimates\n4.1 Egg coloration. The evolutionary role of variation in bird egg coloration remains mysterious to biol-\nogists. One hypothesis suggests that egg color may play a role in sexual selection. For example, perhaps\nhealthier females are able to deposit more blue-green pigment into eggshells instead of using it themselves as\nan antioxidant. Researchers measured the blue-green chroma (BGC) of 70 di ﬀerent collared ﬂycatcher nests\nin an area of the Czech Republic.\nBlue−Green Chroma0.56 0.58 0.60 0.62 0.6405101520\nMin 0.5675\nQ1 0.5977\nMedian 0.6046\nMean 0.6052\nSD 0.0131\nQ3 0.6126\nMax 0.6355\n(a) What is the point estimate for the average BGC of nests?\n(b) What is the point estimate for the standard deviation of the BGC of eggs across nests?\n(c) Would a nest with average BGC of 0.63 be considered unusually high? Explain your reasoning.\n(d) Compute the standard error of the sample mean using the summary statistics.\n228 CHAPTER 4. FOUNDATIONS FOR INFERENCE\n4.2 Heights of adults. Researchers studying anthropometry collected body girth measurements and skeletal\ndiameter measurements, as well as age, weight, height and gender, for 507 physically active individuals. The\nhistogram below shows the sample distribution of heights in centimeters.22\nHeight150 160 170 180 190 200020406080100\nMin 147.2\nQ1 163.8\nMedian 170.3\nMean 171.1\nSD 9.4\nQ3 177.8\nMax 198.1\n(a) What is the point estimate for the average height of active individuals?\n(b) What is the point estimate for the standard deviation of the heights of active individuals? What about the\nIQR?\n(c) Is a person who is 1m 80cm (180 cm) tall considered unusually tall? And is a person who is 1m 55cm\n(155cm) considered unusually short? Explain your reasoning.\n(d) The researchers take another random sample of physically active individuals. Would you expect the mean\nand the standard deviation of this new sample to be the ones given above? Explain your reasoning.\n(e) The sample means obtained are point estimates for the mean height of all active individuals, if the sample\nof individuals is equivalent to a simple random sample. What measure is used to quantify the variability\nof such an estimate? Compute this quantity using the data from the original sample under the condition\nthat the data are a simple random sample.\n4.3 Hen eggs. The distribution of the number of eggs laid by a certain species of hen during their breeding\nperiod is on average, 35 eggs, with a standard deviation of 18.2. Suppose a group of researchers randomly\nsamples 45 hens of this species, counts the number of eggs laid during their breeding period, and records the\nsample mean. They repeat this 1,000 times, and build a distribution of sample means.\n(a) What is this distribution called?\n(b) Would you expect the shape of this distribution to be symmetric, right skewed, or left skewed? Explain\nyour reasoning.\n(c) Calculate the variability of this distribution and state the appropriate term used to refer to this value.\n(d) Suppose the researchers’ budget is reduced and they are only able to collect random samples of 10 hens.\nThe sample mean of the number of eggs is recorded, and we repeat this 1,000 times, and build a new\ndistribution of sample means. How will the variability of this new distribution compare to the variability\nof the original distribution?\n22G. Heinz et al. “Exploring relationships in body dimensions”. In: Journal of Statistics Education 11.2 (2003).\n4.5. EXERCISES 229\n4.5.2 Conﬁdence intervals\n4.4 Mental health, Part I. The 2010 General Social Survey asked the question: “For how many days during\nthe past 30 days was your mental health, which includes stress, depression, and problems with emotions, not\ngood?\" Based on responses from 1,151 US residents, the survey reported a 95% conﬁdence interval of 3.40 to\n4.24 days in 2010.\n(a) Interpret this interval in context of the data.\n(b) What does “95% conﬁdent\" mean? Explain in the context of the application.\n(c) If a new survey were to be done with 500 Americans, would the standard error of the estimate be larger,\nsmaller, or about the same? Assume the standard deviation has remained constant since 2010.\n4.5 Relaxing after work, Part I. The 2010 General Social Survey asked the question: “After an average work\nday, about how many hours do you have to relax or pursue activities that you enjoy?\" to a random sample\nof 1,155 Americans.23A 95% conﬁdence interval for the mean number of hours spent relaxing or pursuing\nactivities they enjoy is (1.38, 1.92).\n(a) Interpret this interval in context of the data.\n(b) Suppose another set of researchers reported a conﬁdence interval with a larger margin of error based on\nthe same sample of 1,155 Americans. How does their conﬁdence level compare to the conﬁdence level of\nthe interval stated above?\n(c) Suppose next year a new survey asking the same question is conducted, and this time the sample size\nis 2,500. Assuming that the population characteristics, with respect to how much time people spend\nrelaxing after work, have not changed much within a year. How will the margin of error of the new 95%\nconﬁdence interval compare to the margin of error of the interval stated above?\n(d) Suppose the researchers think that 90% conﬁdence interval would be more appropriate. Will this new\ninterval be smaller or larger than the original 95% conﬁdence interval? Justify your answer. (Assume that\nthe standard deviation remains constant).\n23National Opinion Research Center, General Social Survey, 2010.\n230 CHAPTER 4. FOUNDATIONS FOR INFERENCE\n4.6 Thanksgiving spending, Part I. The 2009 holiday retail season, which kicked o ﬀon November 27,\n2009 (the day after Thanksgiving), had been marked by somewhat lower self-reported consumer spending\nthan was seen during the comparable period in 2008. To get an estimate of consumer spending, 436 randomly\nsampled American adults were surveyed. Daily consumer spending for the six-day period after Thanksgiving,\nspanning the Black Friday weekend and Cyber Monday, averaged $84.71. A 95% conﬁdence interval based\non this sample is ($80.31, $89.11). Determine whether the following statements are true or false, and explain\nyour reasoning.\nSpending0 50 100 150 200 250 300020406080\n(a) We are 95% conﬁdent that the average spending of these 436 American adults is between $80.31 and\n$89.11.\n(b) This conﬁdence interval is not valid since the distribution of spending in the sample is right skewed.\n(c) 95% of random samples have a sample mean between $80.31 and $89.11.\n(d) We are 95% conﬁdent that the average spending of all American adults is between $80.31 and $89.11.\n(e) A 90% conﬁdence interval would be narrower than the 95% conﬁdence interval.\n(f) The margin of error is 4.4.\n4.7 Waiting at an ER, Part I. A hospital administrator hoping to improve wait times decides to estimate the\naverage emergency room waiting time at her hospital. She collects a simple random sample of 64 patients\nand determines the time (in minutes) between when they checked in to the ER until they were ﬁrst seen by a\ndoctor. A 95% conﬁdence interval based on this sample is (128 minutes, 147 minutes), which is based on the\nnormal model for the mean. Determine whether the following statements are true or false, and explain your\nreasoning.\n(a) This conﬁdence interval is not valid since we do not know if the population distribution of the ER wait\ntimes is nearly Normal.\n(b) We are 95% conﬁdent that the average waiting time of these 64 emergency room patients is between 128\nand 147 minutes.\n(c) We are 95% conﬁdent that the average waiting time of all patients at this hospital’s emergency room is\nbetween 128 and 147 minutes.\n(d) 95% of random samples have a sample mean between 128 and 147 minutes.\n(e) A 99% conﬁdence interval would be narrower than the 95% conﬁdence interval since we need to be more\nsure of our estimate.\n(f) The margin of error is 9.5 and the sample mean is 137.5.\n(g) Halving the margin of error of a 95% conﬁdence interval requires doubling the sample size.\n4.5. EXERCISES 231\n4.8 Age at ﬁrst marriage, Part I. The National Survey of Family Growth conducted by the Centers for\nDisease Control gathers information on family life, marriage and divorce, pregnancy, infertility, use of con-\ntraception, and men’s and women’s health. One of the variables collected on this survey is the age at ﬁrst\nmarriage. The histogram below shows the distribution of ages at ﬁrst marriage of 5,534 randomly sampled\nwomen between 2006 and 2010. The average age at ﬁrst marriage among these women is 23.44 with a stan-\ndard deviation of 4.72.24\nAge at first marriage10 15 20 25 30 35 40 4502004006008001000\nEstimate the average age at ﬁrst marriage of women using a 95% conﬁdence interval, and interpret this inter-\nval in context. Discuss any relevant assumptions.\n4.9 Mental health, Part II. The General Social Survey (GSS) is a sociological survey used to collect data on\ndemographic characteristics and attitudes of residents of the United States. The 2010 General Social Survey\nasked the question, \"For how many days during the past 30 days was your mental health not good?\" Based on\nresponses from 1,151 US adults, the survey reported a 95% conﬁdence interval of (3.40, 4.24) days. Assume\nthat the sampled US adults are representative of all US adults.\n(a) Identify each of the following statements as true or false. Justify your answers.\ni. The conﬁdence interval of (3.40, 4.24) contains the mean days out of the past 30 days that U.S. adults\nexperienced poor mental health.\nii. There is a 95% chance that the mean days out of the past 30 days that U.S. adults experienced poor\nmental health is within the conﬁdence interval (3.40, 4.24).\niii. If we repeated this survey 1,000 times and constructed a 95% conﬁdence interval each time, then\napproximately 950 of those intervals would contain the true mean days out of the past 30 days that\nU.S. adults experienced poor mental health.\niv. The survey provides statistically signiﬁcant evidence at the \u000b= 0:05 signiﬁcance level that the mean\ndays out of the past 30 days that U.S. adults experienced poor mental health is not 4.5 days.\nv. We can be 95% conﬁdent that the mean days out of the past 30 days that U.S. adults experienced poor\nmental health is 3.82 days.\nvi. We can be 95% conﬁdent that the interval (3.40, 4.24) days contains the mean days out of the past 30\ndays that the sampled adults experienced poor mental health.\n(b) Would you expect the 90% conﬁdence interval to be larger or smaller than the 95% conﬁdence interval?\nExplain your reasoning.\n(c) Calculate the 90% conﬁdence interval.\n24Centers for Disease Control and Prevention, National Survey of Family Growth, 2010.\n232 CHAPTER 4. FOUNDATIONS FOR INFERENCE\n4.10 Leisure time, Part III. In 2010, the General Social Survey collected responses from 1,154 US residents.\nThe survey is conducted face-to-face with an in-person interview of a randomly selected sample of adults.\nOne of the questions on the survey is \"After an average workday, about how many hours do you have to relax\nor pursue activities that you enjoy?\" A 95% conﬁdence interval from the 2010 GSS survey for the collected\nanswers is 3.53 to 3.83 hours. Identify each of the following statements as true or false. Explain your answers.\n(a) If the researchers wanted to report a conﬁdence interval with a smaller margin of error based on the same\nsample of 1,154 Americans, the conﬁdence interval would be larger.\n(b) We can be 95% conﬁdent that the interval (3.53, 3.83) hours contains the mean hours that the sampled\nadults have for leisure time after an average workday.\n(c) The conﬁdence interval of (3.53, 3.83) hours contains the mean hours that U.S. adults have for leisure time\nafter an average workday.\n(d) The survey provides statistically signiﬁcant evidence at the \u000b= 0:05 signiﬁcance level that the mean hours\nU.S. adults have for leisure time after the average workday is 3.6 hours.\n(e) There is a 5% chance that the interval (3.53, 3.83) hours does not contain the mean hours that U.S. adults\nhave for leisure time after an average workday.\n(f) The interval (3.53, 3.83) hours provides evidence at the \u000b= 0:05 signiﬁcance level that U.S. adults, on\naverage, have fewer than 3.9 hours of leisure time after a typical workday.\n4.5.3 Hypothesis testing\n4.11 Identify hypotheses, Part I. Write the null and alternative hypotheses in words and then symbols for\neach of the following situations.\n(a) New York is known as “the city that never sleeps\". A random sample of 25 New Yorkers were asked how\nmuch sleep they get per night. Do these data provide convincing evidence that New Yorkers on average\nsleep less than 8 hours a night?\n(b) Employers at a ﬁrm are worried about the e ﬀect of March Madness, a basketball championship held each\nspring in the US, on employee productivity. They estimate that on a regular business day employees\nspend on average 15 minutes of company time checking personal email, making personal phone calls,\netc. They also collect data on how much company time employees spend on such non- business activities\nduring March Madness. They want to determine if these data provide convincing evidence that employee\nproductivity decreases during March Madness.\n4.12 Identify hypotheses, Part II. Write the null and alternative hypotheses in words and using symbols for\neach of the following situations.\n(a) Since 2008, chain restaurants in California have been required to display calorie counts of each menu\nitem. Prior to menus displaying calorie counts, the average calorie intake of diners at a restaurant was\n1100 calories. After calorie counts started to be displayed on menus, a nutritionist collected data on the\nnumber of calories consumed at this restaurant from a random sample of diners. Do these data provide\nconvincing evidence of a di ﬀerence in the average calorie intake of a diners at this restaurant?\n(b) Based on the performance of those who took the GRE exam between July 1, 2004 and June 30, 2007, the\naverage Verbal Reasoning score was calculated to be 462. In 2011 the average verbal score was slightly\nhigher. Do these data provide convincing evidence that the average GRE Verbal Reasoning score has\nchanged since 2004?\n4.5. EXERCISES 233\n4.13 Online communication. A study suggests that the average college student spends 10 hours per week\ncommunicating with others online. You believe that this is an underestimate and decide to collect your own\nsample for a hypothesis test. You randomly sample 60 students from your dorm and ﬁnd that on average they\nspent 13.5 hours a week communicating with others online. A friend of yours, who o ﬀers to help you with the\nhypothesis test, comes up with the following set of hypotheses. Indicate any errors you see.\nH0:¯x<10hours\nHA:¯x>13:5hours\n4.14 Age at ﬁrst marriage, Part II. Exercise 4.8 presents the results of a 2006 - 2010 survey showing that\nthe average age of women at ﬁrst marriage is 23.44. Suppose a social scientist believes that this value has\nincreased in 2012, but she would also be interested if she found a decrease. Below is how she set up her\nhypotheses. Indicate any errors you see.\nH0:¯x= 23:44years\nHA:¯x>23:44years\n4.15 Waiting at an ER, Part II. Exercise 4.7 provides a 95% conﬁdence interval for the mean waiting time\nat an emergency room (ER) of (128 minutes, 147 minutes). Answer the following questions based on this\ninterval.\n(a) A local newspaper claims that the average waiting time at this ER exceeds 3 hours. Is this claim supported\nby the conﬁdence interval? Explain your reasoning.\n(b) The Dean of Medicine at this hospital claims the average wait time is 2.2 hours. Is this claim supported\nby the conﬁdence interval? Explain your reasoning.\n(c) Without actually calculating the interval, determine if the claim of the Dean from part (b) would be\nsupported based on a 99% conﬁdence interval?\n4.16 Gifted children, Part I. Researchers investigating characteristics of gifted children collected data from\nschools in a large city on a random sample of thirty-six children who were identiﬁed as gifted children soon\nafter they reached the age of four. The following histogram shows the distribution of the ages (in months) at\nwhich these children ﬁrst counted to 10 successfully. Also provided are some sample statistics.25\nAge child first counted to 10 (in months)20 25 30 35 40036\nn 36\nmin 21\nmean 30.69\nsd 4.31\nmax 39\n(a) Are conditions for inference satisﬁed?\n(b) Suppose an online survey reports that children ﬁrst count to 10 successfully when they are 32 months\nold, on average. Perform a hypothesis test to evaluate if these data provide convincing evidence that the\naverage age at which gifted children ﬁrst count to 10 successfully is less than the general average of 32\nmonths. Use a signiﬁcance level of 0.10.\n(c) Interpret the p-value in context of the hypothesis test and the data.\n(d) Calculate a 90% conﬁdence interval for the average age at which gifted children ﬁrst count to 10 success-\nfully.\n(e) Do your results from the hypothesis test and the conﬁdence interval agree? Explain.\n25F.A. Graybill and H.K. Iyer. Regression Analysis: Concepts and Applications . Duxbury Press, 1994, pp. 511–516.\n234 CHAPTER 4. FOUNDATIONS FOR INFERENCE\n4.17 Nutrition labels. The nutrition label on a bag of potato chips says that a one ounce (28 gram) serving\nof potato chips has 130 calories and contains ten grams of fat, with three grams of saturated fat. A random\nsample of 35 bags yielded a sample mean of 134 calories with a standard deviation of 17 calories. Is there\nevidence that the nutrition label does not provide an accurate measure of calories in the bags of potato chips?\nWe have veriﬁed the independence, sample size, and skew conditions are satisﬁed.\n4.18 Waiting at an ER, Part III. The hospital administrator mentioned in Exercise 4.7 randomly selected 64\npatients and measured the time (in minutes) between when they checked in to the ER and the time they were\nﬁrst seen by a doctor. The average time is 137.5 minutes and the standard deviation is 39 minutes. She is\ngetting grief from her supervisor on the basis that the wait times in the ER has increased greatly from last\nyear’s average of 127 minutes. However, she claims that the increase is probably just due to chance.\n(a) Calculate a 95% conﬁdence interval. Is the change in wait times statistically signiﬁcant at the \u000b= 0:05\nlevel?\n(b) Would the conclusion in part (a) change if the signiﬁcance level were changed to \u000b= 0:01?\n(c) Is the supervisor justiﬁed in criticizing the hospital administrator regarding the change in ER wait times?\nHow might you present an argument in favor of the administrator?\n4.19 Birth weights. Suppose an investigator takes a random sample of n= 50 birth weights from several\nteaching hospitals located in an inner-city neighborhood. In her random sample, the sample mean xis 3,150\ngrams and the standard deviation is 250 grams.\n(a) Calculate a 95% conﬁdence interval for the population mean birth weight in these hospitals.\n(b) The typical weight of a baby at birth for the US population is 3,250 grams. The investigator suspects that\nthe birth weights of babies in these teaching hospitals is di ﬀerent than 3,250 grams, but she is not sure if\nit is smaller (from malnutrition) or larger (because of obesity prevalence in mothers giving birth at these\nhospitals). Carry out the hypothesis test that she would conduct.\n4.20 Gifted children, Part II. Exercise 4.16 describes a study on gifted children. In this study, along with\nvariables on the children, the researchers also collected data on the mother’s and father’s IQ of the 36 ran-\ndomly sampled gifted children. The histogram below shows the distribution of mother’s IQ. Also provided\nare some sample statistics.\nMother's IQ100 105 110 115 120 125 130 13504812\nn 36\nmin 101\nmean 118.2\nsd 6.5\nmax 131\n(a) Perform a hypothesis test to evaluate if these data provide convincing evidence that the average IQ of\nmothers of gifted children is di ﬀerent than the average IQ for the population at large, which is 100. Use a\nsigniﬁcance level of 0.10.\n(b) Calculate a 90% conﬁdence interval for the average IQ of mothers of gifted children.\n(c) Do your results from the hypothesis test and the conﬁdence interval agree? Explain.\n4.5. EXERCISES 235\n4.21 Testing for ﬁbromyalgia. A patient named Diana was diagnosed with ﬁbromyalgia, a long-term syn-\ndrome of body pain, and was prescribed anti-depressants. Being the skeptic that she is, Diana didn’t initially\nbelieve that anti-depressants would help her symptoms. However after a couple months of being on the med-\nication she decides that the anti-depressants are working, because she feels like her symptoms are in fact\ngetting better.\n(a) Write the hypotheses in words for Diana’s skeptical position when she started taking the anti-depressants.\n(b) What is a Type 1 Error in this context?\n(c) What is a Type 2 Error in this context?\n4.22 Testing for food safety. A food safety inspector is called upon to investigate a restaurant with a few\ncustomer reports of poor sanitation practices. The food safety inspector uses a hypothesis testing framework\nto evaluate whether regulations are not being met. If he decides the restaurant is in gross violation, its license\nto serve food will be revoked.\n(a) Write the hypotheses in words.\n(b) What is a Type 1 Error in this context?\n(c) What is a Type 2 Error in this context?\n(d) Which error is more problematic for the restaurant owner? Why?\n(e) Which error is more problematic for the diners? Why?\n(f) As a diner, would you prefer that the food safety inspector requires strong evidence or very strong evi-\ndence of health concerns before revoking a restaurant’s license? Explain your reasoning.\n4.23 Which is higher? In each part below, there is a value of interest and two scenarios (I and II). For each\npart, report if the value of interest is larger under scenario I, scenario II, or whether the value is equal under\nthe scenarios.\n(a) The standard error of ¯xwhens= 120 and (I) n = 25 or (II) n = 125.\n(b) The margin of error of a conﬁdence interval when the conﬁdence level is (I) 90% or (II) 80%.\n(c) The p-value for a Z-statistic of 2.5 when (I) n = 500 or (II) n = 1000.\n(d) The probability of making a Type 2 Error when the alternative hypothesis is true and the signiﬁcance level\nis (I) 0.05 or (II) 0.10.\n4.24 True or false. Determine if the following statements are true or false, and explain your reasoning. If\nfalse, state how it could be corrected.\n(a) If a given value (for example, the null hypothesized value of a parameter) is within a 95% conﬁdence\ninterval, it will also be within a 99% conﬁdence interval.\n(b) Decreasing the signiﬁcance level ( \u000b) will increase the probability of making a Type 1 Error.\n(c) Suppose the null hypothesis is \u0016= 5 and we fail to reject H0. Under this scenario, the true population\nmean is 5.\n(d) If the alternative hypothesis is true, then the probability of making a Type 2 Error and the power of a test\nadd up to 1.\n(e) With large sample sizes, even small di ﬀerences between the null value and the true value of the parameter,\na diﬀerence often called the e ﬀect size , will be identiﬁed as statistically signiﬁcant.\n236\nChapter 5\nInference for\nnumerical data\n5.1 Single-sample inference with the t-distribution\n5.2 Two-sample test for paired data\n5.3 Two-sample test for independent data\n5.4 Power calculations for a difference of means\n5.5 Comparing means with ANOVA\n5.6 Notes\n5.7 Exercises\n237\nChapter 4 introduced some primary tools of statistical inference—point estimates,\ninterval estimates, and hypothesis tests. This chapter discusses settings where\nthese tools are often used, including the analysis of paired observations and the\ncomparison of two or more independent groups. The chapter also covers the im-\nportant topic of estimating an appropriate sample size when a study is being de-\nsigned. The chapter starts with introducing a new distribution, the t-distribution,\nwhich can be used for small sample sizes.\nFor labs, slides, and other resources, please visit\nwww.openintro.org/book/biostat\n238 CHAPTER 5. INFERENCE FOR NUMERICAL DATA\n5.1 Single-sample inference with the ttt-distribution\nThe tools studied in Chapter 4 all made use of the t-statistic from a sample mean,\nt=x\u0000\u0016\ns=pn;\nwhere the parameter \u0016is a population mean, xandsare the sample mean and standard deviation,\nandnis the sample size. Tests and conﬁdence intervals were restricted to samples of at least 30\nindependent observations from a population where there was no evidence of strong skewness. This\nallowed for the Central Limit Theorem to be applied, justifying use of the normal distribution to\ncalculate probabilities associated with the t-statistic.\nIn sample sizes smaller than 30, if the data are approximately symmetric and there are no large\noutliers, the t-statistic has what is called a t-distribution. When the normal distribution is used as\nthe sampling distribution of the t-statistic,sis essentially being treated as a good replacement\nfor the unknown population standard deviation \u001b. However, the sample standard deviation s, as\nan estimate of \u001b, has its own inherent variability like x. Thetdensity function adjusts for the\nvariability in sby having more probability in the left and right tails than the normal distribution.\n5.1.1 The ttt-distribution\nFigure 5.1 shows a t-distribution and normal distribution. Like the standard normal distribu-\ntion, thet-distribution is unimodal and symmetric about zero. However, the tails of a t-distribution\nare thicker than for the normal, so observations are more likely to fall beyond two standard devia-\ntions from the mean than under the normal distribution.1While the estimate of the standard error\nwill be less accurate with smaller sample sizes, the thick tails of the t-distribution correct for the\nvariability in s.\n−4 −2 0 2 4\nFigure 5.1: Comparison of a t-distribution (solid line) and a normal distribution\n(dotted line).\nThet-distribution can be described as a family of symmetric distributions with a single pa-\nrameter: degrees of freedom, which equals n\u00001. Severalt-distributions are shown in Figure 5.2.\nWhen there are more degrees of freedom, the t-distribution looks very much like the standard\nnormal distribution. With degrees of freedom of 30 or more, the t-distribution is nearly indistin-\nguishable from the normal distribution. Since the t-statistics in Chapter 4 were associated with\nsample sizes of at least 30, the degrees of freedom for the corresponding t-distributions were large\nenough to justify use of the normal distribution to calculate probabilities.\n1The standard deviation of the t-distribution is actually a little more than 1. However, it is useful to think of the\nt-distribution as having a standard deviation of 1 in the context of using it to conduct inference.\n5.1. SINGLE-SAMPLE INFERENCE WITH THE T-DISTRIBUTION 239\n−2 0 2 4 6 8normal\nt, df = 8\nt, df = 4\nt, df = 2\nt, df = 1\nFigure 5.2: The larger the degrees of freedom, the more closely the t-distribution\nresembles the standard normal model.\nDEGREES OF FREEDOM (DF)\nThe degrees of freedom characterize the shape of the t-distribution. The larger the degrees of\nfreedom, the more closely the distribution approximates the normal model.\nProbabilities for the t-distribution can be calculated either by using distribution tables or\nusing statistical software. The use of software has become the preferred method because it is more\naccurate, allows for complete ﬂexibility in the choice of t-values on the horizontal axis, and is not\nlimited to a small range of degrees of freedom. The remainder of this section illustrates the use of\nat-table , partially shown in Figure 5.3, in place of the normal probability table. A larger t-table is\nin Appendix B.2 on page 466. The Rlabs illustrate the use of software to calculate probabilities for\nthet-distribution. Readers intending to use software can skip to the next section.\none tail 0.100 0.050 0.025 0.010 0.005\ntwo tails 0.200 0.100 0.050 0.020 0.010\ndf 1 3.08 6.31 12.71 31.82 63.66\n2 1.89 2.92 4.30 6.96 9.92\n3 1.64 2.35 3.18 4.54 5.84\n:::::::::::::::\n17 1.33 1.74 2.11 2.57 2.90\n18 1.33 1.73 2.10 2.55 2.88\n19 1.33 1.73 2.09 2.54 2.86\n20 1.33 1.72 2.09 2.53 2.85\n:::::::::::::::\n400 1.28 1.65 1.97 2.34 2.59\n500 1.28 1.65 1.96 2.33 2.59\n1 1.28 1.64 1.96 2.33 2.58\nFigure 5.3: An abbreviated look at the t-table. Each row represents a di ﬀerent\nt-distribution. The columns describe the cuto ﬀs for speciﬁc tail areas. The row\nwithdf= 18 has been highlighted .\nEach row in the t-table represents a t-distribution with di ﬀerent degrees of freedom. The\ncolumns correspond to tail probabilities. For instance, for a t-distribution with df= 18, row 18 is\nused (highlighted in Figure 5.3). The value in this row that identiﬁes the cuto ﬀfor an upper tail of\n5% is found in the column where one tail is 0.050. This cuto ﬀis 1.73. The cuto ﬀfor the lower 5% is\n-1.73; just like the normal distribution, all t-distributions are symmetric. If the area in each tail is\n5%, then the area in two tails is 10%; thus, this column can also be described as the column where\ntwo tails is 0.100.\n240 CHAPTER 5. INFERENCE FOR NUMERICAL DATA\nEXAMPLE 5.1\nWhat proportion of the t-distribution with 18 degrees of freedom falls below -2.10?\nJust like for a normal probability problem, it is advisable to start by drawing the distribution\nand shading the area below -2.10, as shown in Figure 5.4. From the table, identify the column\ncontaining the absolute value of -2.10; it is the third column. Since this is just the probability in\none tail, examine the top line of the table; a one tail area for a value in the third column corresponds\nto 0.025. About 2.5% of the distribution falls below -2.10.\n−4 −2 0 2 4\nFigure 5.4: The t-distribution with 18 degrees of freedom. The area below -2.10\nhas been shaded.\nEXAMPLE 5.2\nAt-distribution with 20 degrees of freedom is shown in the left panel of Figure 5.5. Estimate the\nproportion of the distribution falling above 1.65 and below -1.65.\nIdentify the row in the t-table using the degrees of freedom: df\u000020. Then, look for 1.65; the value\nis not listed, and falls between the ﬁrst and second columns. Since these values bound 1.65, their\ntail areas will bound the tail area corresponding to 1.65. The two tail area of the ﬁrst and second\ncolumns is between 0.100 and 0.200. Thus, between 10% and 20% of the distribution is more\nthan 1.65 standard deviations from the mean. The precise area can be calculated using statistical\nsoftware: 0.1146.\n−4 −2 0 2 4\nFigure 5.5: The t-distribution with 20 degrees of freedom, with the area further\nthan 1.65 away from 0 shaded.\n5.1. SINGLE-SAMPLE INFERENCE WITH THE T-DISTRIBUTION 241\n5.1.2 Using the ttt-distribution for tests and conﬁdence intervals for a popu-\nlation mean\nChapter 4 provided formulas for tests and conﬁdence intervals for population means in ran-\ndom samples large enough for the t-statistic to have a nearly normal distribution. In samples\nsmaller than 30 from approximately symmetric distributions without large outliers, the t-statistic\nhas at-distribution with degrees of freedom equal to n\u00001. Just like inference in larger samples,\ninference using the t-distribution also requires that the observations in the sample be indepen-\ndent. Random samples from very large populations always produce independent observations;\nin smaller populations, observations will be approximately independent as long as the size of the\nsample is no larger than 10% of the population.\nFormulas for tests and intervals using the t\u0000distribution are very similar to those using the\nnormal distribution. For a sample of size nwith sample mean xand standard deviation s, two-sided\nconﬁdence intervals with conﬁdence coe ﬃcient 100(1\u0000\u000b)% have the form\nx\u0006t?\ndf\u0002SE;\nwhere SE is the standard error of the sample mean ( s=pn) andt?\ndfis the point on a t-distribution\nwithn\u00001 degrees of freedom and area (1 \u0000\u000b=2) to its left.\nA one-sided interval with the same conﬁdence coe ﬃcient will have the form\nx+t?\ndf\u0002SE (one-sided upper conﬁdence interval) ;or\nx\u0000t?\ndf\u0002SE (one-sided lower conﬁdence interval) ;\nexcept that in this case t?\ndfis the point on a t-distribution with n\u00001 degrees of freedom and area\n(1\u0000\u000b) to its left.\nWith the ability to conveniently calculate t?for any sample size or associated \u000bvia computing\nsoftware, the t-distribution can be used by default over the normal distribution. The rule of thumb\nthatn>30 qualiﬁes as a large enough sample size to use the normal distribution dates back to\nwhen it was necessary to rely on distribution tables.\n242 CHAPTER 5. INFERENCE FOR NUMERICAL DATA\nEXAMPLE 5.3\nDolphins are at the top of the oceanic food chain; as a consequence, dangerous substances such\nas mercury tend to be present in their organs and muscles at high concentrations. In areas where\ndolphins are regularly consumed, it is important to monitor dolphin mercury levels. This example\nuses data from a random sample of 19 Risso’s dolphins from the Taiji area in Japan.2Calculate the\n95% conﬁdence interval for average mercury content in Risso’s dolphins from the Taiji area using\nthe data in Figure 5.6.\nThe observations are a simple random sample consisting of less than 10% of the population, so\nindependence of the observations is reasonable. The summary statistics in Figure 5.6 do not suggest\nany skew or outliers; all observations are within 2.5 standard deviations of the mean. Based on this\nevidence, the approximate normality assumption seems reasonable.\nUse thet-distribution to calculate the conﬁdence interval:\nx\u0006t?\ndf\u0002SE =x\u0006t?\n18\u0002s=p\nn\n= 4:4\u00062:10\u00022:3=p\n19\n= (3:29;5:51)\u0016g/wet g:\nThet?point can be read from the t-table on page 239, in the column with area totaling 0.05 in the\ntwo tails (third column) and the row with 18 degrees of freedom. Based on these data, one can be\n95% conﬁdent the average mercury content of muscles in Risso’s dolphins is between 3.29 and 5.51\n\u0016g/wet gram.\nAlternatively, the t?point can be calculated in Rwith the function qt, which returns a value of\n2.1009.\nnx s minimum maximum\n19 4.4 2.3 1.7 9.2\nFigure 5.6: Summary of mercury content in the muscle of 19 Risso’s dolphins\nfrom the Taiji area. Measurements are in \u0016g/wet g (micrograms of mercury per\nwet gram of muscle).\n2Taiji is a signiﬁcant source of dolphin and whale meat in Japan. Thousands of dolphins pass through the Taiji area\nannually; assume that these 19 dolphins represent a simple random sample. Data reference: Endo T and Haraguchi K.\n2009. High mercury levels in hair samples from residents of Taiji, a Japanese whaling town. Marine Pollution Bulletin\n60(5):743-747.\n5.1. SINGLE-SAMPLE INFERENCE WITH THE T-DISTRIBUTION 243\nGUIDED PRACTICE 5.4\nThe FDA’s webpage provides some data on mercury content of various ﬁsh species.3From a sample\nof 15 white croaker (Paciﬁc), a sample mean and standard deviation were computed as 0.287 and\n0.069 ppm (parts per million), respectively. The 15 observations ranged from 0.18 to 0.41 ppm.\nAssume that these observations are independent. Based on summary statistics, does the normality\nassumption seem reasonable? If so, calculate a 90% conﬁdence interval for the average mercury\ncontent of white croaker (Paciﬁc).4\nEXAMPLE 5.5\nAccording to the EPA, regulatory action should be taken if ﬁsh species are found to have a mercury\nlevel of 0.5 ppm or higher. Conduct a formal signiﬁcance test to evaluate whether the average\nmercury content of croaker white ﬁsh (Paciﬁc) is di ﬀerent from 0.50 ppm. Use \u000b= 0:05.\nThe FDA regulatory guideline is a ‘one-sided’ statement; ﬁsh should not be eaten if the mercury\nlevel is larger than a certain value. However, without prior information on whether the mercury in\nthis species tends to be high or low, it is best to do a two-sided test.\nState the hypotheses: H0:\u0016= 0:5 vsHA:\u0016,0:5. Let\u000b= 0:05.\nCalculate the t-statistic:\nt=x\u0000\u00160\nSE=0:287\u00000:50\n0:069=p\n15=\u000011:96\nThe probability that the absolute value of a t-statistic with 14 df is smaller than -11.96 is smaller\nthan 0.01. Thus, p<0:01. There is evidence to suggest at the \u000b= 0:05 signiﬁcance level that the\naverage mercury content of this ﬁsh species is lower than 0.50 ppm, since xis less than 0.50.\n3www.fda.gov/food/foodborneillnesscontaminants/metals/ucm115644.htm\n4There are no obvious outliers; all observations are within 2 standard deviations of the mean. If there is skew, it is not\nevident. There are no red ﬂags for the normal model based on this (limited) information. x\u0006t?\n14\u0002SE!0:287\u00061:76\u0002\n0:0178!(0:256;0:318). We are 90% conﬁdent that the average mercury content of croaker white ﬁsh (Paciﬁc) is between\n0.256 and 0.318 ppm.\n244 CHAPTER 5. INFERENCE FOR NUMERICAL DATA\n5.2 Two-sample test for paired data\nIn the 2000 Olympics, was the use of a new wetsuit design responsible for an observed in-\ncrease in swim velocities? In a study designed to investigate this question, twelve competitive\nswimmers swam 1500 meters at maximal speed, once wearing a wetsuit and once wearing a regu-\nlar swimsuit.5The order of wetsuit versus swimsuit was randomized for each of the 12 swimmers.\nFigure 5.7 shows the average velocity recorded for each swimmer, measured in meters per second\n(m/s).6\nswimmer.number wet.suit.velocity swim.suit.velocity velocity.di ﬀ\n1 1 1.57 1.49 0.08\n2 2 1.47 1.37 0.10\n3 3 1.42 1.35 0.07\n4 4 1.35 1.27 0.08\n5 5 1.22 1.12 0.10\n6 6 1.75 1.64 0.11\n7 7 1.64 1.59 0.05\n8 8 1.57 1.52 0.05\n9 9 1.56 1.50 0.06\n10 10 1.53 1.45 0.08\n11 11 1.49 1.44 0.05\n12 12 1.51 1.41 0.10\nFigure 5.7: Paired Swim Suit Data\nThe swimsuit velocity data are an example of paired data , in which two sets of observations\nare uniquely paired so that an observation in one set matches an observation in the other; in this\ncase, each swimmer has two measured velocities, one with a wetsuit and one with a swimsuit. A\nnatural measure of the e ﬀect of the wetsuit on swim velocity is the di ﬀerence between the mea-\nsured maximum velocities ( velocity.diff = wet.suit.velocity - swim.suit.velocity ). Even\nthough there are two measurements per swimmer, using the di ﬀerence in velocities as the variable\nof interest allows for the problem to be approached like those in Section 5.1. Although it was not\nexplicitly noted, the data used in Section 4.3.1 were paired; each respondent had both an actual\nand desired weight.\nSuppose the parameter \u000eis the population average of the di ﬀerence in maximum velocities\nduring a 1500m swim if all competitive swimmers recorded swim velocities with each suit type. A\nhypothesis test can then be conducted with the null hypothesis that the mean population di ﬀerence\nin swim velocities between suit types equals 0 (i.e., there is no di ﬀerence in population average\nswim velocities), H0:\u000e= 0, against the alternative that the di ﬀerence is non-zero, HA:\u000e,0.\nSTATING HYPOTHESES FOR PAIRED DATA\nWhen testing a hypothesis about paired data, compare the groups by testing whether the pop-\nulation mean of the di ﬀerences between the groups equals 0.\n– For a two-sided test, H0:\u000e= 0;HA:\u000e,0.\n– For a one-sided test, either H0:\u000e= 0;HA:\u000e>0 orH0:\u000e= 0;HA:\u000e<0.\n5De Lucas et. al, The e ﬀects of wetsuits on physiological and biomechanical indices during swimming. Journal of Science\nand Medicine in Sport, 2000; 3(1): 1-8\n6The data are available as swim in the oibiostat Rpackage. The data are also used in Lock et. al Statistics, Unlocking the\nPower of Data , Wiley, 2013.\n5.2. TWO-SAMPLE TEST FOR PAIRED DATA 245\nSome important assumptions are being made. First, it is assumed that the data are a random\nsample from the population. While the observations are likely independent, it is more di ﬃcult to\njustify that this sample of 12 swimmers is randomly drawn from the entire population of compet-\nitive swimmers. Nevertheless, it is often assumed in problems such as these that the participants\nare reasonably representative of competitive swimmers. Second, it is assumed that the population\nof diﬀerences is normally distributed. This is a small sample, one in which normality would be\ndiﬃcult to conﬁrm. The dot plot for the di ﬀerence in velocities in Figure 5.8 shows approximate\nsymmetry.\nDifference in Swim Velocities (m/s)0.05 0.06 0.07 0.08 0.09 0.10 0.11\nFigure 5.8: A dot plot of di ﬀerences in swim velocities.\nLetxdiﬀdenote the sample average of the di ﬀerences in maximum velocity, sdiﬀthe sample\nstandard deviation of the di ﬀerences, and nthe number of pairs in the dataset. The t-statistic used\nto testH0vs.HAis:\nxdiﬀ\u0000\u000e0\nsdiﬀ=pn;\nwhere in this case \u000e0= 0.7\nEXAMPLE 5.6\nUsing the data in Figure 5.7, conduct a two-sided hypothesis test at \u000b= 0:05 to assess whether\nthere is evidence to suggest that wetsuits have an e ﬀect on swim velocities during a 1500m swim.\nThe hypotheses are H0:\u000e= 0 andHA:\u000e,0. Let\u000b= 0:05.\nCalculate the t-statistic:\nt=xdiﬀ\u0000\u000e0\nsdiﬀ=pn=0:078\u00000\n0:022=p\n12= 12:32\nThe two-sided p-value is\np=P(T <\u000012.32) +P(T >12.32);\nwherethas at-distribution with n\u00001 = 11 degrees of freedom. The t-table shows that p<0:01.\nSoftware can be used to show that p= 8:9\u000210\u00008, a very small value indeed.\nThe data support the claim that the wetsuits changed swim velocity in a 1500m swim. The observed\naverage increase of 0.078 m/s is signiﬁcantly di ﬀerent than the null hypothesis of no change, and\nsuggests that swim velocities are higher when swimmers wear wetsuits as opposed to swimsuits.\nCalculating conﬁdence intervals for paired data is also based on the di ﬀerences between the\nvalues in each pair; the same approach as for single-sample data can be applied on the di ﬀerences.\nFor example, a two-sided 95% conﬁdence interval for paired data has the form:\n \nxdiﬀ\u0000t?\ndf\u0002sdiﬀpn;xdiﬀ+t?\ndf\u0002sdiﬀpn!\n;\nwheret?is the point on a t-distribution with df=n\u00001 fornpairs, with area 0.025 to its right.\n7This value is speciﬁed by the null hypothesis of no di ﬀerence.\n246 CHAPTER 5. INFERENCE FOR NUMERICAL DATA\nGUIDED PRACTICE 5.7\nUsing the data in Figure 5.7, calculate a 95% conﬁdence interval for the average di ﬀerence in swim\nvelocities during a 1500m swim. Is the interval consistent with the results of the hypothesis test?8\nThe general approach when analyzing paired data is to ﬁrst calculate the di ﬀerences between\nthe values in each pair, then use those di ﬀerences in methods for conﬁdence intervals and tests for\na single sample. Any conclusion from an analysis should be stated in terms of the original paired\nmeasurements.\n8Use the values of xdiﬀandsdiﬀas calculated previously: 0.078 and 0.022. The t?value of 2.20 has df= 11 and 0.025\narea to the right. The conﬁdence interval is (0 :078\u00060:022p\n12)!(0.064, 0.091) m/s. With 95% conﬁdence, \u000elies between 0.064\nm/s and 0.09 m/s. The interval does not include 0 (no change), which is consistent with the result of the hypothesis test.\n5.3. TWO-SAMPLE TEST FOR INDEPENDENT DATA 247\n5.3 Two-sample test for independent data\nDoes treatment using embryonic stem cells (ESCs) help improve heart function following a\nheart attack? New and potentially risky treatments are sometimes tested in animals before studies\nin humans are conducted. In a 2005 paper in Lancet , Menard, et al. describe an experiment in\nwhich 18 sheep with induced heart attacks were randomly assigned to receive cell transplants\ncontaining either ESCs or inert material.9Various measures of cardiac function were measured 1\nmonth after the transplant.\nThis design is typical of an intervention study. The analysis of such an experiment is an\nexample of drawing inference about the di ﬀerence in two population means, \u00161\u0000\u00162, when the data\nare independent, i.e., not paired. The point estimate of the di ﬀerence,x1\u0000x2, is used to calculate a\nt-statistic that is the basis of conﬁdence intervals and tests.\n5.3.1 Conﬁdence interval for a difference of means\nFigure 5.9 contains summary statistics for the 18 sheep.10Percent change in heart pumping\ncapacity was measured for each sheep. A positive value corresponds to increased pumping ca-\npacity, which generally suggests a stronger recovery from the heart attack. Is there evidence for a\npotential treatment e ﬀect of administering stem cells?\nnx s\nESCs 9 3.50 5.17\ncontrol 9 -4.33 2.76\nFigure 5.9: Summary statistics of the embryonic stem cell study.\n9Menard C, et al., Transplantation of cardiac-committed mouse embryonic stem cells to infarcted sheep myocardium:\na preclinical 2005; 366:1005-12, doi https://doi.org/10.1016/S0140-6736(05)67380-1\n10The data are accessible as the dataset stem.cells in the openintro Rpackage.\n248 CHAPTER 5. INFERENCE FOR NUMERICAL DATA\nfrequency\n−10% −5% 0% 5% 10% 15%Embryonic stem cell transplant\nChange in heart pumping function0123frequency\n−10% −5% 0% 5% 10% 15%0123Control (no treatment)\nChange in heart pumping function\nFigure 5.10: Histograms for both the embryonic stem cell group and the control\ngroup. Higher values are associated with greater improvement.\nFigure 5.10 shows that the distributions of percent change do not have any prominent outliers,\nwhich would indicate a deviation from normality; this suggests that each sample mean can be\nmodeled using a t-distribution. Additionally, the sheep in the study are independent of each other,\nand the sheep between groups are also independent. Thus, the t-distribution can be used to model\nthe diﬀerence of the two sample means.\nUSING THE ttt-DISTRIBUTION FOR A DIFFERENCE IN MEANS\nThet-distribution can be used for inference when working with the standardized di ﬀerence\nof two means if (1) each sample meets the conditions for using the t-distribution and (2) the\nsamples are independent.\nA conﬁdence interval for a di ﬀerence of two means has the same basic structure as previously\ndiscussed conﬁdence intervals:\n(x1\u0000x2)\u0006t?\ndf\u0002SE:\nThe following formula is used to calculate the standard error of x1\u0000x2. Since\u001bis typically\nunknown, the standard error is estimated by using sin place of\u001b.\nSEx1\u0000x2=s\n\u001b2\n1\nn1+\u001b2\n2\nn2\u0019s\ns2\n1\nn1+s2\n2\nn2:\nIn this setting, the t-distribution has a somewhat complicated formula for the degrees of free-\ndom that is usually calculated with software.11An alternative approach uses the smaller of n1\u00001\nandn2\u00001 as the degrees of freedom.12\n11See Section 5.6 for the formula.\n12This technique for degrees of freedom is conservative with respect to a Type 1 Error; it is more di ﬃcult to reject the\nnull hypothesis using this approach for degrees of freedom.\n5.3. TWO-SAMPLE TEST FOR INDEPENDENT DATA 249\nDISTRIBUTION OF A DIFFERENCE OF SAMPLE MEANS\nThe sample di ﬀerence of two means, x1\u0000x2, can be modeled using the t-distribution and the\nstandard error\nSEx1\u0000x2=q\ns2\n1\nn1+s2\n2\nn2(5.8)\nwhen each sample mean can itself be modeled using a t-distribution and the samples are\nindependent. To calculate the degrees of freedom without using software, use the smaller of\nn1\u00001 andn2\u00001.\nEXAMPLE 5.9\nCalculate and interpret a 95% conﬁdence interval for the e ﬀect of ESCs on the change in heart\npumping capacity of sheep following a heart attack.\nThe point estimate for the di ﬀerence isx1\u0000x2=xesc\u0000xcontrol = 7:83.\nThe standard error is: s\ns2\n1\nn1+s2\n2\nn2=r\n5:172\n9+2:762\n9= 1:95:\nSincen1=n2= 9, usedf= 8;t?\n8= 2:31 for a 95% conﬁdence interval. Alternatively, computer\nsoftware can provide more accurate values: df= 12:225;t?= 2:174.\nThe conﬁdence interval is given by:\n(x1\u0000x2)\u0006t?\ndf\u0002SE! 7:83\u00062:31\u00021:95! (3:38;12:38):\nWith 95% conﬁdence, the average amount that ESCs improve heart pumping capacity lies between\n3.38% to 12.38%.13The data provide evidence for a treatment e ﬀect of administering stem cells.\n5.3.2 Hypothesis tests for a difference in means\nIs there evidence that newborns from mothers who smoke have a di ﬀerent average birth\nweight than newborns from mothers who do not smoke? The dataset births contains data from a\nrandom sample of 150 cases of mothers and their newborns in North Carolina over a year; there\nare 50 cases in the smoking group and 100 cases in the nonsmoking group.14\nfAge mAge weeks weight sexBaby smoke\n1 NA 13 37 5.00 female nonsmoker\n2 NA 14 36 5.88 female nonsmoker\n3 19 15 41 8.13 male smoker\n::::::::::::::::::\n150 45 50 36 9.25 female nonsmoker\nFigure 5.11: Four cases from the births dataset.\n13From software, the conﬁdence interval is (3.58, 12.08).\n14This dataset is available in the openintro Rpackage.\n250 CHAPTER 5. INFERENCE FOR NUMERICAL DATA\nEXAMPLE 5.10\nEvaluate whether it is appropriate to apply the t-distribution to the di ﬀerence in sample means\nbetween the two groups.\nSince the data come from a simple random sample and consist of less than 10% of all such cases,\nthe observations are independent. While each distribution is strongly skewed, the large sample\nsizes of 50 and 100 allow for the use of the t-distribution to model each mean separately. Thus, the\ndiﬀerence in sample means may be modeled using a t-distribution.\nNewborn weights (lbs) from mothers who smoked0246810\nNewborn weights (lbs) from mothers who did not smoke0246810\nFigure 5.12: The top panel represents birth weights for infants whose moth-\ners smoked. The bottom panel represents the birth weights for infants whose\nmothers who did not smoke. The distributions exhibit moderate-to-strong and\nstrong skew, respectively.\nA hypothesis test can be conducted to evaluate whether there is a relationship between mother’s\nsmoking status and average newborn birth weight. The null hypothesis represents the case of no\ndiﬀerence between the groups, H0:\u0016ns\u0000\u0016s= 0, where\u0016nsrepresents the population mean of new-\nborn birthweight for infants with mothers who did not smoke, and \u0016srepresents mean newborn\nbirthweight for infants with mothers who smoked. Under the alternative hypothesis, there is some\ndiﬀerence in average newborn birth weight between the groups, HA:\u0016ns\u0000\u0016s,0. The hypotheses\ncan also be written as H0:\u0016ns=\u0016sandHA:\u0016ns,\u0016s.\nSTATING HYPOTHESES FOR TWO-GROUP DATA\nWhen testing a hypothesis about two independent groups, directly compare the two popula-\ntion means and state hypotheses in terms of \u00161and\u00162.\n– For a two-sided test, H0:\u00161=\u00162;HA:\u00161,\u00162.\n– For a one-sided test, either H0:\u00161=\u00162;HA:\u00161>\u00162orH0:\u00161=\u00162;HA:\u00161<\u00162.\n5.3. TWO-SAMPLE TEST FOR INDEPENDENT DATA 251\nIn this setting, the formula for a t-statistic is:\nt=(x1\u0000x2)\u0000(\u00161\u0000\u00162)\nSEx1\u0000x2=(x1\u0000x2)\u0000(\u00161\u0000\u00162)s\ns2\n1\nn1+s2\n2\nn2:\nUnder the null hypothesis of no di ﬀerence between the groups, H0:\u00161\u0000\u00162= 0, the formula sim-\npliﬁes to\nt=(x1\u0000x2)s\ns2\n1\nn1+s2\n2\nn2:\nEXAMPLE 5.11\nUsing Figure 5.13, conduct a hypothesis test to evaluate whether there is evidence that newborns\nfrom mothers who smoke have a di ﬀerent average birth weight than newborns from mothers who\ndo not smoke.\nThe hypotheses are H0:\u00161=\u00162andHA:\u00161,\u00162, where\u00161represents the average newborn birth\nweight for nonsmoking mothers and \u00162represents average newborn birth weight for mothers who\nsmoke. Let\u000b= 0:05.\nCalculate the t-statistic:\nt=(x1\u0000x2)s\ns2\n1\nn1+s2\n2\nn2=7:18\u00006:78q\n1:602\n100+1:432\n50= 1:54:\nApproximate the degrees of freedom as 50 \u00001 = 49. The t-score of 1.49 falls between the ﬁrst and\nsecond columns in the df= 49 row of the t-table, so the two-sided p-value is between 0.10 and\n0.20.15\nThisp-value is larger than the signiﬁcance value, 0.05, so the null hypothesis is not rejected. There\nis insuﬃcient evidence to state there is a di ﬀerence in average birth weight of newborns from North\nCarolina mothers who did smoke during pregnancy and newborns from North Carolina mothers\nwho did not smoke during pregnancy.\nsmoker nonsmoker\nmean 6.78 7.18\nst. dev. 1.43 1.60\nsamp. size 50 100\nFigure 5.13: Summary statistics for the births dataset.\n15From R,df= 89:277 andp= 0:138.\n252 CHAPTER 5. INFERENCE FOR NUMERICAL DATA\n5.3.3 The paired test vs. independent group test\nIn the two-sample setting, students often ﬁnd it di ﬃcult to determine whether a paired test\nor an independent group test should be used. The paired test applies only in situations where\nthere is a natural pairing of observations between groups, such as in the swim data. Pairing can be\nobvious, such as the two measurements for each swimmer, or more subtle, such as measurements\nof respiratory function in twins, where one member of the twin pair is treated with an experimental\ntreatment and the other with a control. In the case of two independent groups, there is no natural\nway to pair observations.\nA common error is to overlook pairing in data and assume that two groups are independent.\nThe swimsuit data can be used to illustrate the possible harm in conducting an independent group\ntest rather than a paired test. In Section 5.2, the paired t-test showed a signiﬁcant di ﬀerence in\nthe swim velocities between swimmers wearing wetsuits versus regular swimsuits. Suppose the\nanalysis had been conducted without accounting for the fact that the measurements were paired.\nThe mean and standard deviation for the 12 wet suit velocities are 1.51 and 0.14 (m/sec),\nrespectively, and 1.43 and 0.14 (m/sec) for the 12 swim suit velocities. A two-group test statistic is:\nt=1:52\u00001:43p\n0:142=12 + 0:142=12= 1:37:\nIf the degrees of freedom are approximated as 11 = 12 \u00001, the two-sided p-value as calculated from\nsoftware is 0.20. According to this method, the null hypothesis of equal mean velocities for the two\nsuit types would not be rejected.\nIt is not di ﬃcult to show that the numerator of the paired test (the average of the within\nswimmer di ﬀerences) and the numerator of the two-group test (the di ﬀerence of the average times\nfor the two groups) are identical. The values of the test statistics di ﬀer because the denominators\nare diﬀerent—speciﬁcally, the standard errors associated with each statistic are di ﬀerent. For the\npaired test statistic, the standard error uses the standard deviation of the within pair di ﬀerences\n(0.22) and has value 0 :022=p\n12 = 0:006. The two-group test statistic combines the standard devia-\ntions for the original measurements and has valuep\n0:142=12 + 0:142=12 = 0:06. The standard error\nfor the two-group test is 10-fold larger than for the paired test.\nThis striking di ﬀerence in the standard errors is caused by the much lower variability of the\nindividual velocity di ﬀerences compared to the variability of the original measurements. Due to\nthe correlation between swim velocities for a single swimmer, the di ﬀerences in the two velocity\nmeasurements for each swimmer are consistently small, resulting in low variability. Pairing has\nallowed for increased precision in estimating the di ﬀerence between groups.\nThe swim suit data illustrates the importance of context, which distinguishes a statistical\nproblem from a purely mathematical one. While both the paired and two-group tests are nu-\nmerically feasible to calculate, without an apparent error, the context of the problem dictates that\nthe correct approach is to use a paired test.\nGUIDED PRACTICE 5.12\nPropose an experimental design for the embryonic stem cell study in sheep that would have re-\nquired analysis with a paired t-test.16\n16The experiment could have been done on pairs of siblings, with one assigned to the treatment group and one assigned\nto the control group. Alternatively, sheep could be matched up based on particular characteristics relevant to the experi-\nment; for example, sheep could be paired based on similar weight or age. Note that in this study, a design involving two\nmeasurements taken on each sheep would be impractical.\n5.3. TWO-SAMPLE TEST FOR INDEPENDENT DATA 253\n5.3.4 Case study: discrimination in developmental disability support\nSection 1.7.1 presented an analysis of the relationship between age, ethnicity, and amount of\nexpenditures for supporting developmentally disabled residents in the state of California, using the\ndds.discr dataset. When the variable ageis ignored, the expenditures per consumer is larger on\naverage for White non-Hispanics than Hispanics, but Figure 1.53 showed that average di ﬀerences\nby ethnicity were much smaller within age cohorts. This section demonstrates the use of t-tests\nto conduct a more formal analysis of possible di ﬀerences in expenditure by ethnicity, both overall\n(i.e., ignoring age) and within age cohorts.\nComparing expenditures overall\nWhen ignoring age, expenditures within the ethnicity groups Hispanic and White non-Hispanic\nshow substantial right-skewing (Figure 1.45). A transformation is advisable before conducting a\nt-test. As shown in Figure 5.14, a natural log transformation e ﬀectively eliminates skewing.\nEthnicityLog Expenditures (log(USD))\nHispanic White not Hispanic4681012\nFigure 5.14: A plot of log(expenditures) byethnicity .\nIs there evidence of a di ﬀerence in mean expenditures by ethnic group? Conduct a t-test of\nthe null hypothesis H0:\u00161=\u00162versus the two-sided alternative HA:\u00161,\u00162, where\u00161is the\npopulation mean log expenditure in Hispanics and \u00162is the population mean log expenditure in\nWhite non-Hispanics.\nEthnicity nx s\n1 Hispanic 376 8.56 1.17\n2 White non Hispanic 401 9.47 1.35\nFigure 5.15: Summary statistics for the transformed variable log(expenditures)\nin the dds.discr data.\nThe summary statistics required to calculate the t-statistic are shown in Figure 5.15. The\nt-statistic for the test is\nt=9:47\u00008:56p\n1:352=401 + 1:172=376= 10:1:\n254 CHAPTER 5. INFERENCE FOR NUMERICAL DATA\nThe degrees of freedom of the test can be approximated as 376 \u00001 = 375; the p-value can\nbe calculated using a normal approximation. Regardless of whether a tor normal distribution is\nused, the probability of a test statistic with absolute value larger than 10 is vanishingly small—the\np-value is less than 0.001. When ignoring age, there is signiﬁcant evidence of a di ﬀerence in mean\nexpenditures between Hispanics and White non-Hispanics. It appears that on average, White non-\nHispanics receive a higher amount of developmental disability support from the state of California\n(x1<x2).\nHowever, as indicated in Section 1.7.1, this is a misleading result. The analysis as conducted\ndoes not account for the confounding e ﬀect of age, which is associated with both expenditures and\nethnicity. As individuals age, they typically require more support from the government. In this\ndataset, White non-Hispanics tend to be older than Hispanics; this di ﬀerence in age distribution\ncontributes to the apparent di ﬀerence in expenditures between two groups.\nComparing expenditures within age cohorts\nOne way to account for the e ﬀect of age is to compare mean expenditures within age cohorts.\nWhen comparing individuals of similar ages but di ﬀerent ethnic groups, are the di ﬀerences in\nmean expenditures larger than would be expected by chance alone?\nFigure 1.52 shows that the age cohort 13-17 is the largest among the Hispanic consumers,\nwhile the cohort 22-50 is the largest among White non-Hispanics. This section will examine the\nevidence against the null hypothesis of no di ﬀerence in mean expenditures within these two co-\nhorts.\nFigure 5.16 shows that within both the age cohorts of 13-17 years and 22-50 years, the distri-\nbution of expenditures is reasonably symmetric; there is no need to apply a transformation before\nconducting a t-test. The skewing evident when age was ignored is due to the di ﬀering distributions\nof age within ethnicities.\nEthnicityExpenditures (USD)\nHispanic White not Hispanic02000400060008000\n(a)\nEthnicityExpenditures (USD)\nHispanic White not Hispanic0100002000030000400005000060000 (b)\nFigure 5.16: (a)A plot of expenditures byethnicity in the age cohort 13 - 17. (b)\nA plot of expenditures byethnicity in the age cohort 22 - 50.\n5.3. TWO-SAMPLE TEST FOR INDEPENDENT DATA 255\nFigure 5.17 contains the summary statistics for computing the test statistic to compare expen-\nditures in the two groups within this age cohort. The test statistic has value t= 0:318, with degrees\nof freedom 66. The two-sided p-value is 0.75. There is not evidence of a di ﬀerence between mean\nexpenditures in Hispanics and White non-Hispanics ages 13-17.\nEthnicity nx s\n1 Hispanic 103 3955.28 938.82\n2 White not Hispanic 67 3904.36 1071.02\nFigure 5.17: Summary statistics for expenditures , Ages 13-17.\nThe analysis of the age cohort 22 - 50 years shows the same qualitative result. The t-statistic\ncalculated from the summary statistics in Figure 5.18 has value t= 0:659 andp-value 0.51. Just as\nin the 13-17 age cohort, there is insu ﬃcient evidence to reject the null hypothesis of no di ﬀerence\nbetween the means.\nEthnicity n x s\n1 Hispanic 43 40924.12 6467.09\n2 White not Hispanic 133 40187.62 6081.33\nFigure 5.18: Summary statistics for expenditures , Ages 22 - 50.\nThe inference-based analyses for these two age cohorts support the conclusions reached through\nthe exploratory approach used in Section 1.7.1—comparing individuals of similar ages shows that\nthere are not large di ﬀerences between mean expenditures for White non-Hispanics versus Hispan-\nics. An analysis that accounts for age as a confounding variable does not suggest there is evidence\nof ethnic discrimination in developmental disability support provided by the State of California.\n256 CHAPTER 5. INFERENCE FOR NUMERICAL DATA\n5.3.5 Pooled standard deviation estimate\nOccasionally, two populations will have standard deviations that are so similar that they can\nbe treated as identical. For example, historical data or a well-understood biological mechanism\nmay justify this strong assumption. In such cases, it can be more precise to use a pooled standard\ndeviation to make inferences about the di ﬀerence in population means.\nThe pooled standard deviation of two groups uses data from both samples to estimate the\ncommon standard deviation and standard error. If there are good reasons to believe that the popu-\nlation standard deviations are equal, an improved estimate of the group variances can be obtained\nby pooling the data from the two groups:\ns2\npooled=s2\n1(n1\u00001) +s2\n2(n2\u00001)\nn1+n2\u00002;\nwheren1andn2are the sample sizes, and s1ands2represent the sample standard deviations.\nIn this setting, the t-statistic uses s2\npooledin place ofs2\n1ands2\n2in the standard error formula, and\nthe degrees of freedom for the t\u0000statistic is the sum of the degrees of freedom for the two sample\nvariances:\ndf = (n1\u00001) + (n2\u00001) =n1+n2\u00002:\nThet-statistic for testing the null hypothesis of no di ﬀerence between population means becomes\nt=x1\u0000x2\nspooledq\n1\nn1+1\nn2:\nThe formula for the two-sided conﬁdence interval for the di ﬀerence in population means is\n(x1\u0000x2)\u0006t?\u0002spooledr\n1\nn1+1\nn2;\nwheret?is the point on a t-distribution with n1+n2\u00002 degrees of freedom chosen according to the\nconﬁdence coe ﬃcient.\nThe beneﬁts of pooling the standard deviation are realized through obtaining a better estimate\nof the standard deviation for each group and using a larger degrees of freedom parameter for the t-\ndistribution. Both of these changes may permit a more accurate model of the sampling distribution\nofx1\u0000x2, if the standard deviations of the two groups are indeed equal. In most applications,\nhowever, it is di ﬃcult to verify the assumption of equal population standard deviations, and thus\nsafer to use the methods discussed in Sections 5.3.1 and 5.3.2.\n5.4. POWER CALCULATIONS FOR A DIFFERENCE OF MEANS 257\n5.4 Power calculations for a difference of means\nDesigning a study often involves many complex issues; perhaps the most important statistical\nissue in study design is the choice of an appropriate sample size. The power of a statistical test is\nthe probability that the test will reject the null hypothesis when the alternative hypothesis is true;\nsample sizes are chosen to make that probability su ﬃciently large, typically between 80% and 90%.\nTwo competing considerations arise when choosing a sample size. The sample size should be\nsuﬃciently large to allow for important group di ﬀerences to be detected in a hypothesis test. Prac-\ntitioners often use the term ‘detecting a di ﬀerence’ to mean correctly rejecting a null hypothesis,\ni.e., rejecting a null hypothesis when the alternative is true. If a study is so small that detecting\na statistically signiﬁcant di ﬀerence is unlikely even when there are potentially important di ﬀer-\nences, enrolling participants might be unethical, since subjects could potentially be exposed to a\ndangerous experimental treatment. However, it is also unethical to conduct studies with an overly\nlarge sample size, since more participants than necessary would be exposed to an intervention with\nuncertain value. Additionally, collecting data is typically expensive and time consuming; it would\nbe a waste of valuable resources to design a study with an overly large sample size.\nThis section begins by illustrating relevant concepts in the context of a hypothetical clinical\ntrial, where the goal is to calculate a su ﬃcient sample size for being 80% likely to detect practically\nimportant e ﬀects.17Afterwards, formulas are provided for directly calculating sample size, as well\nas references to software that can perform the calculations.\n5.4.1 Reviewing the concepts of a test\nEXAMPLE 5.13\nA company would like to run a clinical trial with participants whose systolic blood pressures are\nbetween 140 and 180 mmHg. Suppose previously published studies suggest that the standard\ndeviation of patient blood pressures will be about 12 mmHg, with an approximately symmetric\ndistribution.18What would be the approximate standard error for xtrmt\u0000xctrlif 100 participants\nwere enrolled in each treatment group?\nThe standard error is calculated as follows:\nSExtrmt\u0000xctrl=s\ns2\ntrmt\nntrmt+s2\nctrl\nnctrl=r\n122\n100+122\n100= 1:70:\nThis may be an imperfect estimate of SExtrmt\u0000xctrl, since the standard deviation estimate of 12 mmHg\nfrom prior data may not be correct. However, it is su ﬃcient for getting started, and making an\nassumption like this is often the only available option.\n17While sample size planning is also important for observational studies, those techniques are not discussed here.\n18In many studies like this one, each participant’s blood pressure would be measured at the beginning and end of\nthe study, and the outcome measurement for the study would be the average di ﬀerence in blood pressure in each of the\ntreatment groups. For this hypothetical study, we assume for simplicity that blood pressure is measured at only the end\nof the study, and that the randomization ensures that blood pressures at the beginning of the study are equal (on average)\nbetween the two groups.\n258 CHAPTER 5. INFERENCE FOR NUMERICAL DATA\nSince the degrees of freedom are greater than 30, the distribution of xtrmt\u0000xctrlwill be ap-\nproximately normal. Under the null hypothesis, the mean is 0 and the standard deviation is 1.70\n(from the standard error).\n−9 −6 −3 0 3 6 9\nxtrmt−xctrlNull distribution\nFigure 5.19: Null distribution for the t-statistic in Example 5.14.\nEXAMPLE 5.14\nFor what values of xtrmt\u0000xctrlwould the null hypothesis be rejected, using \u000b= 0:05?\nIf the observed di ﬀerence is in the far left or far right tail of the null distribution, there is su ﬃcient\nevidence to reject the null hypothesis. For \u000b= 0:05,H0is rejected if the di ﬀerence is in the lower\n2.5% or upper 2.5% tail:\nLower 2.5%: For the normal model, this is 1.96 standard errors below 0, so any di ﬀerence smaller\nthan\u00001:96\u00021:70 =\u00003:332 mmHg.\nUpper 2.5%: For the normal model, this is 1.96 standard errors above 0, so any di ﬀerence larger\nthan 1:96\u00021:70 = 3:332 mmHg.\nThe boundaries of these rejection regions are shown below. Note that if the new treatment is\neﬀective, mean blood pressure should be lower in the treatment group than in the control group;\ni.e., the di ﬀerence should be in the lower tail.\n−9 −6 −3 0 3 6 9\nxtrmt−xctrlNull distribution\nReject H0Do not\nreject H0Reject H0\nThe next step is to perform some hypothetical calculations to determine the probability of\nrejecting the null hypothesis if the alternative hypothesis were true.\n5.4. POWER CALCULATIONS FOR A DIFFERENCE OF MEANS 259\n5.4.2 Computing the power for a 2-sample test\nIf there is a real e ﬀect from an intervention, and the e ﬀect is large enough to have practical\nvalue, the probability of detecting that e ﬀect is referred to as the power . Power can be computed\nfor diﬀerent sample sizes or di ﬀerent e ﬀect sizes.\nThere is no easy way to deﬁne when an e ﬀect size is large enough to be of value; this is not a\nstatistical issue. For example, in a clinical trial, the scientiﬁcally signiﬁcant e ﬀect is the incremental\nvalue of the intervention that would justify changing current clinical recommendations from an\nexisting intervention to a new one. In such a setting, the e ﬀect size is usually determined from long\ndiscussions between the research team and study sponsors.\nSuppose that for this hypothetical blood pressure medication study, the researchers are in-\nterested in detecting any e ﬀect on blood pressure that is 3 mmHg or larger than the standard\nmedication. Here, 3 mmHg is the minimum population e ﬀect size of interest.\nEXAMPLE 5.15\nSuppose the study proceeded with 100 patients per treatment group and the new drug does reduce\naverage blood pressure by an additional 3 mmHg relative to the standard medication. What is the\nprobability of detecting this e ﬀect?\nDetermine the sampling distribution for xtrmt\u0000xctrlwhen the true di ﬀerence is\u00003 mmHg; this has\nthe same standard deviation of 1.70 as the null distribution, but the mean is shifted 3 units to the\nleft. Then, calculate the fraction of the distribution for xtrmt\u0000xctrlthat falls within the rejection\nregion for the null distribution, as shown in Figure 5.20.\nThe probability of being in the left side of the rejection region ( x<\u00003:332) can be calculated by\nconverting to a Z-score and using either the normal probability table or statistical software.19\nZ=\u00003:332\u0000(\u00003)\n1:7=\u00000:20!P(Z\u0014\u00000:20) = 0:4207:\nThe power for the test is about 42% when \u0016trmt\u0000\u0016ctrl=\u00003 mm/Hg and each group has a sample\nsize of 100.\n−9 −6 −3 0 3 6 9\nxtrmt−xctrlNull distribution Distribution with\nµtrmt−µctrl = −3\nFigure 5.20: The rejection regions are outside of the dotted lines. Recall that the\nboundaries for \u000b= 0:05 were calculated to be \u00063:332 mmHg.\n19The probability of being in the right side of the rejection region is negligible and can be ignored.\n260 CHAPTER 5. INFERENCE FOR NUMERICAL DATA\n5.4.3 Determining a proper sample size\nThe last example demonstrated that with a sample size of 100 in each group, there is a prob-\nability of about 0.42 of detecting an e ﬀect size of 3 mmHg. If the study were conducted with this\nsample size, even if the new medication reduced blood pressure by 3 mmHg compared to the con-\ntrol group, there is a less than 50% chance of concluding that the medication is beneﬁcial. Studies\nwith low power are often inconclusive, and there are important reasons to avoid such a situation:\n– Participants were subjected to a drug for a study that may have little scientiﬁc value.\n– The company may have invested hundreds of millions of dollars in developing the new drug,\nand may now be left with uncertainty about its potential.\n– Another clinical trial may need to be conducted to obtain a more conclusive answer as to\nwhether the drug does hold any practical value, and that would require substantial time and\nexpense.\nTo ensure a higher probability of detecting a clinically important e ﬀect, a larger sample size\nshould be chosen. What about a study with 500 patients per group?\nGUIDED PRACTICE 5.16\nCalculate the power to detect a change of -3 mmHg using a sample size of 500 per group. Recall\nthat the standard deviation of patient blood pressures was expected to be about 12 mmHg.20\n(a) Determine the standard error.\n(b) Identify the null distribution and rejection regions, as well as the alternative distribution when\n\u0016trmt\u0000\u0016ctrl=\u00003.\n(c) Compute the probability of rejecting the null hypothesis.\nWith a sample size of 500 per group, the power of the test is much larger than necessary. Not\nonly does this lead to a study that would be overly expensive and time consuming, it also exposes\nmore patients than necessary to the experimental drug.\nSample sizes are generally chosen such that power is around 80%, although in some cases\n90% is the target. Other values may be reasonable for a speciﬁc context, but 80% and 90% are\nmost commonly chosen as a good balance between high power and limiting the number of patients\nexposed to a new treatment (as well as reducing experimental costs).\n20(a) The standard error will now be SE=q\n122\n500+122\n500= 0:76.\n(b) The null distribution, rejection boundaries, and alternative distribution are shown below. The rejection regions are the\nareas outside the two dotted lines at xtrmt\u0000xctrl\u00060:76\u00021:96 =\u00061:49.\n−9 −6 −3 0 3 6 9\nxtrmt−xctrlNull distribution Distribution with\nµtrmt−µctrl = −3\n(c) Compute the Z-score and ﬁnd the tail area, Z=\u00001:49\u0000(\u00003)\n0:76= 1:99!P(Z\u00141:99) = 0:9767, which is the power of the\ntest for a di ﬀerence of 3 mmHg. With 500 patients per group, the study would be 97.7% likely to detect an e ﬀect size of\n3 mmHg.\n5.4. POWER CALCULATIONS FOR A DIFFERENCE OF MEANS 261\nEXAMPLE 5.17\nIdentify the sample size that would lead to a power of 80%.\nTheZ-score that deﬁnes a lower tail area of 0.80 is about Z= 0:84. In other words, 0.84 standard\nerrors from -3, the mean of the alternative distribution.\n−9 −6 −3 0 3 6 9\nxtrmt−xctrlNull distribution Distribution with\nµtrmt−µctrl = −3\n0.84 SE 1.96 SE\nFor\u000b= 0:05, the rejection region always extends 1.96 standard errors from 0, the center of the null\ndistribution.\nThe distance between the centers of the null and alternative distributions can be expressed in terms\nof the standard error:\n(0:84\u0002SE) + (1:96\u0002SE) = 2:8\u0002SE:\nThis quantity necessarily equals the minimum e ﬀect size of interest, 3 mmHg, which is the distance\nbetween -3 and 0. It is then possible to solve for n:\n3 = 2:8\u0002SE\n3 = 2:8\u0002r\n122\nn+122\nn\nn=2:82\n32\u0002\u0010\n122+ 122\u0011\n= 250:88\nThe study should enroll at least 251 patients per group for 80% power. Note that sample size\nshould always be rounded up in order to achieve the desired power. Even if the calculation had\nyielded a number closer to 250 (e.g., 250.25), the study should still enroll 251 patients per grou,\nsince having 250 patients per group would result in a power lower than 80%.\nGUIDED PRACTICE 5.18\nSuppose the targeted power is 90% and \u000b= 0:01. How many standard errors should separate the\ncenters of the null and alternative distributions, where the alternative distribution is centered at\nthe minimum e ﬀect size of interest? Assume the test is two-sided.21\n21Find theZ-score such that 90% of the distribution is below it: Z= 1:28. Next, ﬁnd the cuto ﬀs for the rejection regions:\n\u00062:58. Thus, the centers of the null and alternative distributions should be about 1 :28 + 2:58 = 3:86 standard errors apart.\n262 CHAPTER 5. INFERENCE FOR NUMERICAL DATA\nFigure 5.21 shows the power for sample sizes from 20 participants to 5,000 participants when\n\u000b= 0:05 and the true di ﬀerence is -3 mmHg. While power increases with sample size, having more\nthan 250-300 participants provides little additional value towards detecting an e ﬀect.\nSample Size Per GroupPower\n20 50100 200 5001000 2000 50000.00.20.40.60.81.0\nFigure 5.21: The curve shows the power for di ﬀerent sample sizes in the context\nof the blood pressure example when the true di ﬀerence is -3.\n5.4.4 Formulas for power and sample size\nThe previous sections have illustrated how power and sample size can be calculated from\nﬁrst principles, using the fundamental ideas behind distributions and testing. In practice, power\nand sample size calculations are so important that statistical software should be the method of\nchoice; there are many commercially available and public domain programs for performing such\ncalculations. However, hand calculations using formulas can provide quick estimates in the early\nstages of planning a study.\nUse the following formula to calculate sample size for comparing two means, assuming each\ngroup will have nparticipants:\nn=(\u001b2\n1+\u001b2\n2)(z1\u0000\u000b=2+z1\u0000\f)2\n\u00012:\nIn this formula:\n–\u00161;\u00162;\u001b1;and\u001b2are the population means and standard deviations of the two groups.\n–\u0001=\u00161\u0000\u00162is the minimally important di ﬀerence that investigators wish to detect.\n– The null and alternative hypotheses are H0:\u0001= 0 (i.e., no di ﬀerence between the means) and\nHA:\u0001,0, i.e., a two-sided alternative.\n– The two-sided signiﬁcance level is \u000b, andz1\u0000\u000b=2is the point on a standard normal distribution\nwith area 1\u0000\u000b=2 to its left and \u000b=2 area to its right.\n–\fis the probability of incorrectly failing to reject H0for a speciﬁed value of \u0001; 1\u0000\fis the\npower. The value z1\u0000\fis the point on a standard normal distribution with area 1 \u0000\fto its left.\n5.4. POWER CALCULATIONS FOR A DIFFERENCE OF MEANS 263\nFor a study with sample size nper group, where Zis a normal random variable with mean 0\nand standard deviation 1, power is given by:\nPower =P0\nBBBBBBBB@Z<\u0000z1\u0000\u000b=2+\u0001q\n\u001b2\n1=n+\u001b2\n2=n1\nCCCCCCCCA:\nThese formulas could have been used to do the earlier power and sample size calculations\nfor the hypothetical study of blood pressure lowering medication. To calculate the sample size\nneeded for 80% power in detecting a change of 3 mmHg, \u000b= 0:05, 1\u0000\f= 0:80,\u0001= 3 mmHg, and\n\u001b1=\u001b2= 12 mmHg. The formula yields a sample size nper group of\nn=(122+ 122)(1:96 + 0:84)2\n(\u00003:0)2= 250:88;\nwhich can be rounded up to 251.\nThe formula for power can be used to verify the sample size of 251:\nPower =P \nZ<\u00001:96 +3p\n122=251 + 122=251!\n=P(Z<1:25)\n= 0:85:\nThe calculated power is slightly larger than 80% because of the rounding to 251.\nThe sample size calculations done before any data are collected are one of the most critical\naspects of conducting a study. If an analysis is done incorrectly, it can be redone once the error\nis discovered. However, if data were collected for a sample size that is either too large or too\nsmall, it can be impossible to correct the error, especially in studies with human subjects. As a\nresult, sample size calculations are nearly always done using software. For two-sample t-tests, the\nRfunction power.t.test is both freely available and easy to use.\n264 CHAPTER 5. INFERENCE FOR NUMERICAL DATA\n5.5 Comparing means with ANOVA\nIn some settings, it is useful to compare means across several groups. It might be tempting\nto do pairwise comparisons between groups; for example, if there are three groups ( A;B;C ), why\nnot conduct three separate t-tests (Avs.B,Avs.C,Bvs.C)? Conducting multiple tests on the\nsame data increases the rate of Type I error, making it more likely that a di ﬀerence will be found\nby chance, even if there is no di ﬀerence among the population means. Multiple testing is discussed\nfurther in Section 5.5.3.\nInstead, the methodology behind a t-test can be generalized to a procedure called analysis of\nvariance (ANOV A) , which uses a single hypothesis test to assess whether the means across several\ngroups are equal. Strong evidence favoring the alternative hypothesis in ANOV A is described by\nunusually large di ﬀerences among the group means.\nH0: The mean outcome is the same across all kgroups. In statistical notation, \u00161=\u00162=\u0001\u0001\u0001=\u0016k\nwhere\u0016irepresents the mean of the outcome for observations in category i.\nHA: At least one mean is di ﬀerent.\nThere are three conditions on the data that must be checked before performing ANOV A: 1)\nobservations are independent within and across groups, 2) the data within each group are nearly\nnormal, and 3) the variability across the groups is about equal.\nEXAMPLE 5.19\nExamine Figure 5.22. Compare groups I, II, and III. Is it possible to visually determine if the\ndiﬀerences in the group centers is due to chance or not? Now compare groups IV, V, and VI. Do the\ndiﬀerences in these group centers appear to be due to chance?\nIt is diﬃcult to discern a di ﬀerence in the centers of groups I, II, and III, because the data within\neach group are quite variable relative to any di ﬀerences in the average outcome. However, there\nappear to be di ﬀerences in the centers of groups IV, V, and VI. For instance, group V appears to\nhave a higher mean than that of the other two groups. The di ﬀerences in centers for groups IV, V,\nand VI are noticeable because those di ﬀerences are large relative to the variability in the individual\nobservations within each group.\noutcome\n−101234\nIIIIIIIVVVI\nFigure 5.22: Side-by-side dot plot for the outcomes for six groups.\n5.5. COMPARING MEANS WITH ANOV A 265\n5.5.1 Analysis of variance (ANOVA) and the FFF-test\nThe famuss dataset was introduced in Chapter 1, Section 1.2.2. In the FAMuSS study, researchers\nexamined the relationship between muscle strength and genotype at a location on the ACTN3 gene.\nThe measure for muscle strength is percent change in strength in the non-dominant arm ( ndrm.ch ).\nIs there a di ﬀerence in muscle strength across the three genotype categories ( CC,CT,TT)?\nGUIDED PRACTICE 5.20\nThe null hypothesis under consideration is the following: \u0016CC=\u0016CT=\u0016TT. Write the null and\ncorresponding alternative hypotheses in plain language.22\nFigure 5.23 provides summary statistics for each group. A side-by-side boxplot for the change\nin non-dominant arm strength is shown in Figure 5.24; Figure 5.25 shows the Q-Q plots by each\ngenotype. Notice that the variability appears to be approximately constant across groups; nearly\nconstant variance across groups is an important assumption that must be satisﬁed for using ANOV A.\nBased on the Q-Q plots, there is evidence of moderate right skew; the data do not follow a normal\ndistribution very closely, but could be considered to ’loosely’ follow a normal distribution.23It is\nreasonable to assume that the observations are independent within and across groups; it is unlikely\nthat participants in the study were related, or that data collection was carried out in a way that one\nparticipant’s change in arm strength could inﬂuence another’s.\nCC CT TT\nSample size ( ni) 173 261 161\nSample mean ( ¯xi) 48.89 53.25 58.08\nSample SD ( si) 29.96 33.23 35.69\nFigure 5.23: Summary statistics of change in non-dominant arm strength, split by\ngenotype.\nGenotypeChange in Non−Dominant Arm Strength (%)\nCC CT TT050100150200250\nFigure 5.24: Side-by-side box plot of the change in non-dominant arm strength\nfor 595 participants across three groups.\n22H0: The average percent change in non-dominant arm strength is equal across the three genotypes. HA: The average\npercent change in non-dominant arm strength varies across some (or all) groups.\n23In a more advanced course, it can be shown that the ANOV A procedure still holds with deviations from normality\nwhen sample sizes are moderately large. Additionally, a more advanced course would discuss appropriate transformations\nto induce normality.\n266 CHAPTER 5. INFERENCE FOR NUMERICAL DATA\n●●\n●●\n●\n●\n●●●\n●\n●●\n●\n●●\n●\n●●\n●●●\n●●●\n●●\n●●●●●\n●●●\n●●●\n●●\n●\n●●●\n●●\n●●\n●\n●●\n●●\n●●\n●●●●\n●●\n●●●●\n●●●\n●\n●●●\n●●\n●●\n●●\n●●●●\n●●\n●\n●●\n●\n●●\n●●●\n●●\n●\n●●●●\n●●\n●●●\n●●●●\n●●\n●●●\n●\n●●\n●\n●●●\n●●\n●●\n●●\n●●\n●●\n●●●●\n●\n●●\n●\n●●\n●●\n●●●\n●●\n●\n●●\n●●●\n●\n●●●\n●●●\n●\n●●\n●\n●●\n●●\n●\n●●●\n●\n−2−1012050100150Q−Q for CC genotype\nTheoretical QuantilesSample Quantiles\n●●●\n●\n●\n●●●\n●●●\n●●●\n●\n●●\n●●●●●●●●●●\n●●\n●●\n●●●●\n●●\n●\n●●●●\n●\n●●●●\n●●\n●●●\n●●●\n●\n●●●\n●●●●●\n●●\n●●\n●●\n●●●\n●●\n●●\n●●●\n●\n●\n●●\n●\n●●●●\n●\n●●\n●●●\n●●●\n●●\n●●●\n●\n●\n●●\n●●\n●●\n●\n●●\n●●●●\n●●●●●●\n●\n●●\n●●●\n●\n●●\n●●●\n●\n●●\n●●\n●●\n●●●\n●●\n●●●●●\n●●\n●●\n●\n●●●\n●●●\n●●●●\n●●●\n●●●●●●\n●●●\n●●●\n●●●\n●\n●●●●\n●●\n●●\n●●●●●\n●\n●●●\n●●●●\n●●\n●●\n●\n●●●●\n●●●\n●\n●●\n●●●●\n●\n●●●●●\n●●●\n●\n●\n●●\n●●●\n●●●●\n●●\n●\n●●●\n●●●\n●\n●●●\n●\n−3−2−10123050100150200250Q−Q for CT genotype\nTheoretical QuantilesSample Quantiles●\n●\n●●●●●●\n●\n●●\n●●●\n●●\n●●\n●●\n●●●\n●●●●\n●●●\n●\n●●●\n●\n●\n●●\n●●\n●●●\n●●●\n●\n●●\n●●\n●●●●\n●●●●●\n●●\n●●\n●●●\n●●●\n●●\n●●●\n●●●●\n●●\n●●●\n●●\n●●\n●\n●\n●\n●\n●●●●●\n●●●\n●●●\n●●\n●\n●●●●\n●●\n●●\n●\n●●●\n●●\n●●●\n●\n●●●●●\n●●●\n●\n●●\n●\n●●●\n●●\n●●\n●●\n●●●\n●●●\n●●●●●\n●●\n●●●\n−2−1012050100150200Q−Q for TT genotype\nTheoretical QuantilesSample Quantiles\nFigure 5.25: Q-Q plots of the change in non-dominant arm strength for 595 par-\nticipants across three groups.\nEXAMPLE 5.21\nThe largest di ﬀerence between the sample means is between the CCand TTgroups. Consider again\nthe original hypotheses:\nH0:\u0016CC=\u0016CT=\u0016TT\nHA: The average percent change in non-dominant arm strength ( \u0016i) varies across some (or all)\ngroups.\nWhy might it be inappropriate to run the test by simply estimating whether the di ﬀerence of\u0016CC\nand\u0016TTis statistically signiﬁcant at a 0.05 signiﬁcance level?\nIt is inappropriate to informally examine the data and decide which groups to formally test. This is\na form of data ﬁshing ; choosing the groups with the largest di ﬀerences for the formal test will lead\nto an increased chance of incorrectly rejecting the null hypothesis (i.e., an inﬂation in the Type I\nerror rate). Instead, all the groups should be tested using a single hypothesis test.\nAnalysis of variance focuses on answering one question: is the variability in the sample means\nlarge enough that it seems unlikely to be from chance alone? The variation between groups is\nreferred to as the mean square between groups ( MSG ); theMSG is a measure of how much each\ngroup mean varies from the overall mean. Let xrepresent the mean of outcomes across all groups,\nwherexiis the mean of outcomes in a particular group iandniis the sample size of group i. The\nmean square between groups is:\nMSG =1\nk\u00001kX\ni=1ni(xi\u0000x)2=1\ndfGSSG;\nwhereSSG is the sum of squares between groups ,Pk\ni=1ni(xi\u0000x)2, and dfG=k\u00001 is the degrees\nof freedom associated with the MSG when there are kgroups.\n5.5. COMPARING MEANS WITH ANOV A 267\nUnder the null hypothesis, any observed variation in group means is due to chance and there\nis no real di ﬀerence between the groups. In other words, the null hypothesis assumes that the\ngroupings are non-informative, such that all observations can be thought of as belonging to a single\ngroup. If this scenario is true, then it is reasonable to expect that the variability between the group\nmeans should be equal to the variability observed within a single group. The mean square error\n(MSE )is a pooled variance estimate with associated degrees of freedom df E=n\u0000kthat provides a\nmeasure of variability within the groups. The mean square error is computed as:\nMSE =1\nn\u0000kkX\ni=1(ni\u00001)s2\ni=1\ndfESSE;\nwhere theSSE is the sum of squared errors ,niis the sample size of group i, andsiis the standard\ndeviation of group i.\nUnder the null hypothesis that all the group means are equal, any di ﬀerences among the\nsample means are only due to chance; thus, the MSG andMSE should also be equal. ANOV A is\nbased on comparing the MSG andMSE . The test statistic for ANOV A, the F-statistic , is the ratio\nof the between-group variability to the within-group variability:\nF=MSG\nMSE: (5.22)\nEXAMPLE 5.23\nCalculate the F-statistic for the famuss data summarized in Figure 5.23. The overall mean xacross\nall observations is 53.29.\nFirst, calculate the MSG andMSE .\nMSG =1\nk\u00001kX\ni=1ni(¯xi\u0000¯x)2\n=1\n3\u00001[(173)(48:89\u000053:29)2+ (261)(53:25\u000053:29)2+ (161)(58:08\u000053:29)2]\n=3521:69\nMSE =1\nn\u0000kkX\ni=1(ni\u00001)s2\ni\n=1\n595\u00003[(173\u00001)(29:962) + (261\u00001)(33:232) + (161\u00001)(35:692)]\n=1090:02\nTheF-statistic is the ratio:\nMSG\nMSE=3521:69\n1090:02= 3:23:\n268 CHAPTER 5. INFERENCE FOR NUMERICAL DATA\nAp-value can be computed from the F-statistic using an F-distribution, which has two as-\nsociated parameters: df 1and df 2. For theF-statistic in ANOV A, df 1= dfGand df 2= dfE. AnF\ndistribution with 2 and 592 degrees of freedom, corresponding to the F-statistic for the genotype\nand muscle strength hypothesis test, is shown in Figure 5.26.\n0123456\nFigure 5.26: An F-distribution with df 1= 2 and df 2= 592. The tail area greater\nthanF= 3:23 is shaded.\nThe larger the observed variability in the sample means ( MSG ) relative to the within-group\nvariability ( MSE ), the larger Fwill be. Larger values of Frepresent stronger evidence against the\nnull hypothesis. The upper tail of the distribution is used to compute a p-value, which is typically\ndone using statistical software.\nEXAMPLE 5.24\nThep-value corresponding to the test statistic is equal to about 0.04. Does this provide strong\nevidence against the null hypothesis at signiﬁcance level \u000b= 0:05?\nThep-value is smaller than 0.05, indicating the evidence is strong enough to reject the null hy-\npothesis at a signiﬁcance level of 0.05. The data suggest that average change in strength in the\nnon-dominant arm varies by participant genotype.\nTHEFFF-STATISTIC AND THE FFF-TEST\nAnalysis of variance (ANOV A) is used to test whether the mean outcome di ﬀers across two or\nmore groups. ANOV A uses a test statistic F, which represents a standardized ratio of vari-\nability in the sample means relative to the variability within the groups. If H0is true and\nthe model assumptions are satisﬁed, the statistic Ffollows anFdistribution with parameters\ndf1=k\u00001 and df 2=n\u0000k. The upper tail of the F-distribution is used to calculate the p-value.\n5.5. COMPARING MEANS WITH ANOV A 269\n5.5.2 Reading an ANOVA table from software\nThe calculations required to perform an ANOV A by hand are tedious and prone to human\nerror. Instead, it is common to use statistical software to calculate the F-statistic and associated\np-value. The results of an ANOV A can be summarized in a table similar to that of a regression\nsummary, which will be discussed in Chapters 6 and 7.\nFigure 5.27 shows an ANOV A summary to test whether the mean change in non-dominant\narm strength varies by genotype. Many of these values should look familiar; in particular, the\nF-statistic and p-value can be retrieved from the last two columns.\nDf Sum Sq Mean Sq F value Pr( >F)\nfamuss$actn3.r577x 2 7043 3522 3.231 0.0402\nResiduals 592 645293 1090\nFigure 5.27: ANOV A summary for testing whether the mean change in non-\ndominant arm strength varies by genotype at the actn3.r577x location on the\nACTN3 gene.\n5.5.3 Multiple comparisons and controlling Type I Error rate\nRejecting the null hypothesis in an ANOV A analysis only allows for a conclusion that there\nis evidence for a di ﬀerence in group means. In order to identify the groups with di ﬀerent means,\nit is necessary to perform further testing. For example, in the famuss analysis, there are three\ncomparisons to make: CCtoCT,CCtoTT, and CTtoTT. While these comparisons can be made\nusing two sample t-tests, it is important to control the Type I error rate. One of the simplest ways\nto reduce the overall probability of identifying a signiﬁcant di ﬀerence by chance in a multiple\ncomparisons setting is to use the Bonferroni correction procedure.\nIn the Bonferroni correction procedure, the p-value from a two-sample t-test is compared to\na modiﬁed signiﬁcance level, \u000b?;\u000b?=\u000b=K, whereKis the total number of comparisons being\nconsidered. For kgroups,K=k(k\u00001)\n2. When calculating the t-statistic, use the pooled estimate\nof standard deviation between groups (which equalsp\nMSE ); to calculate the p-value, use a t-\ndistribution with df 2. It is typically more convenient to do these calculations using software.\nBONFERRONI CORRECTION\nThe Bonferroni correction suggests that a more stringent signiﬁcance level is appropriate\nwhen conducting multiple tests:\n\u000b?=\u000b=K\nwhereKis the number of comparisons being considered. For kgroups,K=k(k\u00001)\n2.\n270 CHAPTER 5. INFERENCE FOR NUMERICAL DATA\nEXAMPLE 5.25\nThe ANOV A conducted on the famuss dataset showed strong evidence of di ﬀerences in the mean\nstrength change in the non-dominant arm between the three genotypes. Complete the three possi-\nble pairwise comparisons using the Bonferroni correction and report any di ﬀerences.\nUse a modiﬁed signiﬁcance level of \u000b?= 0:05=3 = 0:0167. The pooled estimate of the standard\ndeviation isp\nMSE =p\n1090:02 = 33:02.\nGenotype CCversus Genotype CT:\nt=x1\u0000x2\nspooledq\n1\nn1+1\nn2=48:89\u000053:25\n33:02q\n1\n173+1\n261=\u00001:35:\nThis results in a p-value of 0.18 on df= 592. This p-value is larger than \u000b?= 0:0167, so there is\nnot evidence of a di ﬀerence in the means of genotypes CCand CT.\nGenotype CCversus Genotype TT:\nt=x1\u0000x2\nspooledq\n1\nn1+1\nn2=48:89\u000058:08\n33:02q\n1\n173+1\n161=\u00002:54:\nThis results in a p-value of 0.01 on df= 592. This p-value is smaller than \u000b?= 0:0167, so there is\nevidence of a di ﬀerence in the means of genotypes CCand TT.\nGenotype CTversus Genotype TT:\nt=x1\u0000x2\nspooledq\n1\nn1+1\nn2=53:25\u000058:08\n33:02q\n1\n261+1\n161=\u00001:46:\nThis results in a p-value of 0.14 on df= 592. This p-value is larger than \u000b?= 0:0167, so there is\nnot evidence of a di ﬀerence in the means of genotypes CTand TT.\nIn summary, the mean percent strength change in the non-dominant arm for genotype CTindividu-\nals is not statistically distinguishable from those of genotype CCand TTindividuals. However, there\nis evidence that mean percent strength change in the non-dominant arm di ﬀers between individu-\nals of genotype CCand TTare diﬀerent.\n5.5. COMPARING MEANS WITH ANOV A 271\n5.5.4 Reading the results of pairwise ttt-tests from software\nStatistical software can be used to calculate the p-values associated with each possible pairwise\ncomparison of the groups in ANOV A. The results of the pairwise tests are summarized in a table\nthat shows the p-value for each two-group test.\nFigure 5.28 shows the p-values from the three possible two-group t-tests comparing change in\nnon-dominant arm strengths between individuals with genotypes CC,CT, and TT. For example, the\ntable indicates that when comparing mean change in non-dominant arm strength between TTand\nCCindividuals, the p-value is 0.01. This coheres with the calculations above, and these unadjusted\np-values should be compared to \u000b?= 0:0167.\nCC CT\nCT 0.18 -\nTT 0.01 0.14\nFigure 5.28: Unadjusted p-values for pairwise comparisons testing whether\nthe mean change in non-dominant arm strength varies by genotype at the\nactn3.r577x location on ACTN3 gene.\nThe use of statistical software makes it easier to apply corrections for multiple testing, such\nthat it is not necessary to explicitly calculate the value of \u000b?. Figure 5.29 shows the Bonferroni-\nadjustedp-values from the three possible tests. When statistical software applies the Bonferroni\ncorrection, the unadjusted p-value is multiplied by K, the number of comparisons, allowing for the\nvalues to be directly compared to \u000b, not\u000b?. Comparing an unadjusted p-value to\u000b=K is equivalent\nto comparing the quantity ( K\u0002p-value) to\u000b.\nCC CT\nCT 0.54 -\nTT 0.03 0.43\nFigure 5.29: Bonferroni-adjusted p-values for pairwise comparisons testing\nwhether the mean change in non-dominant arm strength varies by genotype at\ntheactn3.r577x location on ACTN3 gene.\n272 CHAPTER 5. INFERENCE FOR NUMERICAL DATA\n5.6 Notes\nThe material in this chapter is particularly important. For many applications, t-tests and\nAnalysis of Variance (ANOV A) are an essential part of the core of statistics in medicine and the life\nsciences. The comparison of two or more groups is often the primary aim of experiments both in\nthe laboratory and in studies with human subjects. More generally, the approaches to interpreting\nand drawing conclusions from testing demonstrated in this chapter are used throughout the rest\nof the text and, indeed, in much of statistics.\nWhile it is important to master the details of the techniques of testing for di ﬀerences in two\nor more groups, it is even more critical to not lose sight of the fundamental principles behind the\ntests. A statistically signiﬁcant di ﬀerence in group means does not necessarily imply that group\nmembership is the reason for the observed association. A signiﬁcant association does not neces-\nsarily imply causation, even if it is highly signiﬁcant; confounding variables may be involved. In\nmost cases, causation can only be inferred in controlled experiments when interventions have been\nassigned randomly. It is also essential to carefully consider the context of a problem. For instance,\nstudents often ﬁnd the distinction between paired and independent group comparisons confusing;\nunderstanding the problem context is the only reliable way to choose the correct approach.\nIt is generally prudent to use the form of the t-test that does not assume equal standard de-\nviations, but the power calculations described in Section 5.4 assume models with equal standard\ndeviations. The formulas are simpler when standard deviations are equal, and software is more\nwidely available for that case. The di ﬀerences in sample sizes are usually minor and less important\nthan assumptions about target di ﬀerences or the values of the standard deviations. If the standard\ndeviations are expected to be very di ﬀerent, then more specialized software for computing sample\nsize and power should be used. The analysis done after the study has been completed should then\nuse thet-test for unequal standard deviations.\nTests for signiﬁcant di ﬀerences are sometimes overused in science, with not enough attention\npaid to estimates and conﬁdence intervals. Conﬁdence intervals for the di ﬀerence of two popu-\nlation means show a range of underlying di ﬀerences in means that are consistent with the data,\nand often lead to insights not possible from only the test statistic and p-value. Wide conﬁdence\nintervals may show that a non-signiﬁcant test is the result of high variability in the test statistic,\nperhaps caused by a sample size that was too small. Conversely, a highly signiﬁcant p-value may be\nthe result of such a large sample size that the observed di ﬀerences are not scientiﬁcally meaningful;\nthat may be evident from conﬁdence intervals with very narrow width.\n5.6. NOTES 273\nFinally, the formula used to approximate degrees of freedom \u0017for the independent two-group\nt-test that does not assume equal variance is\n\u0017=h\n(s2\n1=n1) + (s2\n2=n2)i2\nh\n(s2\n1=n1)2=(n1\u00001) + (s2\n2=n2)2=(n2\u00001)i;\nwheren1;s1are the sample size and standard deviation for the ﬁrst sample, and n2;s2are the\ncorresponding values for the second sample. Since \u0017is routinely provided in the output from\nstatistical software, there is rarely any need to calculate it by hand. The approximate formula\ndf = min(n1\u00001;n2\u00001) always produces a smaller value for degrees of freedom and hence a larger\np-value.\nThe labs for this chapter are structured around particularly important problems in practice:\ncomparing two groups, such as a treatment and control group (Lab 1); assessing before starting a\nstudy whether a sample size is large enough to make it likely that important di ﬀerences will be\ndetected (Lab 2); comparing more than two groups using analysis of variance (Lab 3); controlling\nerror rates when looking at many comparisons in a dataset (Lab 4); and thinking about hypothesis\ntesting in the larger context of reproducibility (Lab 5). The ﬁrst four labs provide guidance on\nhow to conduct and interpret speciﬁc types of analyses. Students may ﬁnd the last lab particularly\nuseful in understanding the distinction between a p-value and other probabilities relevant in an\ninferential setting, such as power.\n274 CHAPTER 5. INFERENCE FOR NUMERICAL DATA\n5.7 Exercises\n5.7.1 Single-sample inference with the ttt-distribution\n5.1 Identify the critical ttt.An independent random sample is selected from an approximately normal pop-\nulation with unknown standard deviation. Find the degrees of freedom and the critical t-value (t?) for the\ngiven sample size and conﬁdence level.\n(a)n= 6, CL = 90%\n(b)n= 21, CL = 98%\n(c)n= 29, CL = 95%\n(d)n= 12, CL = 99%\n5.2 Find the p-value, Part I. An independent random sample is selected from an approximately normal\npopulation with an unknown standard deviation. Find the p-value for the given sets of alternative hypothesis\nand test statistic, and determine if the null hypothesis would be rejected at \u000b= 0:05.\n(a)HA:\u0016>\u0016 0,n= 11,T= 1:91\n(b)HA:\u0016<\u0016 0,n= 17,T=\u00003:45\n(c)HA:\u0016,\u00160,n= 7,T= 0:83\n(d)HA:\u0016>\u0016 0,n= 28,T= 2:13\n5.3 Cutoff values. The following are cuto ﬀvalues for the upper 5% of a t-distribution with either degrees of\nfreedom 10, 50, or 100: 2.23, 1.98, and 2.01. Identify which value belongs to which distribution and explain\nyour reasoning.\n5.4 Find the p-value, Part II. An independent random sample is selected from an approximately normal\npopulation with an unknown standard deviation. Find the p-value for the given sets of alternative hypothesis\nand test statistic, and determine if the null hypothesis would be rejected at \u000b= 0:01.\n(a)HA:\u0016>0:5,n= 26,T= 2:485\n(b)HA:\u0016<3,n= 18,T= 0:5\n5.5 Working backwards, Part I. A 95% conﬁdence interval for a population mean, \u0016, is given as (18.985,\n21.015). This conﬁdence interval is based on a simple random sample of 36 observations. Calculate the\nsample mean and standard deviation. Assume that all conditions necessary for inference are satisﬁed. Use\nthet-distribution in any calculations.\n5.6 Working backwards, Part II. A 90% conﬁdence interval for a population mean is (65, 77). The popula-\ntion distribution is approximately normal and the population standard deviation is unknown. This conﬁdence\ninterval is based on a simple random sample of 25 observations. Calculate the sample mean, the margin of\nerror, and the sample standard deviation.\n5.7. EXERCISES 275\n5.7 Sleep habits of New Yorkers. New York is known as \"the city that never sleeps\". A random sample of\n25 New Yorkers were asked how much sleep they get per night. Statistical summaries of these data are shown\nbelow. Do these data provide strong evidence that New Yorkers sleep less than 8 hours a night on average?\nn ¯x s min max\n25 7.73 0.77 6.17 9.78\n(a) Write the hypotheses in symbols and in words.\n(b) Check conditions, then calculate the test statistic, T, and the associated degrees of freedom.\n(c) Find and interpret the p-value in this context. Drawing a picture may be helpful.\n(d) What is the conclusion of the hypothesis test?\n(e) If you were to construct a 90% conﬁdence interval that corresponded to this hypothesis test, would you\nexpect 8 hours to be in the interval?\n5.8 Heights of adults. Researchers studying anthropometry collected body girth measurements and skeletal\ndiameter measurements, as well as age, weight, height and gender, for 507 physically active individuals. The\nhistogram below shows the sample distribution of heights in centimeters.24\nHeight150 160 170 180 190 200020406080100\nMin 147.2\nQ1 163.8\nMedian 170.3\nMean 171.1\nSD 9.4\nQ3 177.8\nMax 198.1\n(a) What is the point estimate for the average height of active individuals? What about the median?\n(b) What is the point estimate for the standard deviation of the heights of active individuals? What about the\nIQR?\n(c) Is a person who is 1m 80cm (180 cm) tall considered unusually tall? And is a person who is 1m 55cm\n(155cm) considered unusually short? Explain your reasoning.\n(d) The researchers take another random sample of physically active individuals. Would you expect the mean\nand the standard deviation of this new sample to be the ones given above? Explain your reasoning.\n(e) The sample means obtained are point estimates for the mean height of all active individuals, if the sample\nof individuals is equivalent to a simple random sample. What measure do we use to quantify the vari-\nability of such an estimate? Compute this quantity using the data from the original sample under the\ncondition that the data are a simple random sample.\n5.9 Find the mean. You are given the following hypotheses:\nH0:\u0016= 60\nHA:\u0016<60\nWe know that the sample standard deviation is 8 and the sample size is 20. For what sample mean would the\np-value be equal to 0.05? Assume that all conditions necessary for inference are satisﬁed.\n24G. Heinz et al. “Exploring relationships in body dimensions”. In: Journal of Statistics Education 11.2 (2003).\n276 CHAPTER 5. INFERENCE FOR NUMERICAL DATA\n5.10ttt?vs.zzz?.For a given conﬁdence level, t?\ndfis larger than z?. Explain how t\u0003\ndfbeing slightly larger than\nz\u0003aﬀects the width of the conﬁdence interval.\n5.11 Play the piano. Georgianna claims that in a small city renowned for its music school, the average child\ntakes less than 5 years of piano lessons. We have a random sample of 20 children from the city, with a mean\nof 4.6 years of piano lessons and a standard deviation of 2.2 years.\n(a) Evaluate Georgianna’s claim using a hypothesis test.\n(b) Construct a 95% conﬁdence interval for the number of years students in this city take piano lessons, and\ninterpret it in context of the data.\n(c) Do your results from the hypothesis test and the conﬁdence interval agree? Explain your reasoning.\n5.12 Auto exhaust and lead exposure. Researchers interested in lead exposure due to car exhaust sampled\nthe blood of 52 police o ﬃcers subjected to constant inhalation of automobile exhaust fumes while working\ntraﬃc enforcement in a primarily urban environment. The blood samples of these o ﬃcers had an average lead\nconcentration of 124.32 \u0016g/l and a SD of 37.74 \u0016g/l; a previous study of individuals from a nearby suburb,\nwith no history of exposure, found an average blood level concentration of 35 \u0016g/l.25\n(a) Write down the hypotheses that would be appropriate for testing if the police o ﬃcers appear to have been\nexposed to a higher concentration of lead.\n(b) Explicitly state and check all conditions necessary for inference on these data.\n(c) Test the hypothesis that the downtown police o ﬃcers have a higher lead exposure than the group in the\nprevious study. Interpret your results in context.\n(d) Based on your preceding result, without performing a calculation, would a 99% conﬁdence interval for\nthe average blood concentration level of police o ﬃcers contain 35 \u0016g/l?\n(e) Based on your preceding result, without performing a calculation, would a 99% conﬁdence interval for\nthis diﬀerence contain 0? Explain why or why not.\n5.13 Car insurance savings. A market researcher wants to evaluate car insurance savings at a competing\ncompany. Based on past studies he is assuming that the standard deviation of savings is $100. He wants to\ncollect data such that he can get a margin of error of no more than $10 at a 95% conﬁdence level. How large\nof a sample should he collect?\n25WI Mortada et al. “Study of lead exposure from automobile exhaust as a risk for nephrotoxicity among tra ﬃc police-\nmen.” In: American journal of nephrology 21.4 (2000), pp. 274–279.\n5.7. EXERCISES 277\n5.7.2 Two-sample test for paired data\n5.14 Air quality. Air quality measurements were collected in a random sample of 25 country capitals in\n2013, and then again in the same cities in 2014. We would like to use these data to compare average air\nquality between the two years. Should we use a paired or non-paired test? Explain your reasoning.\n5.15 Paired or not, Part I. In each of the following scenarios, determine if the data are paired.\n(a) Compare pre- (beginning of semester) and post-test (end of semester) scores of students.\n(b) Assess gender-related salary gap by comparing salaries of randomly sampled men and women.\n(c) Compare artery thicknesses at the beginning of a study and after 2 years of taking Vitamin E for the same\ngroup of patients.\n(d) Assess e ﬀectiveness of a diet regimen by comparing the before and after weights of subjects.\n5.16 Paired or not, Part II. In each of the following scenarios, determine if the data are paired.\n(a) We would like to know if Intel’s stock and Southwest Airlines’ stock have similar rates of return. To ﬁnd\nout, we take a random sample of 50 days, and record Intel’s and Southwest’s stock on those same days.\n(b) We randomly sample 50 items from Target stores and note the price for each. Then we visit Walmart and\ncollect the price for each of those same 50 items.\n(c) A school board would like to determine whether there is a di ﬀerence in average SAT scores for students at\none high school versus another high school in the district. To check, they take a simple random sample of\n100 students from each high school.\n5.17 Global warming, Part I. Let’s consider a limited set of climate data, examining temperature di ﬀerences\nin 1948 vs 2018. We sampled 197 locations from the National Oceanic and Atmospheric Administration’s\n(NOAA) historical data, where the data was available for both years of interest. We want to know: were\nthere more days with temperatures exceeding 90°F in 2018 or in 1948?26The diﬀerence in number of days\nexceeding 90°F (number of days in 2018 - number of days in 1948) was calculated for each of the 197 locations.\nThe average of these di ﬀerences was 2.9 days with a standard deviation of 17.2 days. We are interested in\ndetermining whether these data provide strong evidence that there were more days in 2018 that exceeded\n90°F from NOAA’s weather stations.\n(a) Is there a relationship between the observations collected in 1948\nand 2018? Or are the observations in the two groups independent?\nExplain.\n(b) Write hypotheses for this research in symbols and in words.\n(c) Check the conditions required to complete this test. A histogram of\nthe diﬀerences is given to the right.\n(d) Calculate the test statistic and ﬁnd the p-value.\n(e) Use\u000b= 0:05 to evaluate the test, and interpret your conclusion in\ncontext.\n(f) What type of error might we have made? Explain in context what\nthe error means.\n(g) Based on the results of this hypothesis test, would you expect a con-\nﬁdence interval for the average di ﬀerence between the number of\ndays exceeding 90°F from 1948 and 2018 to include 0? Explain\nyour reasoning.\nDifferences in Number of Days−60−40−20020406001020304050\n−60−40−200204060\n26NOAA, www.ncdc.noaa.gov/cdo-web/datasets, April 24, 2019.\n278 CHAPTER 5. INFERENCE FOR NUMERICAL DATA\n5.18 High School and Beyond, Part I. The National Center of Education Statistics conducted a survey of\nhigh school seniors, collecting test data on reading, writing, and several other subjects. Here we examine a\nsimple random sample of 200 students from this survey. Side-by-side box plots of reading and writing scores\nas well as a histogram of the di ﬀerences in scores are shown below.\nyscores\nread write20406080\nDifferences in scores (read − write)−20 −10 010 20010203040\n(a) Is there a clear di ﬀerence in the average reading and writing scores?\n(b) Are the reading and writing scores of each student independent of each other?\n(c) The average observed di ﬀerence in scores is ¯xread\u0000write =\u00000:545, and the standard deviation of the dif-\nferences is 8.887 points. Do these data provide convincing evidence of a di ﬀerence between the average\nscores on the two exams? Conduct a hypothesis test; interpret your conclusions in context.\n(d) Based on the results of this hypothesis test, would you expect a conﬁdence interval for the average di ﬀer-\nence between the reading and writing scores to include 0? Explain your reasoning.\n5.19 Global warming, Part II. We considered the change in the number of days exceeding 90°F from 1948\nand 2018 at 197 randomly sampled locations from the NOAA database in Exercise 5.17. The mean and stan-\ndard deviation of the reported di ﬀerences are 2.9 days and 17.2 days.\n(a) Calculate a 90% conﬁdence interval for the average di ﬀerence between number of days exceeding 90°F\nbetween 1948 and 2018. We’ve already checked the conditions for you.\n(b) Interpret the interval in context.\n(c) Does the conﬁdence interval provide convincing evidence that there were more days exceeding 90°F in\n2018 than in 1948 at NOAA stations? Explain.\n5.20 High school and beyond, Part II. We considered the di ﬀerences between the reading and writing\nscores of a random sample of 200 students who took the High School and Beyond Survey in Exercise 5.18.\nThe mean and standard deviation of the di ﬀerences are ¯xread\u0000write =\u00000:545 and 8.887 points.\n(a) Calculate a 95% conﬁdence interval for the average di ﬀerence between the reading and writing scores of\nall students.\n(b) Interpret this interval in context.\n(c) Does the conﬁdence interval provide convincing evidence that there is a real di ﬀerence in the average\nscores? Explain.\n5.7. EXERCISES 279\n5.21 Gifted children. Researchers collected a simple random sample of 36 children who had been identiﬁed\nas gifted in a large city. The following histograms show the distributions of the IQ scores of mothers and\nfathers of these children. Also provided are some sample statistics.27\nMother's IQ100 120 14004812\nFather's IQ110 120 13004812\nDiff.−20 0 2004812\nMother Father Di ﬀ.\nMean 118.2 114.8 3.4\nSD 6.5 3.5 7.5\nn 36 36 36\n(a) Are the IQs of mothers and the IQs of fathers in this data set related? Explain.\n(b) Conduct a hypothesis test to evaluate if the scores are equal on average. Make sure to clearly state your\nhypotheses, check the relevant conditions, and state your conclusion in the context of the data.\n5.22 DDT exposure. Suppose that you are interested in determining whether exposure to the organochlo-\nride DDT, which has been used extensively as an insecticide for many years, is associated with breast cancer in\nwomen. As part of a study that investigated this issue, blood was drawn from a sample of women diagnosed\nwith breast cancer over a six-year period and a sample of healthy control subjects matched to the cancer pa-\ntients on age, menopausal status, and date of blood donation. Each woman’s blood level of DDE (an important\nbyproduct of DDT in the human body) was measured, and the di ﬀerence in levels for each patient and her\nmatched control calculated. A sample of 171 such di ﬀerences has mean d= 2:7 ng/mL and standard deviation\nsd= 15:9 ng/mL. Di ﬀerences were calculated as DDEcancer\u0000DDEcontrol .\n(a) Test the null hypothesis that the mean blood levels of DDE are identical for women with breast cancer\nand for healthy control subjects. What do you conclude?\n(b) Would you expect a 95% conﬁdence interval for the true di ﬀerence in population mean DDE levels to\ncontain the value 0?\n5.23 Blue-green eggshells. It is hypothesized that the blue-green color of the eggshells of many avian\nspecies represents an informational signal as to the health of the female that laid the eggs. To investigate\nthis hypothesis, researchers conducted a study in which birds assigned to the treatment group were provided\nwith supplementary food before and during laying; they predict that if eggshell coloration is related to fe-\nmale health at laying, females given supplementary food will lay more intensely blue-green eggs than control\nfemales. Nests were paired according to when nest construction began, and the study examined 16 nest pairs.\n(a) The blue-green chroma (BGC) of eggs was measured on the day of laying; BGC refers to the proportion\nof total reﬂectance that is in the blue-green region of the spectrum, with a higher value representing a\ndeeper blue-green color. In the food supplemented group, BGC chroma had x= 0:594 ands= 0:010; in\nthe control group, BGC chroma had x= 0:586 ands= 0:009. A paired t-test resulted in t= 2:28 and\np= 0:038. Interpret the results in the context of the data.\n(b) In general, healthier birds are also known to lay heavier eggs. Egg mass was also measured for both\ngroups. In the food supplemented group, egg mass had x= 1:70 grams and s= 0:11 grams; in the control\ngroup, egg mass had x= 0:586 grams and s= 0:009 grams. The test statistic from a paired t-test was\n2.64 withp-value 0.019. Compute and interpret a 95% conﬁdence interval for \u000e, the population mean\ndiﬀerence in egg mass between the groups.\n27F.A. Graybill and H.K. Iyer. Regression Analysis: Concepts and Applications . Duxbury Press, 1994, pp. 511–516.\n280 CHAPTER 5. INFERENCE FOR NUMERICAL DATA\n5.7.3 Two-sample test for independent data\n5.24 Diamond prices, Part I. A diamond’s price is determined by various measures of quality, including\ncarat weight. The price of diamonds increases as carat weight increases. While the di ﬀerence between the size\nof a 0.99 carat diamond and a 1 carat diamond is undetectable to the human eye, the price di ﬀerence can be\nsubstantial.28\n0.99 carats 1 carat\nMean $ 44.51 $ 56.81\nSD $ 13.32 $ 16.13\nn 23 23\nPoint price (in dollars)\n0.99 carats 1 carat20406080\n(a) Use the data to assess whether there is a di ﬀerence between the average standardized prices of 0.99 and 1\ncarat diamonds.\n(b) Construct a 95% conﬁdence interval for the average di ﬀerence between the standardized prices of 0.99\nand 1 carat diamonds.\n5.25 Friday the 13th, Part I. In the early 1990’s, researchers in the UK collected data on tra ﬃc ﬂow, number\nof shoppers, and tra ﬃc accident related emergency room admissions on Friday the 13thand the previous\nFriday, Friday the 6th. The histograms below show the distribution of number of cars passing by a speciﬁc\nintersection on Friday the 6thand Friday the 13thfor many such date pairs. Also given are some sample\nstatistics, where the di ﬀerence is the number of cars on the 6thminus the number of cars on the 13th.29\nFriday the 6th120000 130000 14000001234\nFriday the 13th120000 130000 1400000123\nDifference0 2000 4000012345\n6th13thDiﬀ.\n¯x128,385 126,550 1,835\ns 7,259 7,664 1,176\nn 10 10 10\n(a) Are there any underlying structures in these data that should be considered in an analysis? Explain.\n(b) What are the hypotheses for evaluating whether the number of people out on Friday the 6this diﬀerent\nthan the number out on Friday the 13th?\n(c) Check conditions to carry out the hypothesis test from part (b).\n(d) Calculate the test statistic and the p-value.\n(e) What is the conclusion of the hypothesis test?\n(f) Interpret the p-value in this context.\n(g) What type of error might have been made in the conclusion of your test? Explain.\n28H. Wickham. ggplot2: elegant graphics for data analysis . Springer New York, 2009.\n29T.J. Scanlon et al. “Is Friday the 13th Bad For Your Health?” In: BMJ 307 (1993), pp. 1584–1586.\n5.7. EXERCISES 281\n5.26 Egg volume. In a study examining 131 collared ﬂycatcher eggs, researchers measured various charac-\nteristics in order to study their relationship to egg size (assayed as egg volume, in mm3). These characteristics\nincluded nestling sex and survival. A single pair of collared ﬂycatchers generally lays around 6 eggs per\nbreeding season; laying order of the eggs was also recorded.\n(a) Is there evidence at the \u000b= 0:10 signiﬁcance level to suggest that egg size di ﬀers between male and female\nchicks? If so, do heavier eggs tend to contain males or females? For male chicks, x= 1619:95,s= 127:54,\nandn= 80. For female chicks, x= 1584:20,s= 102:51, andn= 48. Sex was only recorded for eggs that\nhatched.\n(b) Construct a 95% conﬁdence interval for the di ﬀerence in egg size between chicks that successfully ﬂedged\n(developed capacity to ﬂy) and chicks that died in the nest. From the interval, is there evidence of a size\ndiﬀerence in eggs between these two groups? For chicks that ﬂedged, x= 1605:87,s= 126:32, andn= 89.\nFor chicks that died in the nest, x= 1606:91,s= 103:46,n= 42.\n(c) Are eggs that are laid ﬁrst a signiﬁcantly di ﬀerent size compared to eggs that are laid sixth? For eggs laid\nﬁrst,x= 1581:98,s= 155:95, andn= 22. For eggs laid sixth, x= 1659:62,s= 124:59, andn= 20.\n5.27 Friday the 13th, Part II. The Friday the 13thstudy reported in Exercise 5.25 also provides data on\ntraﬃc accident related emergency room admissions. The distributions of these counts from Friday the 6thand\nFriday the 13thare shown below for six such paired dates along with summary statistics. You may assume\nthat conditions for inference are met.\nFriday the 6th5 10012\nFriday the 13th5 10012\nDifference−5 0012\n6th13thdiﬀ\nMean 7.5 10.83 -3.33\nSD 3.33 3.6 3.01\nn 6 6 6\n(a) Conduct a hypothesis test to evaluate if there is a di ﬀerence between the average numbers of tra ﬃc acci-\ndent related emergency room admissions between Friday the 6thand Friday the 13th.\n(b) Calculate a 95% conﬁdence interval for the di ﬀerence between the average numbers of tra ﬃc accident\nrelated emergency room admissions between Friday the 6thand Friday the 13th.\n(c) The conclusion of the original study states, “Friday 13th is unlucky for some. The risk of hospital ad-\nmission as a result of a transport accident may be increased by as much as 52%. Staying at home is\nrecommended.” Do you agree with this statement? Explain your reasoning.\n282 CHAPTER 5. INFERENCE FOR NUMERICAL DATA\n5.28 Avian inﬂuenza, Part I. In recent years, widespread outbreaks of avian inﬂuenza have posed a global\nthreat to both poultry production and human health. One strategy being explored by researchers involves\ndeveloping chickens that are genetically resistant to infection. In 2011, a team of investigators reported in\nScience that they had successfully generated transgenic chickens that are resistant to the virus. As a part of\nassessing whether the genetic modiﬁcation might be hazardous to the health of the chicks, hatch weights\nbetween transgenic chicks and non-transgenic chicks were collected. Does the following data suggest that\nthere is a di ﬀerence in hatch weights between transgenic and non-transgenic chickens?\ntransgenic chicks (g) non-transgenic chicks (g)\n¯x 45.14 44.99\ns 3.32 4.57\nn 54 54\n5.29 Chicken diet and weight, Part I. Chicken farming is a multi-billion dollar industry, and any methods\nthat increase the growth rate of young chicks can reduce consumer costs while increasing company proﬁts,\npossibly by millions of dollars. An experiment was conducted to measure and compare the e ﬀectiveness of\nvarious feed supplements on the growth rate of chickens. Newly hatched chicks were randomly allocated into\nsix groups, and each group was given a di ﬀerent feed supplement. Below are some summary statistics from\nthis data set along with box plots showing the distribution of weights by feed type.30\nWeight (in grams)\ncasein horsebean linseed meatmeal soybean sunflower100150200250300350400●\n●\n●\nMean SD n\ncasein 323.58 64.43 12\nhorsebean 160.20 38.63 10\nlinseed 218.75 52.24 12\nmeatmeal 276.91 64.90 11\nsoybean 246.43 54.13 14\nsunﬂower 328.92 48.84 12\n(a) Describe the distributions of weights of chickens that were fed linseed and horsebean.\n(b) Do these data provide strong evidence that the average weights of chickens that were fed linseed and\nhorsebean are di ﬀerent? Use a 5% signiﬁcance level.\n(c) What type of error might we have committed? Explain.\n(d) Would your conclusion change if we used \u000b= 0:01?\n5.30 Fuel efﬁciency of manual and automatic cars, Part I. Each year the US Environmental Protection\nAgency (EPA) releases fuel economy data on cars manufactured in that year. Below are summary statistics on\nfuel eﬃciency (in miles/gallon) from random samples of cars with manual and automatic transmissions man-\nufactured in 2012. Do these data provide strong evidence of a di ﬀerence between the average fuel e ﬃciency of\ncars with manual and automatic transmissions in terms of their average city mileage? Assume that conditions\nfor inference are satisﬁed.31\nCity MPG\nAutomatic Manual\nMean 16.12 19.85\nSD 3.58 4.51\nn 26 26\nCity MPGautomatic manual152535\n30Chicken Weights by Feed Type, from the datasets package in R..\n31U.S. Department of Energy, Fuel Economy Data, 2012 Dataﬁle.\n5.7. EXERCISES 283\n5.31 Chicken diet and weight, Part II. Casein is a common weight gain supplement for humans. Does it\nhave an e ﬀect on chickens? Using data provided in Exercise 5.29, test the hypothesis that the average weight\nof chickens that were fed casein is di ﬀerent than the average weight of chickens that were fed soybean. If\nyour hypothesis test yields a statistically signiﬁcant result, discuss whether or not the higher average weight\nof chickens can be attributed to the casein diet. Assume that conditions for inference are satisﬁed.\n5.32 Fuel efﬁciency of manual and automatic cars, Part II. The table provides summary statistics on\nhighway fuel economy of cars manufactured in 2012 (from Exercise 5.30). Use these statistics to calculate a\n98% conﬁdence interval for the di ﬀerence between average highway mileage of manual and automatic cars,\nand interpret this interval in the context of the data.32\nHwy MPG\nAutomatic Manual\nMean 22.92 27.88\nSD 5.29 5.01\nn 26 26\nHwy MPGautomatic manual152535\n5.33 Gaming and distracted eating. A group of researchers are interested in the possible e ﬀects of dis-\ntracting stimuli during eating, such as an increase or decrease in the amount of food consumption. To test\nthis hypothesis, they monitored food intake for a group of 44 patients who were randomized into two equal\ngroups. The treatment group ate lunch while playing solitaire, and the control group ate lunch without any\nadded distractions. Patients in the treatment group ate 52.1 grams of biscuits, with a standard deviation of\n45.1 grams, and patients in the control group ate 27.1 grams of biscuits, with a standard deviation of 26.4\ngrams. Do these data provide convincing evidence that the average food intake (measured in amount of bis-\ncuits consumed) is di ﬀerent for the patients in the treatment group? Assume that conditions for inference are\nsatisﬁed.33\n5.34 Placebos without deception. While placebo treatment can inﬂuence subjective symptoms, it is typi-\ncally believed that patient response to placebo requires concealment or deception; in other words, a patient\nmust believe that they are receiving an e ﬀective treatment in order to experience the beneﬁts of being treated\nwith an inert substance. Researchers recruited patients su ﬀering from irritable bowel syndrome (IBS) to test\nwhether placebo responses are neutralized by awareness that the treatment is a placebo.\nPatients were randomly assigned to either the treatment arm or control arm. Those in the treatment\narm were given placebo pills, which were described as \"something like sugar pills, which have been shown in\nrigorous clinical testing to produce signiﬁcant mind-body self-healing processes\". Those in the control arm\ndid not receive treatment. At the end of the study, all participants answered a questionnaire called the IBS\nGlobal Improvement Scale (IBS-GIS) which measures whether IBS symptoms have improved; higher scores\nare indicative of more improvement.\nAt the end of the study, the 37 participants in the open placebo group had IBS-GIS scores with x= 5:0\nands= 1:5, while the 43 participants in the no treatment group had IBS-GIS scores with x= 3:9 ands= 1:3.\nBased on an analysis of the data, summarize whether the study demonstrates evidence that placebos\nadministered without deception may be an e ﬀective treatment for IBS.\n32U.S. Department of Energy, Fuel Economy Data, 2012 Dataﬁle.\n33R.E. Oldham-Cooper et al. “Playing a computer game during lunch a ﬀects fullness, memory for lunch, and later snack\nintake”. In: The American Journal of Clinical Nutrition 93.2 (2011), p. 308.\n284 CHAPTER 5. INFERENCE FOR NUMERICAL DATA\n5.35 Prison isolation experiment, Part I. Subjects from Central Prison in Raleigh, NC, volunteered for\nan experiment involving an “isolation” experience. The goal of the experiment was to ﬁnd a treatment that\nreduces subjects’ psychopathic deviant T scores. This score measures a person’s need for control or their re-\nbellion against control, and it is part of a commonly used mental health test called the Minnesota Multiphasic\nPersonality Inventory (MMPI) test. The experiment had three treatment groups:\n(1) Four hours of sensory restriction plus a 15 minute “therapeutic\" tape advising that professional help is\navailable.\n(2) Four hours of sensory restriction plus a 15 minute “emotionally neutral” tape on training hunting dogs.\n(3) Four hours of sensory restriction but no taped message.\nForty-two subjects were randomly assigned to these treatment groups, and an MMPI test was administered\nbefore and after the treatment. Distributions of the di ﬀerences between pre and post treatment scores (pre\n- post) are shown below, along with some sample statistics. Use this information to independently test the\neﬀectiveness of each treatment. Make sure to clearly state your hypotheses, check conditions, and interpret\nresults in the context of the data.34\nTreatment 10 20 400246\nTreatment 2−20 −10 0 10 20024\nTreatment 3−20 −10 0024\nTr 1 Tr 2 Tr 3\nMean 6.21 2.86 -3.21\nSD 12.3 7.94 8.57\nn 14 14 14\n5.7.4 Power calculations for a difference of means\n5.36 Email outreach efforts. A medical research group is recruiting people to complete short surveys about\ntheir medical history. For example, one survey asks for information on a person’s family history in regards to\ncancer. Another survey asks about what topics were discussed during the person’s last visit to a hospital. So\nfar, as people sign up, they complete an average of just 4 surveys, and the standard deviation of the number\nof surveys is about 2.2. The research group wants to try a new interface that they think will encourage new\nenrollees to complete more surveys, where they will randomize each enrollee to either get the new interface\nor the current interface. How many new enrollees do they need for each interface to detect an e ﬀect size of 0.5\nsurveys per enrollee, if the desired power level is 80%?\n5.37 Increasing corn yield. A large farm wants to try out a new type of fertilizer to evaluate whether it will\nimprove the farm’s corn production. The land is broken into plots that produce an average of 1,215 pounds\nof corn with a standard deviation of 94 pounds per plot. The owner is interested in detecting any average\ndiﬀerence of at least 40 pounds per plot. How many plots of land would be needed for the experiment if the\ndesired power level is 90%? Assume each plot of land gets treated with either the current fertilizer or the new\nfertilizer.\n34Prison isolation experiment, stat.duke.edu/resources/datasets/prison-isolation.\n5.7. EXERCISES 285\n5.7.5 Comparing means with ANOVA\n5.38 Fill in the blank. When doing an ANOV A, you observe large di ﬀerences in means between groups.\nWithin the ANOV A framework, this would most likely be interpreted as evidence strongly favoring the\nhypothesis.\n5.39 Chicken diet and weight, Part III. In Exercises 5.29 and 5.31 we compared the e ﬀects of two types\nof feed at a time. A better analysis would ﬁrst consider all feed types at once: casein, horsebean, linseed,\nmeat meal, soybean, and sunﬂower. The ANOV A output below can be used to test for di ﬀerences between the\naverage weights of chicks on di ﬀerent diets.\nDf Sum Sq Mean Sq F value Pr( >F)\nfeed 5 231,129.16 46,225.83 15.36 0.0000\nResiduals 65 195,556.02 3,008.55\nConduct a hypothesis test to determine if these data provide convincing evidence that the average weight\nof chicks varies across some (or all) groups. Make sure to check relevant conditions. Figures and summary\nstatistics are shown below.\nWeight (in grams)\ncaseinhorsebean linseed meatmeal soybean sunflower100150200250300350400●\n●\n●\nMean SD n\ncasein 323.58 64.43 12\nhorsebean 160.20 38.63 10\nlinseed 218.75 52.24 12\nmeatmeal 276.91 64.90 11\nsoybean 246.43 54.13 14\nsunﬂower 328.92 48.84 12\n5.40 Teaching descriptive statistics. A study compared ﬁve di ﬀerent methods for teaching descriptive\nstatistics. The ﬁve methods were traditional lecture and discussion, programmed textbook instruction, pro-\ngrammed text with lectures, computer instruction, and computer instruction with lectures. 45 students were\nrandomly assigned, 9 to each method. After completing the course, students took a 1-hour exam.\n(a) What are the hypotheses for evaluating if the average test scores are di ﬀerent for the di ﬀerent teaching\nmethods?\n(b) What are the degrees of freedom associated with the F-test for evaluating these hypotheses?\n(c) Suppose the p-value for this test is 0.0168. What is the conclusion?\n286 CHAPTER 5. INFERENCE FOR NUMERICAL DATA\n5.41 Coffee, depression, and physical activity. Caﬀeine is the world’s most widely used stimulant, with\napproximately 80% consumed in the form of co ﬀee. Participants in a study investigating the relationship\nbetween co ﬀee consumption and exercise were asked to report the number of hours they spent per week\non moderate (e.g., brisk walking) and vigorous (e.g., strenuous sports and jogging) exercise. Based on these\ndata the researchers estimated the total hours of metabolic equivalent tasks (MET) per week, a value always\ngreater than 0. The table below gives summary statistics of MET for women in this study based on the amount\nof coﬀee consumed.35\nCaﬀeinated co ﬀee consumption\n\u00141 cup/week 2-6 cups/week 1 cup/day 2-3 cups/day \u00154 cups/day Total\nMean 18.7 19.6 19.3 18.9 17.5\nSD 21.1 25.5 22.5 22.0 22.0\nn 12,215 6,617 17,234 12,290 2,383 50,739\n(a) Write the hypotheses for evaluating if the average physical activity level varies among the di ﬀerent levels\nof coﬀee consumption.\n(b) Check conditions and describe any assumptions you must make to proceed with the test.\n(c) Below is part of the output associated with this test. Fill in the empty cells.\nDf Sum Sq Mean Sq F value Pr( >F)\ncoﬀee XXXXX XXXXX XXXXX XXXXX 0.0003\nResiduals XXXXX 25,564,819 XXXXX\nTotal XXXXX 25,575,327\n(d) What is the conclusion of the test?\n5.42 Student performance across discussion sections. A professor who teaches a large introductory\nstatistics class (197 students) with eight discussion sections would like to test if student performance di ﬀers\nby discussion section, where each discussion section has a di ﬀerent teaching assistant. The summary table\nbelow shows the average ﬁnal exam score for each discussion section as well as the standard deviation of\nscores and the number of students in each section.\nSec 1 Sec 2 Sec 3 Sec 4 Sec 5 Sec 6 Sec 7 Sec 8\nni 33 19 10 29 33 10 32 31\n¯xi92.94 91.11 91.80 92.45 89.30 88.30 90.12 93.35\nsi 4.21 5.58 3.43 5.92 9.32 7.27 6.93 4.57\nThe ANOV A output below can be used to test for di ﬀerences between the average scores from the di ﬀerent\ndiscussion sections.\nDf Sum Sq Mean Sq F value Pr( >F)\nsection 7 525.01 75.00 1.87 0.0767\nResiduals 189 7584.11 40.13\nConduct a hypothesis test to determine if these data provide convincing evidence that the average score varies\nacross some (or all) groups. Check conditions and describe any assumptions you must make to proceed with\nthe test.\n35M. Lucas et al. “Co ﬀee, caﬀeine, and risk of depression among women”. In: Archives of internal medicine 171.17 (2011),\np. 1571.\n5.7. EXERCISES 287\n5.43 GPA and major. Undergraduate students taking an introductory statistics course at Duke University\nconducted a survey about GPA and major. The side-by-side box plots show the distribution of GPA among\nthree groups of majors. Also provided is the ANOV A output.\nGPA\n●\nArts and Humanities Natural Sciences Social Sciences2.73.03.33.63.9\nDf Sum Sq Mean Sq F value Pr( >F)\nmajor 2 0.03 0.015 0.185 0.8313\nResiduals 195 15.77 0.081\n(a) Write the hypotheses for testing for a di ﬀerence between average GPA across majors.\n(b) What is the conclusion of the hypothesis test?\n(c) How many students answered these questions on the survey, i.e. what is the sample size?\n5.44 Work hours and education. The General Social Survey collects data on demographics, education, and\nwork, among many other characteristics of US residents.36Using ANOV A, we can consider educational at-\ntainment levels for all 1,172 respondents at once. Below are the distributions of hours worked by educational\nattainment and relevant summary statistics that will be helpful in carrying out this analysis.\nEducational attainment\nLess than HS HS Jr Coll Bachelor’s Graduate Total\nMean 38.67 39.6 41.39 42.55 40.85 40.45\nSD 15.81 14.97 18.1 13.62 15.51 15.17\nn 121 546 97 253 155 1,172\nHours worked per week\nLess than HS HS Jr Coll Bachelor's Graduate020406080\n(a) Write hypotheses for evaluating whether the average number of hours worked varies across the ﬁve\ngroups.\n(b) Check conditions and describe any assumptions you must make to proceed with the test.\n(c) Below is part of the output associated with this test. Fill in the empty cells.\nDf Sum Sq Mean Sq F value Pr( >F)\ndegree XXXXX XXXXX 501.54 XXXXX 0.0682\nResiduals XXXXX 267,382 XXXXX\nTotal XXXXX XXXXX\n(d) What is the conclusion of the test?\n36National Opinion Research Center, General Social Survey, 2010.\n288 CHAPTER 5. INFERENCE FOR NUMERICAL DATA\n5.45 True / False: ANOVA, Part I. Determine if the following statements are true or false in ANOV A, and\nexplain your reasoning for statements you identify as false.\n(a) As the number of groups increases, the modiﬁed signiﬁcance level for pairwise tests increases as well.\n(b) As the total sample size increases, the degrees of freedom for the residuals increases as well.\n(c) The constant variance condition can be somewhat relaxed when the sample sizes are relatively consistent\nacross groups.\n(d) The independence assumption can be relaxed when the total sample size is large.\n5.46 Child care hours. The China Health and Nutrition Survey aims to examine the e ﬀects of the health,\nnutrition, and family planning policies and programs implemented by national and local governments.37\nIt, for example, collects information on number of hours Chinese parents spend taking care of their children\nunder age 6. The side-by-side box plots below show the distribution of this variable by educational attainment\nof the parent. Also provided below is the ANOV A output for comparing average hours across educational\nattainment categories.\nChild care hours\nPrimary school Lower middle school Upper middle school Technical or vocational College050100150\nDf Sum Sq Mean Sq F value Pr( >F)\neducation 4 4142.09 1035.52 1.26 0.2846\nResiduals 794 653047.83 822.48\n(a) Write the hypotheses for testing for a di ﬀerence between the average number of hours spent on child care\nacross educational attainment levels.\n(b) What is the conclusion of the hypothesis test?\n37UNC Carolina Population Center, China Health and Nutrition Survey, 2006.\n5.7. EXERCISES 289\n5.47 Prison isolation experiment, Part II. Exercise 5.35 introduced an experiment that was conducted with\nthe goal of identifying a treatment that reduces subjects’ psychopathic deviant T scores, where this score\nmeasures a person’s need for control or his rebellion against control. In Exercise 5.35 you evaluated the\nsuccess of each treatment individually. An alternative analysis involves comparing the success of treatments.\nThe relevant ANOV A output is given below.\nDf Sum Sq Mean Sq F value Pr( >F)\ntreatment 2 639.48 319.74 3.33 0.0461\nResiduals 39 3740.43 95.91\nspooled = 9:793 ondf= 39\n(a) What are the hypotheses?\n(b) What is the conclusion of the test? Use a 5% signiﬁcance level.\n(c) If in part (b) you determined that the test is signiﬁcant, conduct pairwise tests to determine which groups\nare diﬀerent from each other. If you did not reject the null hypothesis in part (b), recheck your answer.\n5.48 True / False: ANOVA, Part II. Determine if the following statements are true or false, and explain your\nreasoning for statements you identify as false.\nIf the null hypothesis that the means of four groups are all the same is rejected using ANOV A at a 5%\nsigniﬁcance level, then ...\n(a) we can then conclude that all the means are di ﬀerent from one another.\n(b) the standardized variability between groups is higher than the standardized variability within groups.\n(c) the pairwise analysis will identify at least one pair of means that are signiﬁcantly di ﬀerent.\n(d) the appropriate \u000bto be used in pairwise comparisons is 0.05 / 4 = 0.0125 since there are four groups.\n290\nChapter 6\nSimple linear regression\n6.1 Examining scatterplots\n6.2 Estimating a regression line using least squares\n6.3 Interpreting a linear model\n6.4 Statistical inference with regression\n6.5 Interval estimates with regression\n6.6 Notes\n6.7 Exercises\n291\nThe relationship between two numerical variables can be visualized using a scat-\nterplot in the xy-plane. The predictor orexplanatory variable is plotted on the\nhorizontal axis, while the response variable is plotted on the vertical axis.1\nThis chapter explores simple linear regression, a technique for estimating a\nstraight line that best ﬁts data on a scatterplot.2A line of best ﬁt functions as a\nlinear model that can not only be used for prediction, but also for inference. Lin-\near regression should only be used with data that exhibit linear or approximately\nlinear relationships.\nFor example, scatterplots in Chapter 1 illustrated the linear relationship be-\ntween height and weight in the NHANES data, with height as a predictor of weight.\nAdding a best-ﬁtting line to these data using regression techniques would allow for\nprediction of an individual’s weight based on their height. The linear model could\nalso be used to investigate questions about the population-level relationship be-\ntween height and weight, since the data are a random sample from the population\nof adults in the United States.\nNot all relationships in data are linear. For example, the scatterplot in Fig-\nure 1.28 of Chapter 1 shows a highly non-linear relationship between between\nannual per capita income and life expectancy for 165 countries in 2011. Relation-\nships are called strong relationships if the pattern of the dependence between the\npredictor and response variables is clear, even if it is nonlinear as in Figure 1.28.\nAweak relationship is one in which the points in the scatterplot are so di ﬀuse as\nto make it di ﬃcult to discern any relationship. Figure 1.29 in Chapter 1 showed\nrelationships progressing from weak to strong moving from left to right in the top\nand bottom panels. Each of the relationships shown in the second panels from\nthe left are moderate relationships . Finally, changing the scale of measurement\nof one or both variables, such as changing age from age in years to age in months,\nsimply stretches or compresses one or both axes and does not change the nature\nof the relationship. If a relationship is linear it will remain so, and with a simple\nchange of scale, a nonlinear relationship will remain nonlinear.\n1Sometimes, the predictor variable is referred to as the independent variable, and the response variable referred to as\nthe dependent variable.\n2Although the response variable in linear regression is necessarily numerical, the predictor variable can be numerical\nor categorical.\n292\nThe next chapter covers multiple regression, a statistical model used to esti-\nmate the relationship between a single numerical response variable and several\npredictor variables.\nFor labs, slides, and other resources, please visit\nwww.openintro.org/book/biostat\n6.1. EXAMINING SCATTERPLOTS 293\n6.1 Examining scatterplots\nVarious demographic and cardiovascular risk factors were collected as a part of the Prevention\nof REnal and Vascular END-stage Disease (PREVEND) study, which took place in the Netherlands.\nThe initial study population began as 8,592 participants aged 28-75 years who took a ﬁrst survey\nin 1997-1998.3Participants were followed over time; 6,894 participants took a second survey in\n2001-2003, and 5,862 completed the third survey in 2003-2006. In the third survey, measurement\nof cognitive function was added to the study protocol. Data from 4,095 individuals who completed\ncognitive testing are in the prevend dataset, available in the Rpackage oibiostat .\nAs adults age, cognitive function changes over time, largely due to various cerebrovascular\nand neurodegenerative changes. It is thought that cognitive decline is a long-term process that\nmay start as early as 45 years of age.4The Ru ﬀFigural Fluency Test (RFFT) is one measure of\ncognitive function that provides information about cognitive abilities such as planning and the\nability to switch between di ﬀerent tasks. The test consists of drawing as many unique designs as\npossible from a pattern of dots, under timed conditions; scores range from 0 to 175 points (worst\nand best score, respectively).\nRFFT scores for a random sample of 500 individuals are shown in Figure 6.1, plotted against\nage at enrollment, which is measured in years. The variables Ageand RFFT are negatively associated;\nolder participants tend to have lower cognitive function. There is an approximately linear trend\nobservable in the data, which suggests that adding a line could be useful for summarizing the\nrelationship between the two variables.\nIt is important to avoid adding straight lines to non-linear data, such as in Figure 1.28.\n40 50 60 70 8020406080100120140RFFT Score●●●\n●\n●\n●●●●●\n●\n●●●\n●●\n●●●\n●●●\n●\n●●\n●● ●●\n●●●●●●\n●●●\n●●● ●●\n●●\n●●\n●●●●\n●●● ●●●\n●\n●●\n●●\n●\n●\n●●\n●●●\n●\n●●\n●●\n●●\n●●●\n●\n●\n●● ●\n●●\n●●●\n●\n●●\n●\n●●\n●●●\n●\n●●●●\n●●\n●●●\n●●●●\n●●●\n●●● ●\n●●\n●●\n●●\n●●\n●\n●\n●●\n●●\n●●●\n●●\n●●●●●\n●\n●●\n●\n●\n●●●\n●●\n●●\n●●●●●\n●\n●●\n●\n●\n●●●●\n●●\n●●\n●●\n●\n●\n●●\n● ●\n●\n●●\n●\n●●\n●\n●●●●●\n●\n●●\n●●\n●●\n●●\n●● ●\n●\n●●●●\n●●●\n●\n●\n●●\n● ●●\n●●●\n●●●●\n●\n●●●\n●\n●●●\n●●\n●\n●●●\n●\n●\n●●\n●●●●\n●●●●\n●●●\n●●\n●●●●\n●\n●●\n●●●\n● ●●\n●●\n●●\n●● ●\n●●\n●●\n●●\n●● ●●●●●\n●●\n●\n●●●\n●●\n●●●\n●●\n●\n●\n●●●●\n●\n●●\n●●\n●\n●●\n●\n●\n● ●● ●●●●\n●●\n●\n●●\n●\n●●\n●●\n●\n●●\n●\n●●\n●●\n●●●\n●●●●\n●\n●●\n●●\n●●\n●●●\n●●●\n●●●\n●●\n●●\n●\n●●●●\n●●\n●●\n●●\n●\n●●●●●\n●●\n●●\n●\n●●\n●●\n●●\n●\n●●●●\n●●●\n●●\n●\n●●\n●●\n●\n● ●●●●\n●●\n●●●\n●\n●●●\n●\n●●●\n●\n●●\n●●\n●\n●●\n●●●●\n●●●\n●●\n●\n●\n●●●\n●●\n●●● ●\n●●\n●●\n●●●\n●●\n●●\n●\n●●\n●●\n●●\n●●\n●\n●●\n●\n●●\n●●\n●●●●\n●●\n●\n●●●\nAge (yrs)\nFigure 6.1: A scatterplot showing agevs.RFFT . Age is the predictor variable, while\nRFFT score is the response variable.\n3Participants were selected from the city of Groningen on the basis of their urinary albumin excretion; urinary albumin\nexcretion is known to be associated with abnormalities in renal function.\n4Joosten H, et al. Cardiovascular risk proﬁle and cognitive function in young, middle-aged, and elderly subjects. Stroke.\n2013;44:1543-1549, https://doi.org/10.1161/STROKEAHA.111.000496\n294 CHAPTER 6. SIMPLE LINEAR REGRESSION\nThe following conditions should be true in a scatterplot for a line to be considered a reasonable\napproximation to the relationship in the plot and for the application of the methods of inference\ndiscussed later in the chapter:\n1 Linearity. The data shows a linear trend. If there is a nonlinear trend, an advanced regression\nmethod should be applied; such methods are not covered in this text. Occasionally, a trans-\nformation of the data will uncover a linear relationship in the transformed scale.\n2 Constant variability. The variability of the response variable about the line remains roughly\nconstant as the predictor variable changes.\n3 Independent observations. The (x;y) pairs are independent; i.e., the value of one pair provides\nno information about other pairs. Be cautious about applying regression to sequential ob-\nservations in time ( time series data), such as height measurements taken over the course of\nseveral years. Time series data may have a complex underlying structure, and the relationship\nbetween the observations should be accounted for in a model.\n4 Residuals that are approximately normally distributed. This condition can be checked only af-\nter a line has been ﬁt to the data and will be explained in Section 6.3.1, where the term\nresidual is deﬁned. In large datasets, it is su ﬃcient for the residuals to be approximately\nsymmetric with only a few outliers. This condition becomes particularly important when\ninferences are made about the line, as discussed in Section 6.4.\nGUIDED PRACTICE 6.1\nFigure 6.2 shows the relationship between clutch.volume and body.size in the frog data. The plot\nalso appears as Figure 1.26 in Chapter 1. Are the ﬁrst three conditions met for linear regression?5\n4.0 4.5 5.0 5.5 6.05001000150020002500Clutch Volume  (mm3)\n●●●●●●●●●●●\n●●●●●\n●●\n●\n●●\n●●\n●●●●●●●●●\n●●●\n●●\n●●●\n●●\n●●●●\n●●●\n●●\n●●\n●\n●●●\n●●●\n●●●●●\n●●●●●●\n●●\n●●●●●\n●●●\n●●●\n●●●●\n●●\n●●\n●●\n●●\n●\n●●●●●\n●●●●\n●●●\n●\n●●●\n●●●\n●●●\n●●●●\n●●\n●●●\n●\nFemale Body Size (cm)\nFigure 6.2: A plot of clutch.volume versus body.size in the frog data.\n5No. While the relationship appears linear and it is reasonable to assume the observations are independent (based on\ninformation about the frogs given in Chapter 1), the variability in clutch.volume is noticeably less for smaller values of\nbody.size than for larger values.\n6.2. ESTIMATING A REGRESSION LINE USING LEAST SQUARES 295\n6.2 Estimating a regression line using least squares\nFigure 6.3 shows the scatterplot of age versus RFFT score, with the least squares regression\nline added to the plot; this line can also be referred to as a linear model for the data. An RFFT\nscore can be predicted for a given age from the equation of the regression line:\nRFFT = 137:55\u00001:26(age):\nThe vertical distance between a point in the scatterplot and the predicted value on the re-\ngression line is the residual for the observation represented by the point; observations below the\nline have negative residuals, while observations above the line have positive residuals. The size\nof a residual is usually discussed in terms of its absolute value; for example, a residual of \u000013 is\nconsidered larger than a residual of 5.\nFor example, consider the predicted RFFT score for an individual of age 56. According to the\nlinear model, this individual has a predicted score of 137 :550\u00001:261(56) = 66 :934 points. In the\ndata, however, there is a participant of age 56 with an RFFT score of 72; their score is about 5 points\nhigher than predicted by the model (this observation is shown on the plot with a “ \u0002”).\n40 50 60 70 8020406080100120140\nAge (yrs)RFFT Score\nFigure 6.3: A scatterplot showing age(horizontal axis) vs. RFFT (vertical axis) with\nthe regression line added to the plot. Three observations are marked in the ﬁgure;\nthe one marked by a “+” has a large residual of about +38, the one marked by a\n“\u0002” has a small residual of about +5, and the one marked by a “ 4” has a moderate\nresidual of about -13. The vertical dotted lines extending from the observations\nto the regression line represent the residuals.\n296 CHAPTER 6. SIMPLE LINEAR REGRESSION\nRESIDUAL: DIFFERENCE BETWEEN OBSERVED AND EXPECTED\nThe residual of the ithobservation ( xi;yi) is the di ﬀerence of the observed response ( yi) and the\nresponse predicted based on the model ﬁt ( byi):\nei=yi\u0000byi\nThe value byiis calculated by plugging xiinto the model equation.\nThe least squares regression line is the line which minimizes the sum of the squared residuals\nfor all the points in the plot. Let ˆyibe the predicted value for an observation with value xifor the\nexplanatory variable. The value ei=yi\u0000ˆyiis the residual for a data point ( xi;yi) in a scatterplot\nwithnpairs of points. The least squares line is the line for which\ne2\n1+e2\n2+\u0001\u0001\u0001+e2\nn (6.2)\nis smallest.\nFor a general population of ordered pairs ( x;y), the population regression model is\ny=\f0+\f1x+\":\nThe term\"is a normally distributed ‘error term’ that has mean 0 and standard deviation \u001b.\nSinceE(\") = 0, the model can also be written\nE(Yjx) =\f0+\f1x;\nwhere the notation E(Yjx) denotes the expected value of Ywhen the predictor variable has value\nx.6For the PREVEND data, the population regression line can be written as\nRFFT =\f0+\f1(age) +\";or asE(RFFTjage) =\f0+\f1(age):\nThe term\f0is the vertical intercept for the line (often referred to simply as the intercept) and\n\f1is the slope. The notation b0andb1are used to represent the point estimates of the parameters\n\f0and\f1. The point estimates b0andb1are estimated from data; \f0and\f1are parameters from\nthe population model for the regression line.\nb0;b1\nSample\nestimates\nof\f0,\f1The regression line can be written as ˆy=b0+b1(x), where ˆyrepresents the predicted value of\nthe response variable. The slope of the least squares line, b1, is estimated by\nb1=sy\nsxr; (6.3)\nwhereris the correlation between the two variables, and sxandsyare the sample standard devia-\ntions of the explanatory and response variables, respectively. The intercept for the regression line\nis estimated by\nb0=y\u0000b1x: (6.4)\nTypically, regression lines are estimated using statistical software.\n6The error term \"can be thought of as a population parameter for the residuals ( e). While\"is a theoretical quantity that\nrefers to the deviation between an observed value and E(Yjx), a residual is calculated as the deviation between an observed\nvalue and the prediction from the linear model.\n6.2. ESTIMATING A REGRESSION LINE USING LEAST SQUARES 297\nEXAMPLE 6.5\nFrom the summary statistics displayed in Figure 6.4 for prevend.samp , calculate the equation of the\nleast-squares regression line for the PREVEND data.\nb1=sy\nsxr=27:40\n11:60(\u00000:534) =\u00001:26\nb0=y\u0000b1x= 68:40\u0000(\u00001:26)(54:82) = 137:55:\nThe results agree with the equation shown at the beginning of this section:\nRFFT = 137:55\u00001:26(age):\nAge (yrs) RFFT score\nmean x= 54:82y= 68:40\nstandard deviation sx= 11:60sy= 27:40\nr=\u00000:534\nFigure 6.4: Summary statistics for ageand RFFT from prevend.samp .\nGUIDED PRACTICE 6.6\nFigure 6.5 shows the relationship between height and weight in a sample from the NHANES\ndataset introduced in Chapter 1. Calculate the equation of the regression line given the summary\nstatistics:x= 168:78;y= 83:83;sx= 10:29;sy= 21:04;r= 0:410.7\n150 160 170 180 19050100150200Weight (kg)\n● ●●●\n●●\n●●\n●\n●\n●●\n●●\n●●\n●●\n●●\n●●● ●\n●●\n●●●●\n●●●\n●\n●● ●● ●●\n●●●●●\n●●\n●●●\n●●\n●●●\n●\n●●●\n●●\n●●●●\n●\n●●●\n●\n●●●\n●●\n●●●\n●\n●●\n●●\n●●●\n● ● ●\n●●●\n●●●\n●●●\n●●●\n●●●●●●●\n●●\n●●\n●\n●●●●\n●\n●●●\n●\n●●●●\n●\n●●\n●●\n●\n●\n●●\n●●\n●●\n●●\n●●\n●\n●● ●\n●●●\n●●●●●\n●\n●●●\n●\n●●●● ●●\n●●\n●●●\n●●\n●\n●●● ●\n● ●●●\n●\n●●\n●●●\n● ●●\n●●●\n●\n●\n● ●●\n●●\n●\n●●\n●●●●\n●●●●\n●●\n●●\n● ●\n●●\n●●●\n●\n●●●●●\n●●●\n●\n●●●\n●●\n●● ●●●\n●●\n●●●\n●●●\n●●●\n●●\n●●●\n●●\n●●\n●\n●●\n●\n●●●●\n●●●\n●●●●\n●●\n●●●●\n●●●\n●●\n● ●\n●●●\n●\n●●●\n●●●\n●●\n●●\n●●\n●●\n● ●●●\n●●\n●\n●●\n●●\n●●●●●●\n●●●\n● ●●\n●●\n●●●●\n●●\n●●\n●●\n●●\n●● ●\n●\n●●\n●●\n●\n●●\n●●\n●●●\n●\n●●\n●●●\n●●●\n● ●●\n●\n●●●●●●\n●\n●●\n●●●\n●●●\n●●\n●●\n●●●\n●●\n●●\n●\n●●\n● ●●●\n●\n●●\n●●●●\n●●●\n●\n●●●●\n●\n●●\n●●\n●\n●●●●●\n●●●\n●●\n●●\n●●●●\n●● ●\n●●\n●●● ●●\n●\n●●\n●● ●\n●●\n●●●\n●●●\n●\n●\n●●●\n●●●\n●●●\n●\n●●●●\n●\n●●\n● ●●\n●●\n●●\n●●\nHeight (cm)\nFigure 6.5: A plot of Height versus Weight innhanes.samp.adult.500 , with a least-\nsquares regression line\nGUIDED PRACTICE 6.7\nPredict the weight in pounds for an adult who is 5 feet, 11 inches tall. 1 cm = .3937 in; 1 lb =\n0.454 kg.8\n7The equation of the line is weight =\u000057:738+0:839(height ), where height is in centimeters and weight is in kilograms.\n85 feet, 11 inches equals 71 =:3937 = 180:34 centimeters. From the regression equation, the predicted weight is \u000057:738+\n0:839(180:34) = 93:567 kilograms. In pounds, this weight is 93 :567=0:454 = 206:280.\n298 CHAPTER 6. SIMPLE LINEAR REGRESSION\n6.3 Interpreting a linear model\nA least squares regression line functions as a statistical model that can be used to estimate the\nrelationship between an explanatory and response variable. While the calculations for constructing\na regression line are relatively simple, interpreting the linear model is not always straightforward.\nIn addition to discussing the mathematical interpretation of model parameters, this section also\naddresses methods for assessing whether a linear model is an appropriate choice, interpreting cat-\negorical predictors, and identifying outliers.\nThe slope parameter of the regression line speciﬁes how much the line rises (positive slope) or\ndeclines (negative slope) for one unit of change in the explanatory variable. In the PREVEND data,\nthe line decreases by 1.26 points for every increase of 1 year. However, it is important to clarify that\nRFFT score tends to decrease as age increases, with average RFFT score decreasing by 1.26 points\nfor each additional year of age. As visible from the scatter of the data around the line, the line does\nnot perfectly predict RFFT score from age; if this were the case, all the data would fall exactly on\nthe line.\nWhen interpreting the slope parameter, it is also necessary to avoid phrasing indicative of a\ncausal relationship, since the line describes an association from data collected in an observational\nstudy. From these data, it is not possible to conclude that increased age causes a decline in cognitive\nfunction.9\nMathematically, the intercept on the vertical axis is a predicted value on the line when the\nexplanatory variable has value 0. In biological or medical examples, 0 is rarely a meaningful value\nof the explanatory variable. For example, in the PREVEND data, the linear model predicts a score\nof 137.55 when age is 0—however, it is nonsensical to predict an RFFT score for a newborn infant.\nIn fact, least squares lines should never be used to extrapolate values outside the range of\nobserved values. Since the PREVEND data only includes participants between ages 36 and 81,\nit should not be used to predict RFFT scores for people outside that age range. The nature of a\nrelationship may change for very small or very large values of the explanatory variable; for exam-\nple, if participants between ages 15 and 25 were studied, a di ﬀerent relationship between age and\nRFFT scores might be observed. Even making predictions for values of the explanatory variable\nslightly larger than the minimum or slightly smaller than the maximum can be dangerous, since in\nmany datasets, observations near the minimum or maximum values (of the explanatory variable)\nare sparse.\nLinear models are useful tools for summarizing a relationship between two variables, but it\nis important to be cautious about making potentially misleading claims based on a regression line.\nThe following subsection discusses two commonly used approaches for examining whether a linear\nmodel can reasonably be applied to a dataset.\n6.3.1 Checking residuals from a linear model\nRecall that there are four assumptions that must be met for a linear model to be considered\nreasonable: linearity, constant variability, independent observations, normally distributed residu-\nals. In the PREVEND data, the relationship between RFFT score and age appears approximately\nlinear, and it is reasonable to assume that the data points are independent. To check the assump-\ntions of constant variability around the line and normality of the residuals, it is helpful to consult\nresidual plots and normal probability plots (Section 3.3.7).10\n9Similarly, avoid language such as increased age leads to orproduces lower RFFT scores.\n10While simple arithmetic can be used to calculate the residuals, the size of most datasets makes hand calculations\nimpractical. The plots here are based on calculations done in R.\n6.3. INTERPRETING A LINEAR MODEL 299\nExamining patterns in residuals\nThere are a variety of residual plots used to check the ﬁt of a least squares line. The plots shown\nin this text are scatterplots in which the residuals are plotted on the vertical axis against predicted\nvalues from the model on the horizontal axis. Other residual plots may instead show values of the\nexplanatory variable or the observed response variable on the horizontal axis. When a least squares\nline ﬁts data very well, the residuals should scatter about the horizontal line y= 0 with no apparent\npattern.\nFigure 6.6 shows three residual plots from simulated data; the plots on the left show data\nplotted with the least squares regression line, and the plots on the right show residuals on the y-\naxis and predicted values on the x-axis. A linear model is a particularly good ﬁt for the data in the\nﬁrst row, where the residual plot shows random scatter above and below the horizontal line. In the\nsecond row, the original data cycles below and above the regression line; this nonlinear pattern is\nmore evident in the residual plot. In the last row, the variability of the residuals is not constant;\nthe residuals are slightly more variable for larger predicted values.\nx predict(g)g$residuals\nx predict(g)g$residuals g$residuals\nFigure 6.6: Sample data with their best ﬁtting lines (left) and their corresponding\nresidual plots (right).\n300 CHAPTER 6. SIMPLE LINEAR REGRESSION\nFigure 6.7 shows a residual plot from the estimated linear model RFFT = 137 :55\u00001:26(age).\nWhile the residuals show scatter around the line, there is less variability for lower predicted RFFT\nscores. A data analyst might still decide to use the linear model, with the knowledge that pre-\ndictions of high RFFT scores may not be as accurate as for lower scores. Reading a residual plot\ncritically can reveal weaknesses about a linear model that should be taken into account when in-\nterpreting model results. More advanced regression methods beyond the scope of this text may be\nmore suitable for these data.\n40 50 60 70 80 90−60−40−200204060\nPredicted RFFT ScoreResiduals\nFigure 6.7: Residual plot for the model in Figure 6.3 using prevend.samp .\nEXAMPLE 6.8\nFigure 6.8 shows a residual plot for the model predicting weight from height using the sample of\n500 adults from the NHANES data, nhanes.samp.adult.500 . Assess whether the constant variabil-\nity assumption holds for the linear model.\nThe residuals above the line are more variable, taking on more extreme values than those below\nthe line. Larger than expected residuals imply that there are many large weights that are under-\npredicted; in other words, the model is less accurate at predicting relatively large weights.\n70 80 90 100050100\nPredicted WeightResiduals\nFigure 6.8: A residual plot from the linear model for height versus weight in\nnhanes.samp.adult.500 .\n6.3. INTERPRETING A LINEAR MODEL 301\nChecking normality of the residuals\nThe normal probability plot, introduced in Section 3.3.7, is best suited for checking normality of\nthe residuals, since normality can be di ﬃcult to assess using histograms alone. Figure 6.9 shows\nboth the histogram and normal probability plot of the residuals after ﬁtting a least squares regres-\nsion to the age versus RFFT data.\nResiduals−60 −200204060\nTheoretical QuantilesSample Quantiles\n−3−2−10123−60−40−200204060\nFigure 6.9: A histogram and normal probability plot of the residuals from the\nlinear model for RFFT versus Age in prevend.samp .\nThe normal probability plot shows that the residuals are nearly normally distributed, with\nonly slight deviations from normality in the left and right tails.\nGUIDED PRACTICE 6.9\nFigure 6.10 shows a histogram and normal probability plot for the linear model to predict weight\nfrom height in nhanes.samp.adult.500 . Evaluate the normality of the residuals.11\nResiduals−60−40−200204060●●\n●●●●\n●●●\n●\n●●\n●●\n●\n●●●\n●●●●\n●\n●\n●●\n●●●●\n●●\n●\n●\n●●\n●●●\n●\n●●●●●\n●●\n●●●\n●●\n●●●\n●\n●●●●●\n●●●●\n●●●●\n●\n●●●\n●●●●●\n●\n●\n●●●\n●●●\n●\n●●\n●●●\n●●●\n●●●\n●●●\n●●\n●●●●●\n●●\n●●\n●\n●●●●\n●\n●●●●\n●●●●\n●\n●●\n●●●\n●\n●●\n●●\n●●\n●●\n●●\n●\n●●\n●\n●●●\n●●●●●\n●\n●●●\n●\n●●\n●●\n●●\n●●\n●●●\n●●\n●●●●\n●\n●\n●●●\n●\n●●\n●●●\n●\n●●\n●●\n●\n●\n●●\n●●\n●●●\n●●\n●●●\n●\n●●●●\n●●\n●●\n●●\n●●●\n●●\n●\n●●\n●●●\n●●●\n●\n●●●\n●●\n●●●●●\n●●\n●\n●●\n●●●\n●●●\n●●\n●\n●●●●\n●●\n●●●\n●\n●\n●●●●\n●●\n●●\n●●\n●●\n●\n●●●\n●\n●●\n●●●\n●\n●●●\n●\n●●●\n●●\n●\n●●●●\n●●\n●●\n●●●●\n●●\n●\n●●\n●●\n●●\n●●●●\n●●●\n●●●\n●●\n●●●●\n●●\n●●\n●●●●\n●●\n●\n●●●\n●●●\n●●\n●●\n●●●\n●\n●●●●●\n●●●\n●●●\n●\n●●●●\n●●\n●\n●●\n●●●\n●●\n●\n●●\n●●\n●\n●●\n●●\n●●\n●\n●●\n●\n●\n●●●\n●●\n●●●\n●●●\n●\n●●\n●●●\n●\n●●\n●●\n●\n●●●●●\n●●●●●\n●●\n●●●●\n●●\n●\n●●\n●●\n●\n●●●\n●●\n●\n●●\n●●\n●●●●●●\n●\n●\n●●●\n●●●\n●●●\n●\n●●●\n●●\n●●\n●●●\n●●\n●●\n●●\nTheoretical QuantilesSample Quantiles\n−3−2−10123050100\nFigure 6.10: A histogram and normal probability plot of the residuals from the\nlinear model for height versus weight in nhanes.samp.adult.500 .\n11The data are roughly normal, but there are deviations from normality in the tails, particularly the upper tail. There\nare some relatively large observations, which is evident from the residual plot shown in Figure 6.8.\n302 CHAPTER 6. SIMPLE LINEAR REGRESSION\n6.3.2 Using R2to describe the strength of a ﬁt\nThe correlation coe ﬃcientrmeasures the strength of the linear relationship between two\nvariables. However, it is more common to measure the strength of a linear ﬁt using r2, which is\ncommonly written as R2in the context of regression.12\nThe quantity R2describes the amount of variation in the response that is explained by the\nleast squares line. While R2can be easily calculated by simply squaring the correlation coe ﬃcient,\nit is easier to understand the interpretation of R2by using an alternative formula:\nR2=variance of predicted y-values\nvariance of observed y-values:\nIt is possible to show that R2can also be written\nR2=s2\ny\u0000s2\nresiduals\ns2y:\nIn the linear model predicting RFFT scores from age, the predicted values on the least squares\nline are the values of RFFT that are ’explained’ by the linear model. The variability of the resid-\nuals about the line represents the remaining variability after the prediction; i.e., the variability\nunexplained by the model. For example, if a linear model perfectly captured all the data, then\nthe variance of the predicted y-values would be equal to the variance of the observed y-values,\nresulting in R2= 1. In the linear model for RFFT , the proportion of variability explained is\nR2=s2\nRFFT\u0000s2\nresiduals\ns2\nRFFT=750:52\u0000536:62\n750:52=213:90\n750:52= 0:285;\nabout 29%. This is equal to the square of the correlation coe ﬃcient,r2=\u00000:5342= 0:285.\nSinceR2in simple linear regression is simply the square of the correlation coe ﬃcient between\nthe predictor and the response, it does not add a new tool to regression. It becomes much more\nuseful in models with several predictors, where it has the same interpretation as the proportion of\nvariability explained by a model but is no longer the square of any one of the correlation coe ﬃcients\nbetween the individual responses and the predictor. Those models are discussed in Chapter 7.\nGUIDED PRACTICE 6.10\nIn the NHANES data, the variance of Weight is 442:53 kg2and the variance of the residuals is 368.1.\nWhat proportion of the variability in the data is explained by the model?13\nGUIDED PRACTICE 6.11\nIf a linear model has a very strong negative relationship with a correlation of -0.97, how much of\nthe variation in the response is explained by the explanatory variable?14\n12In software output, R2is usually labeled R-squared .\n13About 16.8%:s2\nweight\u0000s2\nresiduals\ns2\nweight=442:53\u0000368:1\n442:53=74:43\n442:53= 0:168\n14AboutR2= (\u00000:97)2= 0:94 or 94% of the variation is explained by the linear model.\n6.3. INTERPRETING A LINEAR MODEL 303\n6.3.3 Categorical predictors with two levels\nAlthough the response variable in linear regression is necessarily numerical, the predictor\nvariable may be either numerical or categorical. This section explores the association between a\ncountry’s infant mortality rate and whether or not 50% of the population has access to adequate\nsanitation facilities.\nThe World Development Indicators (WDI) is a database of country-level variables (i.e., indi-\ncators) recording outcomes for a variety of topics, including economics, health, mortality, fertility,\nand education.15The dataset wdi.2011 contains a subset of variables on 165 countries from the\nyear 2011.16The infant mortality rate in a country is recorded as the number of deaths in the ﬁrst\nyear of life per 1,000 live births. Access to sanitation is recorded as the percentage of the popula-\ntion with adequate disposal facilities for human waste. Due to the availability of death certiﬁcates,\ninfant mortality is measured reasonably accurately throughout the world. However, it is more dif-\nﬁcult to obtain precise measurements of the percentage of a population with access to adequate\nsanitation facilities; instead, considering whether half the population has such access may be a\nmore reliable measure. The analysis presented here is based on 163 of the 165 countries; the values\nfor access to sanitation are missing for New Zealand and Turkmenistan.\nFigure 6.11(a) shows that infant mortality rates are highly right-skewed, with a relatively\nsmall number of countries having high infant mortality rates. In 13 countries, infant mortality\nrates are higher than 70 deaths per thousand live births. Figure 6.11(b) shows infant mortality\nafter a log transformation; the following analysis will use the more nearly symmetric transformed\nversion of inf.mortality .\nInfant Mortality (deaths/1,000 live births) 0103050709011001020304050\n(a)\nlog (Infant Mortality)1 2 3 4 50102030 (b)\nFigure 6.11: (a) Histogram of infant mortality, measured in deaths per 1,000\nlive births in the ﬁrst year of life. (b) Histogram of the log-transformed infant\nmortality.\n15http://data.worldbank.org/data-catalog/world-development-indicators\n16The data were collected by a Harvard undergraduate in the Statistics department, and are accessible via the oibiostat\npackage.\n304 CHAPTER 6. SIMPLE LINEAR REGRESSION\nFigure 6.12 shows a scatterplot of log( inf.mortality ) against the categorical variable for san-\nitation access, coded 1if at least 50% of the population has access to adequate sanitation, and\n0otherwise. Since there are only two values of the predictor, the values of infant mortality are\nstacked above the two predictor values 0 and 1.17\nAccess to Sanitation0\n(low)1\n(high)1234log(Infant Mortality)\nFigure 6.12: Country-level infant mortality rates, divided into low access ( x= 0)\nand high access ( x= 1) to sanitation. The least squares regression line is also\nshown.\nThe least squares regression line has the form\nlog(inf.mortality ) =b0+b1(sanit.access ): (6.12)\nThe estimated least squares regression line has intercept and slope parameters of 4.018 and\n-1.681, respectively. While the scatterplot appears unlike those for two numerical variables, the\ninterpretation of the parameters remains unchanged. The slope, -1.681, is the estimated change\nin the logarithm of infant mortality when the categorical predictor changes from low access to\nsanitation facilities to high access. The intercept term 4.018 is the estimated log infant mortality\nfor the set of countries where less than 50% of the population has access to adequate sanitation\nfacilities ( sanit.access = 0).\n17Typically, side-by-side boxplots are used to display the relationship between a numerical variable and a categorical\nvariable. In a regression context, it can be useful to use a scatterplot instead, in order to see the variability around the\nregression line.\n6.3. INTERPRETING A LINEAR MODEL 305\nUsing the model in Equation 6.12, the prediction equation can be written\nlog(inf.mortality ) = 4:018\u00001:681( sanit.access ):\nExponentiating both sides of the equation yields\ninf.mortality =e4:018\u00001:681( sanit.access ):\nWhen sanit.access = 0, the equation simpliﬁes to e4:018= 55:590 deaths among 1,000 live births;\nthis is the estimated infant mortality rate in the countries with low access to sanitation facilities.\nWhen sanit.access = 1, the estimated infant mortality rate is e4:018\u00001:681(1)=e2:337= 10:350 deaths\nper 1,000 live births. The infant mortality rate drops by a factor of 0 :186; i.e., the mortality rate in\nthe high access countries is approximately 20% of that in the low access countries.18\nEXAMPLE 6.13\nCheck the assumptions of constant variability around the regression line and normality of the\nresiduals in the model for the relationship between the transformed infant mortality variable and\naccess to sanitation variable. Residual plots are shown in Figure 6.13.\nWhile the normal probability plot does show that the residuals are approximately normally dis-\ntributed, the residual plot reveals that variability is far from constant around the two predictors.\nAnother method for assessing the relationship between the two groups is advisable; this is dis-\ncussed further in Section 6.4.\n2.5 3.0 3.5 4.0−1012\nPredicted log(Infant Mortality)Residuals\n(a)\nResiduals−2−1012\nTheoretical QuantilesSample Quantiles\n−2−1012−1012 (b)\nFigure 6.13: (a) Residual plot of log(inf.mortality) and sanit.access . (b) His-\ntogram and normal probability plot of the residuals.\n6.3.4 Outliers in regression\nDepending on their position, data points in a scatterplot have varying degrees of contribution\nto the estimated parameters of a regression line. Points that are at particularly low or high values\nof the predictor ( x) variable are said to have high leverage , and have a large inﬂuence on the\nestimated intercept and slope of the regression line; observations with xvalues closer to the center\nof the distribution of xdo not have a large e ﬀect on the slope.\n18When examining event rates in public health, associations are typically measured using rate ratios rather than rate\ndiﬀerences.\n306 CHAPTER 6. SIMPLE LINEAR REGRESSION\nA data point in a scatterplot is considered an outlier in regression if its value for the response\n(y) variable does not follow the general linear trend in the data. Outliers that sit at extreme values\nof the predictor variable (i.e., have high leverage) have the potential to contribute disproportion-\nately to the estimated parameters of a regression line. If an observation does have a strong e ﬀect on\nthe estimates of the line, such that estimates change substantially when the point is omitted, the\nobservation is inﬂuential . These terms are formally deﬁned in advanced regression courses.\nThis section examines the relationship between infant mortality and number of doctors, using\ndata for each state and the District of Columbia.19Infant mortality is measured as the number of\ninfant deaths in the ﬁrst year of life per 1,000 live births, and number of doctors is recorded as\nnumber of doctors per 100,000 members of the population. Figure 6.14 shows scatterplots with\ninfant mortality on the y-axis and number of doctors on the x-axis.\nOne point in Figure 6.14(a), marked in red, is clearly distant from the main cluster of points.\nThis point corresponds to the District of Columbia, where there were approximately 807.2 doctors\nper 100,000 members of the population, and the infant mortality rate was 11.3 per 1,000 live births.\nSince 807.2 is a high value for the predictor variable, this observation has high leverage. It is also an\noutlier; the other points exhibit a downward sloping trend as the number of doctors increases, but\nthis point, with an unusually high y-value paired with a high x-value, does not follow the trend.\nFigure 6.14(b) illustrates that the DC observation is inﬂuential. Not only does the observation\nsimply change the numerical value of the slope parameter, it reverses the direction of the linear\ntrend; the regression line ﬁtted with the complete dataset has a positive slope, but the line re-ﬁtted\nwithout the DC observation has a negative slope. The large number of doctors per population is\ndue to the presence of several large medical centers in an area with a population that is much\nsmaller than a typical state.\nIt seems natural to ask whether or not an inﬂuential point should be removed from a dataset,\nbut that may not be the right question. Instead, it is usually more important to assess whether the\ninﬂuential point might be an error in the data, or whether it belongs in the dataset. In this case,\nthe District of Columbia has certain characteristics that may make comparisons with other states\ninappropriate; this is one argument in favor of excluding the DC observation from the data.\nGenerally speaking, if an inﬂuential point arises from random sampling from a large popu-\nlation and is not a data error, it should be left in the dataset, since it probably represents a small\nsubset of the population from which the data were sampled.\nGUIDED PRACTICE 6.14\nOnce the inﬂuential DC point is removed, assess whether it is appropriate to use linear regression\non these data by checking the four assumptions behind least squares regression: linearity, constant\nvariability, independent observations, and approximate normality of the residuals. Refer to the\nresidual plots shown in Figure 6.15.20\n19Data are from the Statistical Abstract of the United States, published by the US Census Bureau. Data are for 2010, and\navailable as census.2010 in the oibiostat package.\n20The scatterplot in Figure 6.14(b) does not show any nonlinear trends. Similarly, Figure 6.15(a) does not indicate any\nnonlinear trends or noticeable di ﬀerence in the variability of the residuals, although it does show that there are relatively\nfew observations for low values of predicted infant mortality. From Figure 6.15(b), the residuals are approximately normally\ndistributed. Infant mortality across the states reﬂects a complex mix of di ﬀerent levels of income, access to health care, and\nindividual state initiatives in health care; these and other state-speciﬁc features probably act independently across the\nstates, although there is some dependence from federal inﬂuence such as funding for pre-natal care. Overall, independence\nseems like a reasonable assumption.\n6.3. INTERPRETING A LINEAR MODEL 307\n200300400500600700800567891011\nDoctors (per 100,000 members of pop.)Infant Mortality\n(a)\n200 250 300 350 400 4505678910\nDoctors (per 100,000 members of pop.)Infant Mortality\n(b)\nFigure 6.14: (a) Plot including District of Columbia data point. (b) Plot without\ninﬂuential District of Columbia data point.\n5.5 6.0 6.5 7.0 7.5−2−10123\nPredicted Infant MortalityResiduals\n(a)\nResiduals−4−2 024\nTheoretical QuantilesSample Quantiles\n−2−1012−2−10123 (b)\nFigure 6.15: (a) Residual plot of inf.mortality and doctors . (b) Histogram and\nnormal probability plot of the residuals.\n308 CHAPTER 6. SIMPLE LINEAR REGRESSION\n6.4 Statistical inference with regression\nThe previous sections in this chapter have focused on linear regression as a tool for summa-\nrizing trends in data and making predictions. These numerical summaries are analogous to the\nmethods discussed in Chapter 1 for displaying and summarizing data. Regression is also used to\nmake inferences about a population.\nThe same ideas covered in Chapters 4 and 5 about using data from a sample to draw inferences\nabout population parameters apply with regression. Previously, the goal was to draw inference\nabout the population parameter \u0016; in regression, the population parameter of interest is typically\nthe slope parameter \f1. Inference about the intercept term is rare, and limited to the few problems\nwhere the vertical intercept has scientiﬁc meaning.21\nInference in regression relies on the population linear model for the relationship between an\nexplanatory variable Xand a response variable Ygiven by\nY=\f0+\f1X+\"; (6.15)\nwhere\"is assumed to have a normal distribution with mean 0 and standard deviation \u001b(\"\u0018\nN(0;\u001b)). This population model speciﬁes that a response Yhas value\f0+\f1Xplus a random term\nthat pushes Ysymmetrically above or below the value speciﬁed by the line.22\nThe set of ordered pairs ( xi;yi) used when ﬁtting a least squares regression line are assumed\nto have been sampled from a population in which the relationship between the explanatory and\nresponse variables follows Equation 6.15. Under this assumption, the slope and intercept values\nof the least squares regression line, b0andb1, are estimates of the population parameters \f0and\n\f1;b0andb1have sampling distributions, just as Xdoes when thought of as an estimate of a pop-\nulation mean \u0016. A more advanced treatment of regression would demonstrate that the sampling\ndistribution of b1is normal with mean E(b1) =\f1and standard deviation\n\u001bb1=\u001bpP(xi\u0000x)2:\nThe sampling distribution of b0has meanE(b0) =\f0and standard deviation\n\u001bb0=\u001bs\n1\nn+x2\nP(xi\u0000x)2:\nIn both of these expressions, \u001bis the standard deviation of \".\nHypothesis tests and conﬁdence intervals for regression parameters have the same basic form\nas tests and intervals about population means. The test statistic for a null hypothesis H0:\f1=\f0\n1\nabout a slope parameter is\nt=b1\u0000\f0\n1\ns.e.(b1);\nwhere the formula for s.e.( b1) is given below. In this setting, thas at-distribution with n\u00002 degrees\nof freedom, where nis the number of ordered pairs used to estimate the least squares line.\n21In some applications of regression, the predictor xis replaced by x\u0003=x\u0000x. In that case, the vertical intercept is the\nvalue of the line when x\u0003= 0, orx=x.\n22SinceE(\") = 0, this model can also be written as Y\u0018N(\u0016x), with\u0016x=E(Y) =\f0+\f1X. The term\"is the population\nmodel for the observed residuals eiin regression.\n6.4. STATISTICAL INFERENCE WITH REGRESSION 309\nTypically, hypothesis testing in regression involves tests of whether the xandyvariables are\nassociated; in other words, whether the slope is signiﬁcantly di ﬀerent from 0. In these settings, the\nnull hypothesis is that there is no association between the explanatory and response variables, or\nH0:\f1= 0 =\f0\n1, in which case\nt=b1\ns.e.(b1):\nThe hypothesis is rejected in favor of the two-sided alternative HA:\f1,0 with signiﬁcance level \u000b\nwhenjtj\u0015t?\ndf, wheret?\ndfis the point on a t-distribution with n\u00002 degrees of freedom that has \u000b=2\narea to its right (i.e., when p\u0014\u000b).\nA two-sided conﬁdence interval for \f1is given by\nb1\u0006s.e.(b1)\u0002t?\ndf:\nTests for one-sided alternatives and one-sided conﬁdence intervals make the usual adjustments to\nthe rejection rule and conﬁdence interval, and p-values are interpreted just as in Chapters 4 and 5.\nFormulas for calculating standard errors\nStatistical software is typically used to obtain t-statistics and p-values for inference with regression,\nsince using the formulas for calculating standard error can be cumbersome.\nThe standard errors of b0andb1used in conﬁdence intervals and hypothesis tests replace \u001b\nwiths, the standard deviation of the residuals from a ﬁtted line. Formally,\ns=sPe2\ni\nn\u00002=rP(yi\u0000ˆyi)2\nn\u00002: (6.16)\nThe terms2is often called the mean squared error from the regression, and sthe root mean squared\nerror.\nThe two standard errors are\ns.e.(b1) =spP(xi\u0000x)2and s.e.( b0) =ss\n1\nn+x2\nP(xi\u0000x)2:\n310 CHAPTER 6. SIMPLE LINEAR REGRESSION\nEXAMPLE 6.17\nIs there evidence of a signiﬁcant association between number of doctors per 100,000 members of\nthe population in a state and infant mortality rate?\nThe numerical output that Rreturns is shown in Figure 6.16.23\nThe question implies that the District of Columbia should not be included in the analysis. The\nassumptions for applying a least squares regression have been veriﬁed in Exercise 6.14. Whenever\npossible, formal inference should be preceded by a check of the assumptions for regression.\nThe null and alternative hypotheses are H0:\f1= 0 andHA:\f1,0:\nThe estimated slope of the least squares line is -0.0068, with standard error 0.0028. The t-statistic\nequals -2.40, and the probability that the absolute value of a t-statistic with 50 \u00002 = 48 degrees of\nfreedom is smaller than \u00002:40 or larger than 2 :40 is 0.021.\nSincep= 0:021<0:05, the data support the alternative hypothesis that the number of physicians\nis associated with infant mortality at the 0.05 signiﬁcance level. The sign of the slope implies that\nthe association is negative; states with more doctors tend to have lower rates of infant mortality.\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) 8.5991 0.7603 11.31 0.0000\nDoctors Per 100,000 -0.0068 0.0028 -2.40 0.0206\nFigure 6.16: Summary of regression output from Rfor the model predicting infant\nmortality from number of doctors, using the census.2010 dataset.\nCare should be taken in interpreting the above results. The R2for the model is 0.107; the\nmodel explains only about 10% of the state-to-state variability in infant mortality, which suggests\nthere are several other factors a ﬀecting infant mortality that are not accounted for in the model.24\nAdditionally, an important implicit assumption being made in this example is that data from the\nyear 2010 are representative; in other words, that the relationship between number of physicians\nand infant mortality is constant over time, and that the data from 2010 can be used to make infer-\nence about other years.\nNote that it would be incorrect to make claims of causality from these data, such as stating\nthat an additional 100 physicians (per 100,000 residents) would lead to a decrease of 0.68 in the\ninfant mortality rate.\nGUIDED PRACTICE 6.18\nCalculate a 95% two-sided conﬁdence interval for the slope parameter \f1in the state-level infant\nmortality data.25\n23Other software packages, such as Stata or Minitab, provide similar information but with slightly di ﬀerent labeling.\n24Calculations of the R2value are not shown here.\n25Thet?value for at-distribution with 48 degrees of freedom is 2.01, and the standard error of b1is 0.0028. The 95%\nconﬁdence interval is \u00000:0068\u00062:01(0:0028) = (-0.0124, -0.0012).\n6.4. STATISTICAL INFERENCE WITH REGRESSION 311\nConnection to two-group hypothesis testing\nConducting a regression analysis with a numerical response variable and a categorical predictor\nwith two levels is analogous to conducting a two-group hypothesis test.\nFor example, Section 6.3.3 shows a regression model that compares the average infant mortal-\nity rate in countries with low access to sanitation facilities versus high access.26In other words, the\npurpose of the analysis is to compare mean infant mortality rate between the two groups: coun-\ntries with low access versus countries with high access. Recall that the slope parameter b1is the\ndiﬀerence between the means of log(mortality rate). A test of the null hypothesis H0:\f1= 0 in the\ncontext of a categorical predictor with two levels is a test of whether the two means are di ﬀerent,\njust as for the two-group null hypothesis, H0:\u00161=\u00162.\nWhen the pooled standard deviation assumption (Section 5.3.5) is used, the t-statistic and\np-value from a two-group hypothesis test are equivalent to that returned from a regression model.\nFigure 6.17 shows the Routput from a regression model in the wdi.2011 data, in which\nsanit.access = 1 for countries where at least 50% of the population has access to adequate sanita-\ntion and 0 otherwise. The abbreviated Routput from two-group t-tests are shown in Figure 6.18.\nThe version of the t-test that does not assume equal standard deviations and uses non-integer de-\ngrees of freedom is often referred to as the Welch test.\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) 4.0184 0.1100 36.52 < 0.001\nHigh Access -1.6806 0.1322 -12.72 0.001\nFigure 6.17: Regression of log(infant mortality) versus sanitation access.\nTest df t value Pr(>|t|)\nTwo-groupt-test 161 12.72 <0:001\nWelch two-group t-test 155.82 17.36 <0:001\nFigure 6.18: Results from the independent two-group t-test, under di ﬀering as-\nsumptions about standard deviations between groups, for mean log(infant mor-\ntality) between sanitation access groups.\nThe sign of the t-statistic di ﬀers because for the two-group test, the di ﬀerence in mean log(infant\nmortality) was calculated by subtracting the mean in the high access group from the mean in\nthe low access group; in the regression model, the negative sign reﬂects the reduction in mean\nlog(infant mortality) when changing from low access to high access. Since the t-distribution is\nsymmetric, the two-sided p-value is equal. In this case, pis a small number less than 0.001, as\ncalculated from a t-distribution with 163 \u00002 = 161 degrees of freedom (recall that 163 countries\nare represented in the dataset). The degrees of freedom for the pooled two-group test and linear\nregression are equivalent.\nExample 6.13 showed that the constant variability assumption does not hold for these data.\nAs a result, it might be advisable for a researcher interested in comparing the infant mortality\nrates between these two groups to conduct a two-group hypothesis test without using the pooled\nstandard deviation assumption. Since this test uses a di ﬀerent formula for calculating the standard\nerror of the di ﬀerence in means, the t-statistic is di ﬀerent; additionally, the degrees of freedom are\nnot equivalent. In this particular example, there is not a noticeable e ﬀect on thep-value.\n26Recall that a log transformation was used on the infant mortality rate.\n312 CHAPTER 6. SIMPLE LINEAR REGRESSION\n6.5 Interval estimates with regression\nSection 6.4 introduced interval estimates for regression parameters, such as the population\nslope\f1. An estimated regression line can also be used to construct interval estimates for the\nregression line itself and to calculate prediction intervals for a new observation.\n6.5.1 Conﬁdence intervals\nAs initially discussed in Section 6.2, the estimated regression line for the association between\nRFFT score and age from the 500 individuals in prevend.samp is\nRFFT = 137:55\u00001:26(age):\nFigure 6.19 shows the summary output from Rwhen the regression model is ﬁt. Ralso pro-\nvides the value of R2as 0.285 and the value of s, the estimated standard deviation of the residuals,\nas 23.2.\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) 137.55 5.02 27.42 0.000\nAge -1.26 0.09 -14.09 0.000\ndf= 498\nFigure 6.19: Summary of regression output from Rfor the model predicting RFFT\nscore from age, using the prevend.samp dataset.\nA conﬁdence interval for the slope parameter \f1is centered at the point estimate b1, with a\nwidth based on the standard error for the slope. For this model, the 95% conﬁdence interval for\nage is\u00001:26\u0006(1:96)(0:09) = (\u00001:44;\u00001:09) years.27With 95% conﬁdence, each additional year of\nage is associated with between a 1.1 and 1.4 point lower RFFT score.\nA conﬁdence interval can also be calculated for a speciﬁc point on a least squares line. Con-\nsider a speciﬁc value of the predictor variable, x\u0003, such as 60 years of age. At age 60 years, the\npredicted value of RFFT score is 137 :55\u00001:26(60) = 61 :95 points. The ﬁtted line suggests that\nindividuals from this population who are 60 years of age score, on average, about 62 points on the\nRFFT. Each point on the estimated regression line represents the predicted average RFFT score for\na certain age.\nMore generally, the population model for a regression line is E(Yjx) =\f0+\f1x, and at a value\nx\u0003of the predictor x, the ﬁtted regression line\nE(Yjx\u0003) =b0+b1x\u0003\nestimates the mean of Yfor members of the population with predictor value x\u0003.\nThus, each point on a ﬁtted regression line represents a point estimate for E(Yjx\u0003). The cor-\nresponding interval estimate for E(Yjx\u0003) measures the uncertainty in the estimated mean of Yat\npredictor value x\u0003, just as how an interval estimate for the population slope \f1represents the un-\ncertainty around b1.\n27The critical value 1.96 is used here because at degrees of freedom 498, the t-distribution is very close to a normal\ndistribution. From software, t?\n0:975;df=498= 1:9647.\n6.5. INTERV AL ESTIMATES WITH REGRESSION 313\nThe conﬁdence interval for E(Yjx\u0003) is computed using the standard error of the estimated\nmean of the regression model at a value of the predictor:\ns.e.(E(Yjx\u0003)) =s\ns2 1\nn+(x\u0003\u0000x)2\nP(xi\u0000x)2!\n=ss\n1\nn+(x\u0003\u0000x)2\nP(xi\u0000x)2:\nIn this expression, sis given by Equation 6.16, the usual estimate of \u001b, the standard deviation of\nthe error term \u000fin the population linear model Y=\f0+\f1X+\u000f.\nThe standard error of an estimated mean in regression is rarely calculated by hand; with all\nbut the smallest datasets, the calculations are long and best left to software. When necessary, it can\nbe calculated from basic features of the data and summary statistics.\nConsider computing a 95% conﬁdence interval for E(RFFTjage= 60).\n– The sample size is n= 500.\n–s= 23:2 appears in the regression output.\n– The sample mean xof the predictor is age= 54:8 years.\n– (x\u0003\u0000x)2is the squared distance between the predictor value of interest and the sample mean\nof the predictors: (60 \u000054:8)2= 27:04.\n– The sumP(xi\u0000x)2is the numerator in the calculation of the variance of the predictor, and\nequals (n\u00001)Var(x) = (499)(134 :4445) = 67;088.\nUsing these values, the standard error of the estimated mean RFFT score at age 60 is\ns.e.(E(RFFTjage= 60)) = 23:2r\n1\n500+27:04\n67;088= 1:14:\nThus, a 95% conﬁdence interval for the estimated mean is 61 :95\u0006(1:96)(1:14) = (59:72;64:18)\npoints. With 95% conﬁdence, the interval (59.72, 64.18) points contains the average RFFT score of\na 60-year-old individual.\nIt is also possible to calculate approximate conﬁdence intervals for the estimated mean at a\nspeciﬁc value of a predictor. When x\u0003=x, the second term in the square root will be 0, and the\nstandard error of the estimated mean at the average value xwill have the simple form s=pn. For\nvalues close to x, approximating the standard error as s=pnis often su ﬃcient. In the PREVEND\ndata, 60 years is reasonably close to the average age 54.8 years, and the approximate value of the\nstandard error is 23 :2=p\n500 = 1:03. For values x\u0003that are more distant from the mean, the second\nterm in the square root cannot be reasonably ignored.\nThe approximate form of the standard error for the mean at a predictor value, s=pn, makes\nit easier to see that for large n, the standard error approaches 0; thus, the conﬁdence interval\nnarrows as sample size increases, allowing the estimates to become more precise. This behavior is\nidentical to the conﬁdence interval for a simple mean, as one would expect. It is possible to show\nalgebraically that the conﬁdence intervals at any value of the predictor become increasingly narrow\nas the sample size increases.\n314 CHAPTER 6. SIMPLE LINEAR REGRESSION\n6.5.2 Prediction intervals\nAfter ﬁtting a regression line, a prediction interval is used to estimate a range of values for a\nnew observation of the response variable Ywith predictor value x\u0003; that is, an observation not in\nthe data used to estimate the line. The point estimate dYjx\u0003=b0+b1x\u0003is the same asE(Yjx\u0003), but\nthe corresponding interval estimate is wider than a conﬁdence interval for the mean.\nThe width of the interval reﬂects both the uncertainty in the estimate of the mean,E(Yjx\u0003),\nand the inherent variability of the response variable. The standard error for a predicted value dYjx\u0003\nat predictor x\u0003is\ns.e.(dYjx\u0003) =s\ns2+s2 1\nn+(x\u0003\u0000x)2\nP(xi\u0000x)2!\n=ss\n1 +1\nn+(x\u0003\u0000x)2\nP(xi\u0000x)2:\nThe increased variability when estimating Yjx\u0003versusE(Yjx\u0003) is accounted for by the additional s2\nterm inside the square root.\nThe standard error for a prediction can also be calculated from summary statistics; the cal-\nculation is similar to that for the standard error for a mean. From the values of the summary\nstatistics,\ns.e.( RFFTjage= 60) = 23:2r\n1 +1\n500+27:04\n67;088= 23:23:\nThe 95% prediction interval is 61 :95\u0006(1:96)(23:23) = (16:42;107:68) points. These data and the\nmodel suggest that with 95% conﬁdence, a newly selected 60-year-old will score between 16 and\n108 points on the RFFT. This interval is wider than the conﬁdence interval for the mean RFFT score\n(at age 60 years).\nJust as with conﬁdence intervals, an approximate prediction interval for a predictor near the\naverage of the predictors can be constructed by considering the case when x\u0003=xand the standard\nerror reduces to sp\n1 + 1=n. This approximate standard error shows why prediction intervals are\nwider than conﬁdence intervals and do not become narrower as sample size increases. For large\nsample sizes, the term 1 =nis close to 0, and the standard error is close to s, the standard deviation\nof the residuals about the line. Even when the mean is estimated perfectly, a prediction interval\nwill reﬂect the variability in the data (speciﬁcally, the variability in the response variable).\n40 50 60 70 8020406080100120140RFFT Score\nAge (yrs)\nFigure 6.20: A scatterplot showing RFFT versus age, with the regression line in\nblue. The conﬁdence intervals are marked by solid red lines, while the prediction\nintervals are shown in dashed red lines.\n6.5. INTERV AL ESTIMATES WITH REGRESSION 315\nFigure 6.20 visually demonstrates conﬁdence intervals and prediction intervals in the PRE-\nVEND example; the regression line is shown in blue, while the 95% conﬁdence intervals and pre-\ndiction intervals for each value of age are shown in red. At any value of age, the width of the\ninterval estimate at that value is represented by the vertical distance between the two red lines.\nFor example, the width of the conﬁdence interval at age 60 years is represented by the distance be-\ntween the points (60, 59.72) and (60, 64.18); the solid red lines pass through the upper and lower\nconﬁdence bounds calculated in the earlier example. Similarly, the dashed red lines that represent\nprediction intervals pass through (60, 16.42) and (60, 107.68), the 95% upper and lower bounds\nfor the predicted RFFT score of a 60-year-old.\nThe plot shows how the conﬁdence intervals are most narrow at the mean age, 54.8 years,\nand become wider at values of age further from the mean. The prediction intervals are always\nwider than the conﬁdence intervals. While the mean can be estimated with relative precision along\nthe regression line, prediction intervals reﬂect the scatter of RFFT scores about the line (which is\ndirectly related to the inherent variability of RFFT scores within the study participants). While\nlarger sample sizes can lead to narrower conﬁdence intervals, the width of the prediction intervals\nwill remain essentially unchanged unless the sampling scheme is changed in a way that reduces the\nvariability of the response. A sample of individuals restricted to ages 60 - 70 years, for example,\nwould be expected to have less variable RFFT scores, which would allow for narrower prediction\nintervals.\nThe distinction between conﬁdence and prediction intervals is important and often over-\nlooked. A clinical practitioner interested in the expected outcomes of a test generally should rely\non conﬁdence intervals for the mean along a regression line. The PREVEND data suggest that 60\nyear olds will score on average about 62 points on the test, and the average score is between 59.7\npoints and 62.2 points. When the RFFT is administered to a new 60 year old, however, the likely\nrange of responses will be between 16.4 and 107.7. The prediction interval is wide because the\nscores on the test are quite variable.\n316 CHAPTER 6. SIMPLE LINEAR REGRESSION\n6.6 Notes\nThis chapter provides only an introduction to simple linear regression; the next chapter,\nChapter 7, expands on the principles of simple regression to models with more than one predictor\nvariable.\nWhen ﬁtting a simple regression, be sure to visually assess whether the model is appropriate.\nNonlinear trends or outliers are often obvious in a scatterplot with the least squares line plotted.\nIf outliers are evident, the data source should be consulted when possible, since outliers may be\nindicative of errors in data collection. It is also important to consider whether observed outliers\nbelong to the target population of inference, and assess whether the outliers should be included in\nthe analysis.\nThere are several variants of residual plots used for model diagnostics. The ones shown in\nSection 6.3.1, which plot the predicted values on the horizontal axis, easily generalize to settings\nwith multiple predictors, since there is always a single predicted value even when there is more\nthan one predictor. If the only model used is a simple regression, plotting residuals against predic-\ntor values may make it easier to identify a case with a notable residual. Additionally, data analysts\nwill sometimes plot residuals against case number of the predictor, since runs of large or small\nresiduals may indicate that adjacent cases are correlated.\nTheR2statistic is widely used in the social sciences, where the unexplained variability in the\ndata is typically much larger than the variability captured or explained by a model. It is important\nto be aware of what information R2does and does not provide. Even though a model may have\na low proportion of explained variability, regression coe ﬃcients in the model can still be highly\nstatistically signiﬁcant. The R2should not be interpreted as a measure of the quality of the ﬁt of\nthe model. It is possible for R2to be large even when the data do not show a linear relationship.\nLinear regression models are often estimated after an investigator has noticed a linear rela-\ntionship in data, and experienced investigators can often guess correctly that regression coe ﬃcients\nwill be signiﬁcant before calculating a p-value. Unlike with two-sample hypothesis tests, regres-\nsion models are rarely speciﬁed in advance at the design stage. In practice, it is best to be skeptical\nabout a small p-value in a regression setting, and wait to see whether the observed statistically\nsigniﬁcant relationship can be conﬁrmed in an independent dataset. The issue of model valida-\ntion and assessing whether results of a regression analysis will generalize to other datasets is often\ndiscussed at length in advanced courses.\nIn more advanced texts, substantial attention is devoted to the subtleties of ﬁtting straight\nline models. For instance, there are strategies for adjusting an analysis when one or more of the\nassumptions for regression do not hold. There are also speciﬁc methods to numerically assess the\nleverage or inﬂuence that each observation has on a ﬁtted model.\nLab 1 explores the relationship between cognitive function and age in adults by ﬁtting and\ninterpreting a straight line to these variables in the PREVEND dataset, in addition to discussing\nthe statistical model for least squares regression and residual plots used to assess the assumptions\nfor linear regression. The lab is a useful reminder that least squares regression is much more than\nthe mechanics of ﬁnding a line that best ﬁts a dataset. Lab 2 uses simulated data to explore the\nquantityR2. Lab 3 explores the use of binary categorical predictor variables in regression and\nshows how two-sample t-tests can be calculated using linear regression, in addition to introducing\ninference in a regression context. Categorical predictor variables are common in medicine and the\nlife sciences.\n6.7. EXERCISES 317\n6.7 Exercises\n6.7.1 Examining scatterplots\n6.1 Identify relationships, Part I. For each of the six plots, identify the strength of the relationship (e.g.\nweak, moderate, or strong) in the data and whether ﬁtting a linear model would be reasonable.\n●●\n●●\n●●●●\n●\n●\n●\n●●●\n●●\n●●\n●●\n●●●\n●●●\n●\n●●\n●\n●●\n●●●\n●●\n●\n●\n●●\n●●\n●●\n●●\n●●\n●●\n●●\n●●\n●\n●\n●●\n●\n●\n●●●\n●●\n●●\n●\n●●●\n●\n●●\n●\n●●\n●●●●\n●●●\n●●\n●●●\n●●●\n●●\n●●●●\n●●●\n●●\n●●●\n●●●\n●●●●●\n●●●\n●\n●●\n(a)\n●●●\n●●●\n●●\n●\n●●\n●\n●●\n●●●\n●●\n●●●●\n●●\n●●\n●●●\n●●\n●●●●●\n●●●●●●●\n●\n●●●\n●\n●\n●●●\n●\n●\n●●\n●\n●●\n●●●\n●●\n●●\n●●●\n●●\n●●●\n●●\n●●●●●\n●\n●●●\n●\n●●\n●●\n●●●●\n●●●\n●●\n●●\n●●●●●\n●●●●●●\n●●●\n●●\n●●\n●\n(b)\n●●\n●●\n●\n●●\n●●\n●●●\n●\n●\n●●●\n●●●●\n●●●\n●●●\n●●\n●●\n●●●\n●\n●●●\n●●\n●●\n●●●\n●●●\n●●●\n●●\n●●●●\n●\n●●●●●\n●●\n●\n●●\n●●\n●●\n●●\n●\n●●\n●●\n●●●\n●●\n●●●\n●\n●●\n●●\n●●\n●\n●●\n●●\n●\n●●\n●●●●●\n●\n●●●\n●●●\n●●\n●●●\n●●\n(c)\n●\n●\n●●\n●●●●\n●●●●●\n●●●\n●\n●●\n●●●\n●●\n●\n●●\n●●\n●\n●\n●●\n●●\n●●\n●●\n●●\n●●\n●●●●●\n●\n●●\n●●\n●●●\n●●●\n●●●\n●●\n●\n●●●\n●●●\n●\n●●\n●\n●●\n●\n●●●\n●●\n●●\n●●●\n●●●\n●●\n●●\n●\n●●\n●●\n●●\n●●●\n●●\n●●●\n●●●\n●\n●●\n●\n●\n●●●\n(d)\n●●●\n●●●\n●●\n●●\n●●●\n●●\n●\n●●●\n●●●\n●●\n●●●\n●●●●\n●\n●●\n●●●●\n●●●\n●\n●●●●●\n●●\n●●●\n●●●\n●●\n●●●●\n●●\n●\n●●●●\n●\n●●\n●●\n●\n●\n●●●●\n●●\n●●\n●●\n●●●●\n●●●\n●●\n●\n●\n●●●●\n●●●\n●●\n●●●\n●●\n●\n●●\n●●\n●\n●●\n●\n●●\n(e)\n●●\n●●●\n●●\n●\n●●\n●●●\n●●●\n●●\n●●\n●\n●\n●●\n●●\n●●●●\n●\n●\n●●\n●●\n●●\n●●●\n●●\n●●\n●\n●\n●●●\n●\n●\n●●●●●\n●●\n●●●\n●\n●●●\n●\n●●●●\n●●●●●\n●●\n●\n●\n●●\n●\n●●●●●●\n●\n●●●\n●●\n●●\n●\n●●●\n●●●●\n●●\n●\n●\n●●●●\n●●●\n●●●\n●●\n(f)\n6.2 Identify relationships, Part II. For each of the six plots, identify the strength of the relationship (e.g.\nweak, moderate, or strong) in the data and whether ﬁtting a linear model would be reasonable.\n●●\n●●\n●●●●\n●\n●\n●\n●●●\n●●\n●●\n●●\n●●●\n●●●\n●\n●●\n●\n●●\n●●●\n●●\n●\n●\n●●\n●●●●\n●●\n●●\n●●●●\n●●\n●\n●\n●●\n●\n●\n●●●\n●●\n●●\n●\n●●●\n●\n●●\n●\n●●\n●●●●\n●●●\n●●\n●●●\n●●●\n●●\n●●●●\n●●●\n●●\n●●●\n●●●\n●●●●●\n●●●\n●\n●●●●\n●\n●●●\n●●\n●\n●●\n●\n●●\n●●●\n●●\n●\n(a)\n●●●\n●●\n●●\n●●●\n●●\n●●●●●\n●●●●●●●\n●\n●●●\n●\n●\n●●●\n●\n●\n●●●\n●●\n●●●\n●●\n●●\n●●●\n●●\n●●●\n●●\n●●●●●\n●\n●●●\n●\n●●\n●●\n●●●●\n●●●\n●●\n●●\n●●●●●\n●●●●●●\n●●●\n●●\n●●\n●●●\n●●●●●\n●●●●●●●\n●●●\n●●●●\n●●●●●●\n●●●●\n●●●●●●●\n●●\n(b)\n●●\n●●●\n●●●\n●●●\n●●\n●●●●\n●●●●●●\n●●\n●\n●●\n●●\n●●\n●●\n●\n●●\n●●●●●\n●●●●●\n●\n●●\n●●\n●●\n●\n●●\n●●\n●\n●●\n●●●●●\n●●●●\n●●●\n●●\n●●●\n●●●\n●\n●●\n●●●●\n●●●●●\n●●●\n●\n●●\n●●●\n●●\n●●●\n●●\n●\n●\n●●\n●●\n●●\n●●\n●●\n●●\n●●●●●\n●\n●●\n●●\n●●●\n●●●\n●\n(c)\n●●\n●●\n●\n●●●\n●●●\n●\n●●\n●\n●●\n●\n●●●\n●●\n●●\n●●●\n●●●\n●●\n●●\n●\n●●\n●●\n●●\n●●●\n●●\n●●●\n●●●\n●\n●●\n●\n●\n●●●●●●\n●●●\n●●\n●●\n●●●\n●●\n●\n●●●\n●●●\n●●\n●\n●●\n●●●●\n●\n●●\n●●●●\n●●●\n●\n●●●●●\n●●\n●●●\n●●●\n●●\n●●●\n●\n●●\n●\n●●●●\n●\n●●\n●●\n●\n●\n●●●●\n●\n(d)\n●\n●●\n●●\n●●●●\n●\n●●\n●●\n●\n●\n●●●●\n●●●\n●●\n●●●\n●●\n●\n●●\n●●\n●\n●●\n●\n●●\n●●\n●●●\n●●\n●\n●●\n●●●\n●●●\n●●\n●●\n●\n●\n●●\n●●\n●●●●\n●\n●\n●●\n●●\n●●\n●●●\n●●\n●●\n●\n●\n●●●\n●\n●\n●●●●●\n●●\n●\n●●\n●\n●●●\n●\n●●●●\n●●●●●\n●●\n●\n●\n●●\n●\n●●●●●●\n●\n●●●\n●●\n●●\n●\n●●\n(e)\n●\n●●●●\n●●\n●\n●●●●●\n●●●\n●●●\n●●●\n●\n●●\n●\n●●\n●\n●●●\n●●\n●●●●\n●●\n●●\n●●\n●●\n●●\n●\n●●●\n●●●\n●●●\n●●●\n●●●●\n●\n●●●●\n●●●\n●\n●●●\n●\n●\n●●\n●●\n●●●\n●●\n●\n●●\n●\n●●\n●●●\n●\n●●●\n●●\n●\n●\n●●\n●\n●●●●\n●\n●●\n●●\n●\n●\n●●\n●●\n●●\n●●\n●●●●\n●\n●●●\n●●●\n●●●\n(f)\n318 CHAPTER 6. SIMPLE LINEAR REGRESSION\n6.7.2 Estimating a regression line using least squares\n6.3 Body measurements, Part I. Researchers studying anthropometry collected body girth measurements\nand skeletal diameter measurements, as well as age, weight, height and gender for 507 physically active\nindividuals.28The scatterplot below shows the relationship between height and shoulder girth (over deltoid\nmuscles), both measured in centimeters.\n(a) Describe the relationship between\nshoulder girth and height.\n(b) How would the relationship change if\nshoulder girth was measured in inches\nwhile the units of height remained in\ncentimeters?\n90100 110 120 130150160170180190200\nShoulder girth (cm)Height (cm)\n6.4 Body measurements, Part II. The scatterplot below shows the relationship between weight measured\nin kilograms and hip girth measured in centimeters from the data described in Exercise 6.3.\n(a) Describe the relationship between hip\ngirth and weight.\n(b) How would the relationship change if\nweight was measured in pounds while\nthe units for hip girth remained in\ncentimeters?\n80 90100 110 120 130406080100\nHip girth (cm)Weight (kg)\n28G. Heinz et al. “Exploring relationships in body dimensions”. In: Journal of Statistics Education 11.2 (2003).\n6.7. EXERCISES 319\n6.5 Over-under, Part I. Suppose we ﬁt a regression line to predict the shelf life of an apple based on its\nweight. For a particular apple, we predict the shelf life to be 4.6 days. The apple’s residual is -0.6 days. Did\nwe over or under estimate the shelf-life of the apple? Explain your reasoning.\n6.6 Over-under, Part II. Suppose we ﬁt a regression line to predict the number of incidents of skin cancer\nper 1,000 people from the number of sunny days in a year. For a particular year, we predict the incidence of\nskin cancer to be 1.5 per 1,000 people, and the residual for this year is 0.5. Did we over or under estimate the\nincidence of skin cancer? Explain your reasoning.\n6.7 Murders and poverty, Part I. The following regression output is for predicting annual murders per\nmillion from percentage living in poverty in a random sample of 20 metropolitan areas.\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) -29.901 7.789 -3.839 0.001\npoverty% 2.559 0.390 6.562 0.000\ns= 5:512 R2= 70:52% R2\nadj= 68:89%\n(a) Write out the linear model.\n(b) Interpret the intercept.\n(c) Interpret the slope.\n(d) Calculate the correlation coe ﬃcient.\n●●●\n●●\n●●●\n●●●●\n●●\n●●\n●●\n●●\nPercent in PovertyAnnual Murders per Million\n14% 18% 22% 26%10203040\n6.8 Cats, Part I. The following regression output is for predicting the heart weight (in g) of cats from their\nbody weight (in kg). The coe ﬃcients are estimated using a dataset of 144 domestic cats.\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) -0.357 0.692 -0.515 0.607\nbody wt 4.034 0.250 16.119 0.000\ns= 1:452 R2= 64:66% R2\nadj= 64:41%\n(a) Write out the linear model.\n(b) Interpret the intercept.\n(c) Interpret the slope.\n(d) Calculate the correlation coe ﬃcient.\nBody weight (kg)Heart weight (g)\n2.0 2.5 3.0 3.5 4.05101520\n320 CHAPTER 6. SIMPLE LINEAR REGRESSION\n6.9 Age and RFFT score, Part I. A linear model ﬁt to RFFT scores and age for 500 randomly sampled\nindividuals from the PREVEND data has equation RFFT = 137:55\u00001:26(Age).\n(a) Interpret the slope and intercept values in the context of the data; i.e., explain the linear model in terms\nthat a non-statistician would understand. Comment on whether the intercept value has any interpretive\nmeaning in this setting.\n(b) Based on the linear model, how much does RFFT score di ﬀer, on average, between an individual who is\n60 years old versus an individual who is 50 years old?\n(c) According to the linear model, what is the average RFFT score for an individual who is 70 years old?\n(d) Examine Figure 6.1. Is it valid to use the linear model to estimate RFFT score for an individual who is 20\nyears old? Explain your answer.\n6.10 Guppies, Part I. Guppies are small, brightly colored tropical ﬁsh often seen in freshwater ﬁsh aquar-\niums. A study was conducted in 147 male guppies to examine the relationship between coloration and het-\nerozygosity; heterozygosity refers to the condition of having di ﬀerent alleles at a given genetic locus. The\nguppies were randomly sampled from a river in the wild.\nIn an initial stage of the study, researchers examined whether length and height are linearly associated.\nThe mean length is 1261.21 cm, with standard deviation 95.62 cm. The mean height is 201.75 cm, with stan-\ndard deviation 20.68. The correlation between length and height is 0.85.\n(a) From a visual inspection, does it seem\nlike the line is a reasonable ﬁt for the\ndata?\n(b) Write the equation of the regression\nline for predicting length from height.\n(c) Estimate the predicted mean length of\na guppy with height 180 cm.\n160180200220240260280110012001300140015001600\nHeight (cm)Length (cm)\n6.7. EXERCISES 321\n6.7.3 Interpreting a linear model\n6.11 Visualize the residuals. The scatterplots shown below each have a superimposed regression line. If\nwe were to construct a residual plot (residuals versus x) for each, describe what those plots would look like.\n●\n●●\n●●●●●●●●●\n●●●\n●●●●\n●●\n●\n●●●●\n●\n●●●\n●●\n●●\n●●●●●●\n●●\n●●●●\n●●●●●●●●\n●●●\n●\n●●●●\n●●●●●\n●●\n●●●●●\n●●\n●●\n●●●●●\n●●●\n●●\n●●\n●●●●\n●●\n●●\n●●\n(a)\n●●●\n●●●\n●●\n●●●\n●●\n●●\n●●\n●●\n●●●●●●\n●●\n●●\n●●\n●●\n●\n●●●●\n●\n●●●\n●●●●\n●●\n●●●\n●●●\n●\n●●\n●●\n●●\n●●\n●●\n●●●●\n●●●●●●●\n●●●●\n●●●●\n●●\n●●●●●●●●●●●●●●\n(b)\n6.12 Trends in the residuals. Shown below are two plots of residuals remaining after ﬁtting a linear model\nto two di ﬀerent sets of data. Describe important features and determine if a linear model would be appropriate\nfor these data. Explain your reasoning.\n●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●\n●●●●●\n●●●●●●●\n●●●●●\n●●●●\n●●●●●●●●●\n●●●\n●●●●\n●●●●●●●\n●●\n●●●●\n●●●\n●\n●●●\n●●●\n●●\n●●●\n●●\n●●●●●\n●●●●●\n●\n●\n●●●\n●\n●●●\n●●\n●●●\n●●●●\n●●●\n●●●●●\n●●●●●●●\n●●\n●●●●\n●●●\n●●\n●●●\n●●\n●●\n●●\n●●●\n●\n●●\n●●\n●●\n●●\n●●●\n●●●●\n●●●●\n●●●\n●\n●●●\n●●\n●●\n●\n●●\n●\n●●\n●●\n●●●\n●●\n●●●\n●●●●●\n●●\n●●\n●●\n●\n●●●\n●●\n●●\n●\n●\n●●\n●\n●\n●●●\n●●●\n●\n●●\n●●●●\n●\n●\n●●\n●●●●\n●●\n●●\n●●●\n●●\n●●\n●●●●\n●●\n●\n●●●\n●●\n●\n●●●\n●●\n●\n●\n●●\n●\n●●\n●●\n(a)\n●●●●\n●\n●●●●\n●●●●●\n●●●\n●●\n●\n●●●\n●●●\n●●●●\n●●●\n●●\n●●●●\n●●●●\n●●●\n●●●●●●\n●●●●●●●\n●●\n●●\n●\n●●●●●●●●●●●\n●●●●●\n●●\n●●\n●●●\n●\n●●●●●●●\n●●\n●●\n●●\n●●●●\n●●●\n●\n●●●●\n●●●\n●●●●\n●●●\n●●\n●●●●●\n●\n●●●\n●●●\n●●\n●●●●●●●\n●●●●\n●●●\n●●\n●\n●●●\n●●\n●●●●●\n●●\n●●\n●●●●●\n●●●●●●●\n●\n●●\n●●\n●●●●●\n●●●\n●●\n●●●●\n●●●\n●●●●●●●\n●●●\n●●●●●\n●●\n●\n●●●●\n●●●●\n●●●●●●\n●●●●●\n●●●●●●●●●\n●●\n●●\n●\n●●●\n●\n●●●●\n●●●●\n●●●\n●●●●\n●\n●●\n●●\n●●●●●\n●●\n●●\n●●●\n●●\n●●●●●\n●●\n(b)\n6.13 Guppies, Part II. Exercise 6.10 showed a plot of length versus height for 147 male guppies with a least\nsquares regression line.\n(a) Identify two points that have relatively high leverage and discuss whether these points seem to be partic-\nularly inﬂuential.\n(b) Based on the plot, comment on whether it is appropriate to use R2as a metric for describing the strength\nof the model ﬁt.\n(c) TheR2for this model is 0.718. Interpret this value in the context of the data.\n322 CHAPTER 6. SIMPLE LINEAR REGRESSION\n6.14 Nutrition at Starbucks, Part I. The scatterplot below shows the relationship between the number of\ncalories and amount of carbohydrates (in grams) Starbucks food menu items contain.29Since Starbucks only\nlists the number of calories on the display items, we are interested in predicting the amount of carbs a menu\nitem has based on its calorie content.\nCaloriesCarbs (grams)\n10020030040050020406080\nCaloriesResiduals\n100200300400500−20020\nResiduals−40 −20 0 20 400510152025\n(a) Describe the relationship between number of calories and amount of carbohydrates (in grams) that Star-\nbucks food menu items contain.\n(b) In this scenario, what are the explanatory and response variables?\n(c) Why might we want to ﬁt a regression line to these data?\n(d) Do these data meet the conditions required for ﬁtting a least squares line?\n6.15 Nutrition at Starbucks, Part II. Exercise 6.14 introduced a data set on nutrition information on Star-\nbucks food menu items. Based on the scatterplot and the residual plot provided, describe the relationship\nbetween the protein content and calories of these menu items, and determine if a simple linear model is\nappropriate to predict amount of protein from the number of calories.\nCaloriesProtein (grams)\n100 200 300 400 5000102030\n−20020\n29Source: Starbucks.com, collected on March 10, 2011,\nwww.starbucks.com/menu/nutrition.\n6.7. EXERCISES 323\n6.16 Body measurements, Part III. Exercise 6.3 introduces data on shoulder girth and height of a group of\nindividuals. The mean shoulder girth is 107.20 cm with a standard deviation of 10.37 cm. The mean height is\n171.14 cm with a standard deviation of 9.41 cm. The correlation between height and shoulder girth is 0.67.\n(a) Write the equation of the regression line for predicting height.\n(b) Interpret the slope and the intercept in this context.\n(c) Calculate R2of the regression line for predicting height from shoulder girth, and interpret it in the context\nof the application.\n(d) A randomly selected student from your class has a shoulder girth of 100 cm. Predict the height of this\nstudent using the model.\n(e) The student from part (d) is 160 cm tall. Calculate the residual, and explain what this residual means.\n(f) A one year old has a shoulder girth of 56 cm. Would it be appropriate to use this linear model to predict\nthe height of this child?\n6.17 Outliers, Part I. Identify the outliers in the scatterplots shown below, and determine what type of\noutliers they are. Explain your reasoning.\n(a)\n(b)\n(c)\n6.18 Outliers, Part II. Identify the outliers in the scatterplots shown below and determine what type of\noutliers they are. Explain your reasoning.\n(a)\n(b)\n(c)\n324 CHAPTER 6. SIMPLE LINEAR REGRESSION\n6.19 Guppies, Part III. The residual plots below are for the linear model ﬁt in Exercise 6.10 predicting length\nfrom height for 147 male guppies.\nHeight(cm)Residuals\n160180200220240260280−20020\n−2 −1 0 1 2−100−50050100150Q−Q Plot of Residuals\nTheoretical QuantilesSample Quantiles\n−2 −1 0 1 2−100−50050100150\n(a) From a plot of residual values versus predicted values, are the assumptions of linearity and constant\nvariability satisﬁed? Explain your answer.\n(b) Is it reasonable to assume that the observations were independent, based on the description of the study?\nExplain your answer.\n(c) Are the residuals approximately normally distributed? Explain your answer.\n6.20 Guppies, Part IV. Multilocus heterozygosity (MLH) is reﬂective of genetic quality; according to sexual\nselection research, it is thought that sexual ornamentation functions as a visual indicator of ﬁtness. By select-\ning males with features such as bright coloration, females can improve the chances of reproductive success.\nMale guppies are covered in a mixture of colored spots; orange coloration is consistently preferred by\nfemales. Heterozygosity was assessed by genotyping 9 loci and calculating the proportion of loci that are\nheterozygous. The research question of interest is whether MLH and orange color are linearly associated.\n0.2 0.3 0.4 0.5 0.6 0.7 0.80.050.100.150.20\nMLHOrange area relative to body size\n0.2 0.3 0.4 0.5 0.6 0.7 0.8−0.050.000.050.10\nRelative Orange AreaResiduals\n−2 −1 0 1 2−0.050.000.050.10Q−Q Plot of Residuals\nTheoretical QuantilesSample Quantiles\n−2 −1 0 1 2−0.050.000.050.10\n(a) Based on the plot of MLH versus relative orange area, describe the nature of the association in language\naccessible to a general audience.\n(b) Comment on whether the assumptions of linearity and constant variability are reasonably met.\n(c) Comment on whether the residuals are approximately normally distributed.\n6.7. EXERCISES 325\n6.21 Diamond prices, Part II. Exercise 5.24 introduced data on the price of diamonds based on whether a\ndiamond is 0.99 carats or 1 carat. Based on the summary statistics, write an estimated model equation pre-\ndicting price from a binary indicator of carat weight. Be sure to clearly deﬁne the variables used in the model.\n0.99 carats 1 carat\nMean $ 44.51 $ 56.81\nSD $ 13.32 $ 16.13\nn 23 23\n6.22 Avian inﬂuenza, Part II. Exercise 5.28 introduced data from an analysis investigating whether hatch\nweights between transgenic and non-transgenic chicks di ﬀer.\ntransgenic chicks (g) non-transgenic chicks (g)\n¯x 45.14 44.99\ns 3.32 4.57\nn 54 54\n(a) Write an estimated least squares regression line for a model predicting hatch weight from chick type,\nwhere non-transgenic chicks are the reference group; i.e., the group for which the binary predictor takes\non value 0.\n(b) Write an estimated least squares regression line for a model predicting hatch weight from chick type,\nwhere transgenic chicks are the reference group.\n326 CHAPTER 6. SIMPLE LINEAR REGRESSION\n6.7.4 Statistical inference with regression\n6.23 Body measurements, Part IV. The scatterplot and least squares summary below show the relationship\nbetween weight measured in kilograms and height measured in centimeters of 507 physically active individ-\nuals.\nHeight (cm)Weight (kg)\n150 175 200507090110\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) -105.0113 7.5394 -13.93 0.0000\nheight 1.0176 0.0440 23.13 0.0000\n(a) Describe the relationship between height and weight.\n(b) Write the equation of the regression line. Interpret the slope and intercept in context.\n(c) Do the data provide strong evidence that an increase in height is associated with an increase in weight?\nState the null and alternative hypotheses, report the p-value, and state your conclusion.\n(d) The correlation coe ﬃcient for height and weight is 0.72. Calculate R2and interpret it in context.\n6.24 Beer and blood alcohol content. Many people believe that gender, weight, drinking habits, and many\nother factors are much more important in predicting blood alcohol content (BAC) than simply considering\nthe number of drinks a person consumed. Here we examine data from sixteen student volunteers at Ohio\nState University who each drank a randomly assigned number of cans of beer. These students were evenly\ndivided between men and women, and they di ﬀered in weight and drinking habits. Thirty minutes later, a\npolice o ﬃcer measured their blood alcohol content (BAC) in grams of alcohol per deciliter of blood.30The\nscatterplot and regression table summarize the ﬁndings.\n●\n●●\n●\n●●\n●\n●\n●●●●\n●●\n●●\n2 4 6 80.050.100.15\nCans of beerBAC (grams / deciliter)\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) -0.0127 0.0126 -1.00 0.3320\nbeers 0.0180 0.0024 7.48 0.0000\n(a) Describe the relationship between the number of cans of beer and BAC.\n(b) Write the equation of the regression line. Interpret the slope and intercept in context.\n(c) Do the data provide strong evidence that drinking more cans of beer is associated with an increase in\nblood alcohol? State the null and alternative hypotheses, report the p-value, and state your conclusion.\n(d) The correlation coe ﬃcient for number of cans of beer and BAC is 0.89. Calculate R2and interpret it in\ncontext.\n(e) Suppose we visit a bar, ask people how many drinks they have had, and also take their BAC. Do you think\nthe relationship between number of drinks and BAC would be as strong as the relationship found in the\nOhio State study?\n30J. Malkevitch and L.M. Lesser. For All Practical Purposes: Mathematical Literacy in Today’s World . WH Freeman & Co,\n2008.\n6.7. EXERCISES 327\n6.25 Husbands and wives, Part I. The scatterplot below summarizes husbands’ and wives’ heights in a\nrandom sample of 170 married couples in Britain, where both partners’ ages are below 65 years. Summary\noutput of the least squares ﬁt for predicting wife’s height from husband’s height is also provided in the table.\nHusband's height (in inches)Wife's height (in inches)\n60 65 70 7555606570\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) 43.5755 4.6842 9.30 0.0000\nheight_husband 0.2863 0.0686 4.17 0.0000\n(a) Is there strong evidence that taller men marry taller women? State the hypotheses and include any infor-\nmation used to conduct the test.\n(b) Write the equation of the regression line for predicting wife’s height from husband’s height.\n(c) Interpret the slope and intercept in the context of the application.\n(d) Given that R2= 0:09, what is the correlation of heights in this data set?\n(e) You meet a married man from Britain who is 5’9\" (69 inches). What would you predict his wife’s height to\nbe? How reliable is this prediction?\n(f) You meet another married man from Britain who is 6’7\" (79 inches). Would it be wise to use the same\nlinear model to predict his wife’s height? Why or why not?\n(g) Is there statistically signiﬁcant evidence of an association between husband height and wife height based\non these data? Explain your answer.\n(h) Would you expect a 95% conﬁdence interval for husband height to contain 0? Explain your answer.\n6.26 Helmets and lunches. The scatterplot shows the relationship between socioeconomic status measured\nas the percentage of children in a neighborhood receiving reduced-fee lunches at school ( lunch ) and the per-\ncentage of bike riders in the neighborhood wearing helmets ( helmet ). The average percentage of children\nreceiving reduced-fee lunches is 30.8% with a standard deviation of 26.7% and the average percentage of bike\nriders wearing helmets is 38.8% with a standard deviation of 16.9%.\n(a) If theR2for the least-squares regression line for\nthese data is 72%, what is the correlation between\nlunch and helmet ?\n(b) Calculate the slope and intercept for the\nleast-squares regression line for these data.\n(c) Interpret the intercept of the least-squares\nregression line in the context of the application.\n(d) Interpret the slope of the least-squares regression\nline in the context of the application.\n(e) What would the value of the residual be for a\nneighborhood where 40% of the children receive\nreduced-fee lunches and 40% of the bike riders\nwear helmets? Interpret the meaning of this\nresidual in the context of the application.\n●●●\n●●\n●\n●●●\n●●●\nRate of Receiving a Reduced−Fee Lunch0%20%40%60%80%0%20%40%60%Rate of Wearing a Helmet\n328 CHAPTER 6. SIMPLE LINEAR REGRESSION\n6.27 Husbands and wives, Part II. Exercise 6.25 presents a scatterplot displaying the relationship between\nhusbands’ and wives’ ages in a random sample of 170 married couples in Britain, where both partners’ ages\nare below 65 years. Given below is summary output of the least squares ﬁt for predicting wife’s age from\nhusband’s age.\nHusband's age (in years)Wife's age (in years)\n20 40 60204060\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) 1.5740 1.1501 1.37 0.1730\nage_husband 0.9112 0.0259 35.25 0.0000\ndf= 168\n(a) We might wonder, is the age di ﬀerence between husbands and wives consistent across ages? If this were\nthe case, then the slope parameter would be \f1= 1. Use the information above to evaluate if there is\nstrong evidence that the di ﬀerence in husband and wife ages di ﬀers for di ﬀerent ages.\n(b) Write the equation of the regression line for predicting wife’s age from husband’s age.\n(c) Interpret the slope and intercept in context.\n(d) Given that R2= 0:88, what is the correlation of ages in this data set?\n(e) You meet a married man from Britain who is 55 years old. What would you predict his wife’s age to be?\nHow reliable is this prediction?\n(f) You meet another married man from Britain who is 85 years old. Would it be wise to use the same linear\nmodel to predict his wife’s age? Explain.\n6.28 Guppies, Part V. Exercise 6.20 introduced a linear model for predicting relative orange area from\nproportion of loci that are heterozygous (MLH). Relative orange area refers to the percentage of the body that\nis orange (rather than a di ﬀerent color).\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) 0.037 0.010 3.68 0.0033\nMLH 0.051 0.018 2.77 0.0063\ndf= 145\n(a) Write the estimated model equation.\n(b) What is the predicted mean relative orange area for a guppy that is heterozygous at 8 out of 9 loci?\n(c) Based on the linear model, how much does mean relative orange area di ﬀer between a guppy that is\nheterozygous at 2 loci versus 4 loci (out of 9 total)?\n(d) Conduct a hypothesis test to determine whether relative orange area is signiﬁcantly associated with MLH.\nDo the results suggest that more elaborate sexual ornaments are associated with increased heterozygosity?\nExplain.\n(e) Compute and interpret a 95% conﬁdence interval for the slope parameter \f1.\n6.7. EXERCISES 329\n6.29 Age and RFFT score, Part II. The following regression output is for predicting RFFT score of 500\nrandomly sampled individuals from the PREVEND data based on age (years).\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) 137.55 5.02 27.42 0.000\nAge -1.26 0.09 -14.09 0.000\ndf= 498\n(a) Do these data provide statistically signiﬁcant evidence at the \u000b= 0:01 signiﬁcance level that age is asso-\nciated with RFFT score? State the null and alternative hypotheses, report the relevant p-value, and state\nyour conclusion.\n(b) Compute and interpret a 99% conﬁdence interval for the population slope.\n6.30 Avian inﬂuenza, Part III. Exercise 5.28 introduced data from an analysis investigating whether hatch\nweights between transgenic and non-transgenic chicks di ﬀer. Based on the results from conducting the two-\ngroup test, explain whether the 95% conﬁdence interval for the \f1parameter in a model predicting hatch\nweight from a group indicator would contain 0.\n6.7.5 Interval estimates with regression\n6.31 Husbands and wives, Part III. Exercise 6.27 introduces data from a random sample of 170 married\ncouples in Britain, where both partners’ ages are below 65 years, and ﬁts a model predicting wife’s age from\nhusband’s age. Wife’s age has a mean of 40.68 years, with standard deviation 11.41 years. Husband’s age has\na mean of 42.92 years, with standard deviation 11.76 years. From software, the residual standard error is\ns= 3:95.\n(a) Use the summary statistics to calculate a 95% conﬁdence interval for the average age of wives whose\nhusbands are 55 years old.\n(b) You meet a married man from Britain who is 55 years old. Predict his wife’s age and give a 95% prediction\ninterval for her age.\n(c) Repeat parts (a) and (b) using the approximate formulas for the appropriate standard errors.\n6.32 Guppies, Part VI. The relationship between length and height for 147 male guppies was introduced\nin Exercise 6.10, which used the summary statistics to calculate the equation of the least squares line for\nlength as a function of height and estimate the mean length of an adult male guppy with height 180 cm. The\nestimated residual standard error from this model is s= 50:93.\n(a) Use the summary statistics given in Exercise 6.10 to construct a 95% conﬁdence interval for the estimated\nmean length when height is 180 cm.\n(b) Use a prediction interval based on the summary statistics to estimate the lengths for a new 180 cm guppy\nthat would be more than two standard deviations above and below the estimated mean.\n(c) Use the approximate formulas for the standard error for a mean and for a prediction to recalculate the\nintervals in parts (a) and (b).\n330\nChapter 7\nMultiple linear regression\n7.1 Introduction to multiple linear regression\n7.2 Simple versus multiple regression\n7.3 Evaluating the ﬁt of a multiple regression model\n7.4 The general multiple linear regression model\n7.5 Categorical predictors with several levels\n7.6 Reanalyzing the PREVEND data\n7.7 Interaction in regression\n7.8 Model selection for explanatory models\n7.9 The connection between ANOVA and regression\n7.10 Notes\n7.11 Exercises\n331\nIn most practical settings, more than one explanatory variable is likely to be asso-\nciated with a response. This chapter discusses how the ideas behind simple linear\nregression can be extended to a model with multiple predictor variables.\nThere are several applications of multiple regression. One of the most com-\nmon applications in a clinical setting is estimating an association between a re-\nsponse variable and primary predictor of interest while adjusting for possible con-\nfounding variables. Sections 7.1 and 7.2 introduce the multiple regression model\nby examining the possible association between cognitive function and the use of\nstatins after adjusting for potential confounders. Section 7.8 discusses another\napplication of multiple regression—constructing a model that e ﬀectively explains\nthe observed variation in the response variable.\nThe other sections in the chapter outline general principles of multiple re-\ngression, including the statistical model, methods for assessing quality of model\nﬁt, categorical predictors with more than two levels, interaction, and the connec-\ntion between ANOV A and regression. The methods used to conduct hypothesis\ntests and construct conﬁdence intervals for regression coe ﬃcients extend natu-\nrally from simple to multiple linear regression, so the section on the statistical\nmodel for multiple regression can be treated as optional.\nFor labs, slides, and other resources, please visit\nwww.openintro.org/book/biostat\n332 CHAPTER 7. MULTIPLE LINEAR REGRESSION\n7.1 Introduction to multiple linear regression\nStatins are a class of drugs widely used to lower cholesterol. There are two main types of\ncholesterol: low density lipoprotein (LDL) and high density lipoprotein (HDL).1Research suggests\nthat adults with elevated LDL may be at risk for adverse cardiovascular events such as a heart attack\nor stroke. In 2013, a panel of experts commissioned by the American College of Cardiology and\nthe American Heart Association recommended that statin therapy be considered in individuals\nwho either have any form of atherosclerotic cardiovascular disease2or have LDL cholesterol levels\n\u0015190 mg/dL, individuals with Type II diabetes ages 40 to 75 with LDL between 70 to 189 mg/dL,\nand non-diabetic individuals ages of 40 to 75 with a predicted probability of future clogged arteries\nof at least 0.075.3\nHealth policy analysts have estimated that if the new guidelines were to be followed, almost\nhalf of Americans ages 40 to 75 and nearly all men over 60 would be prescribed a statin. However,\nsome physicians have raised the question of whether treatment with a statin might be associated\nwith an increased risk of cognitive decline.4, 5Older adults are at increased risk for cardiovascular\ndisease, but also for cognitive decline. A study by Joosten, et al. examined the association of statin\nuse and other variables with cognitive ability in an observational cohort of 4,095 participants from\nthe Netherlands who were part of the larger PREVEND study introduced in Section 6.1.6The\nanalyses presented in this chapter are based on a random sample of 500 participants from the\ncohort.7\nThe investigators behind the Joosten study anticipated an issue in the analysis—statins are\nused more often in older adults than younger adults, and older adults su ﬀer a natural cognitive\ndecline. Age is a potential confounder in this setting. If age is not accounted for in the analysis,\nit may seem that cognitive decline is more common among individuals prescribed statins, simply\nbecause those prescribed statins are simply older and more likely to have reduced cognitive ability\nthan those not prescribed statins.\n1Total cholesterol level is the sum of LDL and HDL levels.\n2i.e., arteries thickening and hardening with plaque\n3Stone NJ, et al. 2013 ACC/AHA Guideline on the Treatment of Blood Cholesterol to Reduce Atherosclerotic Cardio-\nvascular Risk in Adults Circulation. 2014;129:S1-S45. DOI: 10.1161/01.cir.0000437738.63853.7a\n4Muldoon, Matthew F., et al. Randomized trial of the e ﬀects of simvastatin on cognitive functioning in hypercholes-\nterolemic adults. The American journal of medicine 117.11 (2004): 823-829.\n5King, Deborah S., et al. Cognitive impairment associated with atorvastatin and simvastatin. Pharmacotherapy: The\nJournal of Human Pharmacology and Drug Therapy 23.12 (2003): 1663-1667.\n6Joosten H, Visser ST, van Eersel ME, Gansevoort RT, Bilo HJG, et al. (2014) Statin Use and Cognitive Function:\nPopulation-Based Observational Study with Long-Term Follow- Up. PLoS ONE 9(12): e115755. doi:10.1371/ jour-\nnal.pone.0115755\n7The random sample is accessible as prevend.samp in the oibiostat Rpackage.\n7.1. INTRODUCTION TO MULTIPLE LINEAR REGRESSION 333\n40 50 60 70 8020406080100120140RFFT Score\nAge (yrs)\nFigure 7.1: A scatterplot showing agevs. RFFT inprevend.samp . Statin users\nare represented with red points; participants not using statins are shown as blue\npoints.\nFigure 7.1 visually demonstrates why age is a potential confounder for the association be-\ntween statin use and cognitive function, where cognitive function is measured via the Ru ﬀFigural\nFluency Test (RFFT). Scores range from 0 (worst) to 175 (best). The blue points indicate individu-\nals not using statins, while red points indicate statin users. First, it is clear that age and statin use\nare associated, with statin use becoming more common as age increases; the red points are more\nprevalent on the right side of the plot. Second, it is also clear that age is associated with lower\nRFFT scores; ignoring the colors, the point cloud drifts down and to the right. However, a close\ninspection of the plot suggests that for ages in relatively small ranges (e.g., ages 50-60), statin use\nmay not be strongly associated with RFFT score—there are approximately as many red dots with\nlow RFFT scores as with high RFFT scores in a given age range. In other words, for subsets of\nparticipants with approximately similar ages, statin use may not be associated with RFFT. Multiple\nregression provides a way to estimate the association of statin use with RFFT while adjusting for\nage; i.e., accounting for the underlying relationship between age and statin use.\n334 CHAPTER 7. MULTIPLE LINEAR REGRESSION\n7.2 Simple versus multiple regression\nA simple linear regression model can be ﬁt for an initial examination of the association be-\ntween statin use and RFFT score,\nE(RFFT) =\f0+\fStatin (Statin):\nRFFT scores in prevend.samp are approximately normally distributed, ranging between ap-\nproximately 10 and 140, with no obvious outliers (Figure 7.2(a)). The least squares regression line\nshown in Figure 7.2(b) has a negative slope, which suggests a possible negative association.\nRFFT ScoresFrequency\n20406080100120140010203040506070\n(a)\nStatin Use0\n(No)1\n(Yes)20406080100120140 RFFT Score (b)\nFigure 7.2: (a) Histogram of RFFT scores. (b) Scatterplot of RFFT score versus\nstatin use in prevend.samp . The variable Statin is coded 1for statin users, and 0\notherwise.\nFigure 7.3 gives the parameter estimates of the least squares line, and indicates that the as-\nsociation between RFFT score and statin use is highly signiﬁcant. On average, statin users score\napproximately 10 points lower on the RFFT. However, even though the association is statistically\nsigniﬁcant, it is potentially misleading since the model does not account for the underlying re-\nlationship between age and statin use. The association between age and statin use visible from\nFigure 7.1 is even more apparent in Figure 7.4, which shows that the median age of statin users is\nabout 10 years higher than the median age of individuals not using statins.\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) 70.7143 1.3808 51.21 0.0000\nStatin -10.0534 2.8792 -3.49 0.0005\nFigure 7.3: Rsummary output for the simple regression model of RFFT versus\nstatin use in prevend.samp .\n7.2. SIMPLE VERSUS MULTIPLE REGRESSION 335\nStatin UseAge (years)\n0 14050607080\nFigure 7.4: Boxplot of age by statin use in prevend.samp . The variable Statin is\ncoded 1for statin users, and 0otherwise.\nMultiple regression allows for a model that incorporates both statin use and age,\nE(RFFT) =\f0+\fStatin (Statin) +\fAge(Age):\nIn statistical terms, the association between RFFT and Statin is being estimated after adjusting for\nAge. This is an example of one of the more important applications of multiple regression: estimat-\ning an association between a response variable and primary predictor of interest while adjusting\nfor possible confounders. In this setting, statin use is the primary predictor of interest.\nThe principles and assumptions behind the multiple regression model are introduced more\nformally in Section 7.4, along with the method used to estimate the coe ﬃcients. Figure 7.5 shows\nthe parameter estimates for the model from R.\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) 137.8822 5.1221 26.92 0.0000\nStatin 0.8509 2.5957 0.33 0.7432\nAge -1.2710 0.0943 -13.48 0.0000\nFigure 7.5: Rsummary output for the multiple regression model of RFFT versus\nstatin use and age in prevend.samp .\n336 CHAPTER 7. MULTIPLE LINEAR REGRESSION\nEXAMPLE 7.1\nUsing the parameter estimates in Figure 7.5, write the prediction equation for the linear model.\nHow does the predicted RFFT score for a 67-year-old not using statins compare to that of an indi-\nvidual of the same age who does use statins?\nThe equation of the linear model is\nRFFT = 137:8822 + 0:8509(Statin)\u00001:2710(Age):\nThe predicted RFFT score for a 67-year-old not using statins ( Statin = 0 ) is\nRFFT = 137:8822 + (0:8509)(0)\u0000(1:2710)(67) = 52 :7252:\nThe predicted RFFT score for a 67-year-old using statins ( Statin = 1 ) is\nRFFT = 137:8822 + (0:8509)(1)\u0000(1:2710)(67) = 53 :5761:\nThe two calculations di ﬀer only by the value of the coe ﬃcient\fStatin , 0.8509.8Thus, for two in-\ndividuals who are the same age, the model predicts that RFFT score will be 0.8509 higher in the\nindividual taking statins; statin use is associated with a small increase in RFFT score.\nEXAMPLE 7.2\nSuppose two individuals are both taking statins; one individual is 50 years of age, while the other\nis 60 years of age. Compare their predicted RFFT scores.\nFrom the model equation, the coe ﬃcient of age \fAgeis -1.2710; an increase in one unit of age (i.e.,\none year) is associated with a decrease in RFFT score of -1.2710, when statin use is the same. Thus,\nthe individual who is 60 years of age is predicted to have an RFFT score that is about 13 points\nlower ((\u00001:2710)(10) =\u000012:710) than the individual who is 50 years of age.\nThis can be conﬁrmed numerically:\nThe predicted RFFT score for a 50-year-old using statins is\nRFFT = 137:8822 + (0:8509)(1)\u0000(1:2710)(50) = 75 :1831:\nThe predicted RFFT score for a 60-year-old using statins is\nRFFT = 137:8822 + (0:8509)(1)\u0000(1:2710)(60) = 62 :4731:\nThe scores di ﬀer by 62:4731\u000075:1831 =\u000012:710:\n8In most cases, predictions do not need to be calculated to so many signiﬁcant digits, since the coe ﬃcients are only\nestimates. This example uses the additional precision to illustrate the role of the coe ﬃcients.\n7.2. SIMPLE VERSUS MULTIPLE REGRESSION 337\nGUIDED PRACTICE 7.3\nWhat does the intercept represent in this model? Does the intercept have interpretive value?9\nAs in simple linear regression, t-statistics can be used to test hypotheses about the slope coe ﬃ-\ncients; for this model, the two null hypotheses are H0:\fStatin = 0 andH0:\fAge= 0. Thep-values for\nthe tests indicate that at signiﬁcance level \u000b= 0:05, the association between RFFT score and statin\nuse is not statistically signiﬁcant, but the association between RFFT score and age is signiﬁcant.\nIn a clinical setting, the interpretive focus lies on reporting the nature of the association be-\ntween the primary predictor and the response and specifying which confounders have been ad-\njusted for. The results of the analysis might be summarized as follows—\nAlthough the use of statins appeared to be associated with lower RFFT scores when\nno adjustment was made for possible confounders, statin use is not signiﬁcantly asso-\nciated with RFFT score in a regression model that adjusts for age.\nThe results shown in Figure 7.5 do not provide information about either the quality of the\nmodel ﬁt or its value as a prediction model. The next section describes the residual plots that can\nbe used to check model assumptions and the use of R2to estimate how much of the variability in\nthe response variable is explained by the model.\nThere is an important aspect of these data that should not be overlooked. The data do not\ncome from a study in which participants were followed as they aged; i.e., a longitudinal study.\nInstead, this study was a cross-sectional study, in which patient age, statin use, and RFFT score\nwere recorded for all participants during a short time interval. While the results of the study\nsupport the conclusion that older patients tend to have lower RFFT scores, they cannot be used\nto conclude that scores decline with age in individuals; there were no repeated measurements of\nRFFT taken as individual participants aged. Older patients come from an earlier birth cohort, and\nit is possible, for instance, that younger participants have more post-secondary school education\nor better health practices generally; such a cohort e ﬀect may have some explanatory e ﬀect on the\nobserved association. The details of how a study is designed and how data are collected should\nalways be taken into account when interpreting study results.\n9The intercept represents an individual with value 0for both Statin and Age; i.e., an individual not using statins with\nage of 0 years. It is not reasonable to predict RFFT score for a newborn, or to assess statin use; the intercept is meaningless\nand has no interpretive value.\n338 CHAPTER 7. MULTIPLE LINEAR REGRESSION\n7.3 Evaluating the ﬁt of a multiple regression model\n7.3.1 Using residuals to check model assumptions\nThe assumptions behind multiple regression are essentially the same as the four assumptions\nlisted in Section 6.1 for simple linear regression. The assumption of linearity is extended to multi-\nple regression by assuming that when only one predictor variable changes, it is linearly related to\nthe change in the response variable. Assumption 2 becomes the slightly more general assumption\nthat the residuals have approximately constant variance. Assumptions 3 and 4 do not change; it is\nassumed that the observations on each case are independent and the residuals are approximately\nnormally distributed.\nSince it is not possible to make a scatterplot of a response variable against several simultane-\nous predictors, residual plots become even more essential as tools for checking modeling assump-\ntions.\nTo assess the linearity assumption, examine plots of residuals against each of the predictors.\nThese plots might show an nonlinear trend that could be corrected with a transformation. The\nscatterplot of residual values versus age in Figure 7.6 shows no apparent nonlinear trends. It is not\nnecessary to assess linearity against a categorical predictor, since a line drawn through two points\n(i.e., the means of the two groups) is necessarily linear.\n40 50 60 70 80−60−40−200204060Residual●●\n●●\n●\n●●●●●\n●●\n●●\n●●\n●●●\n●●●\n●\n●●\n●●●●\n●\n●●●●●\n●\n●●\n●● ●●●\n●●\n●●\n●\n●●●\n●●●\n●●●\n●\n●●\n●●\n●\n●\n●\n●\n● ●● ●\n●●\n●●\n●● ●\n●●●\n● ●●\n●\n●●\n●●●●\n●● ●\n●●\n●●●\n●\n●\n●●●\n●●\n●●●\n●●●●\n●●\n●\n●●\n●●\n●\n●●●●\n●●●●\n●●●\n●●\n●●●\n●●\n●●●●●\n● ●●\n●\n●●●●\n●●●\n●●\n●●\n●●\n●\n●●●\n●\n●●●●\n●●\n●●\n●●\n●●\n●●\n●\n●●\n●\n● ●\n●●\n●\n●●●\n●●\n●\n●●\n●●\n●●\n● ●●●●\n●\n●●●\n●\n●●●●\n●\n●●\n●\n●●\n●●●\n● ●●●\n●●●●\n● ●\n●●\n●●\n●●●●\n●●\n●●\n●\n●●●\n●●\n●●\n●●●\n●●\n●●●●\n●\n●●\n●\n●●\n●●●\n●●\n●●\n●●●\n●●\n●●\n●●\n●●●\n●\n●●●\n●● ●\n●●\n●●●\n●●\n●\n●● ●\n●\n●\n●●●\n●\n●●\n●●\n●\n●●●\n●\n●\n●●\n● ●●●\n●●●\n●●\n●●●\n●●\n●\n●●●\n●● ●\n●\n●● ●●\n●●●\n●\n●●\n●●\n●●\n●●●\n●●●\n●●●\n●●●●\n●\n●\n●●\n●\n●●\n●●\n●●\n●\n●\n●●●●\n●●\n●●\n●\n●●●●\n●●\n● ●●\n●●\n●\n●●\n●●\n●\n●●\n●●●\n●●●●●\n● ●\n●●●\n●\n●●●\n●●●●\n●●●●●●\n●●\n●●\n●●\n●●●\n●●\n●\n●\n●●●\n●●\n●●●\n●●●\n●●●●●\n●●\n●●\n●●●\n●●●\n●\n●●\n●\n●●\n●●●\n●●\n●●●●\n●●●\n●●●\nAge (yrs)\nFigure 7.6: Residuals versus age in the model for RFFT vs statins and age in the\nPREVEND data.\n7.3. EV ALUATING THE FIT OF A MULTIPLE REGRESSION MODEL 339\nSince each case has one predicted value and one residual, regardless of the number of pre-\ndictors, residuals can still be plotted against predicted values to assess the constant variance as-\nsumption. The scatterplot in the left panel of Figure 7.7 shows that the variance of the residuals is\nslightly smaller for lower predicted values of RFFT, but is otherwise approximately constant.\nJust as in simple regression, normal probability plots can be used to check the normality\nassumption of the residuals. The normal probability plot in the right panel of Figure 7.7 shows\nthat the residuals from the model are reasonably normally distributed, with only slight departures\nfrom normality in the tails.\n405060708090−60−40−200204060\nPredicted ValueResidual●●\n●●\n●\n●●●●●\n●●\n●●\n●●\n●●●\n●●●\n●\n●●\n●●●●\n●\n●●●●●\n●\n●●\n●● ●●●\n●●\n●●\n●\n●●●\n●●●\n●●●\n●\n●●\n●●\n●\n●\n●\n●\n● ●●●\n●●\n●●\n●● ●\n●●●\n● ●●\n●\n●●\n●●●●\n●● ●\n●●\n●●●\n●\n●●●●\n●●\n●●●\n●●●●\n●●\n●\n●●\n●●\n●\n●●●●\n●●●●\n●●●\n●●\n●●●\n●●\n●●●●●\n● ●●\n●\n●●●●\n●●●\n●●\n●●\n●●\n●\n●●●\n●\n●●●●\n●●\n●●\n●●\n●●\n●●\n●\n●●\n●\n● ●\n●●\n●\n●●●\n●●\n●\n●●\n●●\n●●\n● ●●●●\n●\n●●●\n●\n●●●●\n●\n●●\n●\n●●\n●●●\n● ●●●\n●●●●\n● ●\n●●\n●●\n●●●●\n●●●●\n●\n●●●\n●●\n●●\n●●●\n●●\n●●●●\n●\n●●\n●\n●●\n●●●\n●●\n●●\n●●●\n●●\n●●\n●●\n●●●\n●\n●●●\n●●●\n●●\n●●●\n●●●\n●● ●\n●\n●\n●●●\n●\n●●\n●●\n●\n●●●\n●\n●\n●●\n● ●●●\n●●●\n●●\n●●●\n●●\n●\n●●●\n●● ●\n●\n●● ●●\n●●●\n●\n●●\n●●\n●●\n●●●\n●●●\n●●●\n●●●●\n●\n●\n●●\n●\n●●\n●●\n●●\n●\n●\n●●●●\n●●\n●●\n●\n●●●●\n●●\n●●●●●\n●\n●●\n●●\n●\n●●\n●●●\n●●●●●\n● ●\n●●●\n●\n●●●\n●●●●\n●●●●●●\n●●\n●●\n●●\n●●●\n●●\n●\n●\n●●●\n●●\n●●●\n●●●\n●●●●●\n●●\n●●\n●●●\n●●●\n●\n●●\n●●●\n●●●\n● ●\n●●●●\n●●●\n●●●\nTheoretical QuantilesSample Quantiles\n−3−2−10123−60−40−200204060\nFigure 7.7: Residual plots from the linear model for RFFT versus statin use and\nage in prevend.samp .\n340 CHAPTER 7. MULTIPLE LINEAR REGRESSION\nEXAMPLE 7.4\nSection 1.7 featured a case study examining the evidence for ethnic discrimination in the amount of\nﬁnancial support o ﬀered by the State of California to individuals with developmental disabilities.\nAlthough an initial look at the data suggested an association between expenditures and ethnicity ,\nfurther analysis suggested that age is a confounding variable for the relationship.\nA multiple regression model can be ﬁt to these data to model the association between expenditures ,\nage, and ethnicity in a subset that only includes data from Hispanics and White non-Hispanics.\nTwo residual plots from the model ﬁt for\nE(expenditures) = \f0+\fethnicity (ethnicity) + \fage(age)\nare shown in Figure 7.8. From these plots, assess whether a linear regression model is appropriate\nfor these data.\nThe model assumptions are clearly violated. The residual versus ﬁtted plot shows obvious pat-\nterns; the residuals do not scatter randomly about the y= 0 line. Additionally, the variance of the\nresiduals is not constant around the y= 0 line. As shown in the normal probability plot, the resid-\nuals show marked departures from normality, particularly in the upper tail; although this skewing\nmay be partially resolved with a log transformation, the patterns in the residual versus ﬁtted plot\nare more problematic.\nRecall that a residual is the di ﬀerence between an observed value and expected value; for an obser-\nvationi, the residual equals yi\u0000ˆyi. Positive residuals occur when a model’s predictions are smaller\nthat the observed values, and vice versa for negative residuals. In the residual versus ﬁtted plot,\nit can be seen that in the middle range of predicted values, the model consistently under-predicts\nexpenditures; on the upper and lower ends, the model over-predicts. This is a particularly serious\nissue with the model ﬁt.\nA single linear regression model is not appropriate for these data. For a more detailed examination\nof the model residuals, refer to Chapter 7, Lab 2. With some subsetting according to age cohort, it\ncan be reasonable to use linear regression for modeling these data.\n020000 60000−20000−100000100002000030000Residual●\n●●●●●●\n●●●●\n●●●\n●●\n●\n●●\n●●●\n●●\n●●●\n●●\n●\n●●●\n●●●\n●\n●●●\n●●\n●●\n●●\n●●\n●●●\n●●\n●●●●●●\n●●\n●●●\n●\n●●●\n●\n●●\n●\n●●\n●●\n●●●●\n●●●●\n●\n●●●●\n●\n●\n●●\n●●\n●●\n●●●●●\n●\n●●● ●\n●●\n●●●●\n●●●\n●\n●●●\n●\n●\n●● ●\n●●●\n●●\n●●\n●●\n●\n●●\n●●\n●●●●●●●\n●●●●\n●●\n●●\n● ●●\n●●\n●●●\n●\n●●●\n●●●\n●●\n●●\n●●\n●●●●●\n●●●●\n●●●●\n●●\n●●●●\n●●●\n●\n●●●\n●\n●\n●\n●●●●●\n●●\n●●\n●●●●\n●\n●●●\n●\n●●●●\n●●●\n●●\n●●\n●●\n●\n●●●●●●\n●●●\n●●●\n●\n●●●●●\n●●●\n●●\n●●●●●\n●●\n●●\n●●\n●●●\n●●●●\n●●●\n●●●\n●●●\n●●●\n●●●\n●●●●\n●\n●●\n●\n●●●\n●\n●\n●\n●●\n●\n●●●\n●\n●●●\n●●●●\n●\n●\n●\n●●\n●●●\n●●●●\n●●●\n●●●\n●●●●●\n●●●\n●●●●\n●\n●●●\n●●●\n●●●●\n●\n●●●●\n●●●\n●\n●●●\n●●●\n●●\n●●●●\n●●\n●\n●●●●\n●●●●\n●●\n●●●\n●●\n●●\n● ●\n●●●\n●●\n●●●\n●●●●\n●●\n●●●●\n●\n●\n●●●\n● ●\n●●\n●●●●\n●●\n●●\n●●\n●●●\n●●\n●●\n●●●\n●●\n●\n●●●\n●●\n●●●●●●\n●●\n●\n●●●●\n●\n●\n●●●●●\n●●●●\n●●●●\n●●●●\n●\n● ●●●\n●\n●●\n●●\n●●●●●\n●●\n●●\n●●\n●●\n●●\n●●\n●\n●●\n●\n●●\n●●\n●\n●●\n●\n●●\n●●●\n● ●\n●●\n●\n●●\n●●●●\n●●\n●●\n●\n●●●●\n●\n●●●\n●\n●●\n●●●\n●\n●\n●\n●\n●●\n●●\n●\n●\n●●●\n●●\n●●●\n●\n●\n● ●●●●\n●\n●\n● ●●●\n●●●\n●●\n●●\n●\n●●●\n●●\n●●●\n●●\n●●\n●●●●\n●\n●●\n●●\n●●\n●●\n●●●\n●●\n●●\n●●●\n●●\n●●●●\n●●●\n●●\n●●\n●●●\n●\n●●\n●\n●●●●\n●●\n●●●●\n●●●●●\n●\n●●\n●●\n●\n●●●●\n●●●●\n●●●\n●●●\n●\n●\n●●●\n●●\n●\n●●●\n●\n●●●\n●●●\n●●●\n●●●\n●●●\n●\n●●\n●\n●●●●●\n●●\n●\n●●\n●●\n●\n●●●\n●●\n●●●\n●\n●●●●●\n●\n●\n●●\n●\n●●\n●\n●\n●●\n●\n●● ●\n●●●●\n●\n●●●\n●●\nSample Quantiles\n−3−10123−20000−100000100002000030000\nTheoretical Quantiles\nFigure 7.8: Residual versus ﬁtted values plot and residual normal probability\nplot from the linear model for expenditures versus ethnicity and age for a subset\nofdds.discr .\n7.3. EV ALUATING THE FIT OF A MULTIPLE REGRESSION MODEL 341\n7.3.2 Using R2R2R2and adjusted R2R2R2with multiple regression\nSection 6.3.2 provided two deﬁnitions of the R2statistic—it is the square of the correlation\ncoeﬃcientrbetween a response and the single predictor in simple linear regression, and equiv-\nalently, it is the proportion of the variation in the response variable explained by the model. In\nstatistical terms, the second deﬁnition can be written as\nR2=Var(yi)\u0000Var(ei)\nVar(yi)= 1\u0000Var(ei)\nVar(yi);\nwhereyiandeidenote the response and residual values for the ithcase.\nThe ﬁrst deﬁnition cannot be used in multiple regression, since there is a correlation coef-\nﬁcient between each predictor and the response variable. However, since there is a single set of\nresiduals, the second deﬁnition remains applicable.\nAlthoughR2can be calculated directly from the equation, it is rarely calculated by hand since\nstatistical software includes R2as a standard part of the summary output for a regression model.10\nIn the model with response RFFT and predictors Statin and Age,R2= 0:2852. The model explains\nalmost 29% of the variability in RFFT scores, a considerable improvement over the model with\nStatin alone (R2= 0:0239).\nAdding a variable to a regression model always increases the value of R2. Sometimes that\nincrease is large and clearly important, such as when age is added to the model for RFFT scores. In\nother cases, the increase is small, and may not be worth the added complexity of including another\nvariable. The adjusted R-squared is often used to balance predictive ability with complexity in a\nmultiple regression model. Like R2, the adjusted R2is routinely provided in software output.\nADJUSTED R2AS A TOOL FOR MODEL ASSESSMENT\nThe adjusted R2is computed as\nR2\nadj= 1\u0000Var(ei)=(n\u0000p\u00001)\nVar(yi)=(n\u00001)= 1\u0000Var(ei)\nVar(yi)\u0002n\u00001\nn\u0000p\u00001;\nwherenis the number of cases used to ﬁt the model and pis the number of predictor variables\nin the model.\nEssentially, the adjusted R2imposes a penalty for including additional predictors that do not\ncontribute much towards explaining the observed variation in the response variable. The value of\nthe adjusted R2in the model with both Statin and Ageis 0.2823, which is essentially the same\nas theR2value of 0.2852. The additional predictor Ageconsiderably increases the strength of the\nmodel, resulting in only a small penalty to the R2value.\nWhile the adjusted R2is useful as a statistic for comparing models, it does not have an inherent\ninterpretation like R2. Students often confuse the interpretation of R2and adjusted R2; while the\ntwo are similar, adjusted R2isnotthe proportion of variation in the response variable explained by\nthe model. The use of adjusted R2for model selection will be discussed in Section 7.8.\n10InRand other software, R2is typically labeled ’multiple R-squared’.\n342 CHAPTER 7. MULTIPLE LINEAR REGRESSION\n7.4 The general multiple linear regression model\nThis section provides a compact summary of the multiple regression model and contains more\nmathematical detail than most other sections; the next section, Section 7.5, discusses categorical\npredictors with more than two levels. The ideas outlined in this section and the next are illustrated\nwith an extended analysis of the PREVEND data in Section 7.6.\n7.4.1 Model parameters and least squares estimation\nFor multiple regression, the data consist of a response variable Yandpexplanatory variables\nX1;X2;:::;Xp. Instead of the simple regression model\nY=\f0+\f1X+\";\nmultiple regression has the form\nY=\f0+\f1X1+\f2X2+\f3X3+\u0001\u0001\u0001+\fpXp+\";\nor equivalently\nE(Y) =\f0+\f1X1+\f2X2+\f3X3+\u0001\u0001\u0001+\fpXp;\nsince the normally distributed error term \"is assumed to have mean 0. Each predictor xihas an\nassociated coe ﬃcient\fi. In simple regression, the slope coe ﬃcient\fcaptures the change in the\nresponse variable Yassociated with a one unit change in the predictor X. In multiple regression,\nthe coe ﬃcient\fjof a predictor Xjdenotes the change in the response variable Yassociated with a\none unit change in Xjwhen none of the other predictors change; i.e., each \fcoeﬃcient in multiple\nregression plays the role of a slope, as long as the other predictors are not changing.\nMultiple regression can be thought of as the model for the mean of the response Yin a pop-\nulation where the mean depends on the values of the predictors, rather than being constant. For\nexample, consider a setting with two binary predictors such as statin use and sex; the predictors\npartition the population into four subgroups, and the four predicted values from the model are\nestimates of the mean in each of the four groups.\n7.4. THE GENERAL MULTIPLE LINEAR REGRESSION MODEL 343\nGUIDED PRACTICE 7.5\nFigure 7.9 shows an estimated regression model for RFFT with predictors Statin and Gender , where\nGender is coded 0 for males and 1 for females.11Based on the model, what are the estimated mean\nRFFT scores for the four groups deﬁned by these two categorical predictors?12\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) 70.4068 1.8477 38.11 0.0000\nStatin -9.9700 2.9011 -3.44 0.0006\nGender 0.6133 2.4461 0.25 0.8021\nFigure 7.9: Rsummary output for the multiple regression model of RFFT versus\nstatin use and sex in prevend.samp .\nDatasets for multiple regression have ncases, usually indexed algebraically by i, whereitakes\non values from 1 to n; 1 denotes the ﬁrst case in the dataset and ndenotes the last case. The dataset\nprevend.samp containsn= 500 observations. Algebraic representations of the data must indicate\nboth the case number and the predictor in the set of ppredictors. For case iin the dataset, the\nvariableXijdenotes predictor Xj; the response for case iis simplyYi, since there can only be\none response variable. The dataset prevend.samp has many possible predictors, some of which are\nexamined later in this chapter. The analysis in Section 7.2 used p= 2 predictors, Statin and Age.\nJust as in Chapter 2, upper case letters are used when thinking of data as a set of random\nobservations subject to sampling from a population, and lower case letters are used for observed\nvalues. In a dataset, it is common for each row to contain the information on a single case; the\nobservations in row iof a dataset with ppredictors can be written as ( yi;xi1;xi2;:::;xip).\nFor any given set of estimates b1;b2;:::;bpand predictors xi1;xi2;:::;xip, predicted values of\nthe response can be calculated using\nˆyi=b0+b1xi1+b2xi2+\u0001\u0001\u0001+bpxip;\nwhereb0;b1;:::;bpare estimates of the coe ﬃcients\f0;\f1;:::;\fpobtained using the principle of least\nsquares estimation.\nAs in simple regression, each prediction has an associated residual, which is the di ﬀerence\nbetween the observed value yiand the predicted value ˆyi, orei=yi\u0000ˆyi. The least squares estimate\nof the model is the set of estimated coe ﬃcientsb0;b1;:::bpthat minimizes e2\n1+e2\n2+\u0001\u0001\u0001e2n. Explicit\nformulas for the estimates involve advanced matrix theory, but are rarely used in practice. Instead,\nestimates are calculated using software such as as R, Stata, or Minitab.\n11Until recently, it was common practice to use gender to denote biological sex. Gender is di ﬀerent than biological sex,\nbut this text uses the original names in published datasets.\n12The prediction equation for the model is RFFT = 70:41\u00009:97(Statin) + 0 :61(Gender). Both Statin and Gender can take\non values of either 0or1; the four possible subgroups are statin non-user / male ( 0,0), statin non-user / female ( 0,1), statin\nuser / male ( 1,0), statin user / female ( 1,1). Predicted RFFT scores for these groups are 70.41, 71.02, 60.44, and 61.05,\nrespectively.\n344 CHAPTER 7. MULTIPLE LINEAR REGRESSION\n7.4.2 Hypothesis tests and conﬁdence intervals\nUsingt-tests for individual coe ﬃcients\nThe test of the null hypothesis H0:\fk= 0 is a test of whether the predictor Xkis associated with the\nresponse variable. When a coe ﬃcient of a predictor equals 0, the predicted value of the response\ndoes not change when the predictor changes; i.e., a value of 0 indicates there is no association\nbetween the predictor and response. Due to the inherent variability in observed data, an estimated\ncoeﬃcientbkwill almost never be 0 even when the model coe ﬃcient\fkis. Hypothesis testing can\nbe used to assess whether the estimated coe ﬃcient is signiﬁcantly di ﬀerent from 0 by examining\nthe ratio of the estimated coe ﬃcient to its standard error.\nWhen the assumptions of multiple regression hold, at least approximately, this ratio has a\nt-distribution with n\u0000(p+ 1) =n\u0000p\u00001 degrees of freedom when the model coe ﬃcient is 0. The\nformula for the degrees of freedom follows a general rule that appears throughout statistics—the\ndegrees of freedom for an estimated model is the number of cases in the dataset minus the number\nof estimated parameters. There are p+1 parameters in the multiple regression model, one for each\nof theppredictors and one for the intercept.\nSAMPLING DISTRIBUTIONS OF ESTIMATED COEFFICIENTS\nSuppose\nˆy=b0+b1xi+b2xi+\u0001\u0001\u0001+bpxi\nis an estimated multiple regression model from a dataset with nobservations on the response\nand predictor variables, and let bkbe one of the estimated coe ﬃcients. Under the hypothesis\nH0:\fk= 0, the standardized statistic\nbk\ns.e.(bk)\nhas at-distribution with n\u0000p\u00001 degrees of freedom.\nThis sampling distribution can be used to conduct hypothesis tests and construct conﬁdence\nintervals.\nTESTING A HYPOTHESIS ABOUT A REGRESSION COEFFICIENT\nA test of the two-sided hypothesis\nH0:\fk= 0 vs.HA:\fk,0\nis rejected with signiﬁcance level \u000bwhen\njbkj\ns.e.(bk)>t?\ndf;\nwheret?\ndfis the point on a t-distribution with n\u0000p\u00001 degrees of freedom and area (1 \u0000\u000b=2) in\nthe left tail.\n7.4. THE GENERAL MULTIPLE LINEAR REGRESSION MODEL 345\nFor one-sided tests, t?\ndfis the point on a t-distribution with n\u0000p\u00001 degrees of freedom and area\n(1\u0000\u000b) in the left tail. A one-sided test of H0againstHA:\fk>0 rejects when the standardized co-\neﬃcient is greater than t?\ndf; a one-sided test of H0againstHA:\fk<0 rejects when the standardized\ncoeﬃcient is less than t?\ndf.\nCONFIDENCE INTERVALS FOR REGRESSION COEFFICIENT\nA two-sided 100(1 \u0000\u000b)% conﬁdence interval for the model coe ﬃcient\fkis\nbk\u0006s.e.(bk)\u0002t?\ndf:\nAll statistical software packages provide an estimate sof the standard deviation of the resid-\nuals\u000f.\nTheF-statistic for an overall test of the model\nWhen all the model coe ﬃcients are 0, the predictors in the model, considered as a group, are not\nassociated with the response; i.e., the response variable is not associated with any linear combina-\ntion of the predictors. The F-statistic is used to test this null hypothesis of no association, using the\nfollowing idea.\nThe variability of the predicted values about the overall mean response can be estimated by\nMSM =P\ni(ˆyi\u0000y)2\np:\nIn this expression, pis the number of predictors and is the degrees of freedom of the numerator sum\nof squares (derivation not given here). The term MSM is called the model sum of squares because\nit reﬂects the variability of the values predicted by the model ( ˆyi) about the mean ( y) response.13\nIn an extreme case, MSM will have value 0 when all the predicted values coincide with the overall\nmean; in this scenario, a model would be unnecessary for making predictions, since the average of\nall observations could be used to make a prediction.\nThe variability in the residuals can be measured by\nMSE =P\ni(yi\u0000ˆyi)2\nn\u0000p\u00001:\nMSE is called the mean square of the errors since residuals are the observed ‘errors’, the di ﬀerences\nbetween predicted and observed values.\nWhen MSM is small compared to MSE, the model has captured little of the variability in the\ndata, and the model is of little or no value. The F-statistic is given by\nF=MSM\nMSE:\nThe formula is not used for calculation, since the numerical value of the F-statistic is a routine\npart of the output of regression software.\n13It turns out that yis also the mean of the predicted values.\n346 CHAPTER 7. MULTIPLE LINEAR REGRESSION\nTHEFFF-STATISTIC IN REGRESSION\nTheF-statistic in regression is used to test the null hypothesis\nH0:\f1=\f2=\u0001\u0001\u0001=\fp= 0\nagainst the alternative that at least one of the coe ﬃcients is not 0.\nUnder the null hypothesis, the sampling distribution of the F-statistic is an F-distribution\nwith parameters ( p;n\u0000p\u00001), and the null hypothesis is rejected if the value of the F-statistic\nis in the right tail of the distribution of the sampling distribution with area \u000b, where\u000bis the\nsigniﬁcance level of the test.\nTheF-test is inherently one-sided—deviations from the null hypothesis of any form will\npush the statistic to the right tail of the F-distribution. The p-value from the right tail of the\nF-distribution should never be doubled. Students also sometimes make the mistake of assuming\nthat if the null hypothesis of the F-test is rejected, all coe ﬃcients must be non-zero, instead of at\nleast one. A signiﬁcant p-value for the F-statistic suggests that the predictor variables in the model,\nwhen considered as a group, are associated with the response variable.\nIn practice, it is rare for the F-test not to reject the null hypothesis, since most regression\nmodels are used in settings where a scientist has prior evidence that at least some of the predictors\nare useful.\nConﬁdence and Prediction Intervals\nThe conﬁdence and prediction intervals discussed in Section 6.5 can be extended to multiple re-\ngression. Predictions based on speciﬁc values of the predictors are made by evaluating the esti-\nmated model at those values, and both conﬁdence intervals for the mean and prediction intervals\nfor a new observation are constructed using the corresponding standard errors. The formulas for\nstandard errors in the multiple predictor setting are beyond the scope of this text, and there are\nno simple approximate formulas that can be calculated by hand. They are always computed in\nsoftware.\nFigure 7.5 shows the estimated regression model used to examine the association of age and\nstatin use with RFFT score in PREVEND. As shown in Example 7.5, the predicted RFFT score for a\n67-year-old statin user is 57.6 points. Software can be used to show that a 95% conﬁdence interval\nfor the mean RFFT score for 67-year-old statin users is (49.2, 58.9) points, while a 95% prediction\ninterval for the RFFT score of a particular 67-year statin user is (7.8, 99.4) points. Just as with\nsimple linear regression, the prediction interval is wider than the conﬁdence interval for the mean\nbecause it accounts for both variability in the estimated mean and variability in a new observation\nof the response, RFFT score.\n7.5. CATEGORICAL PREDICTORS WITH SEVERAL LEVELS 347\n7.5 Categorical predictors with several levels\nIn the initial model ﬁt with the PREVEND data, the variable Statin is coded 0if the partici-\npant was not using statins, and coded 1if the participant was a statin user. The category coded 0\nis referred to as the reference category; in this model, statin non-users ( Statin = 0 ) are the refer-\nence category. The estimated coe ﬃcient\fStatin is the change in the average response between the\nreference category and the category Statin = 1 .\nSince the variable Statin is categorical, the numerical codes 0and 1are simply labels for statin\nnon-users and users. The labels can be speciﬁed more explicitly in software. For example, in R,\ncategorical variables can be coded as factors; the levels of the variable are displayed as text (such\nas \"NonUser\" or \"User\"), while the data remain stored as integers. The Routput with the variable\nStatin.factor is shown in Figure 7.10, where 0corresponds to the label \"NonUser\" and 1corre-\nsponds to \"User\". The predictor variable is now labeled Statin.factorUser ; the estimate -10.05\nis the change in mean RFFT from the \"NonUser\" (reference) category to the \"User\" category. Note\nhow the reference category is not explicitly labeled; instead, it is contained within the intercept.\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) 70.7143 1.3808 51.21 0.0000\nStatin.factorUser -10.0534 2.8792 -3.49 0.0005\nFigure 7.10: Rsummary output for the simple regression model of RFFT versus\nstatin use in prevend.samp , with Statin converted to a factor called Statin.factor\nthat has levels NonUser and User .\nFor a categorical variable with two levels, estimates from the regression model remain the\nsame regardless of whether the categorical predictor is treated as numerical or not. A \"one unit\nchange\" in the numerical sense corresponds exactly to the switch between the two categories. How-\never, this is not true for categorical variables with more than two levels.\nThis idea will be explored with the categorical variable Education , which indicates the highest\nlevel of education that an individual completed in the Dutch educational system: primary school,\nlower secondary school, higher secondary education, or university education. In the PREVEND\ndataset, educational level is coded as either 0,1,2, or3, where 0denotes at most a primary school\neducation, 1a lower secondary school education, 2a higher secondary education, and 3a university\neducation. Figure 7.11 shows the distribution of RFFT by education level; RFFT scores tend to\nincrease as education level increases.\nIn a regression model with a categorical variable with more than two levels, one of the cate-\ngories is set as the reference category, just as in the setting with two levels for a categorical predictor.\nThe remaining categories each have an estimated coe ﬃcient, which corresponds to the estimated\nchange in response relative to the reference category.\n348 CHAPTER 7. MULTIPLE LINEAR REGRESSION\nEducation levelRFFT Score\n0 1 2 3020406080100120140\nFigure 7.11: Box plots for RFFT score by education level in prevend.samp .\nEXAMPLE 7.6\nIs RFFT score associated with educational level? Interpret the coe ﬃcients from the following\nmodel. Figure 7.12 provides the Routput for the regression model of RFFT versus educational\nlevel in prevend.samp . The variable Education has been converted to Education.factor , which has\nlevels Primary ,LowerSecond ,HigherSecond , and Univ .\nIt is clearest to start with writing the model equation:\nRFFT = 40:94 + 14:78(EduLowerSecond) + 32 :13(EduHigherSecond) + 44 :96(EduUniv)\nEach of the predictor levels can be thought of as binary variables that can take on either 0or1,\nwhere only one level at most can be a 1and the rest must be 0, with 1corresponding to the category\nof interest. For example, the predicted mean RFFT score for individuals in the Lower Secondary\ngroup is given by\nRFFT = 40:94 + 14:78(1) + 32:13(0) + 44:96(0) = 55:72:\nThe value of the LowerSecond coeﬃcient, 14.78, is the change in predicted mean RFFT score from\nthe reference category Primary to the LowerSecond category.\nParticipants with a higher secondary education scored approximately 32.1 points higher on the\nRFFT than individuals with only a primary school education, and have estimated mean RFFT score\n40:94 + 32:13 = 73:07:Those with a university education have estimated mean RFFT score 40 :94 +\n44:96 = 85:90.\nThe intercept value, 40.94, corresponds to the estimated mean RFFT score for individuals who at\nmost completed primary school. From the regression equation,\nRFFT = 40:94 + 14:78(0) + 32:13(0) + 44:96(0) = 40:94:\nThep-values indicate that the change in mean score between participants with only a primary\nschool education and any of the other categories is statistically signiﬁcant.\n7.5. CATEGORICAL PREDICTORS WITH SEVERAL LEVELS 349\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) 40.9412 3.2027 12.78 0.0000\nEducation.factorLowerSecond 14.7786 3.6864 4.01 0.0001\nEducation.factorHigherSecond 32.1335 3.7631 8.54 0.0000\nEducation.factorUniv 44.9639 3.6835 12.21 0.0000\nFigure 7.12: Rsummary output for the regression model of RFFT versus ed-\nucational level in prevend.samp , with Education converted to a factor called\nEducation.factor that has levels Primary ,LowerSecond ,HigherSecond , and Univ .\nEXAMPLE 7.7\nSuppose that the model for predicting RFFT score from educational level is ﬁtted with Education ,\nusing the original numerical coding with 0,1,2, and 3; theRoutput is shown in Figure 7.13. What\ndoes this model imply about the change in mean RFFT between groups? Explain why this model is\nﬂawed.\nAccording to this model, the change in mean RFFT between groups increases by 15.158 for any one\nunit change in Education . For example, the change in means between the groups coded 0and 1\nis necessarily equal to the change in means between the groups coded 2and 3, since the predictor\nchanges by 1 in both cases.\nIt is unreasonable to assume that the change in mean RFFT score when comparing the primary\nschool group to the lower secondary group will be equal to the di ﬀerence in means between the\nhigher secondary group and university group. The numerical codes assigned to the groups are\nsimply short-hand labels, and are assigned arbitrarily. As a consequence, this model would not\nprovide consistent results if the numerical codes were altered; for example, if the primary school\ngroup and lower secondary group were relabeled such that the predictor changes by 2, the esti-\nmated di ﬀerence in mean RFFT would change.\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) 41.148 2.104 19.55 0.0000\nEducation 15.158 1.023 14.81 0.0000\nFigure 7.13: Rsummary output for the simple regression model of RFFT ver-\nsus educational level in prevend.samp , where Education is treated as a numerical\nvariable. Note that it would be incorrect to ﬁt this model; Figure 7.12 shows the\nresults from the correct approach.\nCategorical variables can be included in multiple regression models with other predictors, as\nis shown in the next section. Section 7.9 discusses the connection between ANOV A and regression\nmodels with only one categorical predictor.\n350 CHAPTER 7. MULTIPLE LINEAR REGRESSION\n7.6 Reanalyzing the PREVEND data\nThe earlier models ﬁt to examine the association between cognitive ability and statin use\nshowed that considering statin use alone could be misleading. While older participants tended\nto have lower RFFT scores, they were also more likely to be taking statins. Age was found to be a\nconfounder in this setting—is it the only confounder?\nPotential confounders are best identiﬁed by considering the larger scientiﬁc context of the\nanalysis. For the PREVEND data, there are two natural candidates for potential confounders: ed-\nucation level and presence of cardiovascular disease. The use of medication is known to vary by\neducation levels, often because individuals with more education tend to have higher incomes and\nconsequently, better access to health care; higher educational levels are associated with higher\nRFFT scores, as shown by model 7.12. Individuals with cardiovascular disease are often prescribed\nstatins to lower cholesterol; cardiovascular disease can lead to vascular dementia and cognitive\ndecline.\nFigure 7.14 contains the result of a regression of RFFT with statin use, adding the possible\nconfounders age, educational level, and presence of cardiovascular disease. The variables Statin ,\nEducation and CVDhave been converted to factors, and Ageis a continuous predictor.\nThe coe ﬃcient for statin use shows the importance of adjusting for confounders. In the initial\nmodel for RFFT that only included statin use as a predictor, statin use was signiﬁcantly associated\nwith decreased RFFT scores. After adjusting for age, statins were no longer signiﬁcantly associ-\nated with RFFT scores, but the model suggested that statin use could be associated with increased\nRFFT scores. This ﬁnal model suggests that, after adjusting for age, education, and the presence of\ncardiovascular disease, statin use is associated with an increase in RFFT scores of approximately\n4.7 points. The p-value for the slope coe ﬃcient for statin use is 0.056, which suggests moderately\nstrong evidence of an association (signiﬁcant at \u000b= 0:10, but not\u000b= 0:05).\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) 99.0351 6.3301 15.65 0.0000\nStatin.factorUser 4.6905 2.4480 1.92 0.0559\nAge -0.9203 0.0904 -10.18 0.0000\nEducation.factorLowerSecond 10.0883 3.3756 2.99 0.0029\nEducation.factorHigherSecond 21.3015 3.5777 5.95 0.0000\nEducation.factorUniv 33.1246 3.5471 9.34 0.0000\nCVD.factorPresent -7.5665 3.6516 -2.07 0.0388\nFigure 7.14: Rsummary output for the multiple regression model of RFFT\nversus statin use, age, education, and presence of cardiovascular disease in\nprevend.samp .\nTheR2for the model is 0.4355; a substantial increase from the model with only statin use and\nage as predictors, which had an R2of 0.2852. The adjusted R2for the model is 0.4286, close to the\nR2value, which suggests that the additional predictors increase the strength of the model enough\nto justify the additional complexity.\n7.6. REANALYZING THE PREVEND DATA 351\nFigure 7.15 shows a plot of residuals vs predicted RFFT scores from the model in Figure 7.14\nand a normal probability plot of the residuals. These plots show that the model ﬁts the data rea-\nsonably well. The residuals show a slight increase in variability for larger predicted values, and the\nnormal probability plot shows the residuals depart slightly from normality in the extreme tails.\nModel assumptions never hold exactly, and the possible violations shown in this ﬁgure are not\nsuﬃcient reasons to discard the model.\n20406080100−60−40−200204060\nPredicted ValueResidual●●\n●●\n●\n●●●●\n●\n●●\n●●\n●●\n●\n●●\n●●●\n●\n●●\n●●●\n●\n●●●●●●\n●●● ●\n●●●●\n●●\n●●\n●\n●●\n●\n●●●\n●●●\n●\n●●\n●●\n●●\n●\n●●●●●\n●●\n●●\n●●●\n●●●\n●●●●\n●●\n●●●●\n●●●\n●\n●●●●\n●\n●\n●●●\n●●\n●●●\n●●\n●●\n●●●\n●●●\n●\n●\n●\n●●●\n●●●●\n●●●\n●●\n●●●\n●●\n●●●●\n●\n●●●\n●\n●●●●●●\n●\n●●●●\n●●\n●\n●●●\n●\n● ●●●\n●\n●\n●●\n●●\n●●\n●●\n●\n●●\n●●●\n●●\n●\n●●●\n●●\n●●●\n●●\n●●\n●\n●●\n●●\n●\n●●●\n●\n●●●●\n●\n●●\n●●●●●●\n●●●●\n●\n●●●\n●●\n●●\n●●\n●\n●●●●\n●●●●\n●●\n●\n●●\n●●\n●●●\n●●\n●●●●\n●\n●● ●\n●●\n●●●\n●●\n●●\n● ●●\n●●\n●●\n●●\n●●●\n●\n●●●\n●●●\n●●\n●\n●●\n●\n●●\n●●●\n●\n●\n●●●\n●\n●●●●\n●\n●●\n●\n●\n●\n●●\n●●\n●●\n●●● ●●\n●●●\n●●\n●\n●●\n●●\n●●\n●\n●●●\n●●●●\n●\n●●\n●●\n●●\n●●●\n●●●\n●●●\n●●●●\n●●\n●●●\n●●●\n●●\n●●\n●\n●●●●\n●●\n●●\n●\n●●\n●●\n●●\n●●●●●\n●\n●●●●\n●\n●●\n●●●\n●●\n●\n●●\n●●\n●●●\n●\n●●●\n●●●●\n●●\n●●●●\n●●\n●●●●\n●●\n●\n●●\n●● ●●\n●\n●●\n●●●\n●●●\n●●●●\n●\n●●●●\n●●●\n●\n●●\n●\n●●\n●●●\n●●●\n●●\n●\n●●●\n●●\n●●\n●●\nTheoretical QuantilesSample Quantiles\n−3−2−10123−60−40−200204060\nFigure 7.15: A histogram and normal probability plot of the residuals from the\nlinear model for RFFT vs. statin use, age, educational level and presence of car-\ndiovascular disease in the PREVEND data.\nIt is quite possible that even the model summarized in Figure 7.14 is not the best one to un-\nderstand the association of cognitive ability with statin use. There be other confounders that are\nnot accounted for. Possible predictors that may be confounders but have not been examined are\ncalled residual confounders . Residual confounders can be other variables in a dataset that have\nnot been examined, or variables that were not measured in the study. Residual confounders exist in\nalmost all observational studies, and represent one of the main reasons that observational studies\nshould be interpreted with caution. A randomized experiment is the best way to eliminate resid-\nual confounders. Randomization ensures that, at least on average, all predictors are not associated\nwith the randomized intervention, which eliminates one of the conditions for confounding. A ran-\ndomized trial may be possible in some settings; there have been many randomized trials examining\nthe eﬀect of using statins. However, in many other settings, such as a study of the association of\nmarijuana use and later addiction to controlled substances, randomization may not be possible or\nethical. In those instances, observational studies may be the best available approach.\n352 CHAPTER 7. MULTIPLE LINEAR REGRESSION\n7.7 Interaction in regression\nAn important assumption in the multiple regression model\ny=\f0+\f1x1+\f2x2+:::+\fpxp+\"\nis that when one of the predictor variables xjchanges by 1 unit and none of the other variables\nchange, the predicted response changes by \fj, regardless of the values of the other variables. A\nstatistical interaction occurs when this assumption is not true, such that the relationship of one\nexplanatory variable xjwith the response depends on the particular value(s) of one or more other\nexplanatory variables.\nInteraction is most easily demonstrated in a model with two predictors, where one of the\npredictors is categorical and the other is numerical.14Consider a model that might be used to\npredict total cholesterol level from age and diabetes status (either diabetic or non-diabetic):\nE(TotChol) =\f0+\f1(Age) +\f2(Diabetes): (7.8)\nFigure 7.16 shows the Routput for a regression estimating model 7.8, using data from a sample\nof 500 adults from the NHANES dataset ( nhanes.samp.adult.500 ). Total cholesterol ( TotChol ) is\nmeasured in mmol/L, Ageis recorded in years, and Diabetes is a factor level with the levels No\n(non-diabetic) and Yes(diabetic) where 0corresponds to Noand 1corresponds to Yes.\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) 4.8000 0.1561 30.75 0.0000\nAge 0.0075 0.0030 2.47 0.0137\nDiabetesYes -0.3177 0.1607 -1.98 0.0487\nFigure 7.16: Regression of total cholesterol on age and diabetes, using\nnhanes.samp.adult.500 .\n14Interaction e ﬀects between numerical variables and between more than two variables can be complicated to interpret.\nA more complete treatment of interaction is best left to a more advanced course; this text will only examine interaction in\nthe setting of models with one categorical variable and one numerical variable.\n7.7. INTERACTION IN REGRESSION 353\nEXAMPLE 7.9\nUsing the output in Figure 7.16, write the model equation and interpret the coe ﬃcients for age and\ndiabetes. How does the predicted total cholesterol for a 60-year-old individual compare to that of\na 50-year-old individual, if both have diabetes? What if both individuals do not have diabetes?\nTotChol = 4:80 + 0:0075(Age)\u00000:32(DiabetesYes )\nThe coe ﬃcient for age indicates that with each increasing year of age, predicted total cholesterol\nincreases by 0.0075 mmol/L. The coe ﬃcient for diabetes indicates that diabetics have an average\ntotal cholesterol that is 0.32 mmol/L lower than non-diabetic individuals.\nIf both individuals have diabetes, then the change in predicted total cholesterol level can be de-\ntermined directly from the coe ﬃcient for Age. An increase in one year of age is associated with\na 0.0075 increase in total cholesterol; thus, an increase in ten years of age is associated with\n10(0:0075) = 0:075 mmol/L increase in predicted total cholesterol.\nThe calculation does not di ﬀer if both individuals are non-diabetic. According to the model, the\nrelationship between age and total cholesterol remains the same regardless of the values of the\nother variable in the model.\nEXAMPLE 7.10\nUsing the output in Figure 7.16, write two separate model equations: one for diabetic individuals\nand one for non-diabetic individuals. Compare the two models.\nFor non-diabetics ( Diabetes = 0 ), the linear relationship between average cholesterol and age is\nTotChol = 4 :80 + 0:0075(Age)\u00000:32(0) = 4:80 + 0:0075(Age):\nFor diabetics ( Diabetes = 1 ), the linear relationship between average cholesterol and age is\nTotChol = 4 :80 + 0:0075(Age)\u00000:32(1) = 4:48 + 0:0075(Age):\nThe lines predicting average cholesterol as a function of age in diabetics and non-diabetics are\nparallel, with the same slope and di ﬀerent intercepts. While predicted total cholesterol is higher\noverall in non-diabetics (as indicated by the higher intercept), the rate of change in predicted aver-\nage total cholesterol by age is the same for both diabetics and non-diabetics.\nThis relationship can be expressed directly from the model equation 7.8. For non-diabetics, the\npopulation regression line is E(TotChol) = \f0+\f1(Age). For diabetics, the line is E(TotChol) =\n\f0+\f1(Age)+\f2=\f0+\f2+\f1(Age). The lines have the same slope \f1but intercepts \f0and\f0+\f2.\n354 CHAPTER 7. MULTIPLE LINEAR REGRESSION\nHowever, a model that assumes the relationship between cholesterol and age does not depend\non diabetes status might be overly simple and potentially misleading. Figure 7.17(b) shows a scat-\nterplot of total cholesterol versus age where the least squares models have been ﬁt separately for\nnon-diabetic and diabetic individuals. The blue line in the plot is estimated using only non-diabetic\nindividuals, while the red line was ﬁt using data from diabetic individuals. The lines are not paral-\nlel, and in fact, have slopes with di ﬀerent signs. The plot suggests that among non-diabetics, age is\npositively associated with total cholesterol. Among diabetics, however, age is negatively associated\nwith total cholesterol.\n20 30 40 50 60 70 803456789Total Choelsterol (mmol/L)\nAge (yrs)\n(a)\n20 30 40 50 60 70 803456789Total Cholesterol (mmol/L)\nAge (yrs)\n(b)\nFigure 7.17: Scatterplots of total cholesterol versus age in\nnhanes.samp.adult.500 , where blue represents non-diabetics and red repre-\nsents diabetics. Plot (a) shows the model equations written out in Example 7.10,\nestimated from the entire sample of 500 individuals. Plot (b) shows least squares\nmodels that are ﬁt separately; coe ﬃcients of the blue line are estimated using\nonly data from non-diabetics, while those of the red line are estimated using only\ndata from diabetics.\n7.7. INTERACTION IN REGRESSION 355\nWith the addition of another parameter (commonly referred to as an interaction term), a linear\nregression model can be extended to allow the relationship of one explanatory variable with the\nresponse to vary based on the values of other variables in the model. Consider the model\nE(TotChol) = \f0+\f1(Age) +\f2(Diabetes) + \f3(Diabetes\u0002Age): (7.11)\nThe interaction term allows the slope of the association with age to di ﬀer by diabetes status.\nAmong non-diabetics ( Diabetes = 0), the model reduces to the earlier one,\nE(TotChol) = \f0+\f1(Age):\nAmong the diabetic participants, the model becomes\nE(TotChol) = \f0+\f1(Age) +\f2+\f3(Age)\n=\f0+\f2+ (\f1+\f3)(Age):\nUnlike in the original model, the slopes of the population regression lines for non-diabetics\nand diabetics are now di ﬀerent:\f1versus\f1+\f3.\nFigure 7.18 shows the Routput for a regression estimating model 7.11. In R, the syntax\nAge:DiabetesYes represents the (Age \u0002Diabetes) interaction term.\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) 4.6957 0.1597 29.40 0.0000\nAge 0.0096 0.0031 3.10 0.0020\nDiabetesYes 1.7187 0.7639 2.25 0.0249\nAge:DiabetesYes -0.0335 0.0123 -2.73 0.0067\nFigure 7.18: Regression of total cholesterol on age and diabetes with an interac-\ntion term, using nhanes.samp.adult.500\nEXAMPLE 7.12\nUsing the output in Figure 7.18, write the overall model equation, the model equation for non-\ndiabetics, and the model equation for diabetics.\nThe overall model equation is\nTotChol = 4 :70 + 0:0096(Age) + 1 :72(DiabetesYes)\u00000:034(Age\u0002DiabetesYes) :\nFor non-diabetics ( Diabetes = 0 ), the linear relationship between average cholesterol and age is\nTotChol = 4 :70 + 0:0096(Age) + 1 :72(0)\u00000:034(Age\u00020) = 4:70 + 0:0096(Age):\nFor diabetics ( Diabetes = 1 ), the linear relationship between average cholesterol and age is\nTotChol = 4 :70 + 0:0096(Age) + 1 :72(1)\u00000:034(Age\u00021) = 6:42\u00000:024(Age):\n356 CHAPTER 7. MULTIPLE LINEAR REGRESSION\nThe estimated equations for non-diabetic and diabetic individuals show the same qualitative\nbehavior seen in Figure 7.17(b), where the slope is positive in non-diabetics and negative in diabet-\nics. However, note that the lines plotted in the ﬁgure were estimated from two separate model ﬁts\non non-diabetics and diabetics; in contrast, the equations from the interaction model are ﬁt using\ndata from all individuals.\nIt is more e ﬃcient to model the data using a single model with an interaction term than\nworking with subsets of the data.15Additionally, using a single model allows for the calculation of\nat-statistic and p-value that indicates whether there is statistical evidence of an interaction. The p-\nvalue for the Age:Diabetes interaction term is signiﬁcant at the \u000b= 0:05 level. Thus, the estimated\nmodel suggests there is strong evidence for an interaction between age and diabetes status when\npredicting total cholesterol.\nResidual plots can be used to assess the quality of the model ﬁt. Figure 7.19 shows that the\nresiduals have roughly constant variance in the region with the majority of the data (predicted\nvalues between 4.9 and 5.4 mmol/L). However, there are more large positive residuals than large\nnegative residuals, which suggests that the model tends to underpredict; i.e., predict values of\nTotChol that are smaller than the observed values.16Figure 7.20 shows that the residuals do not\nﬁt a normal distribution in the tails. In the right tails, the sample quantiles are larger than the\ntheoretical quantiles, implying that there are too many large residuals. The left tail is a better ﬁt;\nhowever, there are too few large negative residuals since the sample quantiles in the left tail are\ncloser to 0 than the theoretical quantiles.\n4.6 4.8 5.0 5.2 5.4 5.6−2−101234\nPredicted ValueResidual\nFigure 7.19: A scatterplot of residuals versus predicted values in the model for\ntotal cholesterol that includes age, diabetes status, and the interaction of age and\ndiabetes status.\n15In more complex settings, such as those with potential interaction between several variables or between two numerical\nvariables, it may not be clear how to subset the data in a way that reveals interactions. This is another advantage to using\nan interaction term and single model ﬁt to the entire dataset.\n16Recall that model residuals are calculated as yi\u0000ˆyi; i.e., TotChol i\u0000TotCholi.\n7.7. INTERACTION IN REGRESSION 357\nResiduals−3−2−10123\nTheoretical QuantilesSample Quantiles\n−3−2−10123−2−101234\nFigure 7.20: A histogram of the residuals and a normal probability plot of the\nresiduals from the linear model for total cholesterol versus age, diabetes status,\nand the interaction of age and diabetes status.\nIt is also important to note that the model explains very little of the observed variability in\ntotal cholesterol—the multiple R2of the model is 0.032. While the model falls well short of per-\nfection, it may be reasonably adequate in applied settings. In the setting of a large study, such as\none to examine factors a ﬀecting cholesterol levels in adults, a model like the one discussed here is\ntypically a starting point for building a more reﬁned model. Given these results, a research team\nmight proceed by collecting more data. Regression models are commonly used as tools to work\ntowards understanding a phenomenon, and rarely represent a ’ﬁnal answer’.\nThere are some important general points that should not be overlooked when interpreting\nthis model. The data cannot be used to infer causality; the data simply show associations between\ntotal cholesterol, age, and diabetes status. Each of the NHANES surveys are cross-sectional; they\nare administered to a sample of US residents with various ages and other demographic features\nduring a relatively short period of time. No single individual has had his or her cholesterol levels\nmeasured over a period of many years, so the model slope for diabetes is not indicative of an\nindividual’s cholesterol level declining (or increasing) with age.\nFinally, the interpretation of a model often requires additional contextual information that is\nrelevant to the study population but not captured in the dataset. What might explain increased age\nbeing associated with lower cholesterol for diabetics, but higher cholesterol for non-diabetics? The\nguidelines for the use of cholesterol-lowering statins suggest that these drugs should be prescribed\nmore often in older individuals, and even more so in diabetic individuals. It is a reasonable specu-\nlation that the interaction between age and diabetes status seen in the NHANES data is a result of\nmore frequent statin use in diabetic individuals.\n358 CHAPTER 7. MULTIPLE LINEAR REGRESSION\n7.8 Model selection for explanatory models\nPreviously, multiple regression modeling was shown in the context of estimating an associ-\nation while adjusting for possible confounders. Another application of multiple regression is ex-\nplanatory modeling, in which the goal is to construct a model that explains the observed variation\nin the response variable. In this context, there is no pre-speciﬁed primary predictor of interest;\nexplanatory modeling is concerned with identifying predictors associated with the response. It is\ntypically desirable to have a small model that avoids including variables which do not contribute\nmuch towards the R2.\nThe intended use of a regression model inﬂuences the way in which a model is selected. Ap-\nproaches to model selection vary from those based on careful study of a relatively small set of\npredictors to purely algorithmic methods that screen a large set of predictors and choose a ﬁnal\nmodel by optimizing a numerical criterion. Algorithmic selection methods have gained popular-\nity as researchers have been able to collect larger datasets, but the choice of an algorithm and the\noptimization criterion require more advanced material and are not covered here. This section il-\nlustrates model selection in the context of a small set of potential predictors using only the tools\nand ideas that have been discussed earlier in this chapter and in Chapter 6.\nGenerally, model selection for explanatory modeling follows these steps:\n1.Data exploration. Using numerical and graphical approaches, examine both the distributions\nof individual variables and the relationships between variables.\n2.Initial model ﬁtting. Fit an initial model with the predictors that seem most highly associated\nwith the response variable, based on the data exploration.\n3.Model comparison. Work towards a model that has the highest adjusted R2.\n– Fit new models without predictors that were either not statistically signiﬁcant or only\nmarginally so and compare the adjusted R2between models; drop variables that de-\ncrease the adjusted R2.\n– If the initial set of variables is relatively small, it is prudent to add variables not in the\ninitial model and check the adjusted R2; add variables that increase the adjusted R2.\n– Examine whether interaction terms may improve the adjusted R2.\n4.Model assessment. Use residual plots to assess the ﬁt of the ﬁnal model.\nThe process behind model selection will be illustrated with a case study in which a regression\nmodel is built to examine the association between the abundance of forest birds in a habitat patch\nand features of a patch.\n7.8. MODEL SELECTION FOR EXPLANATORY MODELS 359\nAbundance of forest birds: introduction\nHabitat fragmentation is the process by which a habitat in a large contiguous space is divided into\nsmaller, isolated pieces; human activities such as agricultural development can result in habitat\nfragmentation. Smaller patches of habitat are only able to support limited populations of or-\nganisms, which reduces genetic diversity and overall population ﬁtness. Ecologists study habi-\ntat fragmentation to understand its e ﬀect on species abundance. The forest.birds dataset in the\noibiostat package contains a subset of the variables from a 1987 study analyzing the e ﬀect of habi-\ntat fragmentation on bird abundance in the Latrobe Valley of southeastern Victoria, Australia.17\nThe dataset consists of the following variables, measured for each of the 57 patches.\n–abundance : average number of forest birds observed in the patch, as calculated from several\nindependent 20-minute counting sessions.\n–patch.area : patch area, measured in hectares. 1 hectare is 10,000 square meters and approx-\nimately 2.47 acres.\n–dist.nearest : distance to the nearest patch, measured in kilometers.\n–dist.larger : distance to the nearest patch larger than the current patch, measured in kilo-\nmeters.\n–altitude : patch altitude, measured in meters above sea level.\n–grazing.intensity : extent of livestock grazing, recorded as either \"light\", \"less than average\",\n\"average\", \"moderately heavy\", or \"heavy\".\n–year.of.isolation : year in which the patch became isolated due to habitat fragmentation.\n–yrs.isolation : number of years since patch became isolated due to habitat fragmentation.18\nThe following analysis is similar to analyses that appear in Logan (2011)19and Quinn &\nKeough (2002).20In the approach here, the grazing intensity variable is treated as a categorical\nvariable; Logan and Quinn & Keough treat grazing intensity as a numerical variable, with values\n1-5 corresponding to the categories. The implications of these approaches are discussed at the end\nof the section.\n17Loyn, R.H. 1987. \"E ﬀects of patch area and habitat on bird abundances, species numbers and tree health in fragmented\nVictorian forests.\" Printed in Nature Conservation: The Role of Remnants of Native Vegetation. Saunders DA, Arnold GW,\nBurbridge AA, and Hopkins AJM eds. Surrey Beatty and Sons, Chipping Norton, NSW, 65-77, 1987.\n18The Loyn study completed data collection in 1983; yrs.isolation = 1983\u0000year.of.isolation .\n19Logan, M., 2011. Biostatistical design and analysis using R: a practical guide. John Wiley & Sons, Ch. 9.\n20Quinn, G.P . and Keough, M.J., 2002. Experimental design and data analysis for biologists. Cambridge University\nPress, Ch. 6.\n360 CHAPTER 7. MULTIPLE LINEAR REGRESSION\nData exploration\nThe response variable for the model is abundance . Numerical summaries calculated from software\nshow that abundance ranges from 1.5 to 39.6. Figure 7.21 shows that the distribution of abundance\nis bimodal, with modes at small values of abundance and at between 25 and 30 birds. The me-\ndian (21.0) and mean (19.5) are reasonably close, which conﬁrms the distribution is near enough\nto symmetric to be used in the model without a transformation. The boxplot conﬁrms that the\ndistribution has no outliers.\nAbundanceFrequency\n0 10 20 30 40051015\n(a)\nAbundance\n01020304050 (b)\nFigure 7.21: A histogram (a) and boxplot (b) of abundance in the forest.birds\ndata.\nThere are six potential predictors in the model; the variable year.of.isolation is only used\nto calculate the more informative variable yrs.isolation . The plots in Figure 7.22 reveal right-\nskewing in patch.area ,dist.nearest ,dist.larger , and yrs.isolation ; these might beneﬁt from a\nlog transformation. The variable altitude is reasonably symmetric, and the predictor grazing.factor\nis categorical and so does not take transformations. Figure 7.23 shows the distributions of log.patch.area ,\nlog.dist.nearest ,log.dist.larger , and log.yrs.isolation , which were created through a natu-\nral log transformation of the original variables. All four are more nearly symmetric. These will be\nmore suitable for inclusion in a model than the untransformed versions.\n7.8. MODEL SELECTION FOR EXPLANATORY MODELS 361\nareaFrequency\n05001000 15000510152025\naltitudeFrequency\n1001502002500510152025\ndist.nearestFrequency\n0500 1000 15000510152025\ndist.largerFrequency\n010002000300040000510152025\nyrs.isolationFrequency\n0204060801000510152025\nlight average heavy\ngrazing.intensity0510152025\nFigure 7.22: Histograms and a barplot for the potential predictors of abundance .\nlog(area)Frequency\n−2024680510152025\nlog(dist.nearest)Frequency\n345670510152025\nlog(dist.larger)Frequency\n3456780510152025\nlog(yrs.isolation)Frequency\n2.02.53.03.54.04.50510152025\nFigure 7.23: Histograms of the log-transformed versions of patch.area ,\ndist.nearest ,dist.larger , and yrs.isolation .\n362 CHAPTER 7. MULTIPLE LINEAR REGRESSION\nAscatterplot matrix can be useful for visualizing the relationships between the predictor and\nresponse variables, as well as the relationships between predictors. Each subplot in the matrix is\na simple scatterplot; all possible plots are shown, except for the plots of a variable versus itself.\nThe variable names are listed along the diagonal of the matrix, and the diagonal divides the matrix\ninto symmetric plots. For instance, the ﬁrst plot in the ﬁrst row shows abundance on the vertical\naxis and log.area on the horizontal axis; the ﬁrst plot in the ﬁrst column shows abundance on\nthe horizontal axis and log.area on the vertical axis. Note that for readability, grazing.intensity\nappears with values 1 - 5, with 1 denoting \"light\" and 5 denoting \"heavy\" grazing intensity.\nThe plots in the ﬁrst row of Figure 7.24 show the relationships between abundance and the\npredictors.21There is a strong positive association between abundance with log.area , and a strong\nnegative association between abundance andlog.yrs.isolation . The variables log.dist.near.patch\nand log.dist.larger seem weakly positively associated with abundance . There is high variance of\nabundance and somewhat similar centers for the ﬁrst four categories, but abundance does clearly\ntend to be lower in the \"high grazing\" category versus the others.\nabundance\n−20246\n45678\n2.02.53.03.54.04.5\n01030−226\nlog.area\nlog.dist.nearest\n4567468\nlog.dist.larger\naltitude\n1002002.03.04.0\nlog.yrs.isolation010203040\n4567\n100150200250\n1234512345\ngrazing.intensity\nFigure 7.24: Scatterplot matrix of abundance and the possible predic-\ntors: log.area ,log.dist.near.patch ,log.dist.larger.patch ,altitude ,\nlog.yrs.isolation , and grazing.intensity .\n21Traditionally, the response variable (i.e., the dependent variable) is plotted on the vertical axis; as a result, it seems\nmore natural to look at the ﬁrst row where abundance is on they-axis. It is equally valid, however, to assess the association\nofabundance with the predictors from the plots in the ﬁrst column.\n7.8. MODEL SELECTION FOR EXPLANATORY MODELS 363\nThe variables log.dist.nearest and log.dist.larger appear strongly associated; a model\nmay only need one of the two, as they may be essentially \"redundant\" in explaining the variabil-\nity in the response variable.22In this case, however, since both are only weakly associated with\nabundance , both may be unnecessary in a model.\nA numerical approach conﬁrms some of the features observable from the scatterplot matrix.\nFigure 7.25 shows the correlations between pairs of numerical variables in the dataset. Correla-\ntions between abundance and log.area and between abundance and log.yrs.isolation are rela-\ntively high, at 0.74 and -0.48, respectively. In contrast, the correlation between abundance and the\ntwo variables log.dist.nearest and log.dist.larger are much smaller, at 0.13 and 0.12. Addi-\ntionally, the two potential predictors log.dist.nearest and log.dist.larger have a relatively high\ncorrelation of 0.60.\nabundance log.area log.dist.nearest log.dist.larger altitude log.yrs.isolation\nabundance 1.00 0.74 0.13 0.12 0.39 -0.48\nlog.area 0.74 1.00 0.30 0.38 0.28 -0.25\nlog.dist.nearest 0.13 0.30 1.00 0.60 -0.22 0.02\nlog.dist.larger 0.12 0.38 0.60 1.00 -0.27 0.15\naltitude 0.39 0.28 -0.22 -0.27 1.00 -0.29\nlog.yrs.isolation -0.48 -0.25 0.02 0.15 -0.29 1.00\nFigure 7.25: A correlation matrix for the numerical variables in forest.birds .\nInitial model ﬁtting\nBased on the data exploration, the initial model should include the variables log.area ,altitude ,\nlog.yrs.isolation , and grazing.intensity ; a summary of this model is shown in Figure 7.26. The\nR2and adjusted R2for this model are, respectively, 0.728 and 0.688. The model explains about 73%\nof the variability in abundance .\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) 14.1509 6.3006 2.25 0.0293\nlog.area 3.1222 0.5648 5.53 0.0000\naltitude 0.0080 0.0216 0.37 0.7126\nlog.yrs.isolation 0.1300 1.9193 0.07 0.9463\ngrazing.intensityless than average 0.2967 2.9921 0.10 0.9214\ngrazing.intensityaverage -0.1617 2.7535 -0.06 0.9534\ngrazing.intensitymoderately heavy -1.5936 3.0350 -0.53 0.6019\ngrazing.intensityheavy -11.7435 4.3370 -2.71 0.0094\nFigure 7.26: Initial model: regression of abundance onlog.area ,altitude ,\nlog.yrs.isolation and grazing.intensity .\nTwo of the variables in the model are not statistically signiﬁcant at the \u000b= 0:05 level: altitude\nand log.yrs.isolation . Only one of the categories of grazing.intensity (heavy grazing) is highly\nsigniﬁcant.\n22Typically, the predictor that is less strongly correlated with the response variable is the one that is \"redundant\" and\nwill be statistically insigniﬁcant when included in a model with the more strongly correlated predictor. This is not always\nthe case, and depends on the other variables in the model.\n364 CHAPTER 7. MULTIPLE LINEAR REGRESSION\nModel comparison\nFirst, ﬁt models excluding the predictors that were not statistically signiﬁcant: altitude and\nlog.yrs.isolation . Models excluding either variable have adjusted R2of 0.69, and a model ex-\ncluding both variables has an adjusted R2of 0.70, a small but noticeable increase from the initial\nmodel. This suggests that these two variables can be dropped. At this point, the working model\nincludes only log.area and grazing.intensity ; this model has R2= 0:727 and is shown in Fig-\nure 7.27.\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) 15.7164 2.7674 5.68 0.0000\nlog.area 3.1474 0.5451 5.77 0.0000\ngrazing.intensityless than average 0.3826 2.9123 0.13 0.8960\ngrazing.intensityaverage -0.1893 2.5498 -0.07 0.9411\ngrazing.intensitymoderately heavy -1.5916 2.9762 -0.53 0.5952\ngrazing.intensityheavy -11.8938 2.9311 -4.06 0.0002\nFigure 7.27: Working model: regression of abundance onlog.area and\ngrazing.intensity .\nIt is prudent to check whether the two distance-related variables that were initially excluded\nmight increase the adjusted R2, even though this seems unlikely. When either or both of these\nvariables are added, the adjusted R2decreases from 0.70 to 0.69. Thus, these variables are not\nadded to the working model.\nIn this working model, only one of the coe ﬃcients associated with grazing intensity is statis-\ntically signiﬁcant; when compared to the baseline grazing category (light grazing), heavy grazing\nis associated with a reduced predicted mean abundance of 11.9 birds (assuming that log.area is\nheld constant). Individual categories of a categorical variable cannot be dropped, so a data ana-\nlyst has the choice of leaving the variable as is, or collapsing the variable into fewer categories.\nFor this model, it might be useful to collapse grazing intensity into a two-level variable, with one\ncategory corresponding to the original classiﬁcation of heavy, and another category corresponding\nto the other four categories; i.e., creating a version of grazing intensity that only has the levels\n\"heavy\" and \"not heavy\". This is supported by the data exploration; a plot of abundance versus\ngrazing.intensity shows that the centers of the distributions of abundance in the lowest four graz-\ning intensity categories are roughly similar, relative to the center in the heavy grazing category. The\nmodel with the binary version of grazing intensity, grazing.binary , is shown in Figure 7.28. The\nmodel with grazing.binary has adjusted R2= 0:71, which is slightly larger than 0.70 in the more\ncomplex model with grazing.intensity ; the model explains 72% of the variability in abundance\n(R2= 0:724).\nIncorporating an interaction term did not improve the model; adding a parameter for the\ninteraction between log.area and grazing.binary decreased the adjusted R2to 0.709. Thus, the\nmodel shown in Figure 7.28 is the ﬁnal model.\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) 15.3736 1.4507 10.60 0.0000\nlog.area 3.1822 0.4523 7.04 0.0000\ngrazing.binaryheavy -11.5783 1.9862 -5.83 0.0000\nFigure 7.28: Final model: regression of abundance on log.area and\ngrazing.binary .\n7.8. MODEL SELECTION FOR EXPLANATORY MODELS 365\nModel assessment\nThe ﬁt of a model can be assessed using various residual plots. Figure 7.29 shows a histogram and\nnormal probability plot of the residuals for the ﬁnal model. Both show that the residuals follow\nthe shape of a normal density in the middle range (between -10 and 10) but ﬁt less well in the tails.\nThere are too many large positive and large negative values) residuals.\nResiduals−15 −5051015\nTheoretical QuantilesSample Quantiles\n−2−1012−15−10−50510\nFigure 7.29: Histogram and normal probability plot of residuals in the model for\nabundance with predictors log.area and grazing.binary .\nFigure 7.30 gives a more detailed look at the residuals, plotting the residuals against predicted\nvalues and against the two predictors in the model, log.area and grazing.level . Recall that resid-\nual values closer to 0 are indicative of a more accurate prediction; positive values occur when the\npredicted value from the model is smaller than the observed value, and vice versa for negative\nvalues. Residuals are a measure of the prediction error of a model.\n10203040−15−10−50510\nPredicted ValueResidual●●●●●\n●●\n●\n●●●\n●●\n●●\n●●\n●●\n●●\n●\n●●\n●●\n●●●●\n●●●\n●\n●●\n●●●\n●\n●●●\n●●●●●\n●●●\n●●●\n●●\n−20246−15−10−50510Residual\nlog.areanot heavy heavy−15−10−50510Residual\ngrazing.binary\nFigure 7.30: Scatterplots of residuals versus predicted values and residuals ver-\nsuslog.area , and a side-by-side boxplot of residuals by grazing.binary . In the\nmiddle plot, red points correspond to values where grazing level is \"heavy\" and\nblue points correspond to \"not heavy\".\n366 CHAPTER 7. MULTIPLE LINEAR REGRESSION\nIn the left plot, the large positive and large negative residuals visible from Figure 7.29 are\nevident; the large positive residuals occur across the range of predicted values, while the large\nnegative residuals occur around 20 (predicted birds). The middle plot shows that the large positive\nand negative residuals occur at intermediate values of log.area ; i.e., for values of log.area between\n0 and 4, or equivalently for values of area between exp(0) = 1 and exp(4) = 54 :5 hectares. In\nthe same range, there are also relatively accurate predictions; most residuals are between -5 and\n5. Both the middle plot and the right plot show that the prediction error is smaller for patches\nwith heavy grazing than for patches where grazing intensity was between \"light\" and \"moderately\nheavy\". Patches with heavy grazing are represented with red points; note how the red points mostly\ncluster around the y= 0 line, with the exception of one outlier with a residual value of about 10.\nConclusions\nThe relatively large R2for the ﬁnal model (0.72) suggests that patch area and extent of grazing\n(either heavy or not) explain a large amount of the observed variability in bird abundance. Of\nthe features measured in the study, these two are the most highly associated with bird abundance.\nLarger area is associated with an increase in abundance; when grazing intensity does not change,\nthe model predicts an increase in average abundance by 3.18 birds for every one unit increase in\nlog area (or equivalently, when area is increased by a factor of exp(1) = 2 :7). A patch with heavy\ngrazing is estimated to have a mean abundance of about 11.58 birds lower than a patch that has\nnot been heavily grazed.\nThe residual plots imply that the ﬁnal model may not be particularly accurate. For most\nobservations, the predictions are accurate between \u00065 birds, but there are several instances of over-\npredictions as high as around 10 and under-predictions of about 15. Additionally, the accurate\nand inaccurate predictions occur at similar ranges of of log.area ; if the model only tended to be\ninaccurate at a speciﬁc range, such as for patches with low area, it would be possible to provide\nclearer advice about when the model is unreliable. The residuals plots do suggest that the model\nis more reliable for patches with heavy grazing, although there is a slight tendency towards over-\nprediction.\nBased on these results, the ecologists might decide to proceed by collecting more data. Cur-\nrently, the model seems to adequately explain the variability in bird abundance for patches that\nhave been heavily grazed, but perhaps there are additional variables that are associated with bird\nabundance, especially in patches that are not heavily grazed. Adding these variables might im-\nprove model residuals, in addition to raising R2.\nFinal considerations\nMight a model including all the predictor variables be better than the ﬁnal model with only\nlog.area and grazing.binary ? The model is shown in Figure 7.31. The R2for this model is 0.729\nand the adjusted R2is 0.676. While the R2is essentially the same as for the ﬁnal model, the ad-\njustedR2is noticeably lower. The residual plots in Figure 7.32 do not indicate that this model is an\nespecially better ﬁt, although the residuals are slightly closer to normality. There would be little\ngained from using the larger model.\nIn fact, there is an additional reason to avoid the larger model. When building regression\nmodels, it is important to consider that the complexity of a model is limited by sample size (i.e.,\nthe number of observations in the data). Attempting to estimate too many parameters from a small\ndataset can produce a model with unreliable estimates; the model may be ’overﬁt’, in the sense\nthat it ﬁts the data used to build it particularly well, but will fail to generalize to a new set of data.\nMethods for exploring these issues are covered in more advanced regression courses.\n7.8. MODEL SELECTION FOR EXPLANATORY MODELS 367\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) 10.8120 9.9985 1.08 0.2852\nlog.area 2.9720 0.6587 4.51 0.0000\nlog.dist.near.patch 0.1390 1.1937 0.12 0.9078\nlog.dist.larger.patch 0.3496 0.9301 0.38 0.7087\naltitude 0.0117 0.0233 0.50 0.6169\nlog.yrs.isolation 0.2155 1.9635 0.11 0.9131\ngrazing.intensityless than average 0.5163 3.2631 0.16 0.8750\ngrazing.intensityaverage 0.1344 2.9870 0.04 0.9643\ngrazing.intensitymoderately heavy -1.2535 3.2000 -0.39 0.6971\ngrazing.intensityheavy -12.0642 4.5657 -2.64 0.0112\nFigure 7.31: Full model: regression of abundance on all 6 predictors in\nforest.birds .\n010203040−15−10−50510\nPredicted ValueResidual●●\n●●●\n●●\n●\n●●●\n●●\n●●\n●●\n●●\n●●\n●●●\n●●\n●●●●\n●●●\n●\n●●\n●●●\n●\n●●●●\n●●●●\n●●●\n●●●\n●●\nTheoretical QuantilesSample Quantiles\n−2−1012−15−10−50510\nFigure 7.32: Residual plots for the full model of abundance that includes all pre-\ndictors.\nA general rule of thumb is to avoid ﬁtting a model where there are fewer than 10 observations\nper parameter; e.g., to ﬁt a model with 3 parameters, there should be at least 30 observations in the\ndata. In a regression context, all of the following are considered parameters: an intercept term, a\nslope term for a numerical predictor, a slope term for each level of a categorical predictor, and an\ninteraction term. In forest.birds , there are 56 cases, but ﬁtting the full model involves estimating\n10 parameters. The rule of thumb suggests that for these data, a model can safely support at most\n5 parameters.\nAs mentioned earlier, other analyses of forest.birds have treated grazing.intensity as a nu-\nmerical variable with ﬁve values. One advantage to doing so is to produce a more stable model; only\none slope parameter needs to be estimated, rather than four. However, treating grazing.intensity\nas a numerical variable requires assuming that any one unit change is associated with the same\nchange in population mean abundance ; under this assumption, a change between \"light\" and \"less\nthan average\" (codes 1to2) is associated with the same change in population mean abundance as\nbetween \"moderately heavy\" to \"heavy\" (codes 4to5) grazing. Previous model ﬁtting has shown\nthat this assumption is not supported by the data, and that changes in mean abundance between\nadjacent levels in grazing intensity are not constant. In this text, it is our recommendation that\ncategorical variables should not be treated as numerical variables.\n368 CHAPTER 7. MULTIPLE LINEAR REGRESSION\n7.9 The connection between ANOVA and regression\nRegression with categorical variables and ANOV A are essentially the same method, but with\nsome important di ﬀerences in the information provided by the analysis. Earlier in this chapter, the\nstrength of the association between RFFT scores and educational level was assessed with regres-\nsion. Figure 7.33 shows the results of an ANOV A to analyze the di ﬀerence in RFFT scores between\neducation groups.\nDf Sum Sq Mean Sq F value Pr( >F)\nas.factor(Education) 3 115040.88 38346.96 73.30 0.0000\nResiduals 496 259469.32 523.12\nFigure 7.33: Summary of ANOV A of RFFT by Education Levels\nIn this setting, the F-statistic is used to test the null hypothesis of no di ﬀerence in mean RFFT\nscore by educational level against the alternative that at least two of the means are di ﬀerent. The\nF-statistic is 73.3 and highly signiﬁcant.\nTheF-statistic can also be calculated for regression models, although it has not been shown\nin the regression model summaries in this chapter. In regression, the F-statistic tests the null\nhypothesis that all regression coe ﬃcients are equal to 0 against the alternative that least one of the\ncoeﬃcients is not equal to 0.\nAlthough the phrasing of the hypotheses in ANOV A versus regression may seem di ﬀerent ini-\ntially, they are equivalent. Consider the regression model for predicting RFFT from educational\nlevel—each of the coe ﬃcients in the model is an estimate of the di ﬀerence in mean RFFT for a\nparticular education level versus the baseline category of Education = 0. A signiﬁcant F-statistic\nindicates that at least one of the coe ﬃcients is not zero; i.e., that at least one of the mean levels\nof RFFT di ﬀers from the baseline category. If all the coe ﬃcients were to equal zero, then the dif-\nferences between the means would be zero, implying all the mean RFFT levels are equal. It is\nreasonable, then, that the F-statistic associated with the RFFT versus Education regression model is\nalso 73.3.\nThe assumptions behind the two approaches are identical. Both ANOV A and linear regression\nassume that the groups are independent, that the observations within each group are independent,\nthat the response variable is approximately normally distributed, and that the standard deviations\nof the response are the same across the groups.\nThe regression approach provides estimates of the mean at the baseline category (the inter-\ncept) and the di ﬀerences of the means between each category and the baseline, along with a t-\nstatistic and p-value for each comparison. From regression output, it is easy to calculate all the\nestimated means; to do the same with ANOV A requires calculating summary statistics for each\ngroup. Additionally, diagnostic plots to check model assumptions are generally easily accessible in\nmost computing software.\n7.9. THE CONNECTION BETWEEN ANOV A AND REGRESSION 369\nWhy use ANOV A at all if ﬁtting a linear regression model seems to provide more information?\nA case can be be made that the most important ﬁrst step in analyzing the association between a\nresponse and a categorical variable is to compute and examine the F-statistic for evidence of any\neﬀect, and that only when the F-statistic is signiﬁcant does it become appropriate to proceed to\nexamine the nature of the di ﬀerences. ANOV A displays the F-statistic prominently, emphasizing\nits importance. It is available in regression output, but may not always be easy to locate; the focus of\nregression is on the signiﬁcance of the individual coe ﬃcients. ANOV A has traditionally been used\nin carefully designed experiments. There are complex versions of ANOV A that are appropriate for\nexperiments in which several di ﬀerent factors are set at a range of levels. More complex versions\nof ANOV A are beyond the scope of this text and are covered in more advanced books.\nSection 5.5 discussed the use of Bonferroni corrections when testing hypotheses about pair-\nwise di ﬀerences among the group means when conducting ANOV A. In principle, Bonferroni cor-\nrections can be applied in regression with categorical variables, but that is not often done. In\ndesigned experiments in which ANOV A has historically been used, the goal was typically to show\ndeﬁnitively that a categorical predictor, often a treatment or intervention, was associated with a\nresponse variable so that the treatment could be adopted for clinical use. In experiments where\nthe predictor can be manipulated by a scientist and cases are randomized to one of several levels\nof a predictor, the association can be interpreted as causal. It can be particularly important to con-\ntrol Type I error probabilities in those settings. Regression is often thought of as an exploratory\ntechnique, used in observational studies to discover associations that can be explored in further\nstudies. Strict control of Type I error probabilities may be less critical in such settings.\nAt the introductory level, ANOV A is useful in that it provides more direct access to Type I error\ncontrol and pairwise comparisons with t-tests. In practice, with the use of techniques not covered\nin this text, any analysis done via the ANOV A approach can also be approached with regression\nmodeling.\n370 CHAPTER 7. MULTIPLE LINEAR REGRESSION\n7.10 Notes\nThis chapter and the previous chapter cover only the basic principles behind linear regression,\nand are meant to provide useful tools for getting started with data analysis. This section summa-\nrizes the most important ideas in the chapter and makes reference to some related topics that have\nnot been discussed in detail.\nImportant ideas\nKeep a clear view of the purpose. Is the goal of constructing the model to understand the relationship\nbetween the response and a particular predictor after adjusting for confounders? Or is the\ngoal to understand the joint association between a response and a set of predictors?\nAvoid rushing into model ﬁtting. Before ﬁtting models, examine the data. Assess whether the re-\nsponse variable has an approximate normal distribution, or at least a symmetric distribution;\na log transformation will often produce approximate normality. Examine the relationships\nbetween the response and predictors, as well as the relationships between predictors; check\nfor nonlinear trends or outliers.\nRemember the context of the problem. Context is important at each stage of a regression analysis.\nThe best approach for constructing a model from a small number of potential predictors is\nbased on considering the context of the problem and including predictors that have either\nbeen shown in the past to be associated with the response or for which there is a plausi-\nble working hypothesis about association with the response. When interpreting coe ﬃcients,\nconsider whether the model results cohere with the underlying biological or medical context.\nCritically examine residual plots. All models are approximations, so it is not necessary to be\nconcerned about relatively minor violations of assumptions; residual plots are seldom as\nwell behaved as those for the PREVEND data. In some cases, like with the California DDS\ndata, residual plots show obvious major violations. With intermediate cases such as in the\nforest.birds plots, examine the plots closely and provide a detailed assessment of where the\nmodel seems less reliable.\n7.10. NOTES 371\nRelated topics\nStepwise model selection. Many introductory texts recommend using “stepwise” regression. For-\nward stepwise regression adds predictors one by one according to a set criterion (usually by\nsmallestp-value). Backward stepwise regression eliminates variables one by one from a larger\nmodel until a criterion is met. Stepwise methods can be useful, and are usually automated\nin statistical software. However, there are weaknesses—the ﬁnal models are data-dependent\nand chance alone can lead to spurious variables being included. In very large datasets, step-\nwise regression can lead to substantially incorrect models.\nPrediction models. An application of regression not discussed in this chapter is predictive model-\ning, in which the goal is to construct a model that best predicts outcomes. The focus is on\noverall predictive accuracy; signiﬁcance of individual coe ﬃcients is less important. Evalu-\nating a model’s predictive accuracy involves advanced methods such as cross-validation, in\nwhich the original data sample is divided into a training set and a test set, similar to the ap-\nproach used with the Golub leukemia data in Chapter 1. Prediction models are typically built\nfrom large datasets, using automated model selection procedures like stepwise regression.\nPrediction intervals. Predicted values from regression have an inherent uncertainty because model\nparameters are only estimates. There are two types of interval estimates used with prediction:\nconﬁdence intervals for a predicted mean response from a set of values for the predictors, and\nprediction intervals that show the variability in the predicted value for a new response (i.e.,\nfor a case not in the dataset) given a set of values for the predictor variables. Prediction\nintervals are wider than conﬁdence intervals for a predicted mean because prediction inter-\nvals are subject to both the variability in a predicted mean response and the variability of an\nindividual observation about its mean.\nControlling Type I error in regression. Control of Type I error probabilities becomes more critical\nin regression models with very large numbers of potential predictors. Datasets containing\nmeasurements on genetic data often contain large numbers of potential predictors for a re-\nsponse for many cases; a stricter signiﬁcance level is used to maintain an overall error rate\nof\u000b= 0:05. For example, in genome-wide association studies, the accepted \"genome-wide\nsigniﬁcance rate\" for an individual marker to be considered signiﬁcantly associated with an\noutcome is 5\u000210\u00008.\nBecause there are so many tools available in multiple regression, this chapter has a larger\ncollection of labs than most other chapters. Lab 1 introduces the multiple regression model, il-\nlustrating one its most common uses—estimating an association between a response variable and\npredictor of interest while adjusting for possible confounding. Lab 2 discusses the residual plots\nused to check assumptions for multiple regression and introduces adjusted R2using the California\nDDS dataset initially introduced in Chapter 1.\nLab 3 explores how the association between a response variable and categorical predictors\nwith more than two levels can be be estimated using multiple regression. This topic extends the\nearlier material in Chapter 6, Lab 4. Lab 4 introduces the concept of a statistical interaction using\nthe NHANES dataset, examining whether the association between BMI and age among women is\ndiﬀerent than that among men.\nMultiple regression is often used to examine associations between response variables and a\nsmall set of pre-speciﬁed predictors. It can also be used to explore and select models between a\nresponse variable and a set of candidate predictors. Lab 5 discusses explanatory modeling, in which\nthe goal is to construct a model that e ﬀectively explains the observed variation in the response\nvariable.\n372 CHAPTER 7. MULTIPLE LINEAR REGRESSION\n7.11 Exercises\n7.11.1 Introduction to multiple linear regression\nThere are not currently exercises available for this section.\n7.11.2 Simple versus multiple regression\n7.1 PREVEND, Part I. The summary table below shows the results of a multiple regression model of RFFT\nscore versus statin use and age.\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) 137.8822 5.1221 26.92 0.0000\nStatin 0.8509 2.5957 0.33 0.7432\nAge -1.2710 0.0943 -13.48 0.0000\nIn a clinical setting, the interpretive focus lies on reporting the nature of the association between the\nprimary predictor and the response, while specifying which potential confounders have been adjusted for.\nBrieﬂy respond to a clinician who is concerned about a possible association between statin use and decreased\ncognitive function, based on the above analysis.\n7.2 PREVEND, Part II. Can the results of the analysis in Exercise 7.1 be used to conclude that as one ages,\none’s cognitive function (as measured by RFFT score) declines? Explain your answer.\n7.3 Baby weights, Part I. The Child Health and Development Studies investigate a range of topics. One\nstudy considered all pregnancies between 1960 and 1967 among women in the Kaiser Foundation Health\nPlan in the San Francisco East Bay area. The variable smoke is coded 1 if the mother is a smoker, and 0 if\nnot. The variable parity is 1 if the child is the ﬁrst born, and 0 otherwise. The summary table below shows\nthe results of a linear regression model for predicting the average birth weight of babies, measured in ounces,\nbased on the smoking status of the mother and whether the child is the ﬁrst born.23\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) 123.57 0.72 172.75 0.0000\nsmoke -8.96 1.03 -8.68 0.0000\nparity -1.98 1.15 -1.72 0.0859\n(a) Write the equation of the regression model.\n(b) Interpret the model slopes in the context of the data.\n(c) Calculate the estimated di ﬀerence in mean birth weight for two infants born to non-smoking mothers, if\none is ﬁrst born and the other is not.\n(d) Calculate the estimated di ﬀerence in mean birth weight for two infants born to mothers who are smokers,\nif one is ﬁrst born and the other is not.\n(e) Calculate the predicted mean birth weight for a ﬁrst born baby born to a mother who is not a smoker.\n23Child Health and Development Studies, Baby weights data set.\n7.11. EXERCISES 373\n7.4 Wolbachia, Part I. Wolbachia is a microbial symbiont estimated to be hosted by about 40% of all arthro-\npod species, transmitted primarily from females to their o ﬀspring through the eggs. Researchers conducted\na study on a wasp species to understand the e ﬀect of Wolbachia on the lifetime reproductive success of an in-\nsect host. They estimated the realized lifetime reproductive success of female wasps by collecting them soon\nafter they die naturally in the ﬁeld, counting the number of eggs remaining in their ovaries and quantifying\nWolbachia density in their body.\nIn the ﬁrst stage of the experiment, researchers estimated potential reproductive success by collecting\nfemale wasps as they emerged from eggs then dissecting them to count the number of eggs in their ovaries.\nThese data were used to create a predictive model for initial number of eggs based on tibia length (an indicator\nof body size) and Wolbachia density. Tibia length was measured in \u0016m, and Wolbachia density in units of -ddCt.\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) -18.82 27.26 -0.69 0.497\nwolbachia 1.77 1.07 -1.65 0.111\ntibia 0.357 0.15 2.38 0.0258\n(a) Write the model equation.\n(b) Interpret the model coe ﬃcients in the context of the data.\n(c) Predict mean initial egg count for a wasp with tibia length of 171.4286 \u0016mandWolbachia density of -3.435\n-ddCt.\n7.11.3 Evaluating the ﬁt of a multiple regression model\n7.5 Baby weights, Part III. We considered the variables smoke and parity , one at a time, in modeling birth\nweights of babies in Exercise 7.3. A more realistic approach to modeling infant weights is to consider all\npossibly related variables at once. Other variables of interest include length of pregnancy in days ( gestation ),\nmother’s age in years ( age), mother’s height in inches ( height ), and mother’s pregnancy weight in pounds\n(weight ). Below are three observations from this data set.\nbwt gestation parity age height weight smoke\n1 120 284 0 27 62 100 0\n2 113 282 0 33 64 135 0\n::::::::::::::::::::::::\n1236 117 297 0 38 65 129 0\nThe summary table below shows the results of a regression model for predicting the average birth weight of\nbabies based on all of the variables included in the data set.\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) -80.41 14.35 -5.60 0.0000\ngestation 0.44 0.03 15.26 0.0000\nparity -3.33 1.13 -2.95 0.0033\nage -0.01 0.09 -0.10 0.9170\nheight 1.15 0.21 5.63 0.0000\nweight 0.05 0.03 1.99 0.0471\nsmoke -8.40 0.95 -8.81 0.0000\n(a) Write the equation of the regression model that includes all of the variables.\n(b) Interpret the slopes of gestation and agein this context.\n(c) The coe ﬃcient for parity is diﬀerent than in the linear model shown in Exercise 7.3. Why might there be\na diﬀerence?\n(d) Calculate the residual for the ﬁrst observation in the data set.\n(e) The variance of the residuals is 249.28, and the variance of the birth weights of all babies in the data set\nis 332.57. Calculate the R2and the adjusted R2. Note that there are 1,236 observations in the data set.\n374 CHAPTER 7. MULTIPLE LINEAR REGRESSION\n7.6 Absenteeism, Part I. Researchers interested in the relationship between absenteeism from school and\ncertain demographic characteristics of children collected data from 146 randomly sampled students in rural\nNew South Wales, Australia, in a particular school year. Below are three observations from this data set.\neth sex lrn days\n1 0 1 1 2\n2 0 1 1 11\n:::::::::::::::\n146 1 0 0 37\nThe summary table below shows the results of a linear regression model for predicting the average number of\ndays absent based on ethnic background ( eth: 0 - aboriginal, 1 - not aboriginal), sex ( sex: 0 - female, 1 - male),\nand learner status ( lrn: 0 - average learner, 1 - slow learner).24\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) 18.93 2.57 7.37 0.0000\neth -9.11 2.60 -3.51 0.0000\nsex 3.10 2.64 1.18 0.2411\nlrn 2.15 2.65 0.81 0.4177\n(a) Write the equation of the regression model.\n(b) Interpret each one of the slopes in this context.\n(c) Calculate the residual for the ﬁrst observation in the data set: a student who is aboriginal, male, a slow\nlearner, and missed 2 days of school.\n(d) The variance of the residuals is 240.57, and the variance of the number of absent days for all students in\nthe data set is 264.17. Calculate the R2and the adjusted R2. Note that there are 146 observations in the\ndata set.\n24W. N. Venables and B. D. Ripley. Modern Applied Statistics with S . Fourth Edition. Data can also be found in the R\nMASS package. New York: Springer, 2002.\n7.11. EXERCISES 375\n7.7 Baby weights, Part VI. Exercise 7.5 presents a regression model for predicting the average birth weight\nof babies based on length of gestation, parity, height, weight, and smoking status of the mother. Use the\nfollowing plots to assess whether the assumptions for linear regression are reasonably met. Discuss your\nreasoning.\nResiduals−60−40−200204060050100150200250300\nFitted valuesResiduals\n80 120 160−40040\nOrder of collectionResiduals\n0 400 800 1200−40040\nLength of gestationResiduals\n150 200 250 300 350−40040\nParityResiduals\n0 1−40040\nHeight of motherResiduals\n55 60 65 70−40040\nWeight of motherResiduals\n100 150 200 250−40040\nSmokeResiduals\n0 1−40040\n376 CHAPTER 7. MULTIPLE LINEAR REGRESSION\n7.8 Toxemia and birth weight. A model was ﬁt for a random sample of 100 low birth weight infants born in\ntwo teaching hospitals in Boston, Massachusetts, regressing birthweight on the predictors gestational age and\ntoxemia status. The condition toxemia, also known as preeclampsia, is characterized by high blood pressure\nand protein in urine by the 20thweek of pregnancy; left untreated, toxemia can be life-threatening. Birth\nweight was measured in grams and gestational age measured in weeks.\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) -1286.200 234.918 -5.475 0.0000\ntoxemiaYes -206.591 51.078 -4.045 0.0001\ngestage 84.048 8.251 10.188 0.0000\n24 26 28 30 32 34−600−400−2000200400Residual vs Gestage\nGestage (wks)Residual\n800 1000 1200 1400−600−400−2000200400Residual vs Fitted\nPredicted Birthweight (g)Residual\n−2 −1 0 1 2−600−400−2000200400Normal Q−Q Plot\nTheoretical QuantilesSample Quantiles\n(a) Write the model equation.\n(b) Interpret the coe ﬃcients of the model, and comment on whether the intercept has a meaningful interpre-\ntation.\n(c) Predict the average birth weight for an infant born to a mother diagnosed with toxemia with gestational\nage 31 weeks.\n(d) Evaluate whether the assumptions for linear regression are reasonably satisﬁed.\n(e) A simple regression model with only toxemia status as a predictor had R2= 0:0001 andR2\nadj= 0:010; in\nthis model, the slope estimate for toxemia status is 7.785, with p= 0:907. The simple regression model\nand multiple regression model disagree regarding the nature of the association between birth weight\nand toxemia. Brieﬂy explain a potential reason behind the discrepancy. Which model do you prefer for\nunderstanding the relationship between birth weight and toxemia, and why?\n7.9 Multiple regression fact checking. Determine which of the following statements are true and false.\nFor each statement that is false, explain why it is false.\n(a) Suppose a numerical variable xhas a coe ﬃcient ofb1= 2:5 in the multiple regression model. Suppose\nalso that the ﬁrst observation has x1= 7:2, the second observation has a value of x1= 8:2, and these\ntwo observations have the same values for all other predictors. Then the predicted value of the second\nobservation will be 2.5 higher than the prediction of the ﬁrst observation based on the multiple regression\nmodel.\n(b) If a regression model’s ﬁrst variable has a coe ﬃcient ofb1= 5:7, then if we are able to inﬂuence the data\nso that an observation will have its x1be 1 larger than it would otherwise, the value y1for this observation\nwould increase by 5.7.\n(c) Suppose we ﬁt a multiple regression model based on a data set of 472 observations. We also notice that the\ndistribution of the residuals includes some skew but does not include any particularly extreme outliers.\nBecause the residuals are not nearly normal, we should not use this model and require more advanced\nmethods to model these data.\n7.11. EXERCISES 377\n7.11.4 The general multiple linear regression model\n7.10 Cherry trees. Timber yield is approximately equal to the volume of a tree, however, this value is\ndiﬃcult to measure without ﬁrst cutting the tree down. Instead, other variables, such as height and diameter,\nmay be used to predict a tree’s volume and yield. Researchers wanting to understand the relationship between\nthese variables for black cherry trees collected data from 31 such trees in the Allegheny National Forest,\nPennsylvania. Height is measured in feet, diameter in inches (at 54 inches above ground), and volume in\ncubic feet.25\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) -57.99 8.64 -6.71 0.00\nheight 0.34 0.13 2.61 0.01\ndiameter 4.71 0.26 17.82 0.00\n(a) Calculate a 95% conﬁdence interval for the coe ﬃcient of height, and interpret it in the context of the data.\n(b) One tree in this sample is 79 feet tall, has a diameter of 11.3 inches, and is 24.2 cubic feet in volume.\nDetermine if the model overestimates or underestimates the volume of this tree, and by how much.\n7.11 GPA. A survey of 55 Duke University students asked about their GPA, number of hours they study at\nnight, number of nights they go out, and their gender. Summary output of the regression model is shown\nbelow. Note that male is coded as 1.\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) 3.45 0.35 9.85 0.00\nstudyweek 0.00 0.00 0.27 0.79\nsleepnight 0.01 0.05 0.11 0.91\noutnight 0.05 0.05 1.01 0.32\ngender -0.08 0.12 -0.68 0.50\n(a) Calculate a 95% conﬁdence interval for the coe ﬃcient of gender in the model, and interpret it in the\ncontext of the data.\n(b) Would you expect a 95% conﬁdence interval for the slope of the remaining variables to include 0? Explain\n7.12 Trait inheritance of high blood pressure. One research question of public health interest is to de-\ntermine the extent to which high blood pressure is a genetic phenomenon. In 20 families, the systolic blood\npressure of the mother, father, and ﬁrst-born child in the family were measured (in units of mm Hg). A multi-\nple linear regression model using Y= child’s blood pressure, X1= mother’s blood pressure, and X2= father’s\nblood pressure led to the following estimate of a least squares line: E(Y) =\u000015:69 + 0:415X1+ 0:423X2. The\nstandard errors associated with b0,b1, andb2, respectively, are 23.65, 0.125, and 0.119. The least squares ﬁt\nproducedR2= 0:597 andMSE = 113:8.\n(a) What proportion of the variability of a child’s systolic blood pressure is explained by this model?\n(b) Does the least squares line indicate statistically signiﬁcant associations between each of the parent’s sys-\ntolic blood pressures and that of the child? Explain your answer.\n(c) What is the predicted systolic blood pressure for a child whose mother’s and father’s systolic blood pres-\nsure is 125 mm Hg and 140 mm Hg, respectively?\n(d) A colleague tells you that something must be wrong with your model because your ﬁtted intercept is\nnegative, but blood pressures are never negative. How do you respond?\n(e) Brieﬂy describe three di ﬀerent plots for assessing the appropriateness or ﬁt of the above regression model.\n25D.J. Hand. A handbook of small data sets . Chapman & Hall/CRC, 1994.\n378 CHAPTER 7. MULTIPLE LINEAR REGRESSION\n7.13 Wolbachia, Part II. Exercise 7.4 introduced a study about Wolbachia and reproductive success in a\nwasp host. The following table shows the model coe ﬃcients for a model predicting the number of eggs laid\nover a lifetime from the predictor variables wolbachia density and tibia length. A higher number of eggs laid\nover a lifetime is indicative of greater reproductive success. The model has R2= 0:314 and degrees of freedom\n34. TheF-statistic is 7.782, with p-value 0.0016.\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) -17.88 28.63 -0.62 0.537\nwolbachia 4.28 1.25 3.42 0.002\ntibia 0.272 0.16 1.69 0.010\n(a) Write the model equation.\n(b) Interpret the slope coe ﬃcient of wolbachia .\n(c) Assess the evidence for whether Wolbachia is beneﬁcial for its host in nature, based on these data.\n(d) Compute and interpret a 95% conﬁdence interval for the population slope of wolbachia .\n(e) Interpret the signiﬁcance of the F-statistic.\n7.14 Difﬁcult encounters, Part I. A study was conducted at a university outpatient primary care clinic in\nSwitzerland to identify factors associated with di ﬃcult doctor-patient encounters. The data consist of 527\npatient encounters, conducted by the 27 medical residents employed at the clinic. After each encounter, the\nattending physician completed two questionnaires: the Di ﬃcult Doctor Patient Relationship Questionnaire\n(DDPRQ-10) and the patient’s vulnerability grid (PVG).\nA higher score on the DDPRQ-10 indicates a more di ﬃcult encounter. The maximum possible score is\n60 and encounters with score 30 and higher are considered di ﬃcult.\nA model was ﬁt for the association of DDPRQ-10 score with features of the attending physician: age,\nsex, and years of training. The model has F-statistic of 0.23 on 3 and 286 degrees of freedom, with p-value\n0.876.\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) 30.594 2.886 10.601 0.0000\nage -0.016 0.104 -0.157 0.876\nsexM -0.535 0.781 -0.686 0.494\nyrs.train 0.096 0.215 0.445 0.656\n(a) As a group, are these physician features useful for predicting DDPRQ-10 score?\n(b) Is there evidence of a signiﬁcant association between DDPRQ-10 score and any of the physician features?\n7.11. EXERCISES 379\n7.11.5 Categorical predictors with several levels\n7.15 Prison isolation experiment, Part III.\nExercises 5.35 and 5.47 introduced an experiment conducted with the goal of identifying a treatment\nthat reduces subjects’ psychopathic deviant T scores on the MMPI test. Exercise 5.35 evaluated the success\nof each individual treatment, and in exercise 5.47, ANOV A was used to compare the success of the three\ntreatments. This exercise uses multiple regression to examine the intervention e ﬀect.\nFor this problem, a treatment variable (labeled treatment ) has been constructed with three levels:\n(1)Therapeutic for sensory restriction plus the 15 minute \"therapeutic\" tape advising that professional help\nis available.\n(2)Neutral for sensory restriction plus a 15 minute \"emotionally neutral\" tap on training hunting dogs.\n(3)Absent for sensory restriction but no taped message.\nForty-two subjects were randomly assigned to these treatment groups, and an MMPI test was adminis-\ntered before and after the treatment. Investigators hoped that the interventions would lower MMPI scores.\nThe table below shows the result of a multiple regression in Rwhere the response variable trt.effect is the\nchange in MMPI score (pre-intervention - post-intervention) and the predictor variable is treatment .\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) -3.2143 2.6174 -1.23 0.2268\ntreatmentNeutral 6.0714 3.7015 1.64 0.1090\ntreatmentTherapeutic 9.4286 3.7015 2.55 0.0149\nIn this model, the residual standard error is 9.79, the F-statistic is 3.33 with 2 and 39 degrees of freedom;\nP(F2;39>3:33) = 0:0461.\n(a) Interpret the meaning of a positive value for trt.effect versus a negative value.\n(b) Write the estimated model equation.\n(c) Calculate the predicted value for trt.effect for a patient in the neutral tape group.\n(d) Does the intercept have a meaningful interpretation in this model?\n(e) What is the interpretation of the two slope coe ﬃcients in the regression model?\n(f) Describe the tested hypotheses that correspond to each of the p-values in the last column of the table.\n7.16 Poverty and educational level. This question uses data from 500 randomly selected adults in the\nlarger NHANES dataset. Poverty is measured as a ratio of family income to poverty guidelines. Smaller\nnumbers indicate more poverty, and ratios of 5 or larger were recorded as 5. The Education variable indicates\nthe highest level of education achieved: either 8th grade, 9 - 11th grade, high school, some college, or college\ngrad.\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) 1.4555 0.2703 5.38 0.0000\nEducation9 - 11th Grade 0.9931 0.3302 3.01 0.0028\nEducationHigh School 1.0900 0.3113 3.50 0.0005\nEducationSome College 1.4943 0.2976 5.02 0.0000\nEducationCollege Grad 2.4948 0.2958 8.43 0.0000\nIn this model, the residual standard error is 1.46, the F-statistic is 28.09 with 4 and 456 degrees of\nfreedom;P(F4;456>28:09)<0:0001.\n(a) Write the estimated model equation.\n(b) Calculate the predicted poverty ratio for an individual who at most completed high school.\n(c) Interpret the estimated intercept value.\n(d) Interpret the slope coe ﬃcient for EducationCollege Grad , and describe the tested hypotheses that corre-\nspond to the p-value for this slope coe ﬃcient.\n(e) Assess whether educational level, overall, is associated with poverty. Be sure to include any relevant\nnumerical evidence as part of your answer.\n380 CHAPTER 7. MULTIPLE LINEAR REGRESSION\n7.17 Prison isolation experiment, Part IV. Exercise 7.15 used regression to examine the e ﬀect of three\ninterventions on prisoner MMPI scores. The response variable in the regression was trt.effect , the change\nin MMPI score (pre-intervention - post-intervention).\nInstead of estimating the intervention e ﬀect through the change in scores, suppose one is interested in\npredicting a post-intervention score based on the pre-intervention score for an individual and a particular\nintervention.\n(a) The table below shows an alternative regression model that can be ﬁt to the data. In this model, the\nresponse variable is the post-intervention MMPI value ( post , not shown explicitly in the table) and the\npredictors are the pre-intervention score ( pre) and the treatment, coded as in problem 7.15.\nWrite the estimated equation for this model.\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) 28.4053 12.2949 2.31 0.0264\npre 0.6593 0.1628 4.05 0.0002\ntreatmentNeutral -5.7307 3.5545 -1.61 0.1152\ntreatmentTherapeutic -9.7450 3.5540 -2.74 0.0093\n(b) In this model, describe in general terms the association of the pre-intervention and post-intervention\nscores.\n(c) Does the pre-intervention score appear to be an important predictor of a post intervention score?\n(d) What is the predicted post-intervention score for an individual with a pre-intervention score of 73 and\nreceiving no tape after the isolation?\n(e) Explain the interpretation of the coe ﬃcients for coe ﬃcient of treatmentNeutral . Is there strong statistical\nevidence that it is an important predictor?\n7.18 Resilience, Part I. The American Psychological Association deﬁnes resilience as \"the process of adapt-\ning well in the face of adversity, trauma, tragedy, threats, or even signiﬁcant sources of stress\". Studies have\nsuggested that resilience is an important factor in contributing to how medical students perceive their quality\nof life and educational environment.\nSurvey data were collected from 1,350 students across 25 medical schools. At each school, 54 students\nwere randomly selected to participate in the study. Participants completed questionnaires measuring re-\nsilience, quality of life, perception of educational environment, depression symptoms, and anxiety symptoms.\nThe following regression model was ﬁt to analyze the relationship between resilience and depressive\nsymptoms. Resilience was categorized as: very low, low, moderately low, moderately high, high, and very high.\nDepressive symptoms were measured on a scale of 0 to 63 points, with higher scores indicating either more\nnumerous or more severe depressive symptoms; this questionnaire is called the Beck Depression Inventory\n(BDI).\nIn this model, the residual standard error is 5.867, the F-statistic is 118.1 with 5 and 1344 degrees of\nfreedom;P(F5;1344>118:1)<0:0001.\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) 4.9754 0.4118 12.08 0.0000\nresHigh 2.2936 0.4987 4.60 0.0000\nresModHigh 4.3005 0.5181 8.30 0.0000\nresModLow 6.7108 0.5938 11.30 0.0000\nresLow 9.6538 0.7458 12.94 0.0000\nresVeryLow 15.6453 0.7518 20.81 0.0000\n(a) Describe the overall trend in language accessible to someone who has not taken a statistics course.\n(b) Does the intercept have a meaningful interpretation? Explain your answer.\n(c) Compare the predicted mean BDI score for someone with low resilience to that of someone with very low\nresilience.\n(d) + (e) Continue to the next page for parts (d) and (e).\n7.11. EXERCISES 381\n(d) Assess whether level of resilience, overall, is associated with depressive symptoms as measured by BDI\nscore. Be sure to include any relevant numerical evidence as part of your answer.\n(e) A model was ﬁtted predicting BDI score from resilience, with the categories numerically coded from 1to\n6, with 1being very high resilience and 6being very low resilience. This model has a single slope estimate\nof 2.76 with p-value < 0.0001.\ni. Using this model, compare the predicted mean BDI score for someone with low resilience to that of\nsomeone with very low resilience. Compare this answer to the one from part (c).\nii. What does this model imply about the change in mean BDI score between groups?\niii. Explain why this model is ﬂawed.\n7.11.6 Reanalyzing the PREVEND data\nThere are not currently exercises available for this section.\n7.11.7 Interaction in regression\n7.19 Prison isolation experiment, Part V. Exercise 7.17 used regression to predict a post-intervention score\nbased on pre-intervention score and a particular intervention.\nThe following table shows a model incorporating interaction between pre-intervention score and inter-\nvention.\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) -17.5790 17.7090 -0.99 0.3275\npre 1.2813 0.2376 5.39 0.0000\ntreatmentNeutral 67.7518 30.6168 2.21 0.0333\ntreatmentTherapeutic 64.4183 24.2124 2.66 0.0116\npre:treatmentNeutral -0.9890 0.4082 -2.42 0.0206\npre:treatmentTherapeutic -1.0080 0.3266 -3.09 0.0039\n(a) Write the model equation.\n(b) Interpret the model coe ﬃcients.\n(c) Write a separate model equation for each intervention group.\n(d) Do these data suggest that there is a statistically signiﬁcant di ﬀerence in association between pre- and\npost-intervention scores by treatment group? Explain your answer.\n7.20 Vitamin D. A study was conducted to evaluate Vitamin D status among schoolchildren in Thailand.\nExposure to sunlight allows the body to produce serum 25( OH)D, which is a marker of Vitamin D status;\nserum level is measured in units of nmol/L and having serum level below 50 nmol/L is indicative of Vitamin\nD deﬁciency. The following model was ﬁt to predict serum 25( OH)Dlevel from age, sex, and their interaction.\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) 97.7709 4.7732 20.48 0.0000\nage -3.0156 0.4774 -6.32 0.0000\nsexM -16.2848 7.0740 -2.30 0.0217\nage:sexM 2.9369 0.7054 4.16 0.0000\n(a) Write the model equation.\n(b) Interpret the model coe ﬃcients.\n(c) Is there statistically signiﬁcant evidence that the association between serum 25( OH)Dlevel and age di ﬀers\nby sex? Explain your answer.\n382 CHAPTER 7. MULTIPLE LINEAR REGRESSION\n7.21 PREVEND, Part III. Exercise 7.1 showed a multiple regression model predicting RFFT score from statin\nuse and age. For this problem, an interaction term is added between statin use and age.\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) 140.2031 5.6209 24.94 0.0000\nStatin -13.9720 15.0113 -0.93 0.3524\nAge -1.3149 0.1040 -12.65 0.0000\nStatin:Age 0.2474 0.2468 1.00 0.3166\n(a) Write the model equation.\n(b) Interpret the model coe ﬃcients.\n(c) Is there statistically signiﬁcant evidence that the association between RFFT score and age di ﬀers by\nwhether someone is a statin user? Explain your answer.\n7.22 Antibiotic consumption, Part I. Antibiotic resistance represents a major public health challenge.\nOveruse of antibiotics in clinical settings is thought to be a major contributor to increased antibiotic resis-\ntance. A study was conducted across several regions in China to investigate the impact of a 2011 law pro-\nhibiting over-the-counter (OTC) sales of antibiotics in private pharmacies. The study team collected data on\naverage monthly antibiotic consumption in 621 counties, in addition to information on socioeconomic deter-\nminants such as percentage of population illiterate.\nThe following model was ﬁt to investigate whether the relationship between monthly antibiotic con-\nsumption and percentage of population (over 25 years of age) with an advanced degree di ﬀers between coun-\nties that are located in a metropolitan area and those that are not.\nEstimate Std. Error t value Pr( >jtj)\n(Intercept) 0.8482 0.3311 2.56 0.0107\nmetroYes 2.3035 0.9612 2.40 0.0169\nedu 0.5711 0.0319 17.90 0.0000\nmetroYes:edu -0.1838 0.0752 -2.44 0.0148\n(a) Interpret the model coe ﬃcients, including any relevant inferential results.\n(b) Make a prediction of average monthly antibiotic consumption for a county in a metropolitan area where\n10% of the population over 25 years old has an advanced degree.\n7.11.8 Model selection for explanatory variables\n7.23 Baby weights, Part VII. Suppose the starting point for model selection for the birth weight data were\nthe full model, with all variables. The table below shows the adjusted R2for the full model as well as the\nadjustedR2values for all models with one fewer predictor variable. Based on examining the table from\nExercise 7.5 and the following table, identify which variable, if any, should be removed from the model ﬁrst.\nExplain your answer.\nModel Adjusted R2\n1 Full model 0.2541\n2 No gestation 0.1031\n3 No parity 0.2492\n4 No age 0.2547\n5 No height 0.2311\n6 No weight 0.2536\n7 No smoking status 0.2072\n7.11. EXERCISES 383\n7.24 Absenteeism, Part II. Suppose the starting point for model selection for the absenteeism data were the\nfull model, with all variables. The table below shows the adjusted R2for the full model as well as the adjusted\nR2values for all models with one fewer predictor variable. Based on examining the table from Exercise 7.6\nand the following table, identify which variable, if any, should be removed from the model ﬁrst. Explain your\nanswer.\nModel Adjusted R2\n1 Full model 0.0701\n2 No ethnicity -0.0033\n3 No sex 0.0676\n4 No learner status 0.0723\n7.25 Baby weights, Part VIII. Exercise 7.5 shows a regression model for predicting the average birth weight\nof babies based on all variables included in the dataset: length of pregnancy in days ( gestation ), mother’s age\nin years ( age), mother’s height in inches ( height ), and mother’s pregnancy weight in pounds ( weight ).\nThe following plots show the relationships between the response and the numerical predictor variables,\nin addition to the relationships between the response and two categorical predictor variables.\nnot first born first born6080100120140160180Birthweight (oz)\nBirth Ordernonsmoker smoker6080100120140160180Birthweight (oz)\nMother Smoking Status\nbwt\n150200250300350\n55606570\n60100160150250350\ngestation\nage\n152535455565\nheight6080100120140160180\n15202530354045\n100 200100150200250\nweight\n(a) Examine the relationship between the response variable and the predictor variables. Describe what you\nsee. Which predictor variables seem like they would be useful to include in an initial model?\n(b) Identify any predictors that seem related to each other.\n384 CHAPTER 7. MULTIPLE LINEAR REGRESSION\n7.26 Antibiotic consumption, Part II. Exercise 7.22 introduced a study conducted about antibiotic con-\nsumption in China. One aim of the study was to develop a prediction model for predicting monthly aver-\nage antibiotic consumption based on county-level data. The following plots show the association between\nmonthly average antibiotic consumption and four potential predictor variables: proportion female inhabi-\ntants ( female ), average life expectancy in years ( lifeexp ), proportion of population illiterate ( illiterate ),\nand population density in 1,000 people / km2(popdensity ).\n4648505254010203040Consumption vs Female\nPercentage Female PopConsumption (DID)\n72737475767778010203040Consumption vs Life Expectancy\nLife Expectancy (yrs)Consumption (DID)\n5 10 15010203040Consumption vs Percent Illiterate\nPercentage IlliterateConsumption (DID)\n024681012010203040Consumption vs Pop Density\nDensity (1,000 people per sq km)Consumption (DID)\n(a) Summarize what you see.\n(b) Identify any predictor variables that might beneﬁt from a natural log transformation and brieﬂy justify\nyour choices.\n7.11. EXERCISES 385\n7.11.9 The connection between ANOVA and regression\n7.27 Prison isolation experiment, Part VI. Problem 7.15 used a regression model to examine the e ﬀect of\nthe interventions on possibly reducing psychopathic deviant T scores on prisoners. The regression model is\nshown in the problem statement.\n(a) The value of the F-statistic is 3.33 with 2 and 39 degrees of freedom and P(F2;39>3:33) = 0:0461. In terms\nof the variables in the regression model, state the null hypothesis that corresponds to the F-statistic.\n(b) Describe the relationship between the coe ﬃcients from the linear model and the usual summary statistics\nfor the three sets of di ﬀerence scores.\n(c) Explain why the null hypothesis in this regression model is equivalent to the null hypothesis when these\ndata were analyzed using ANOV A in Problem 5.47.\n(d) Explain whether the assumptions for this regression model di ﬀer from those used in ANOV A.\n7.28 Resilience, Part II. Exercise 7.18 shows a regression model for the association of BDI score with re-\nsilience level.\n(a) In terms of the variables in the regression model, state the null hypothesis that corresponds to the F-\nstatistic.\n(b) Describe the relationship between the coe ﬃcients from the linear model and the usual summary statistics\nfor the six sets of BDI scores.\n(c) Explain why the null hypothesis used in this regression model is equivalent to the null hypothesis that\nwould be used if these data were analyzed with an ANOV A approach.\n386\nChapter 8\nInference for\ncategorical data\n8.1 Inference for a single proportion\n8.2 Inference for the difference of two proportions\n8.3 Inference for two or more groups\n8.4 Chi-square tests for the ﬁt of a distribution\n8.5 Outcome-based sampling: case-control studies\n8.6 Notes\n8.7 Exercises\n387\nPrevious chapters discussed methods of inference for numerical data; in this chap-\nter, those methods are extended to categorical data, such as binomial proportions\nor data in two-way tables. While various details of the methods may change, such\nas the calculations for a test statistic or the distributions used to ﬁnd a p-value, the\ncore ideas and principles behind inference remain the same.\nCategorical data arise frequently in medical research because disease outcomes\nand patient characteristics are often recorded in natural categories such as types\nof treatment received, whether or not disease advanced to a later stage, or whether\nor not a patient responded initially to a treatment. In the simplest settings, a\nbinary outcome (yes/no, success/failure, etc) is recorded for a single group of par-\nticipants, in hopes of learning more about the population from which the partici-\npants were drawn. The binomial distribution is often used for the statistical model\nin this setting, and inference about the binomial probability of success provides in-\nformation about a population proportion p. In more complex settings, participant\ncharacteristics are recorded in a categorical variable with two or more levels, and\nthe outcome or response variable itself has two or more levels. In these instances,\ndata are usually summarized in two-way tables with two or more rows and two or\nmore columns.\nAs with all methods of inference, it is important to understand how the data\nwere collected and whether the data may be viewed as a random sample from\na well-identiﬁed population, at least approximately. This issue is at least as im-\nportant as the formulas for test statistics and conﬁdence intervals, and is often\noverlooked.\nBe careful about the notation in this chapter—since pis the standard nota-\ntion for a population proportion and for a probability, pdoes double duty in this\nchapter as a population parameter and signiﬁcance level.\nFor labs, slides, and other resources, please visit\nwww.openintro.org/book/biostat\n388 CHAPTER 8. INFERENCE FOR CATEGORICAL DATA\n8.1 Inference for a single proportion\nAdvanced melanoma is an aggressive form of skin cancer that until recently was almost uni-\nformly fatal. In rare instances, a patient’s melanoma stopped progressing or disappeared altogether\nwhen the patient’s immune system successfully mounted a response to the cancer. Those observa-\ntions led to research into therapies that might trigger an immune response in cancer. Some of the\nmost notable successes have been in melanoma, particularly with two new therapies, nivolumab\nand ipilimumab.1\nA 2013 report in the New England Journal of Medicine by Wolchok et al. reported the results\nof a study in which patients were treated with both nivolumab and ipilimumab.2Fifty-three pa-\ntients were given the new regimens concurrently, and the response to therapy could be evaluated in\n52 of the 53. Of the 52 evaluable patients, 21 (40%) experienced a response according to commonly\naccepted criteria. In previous studies, the proportion of patients responding to one of these agents\nwas 30% or less. How might one compare the new data to past results?\nThe data from this study are binomial data, with success deﬁned as a response to therapy.\nSuppose the number of patients who respond in a study like this is represented by the random\nvariableX, whereXis binomial with parameters n(the number of trials, where each trial is rep-\nresented by a patient) and p(the unknown population proportion of response). From formulas\ndiscussed in Chapter 3, the mean of Xisnpand the standard deviation of Xisp\nnp(1\u0000p).\nInference about pis based on the sample proportion ˆp, where ˆp=X=n. In this case, ˆp= 21=52 =\n0:404. If the sample proportion is nearly normally distributed, the normal approximation to the\nbinomial distribution can be used to conduct inference; this method is commonly used. When X\ndoes not have an approximately normal distribution, exact inference can based on the binomial\ndistribution for X. Both the normal approximation and exact methods are covered in this chapter.\n8.1.1 Inference using the normal approximation\nAsample proportion can be described as a sample mean. If each success in the melanoma\ndata is represented as a 1and each failure as a 0, then the sample proportion is the mean of the 52\nnumerical outcomes:\nˆp=0 + 1 + 1 +\u0001\u0001\u0001+ 0\n52= 0:404:\nThe distribution of ˆpis nearly normal when the distribution of successes and failures is not too\nstrongly skewed.\n1The -mab su ﬃx in these therapies stands for monoclonal antibody, a therapeutic agent made by identical immune cells\nthat are all clones of a unique parent cell from a patient.\n2N Engl J Med 2013;369:122-33. DOI: 10.1056/NEJMoa1302369\n8.1. INFERENCE FOR A SINGLE PROPORTION 389\nCONDITIONS FOR THE SAMPLING DISTRIBUTION OF ˆpˆpˆpBEING NEARLY NORMAL\nThe sampling distribution for ˆp, calculated from a sample of size nfrom a population with a\nsuccess proportion p, is nearly normal when\n1. the sample observations are independent and\n2. at least 10 successes and 10 failures are expected in the sample, i.e. np\u001510 andn(1\u0000p)\u0015\n10. This is called the success-failure condition .\nIf these conditions are met, then the sampling distribution of ˆpis approximately normal with\nmeanpand standard error\nSEˆp=r\np(1\u0000p)\nn: (8.1)ˆp\nsample\nproportion\np\npopulation\nproportion\nWhen conducting inference, the population proportion pis unknown. Thus, to construct a\nconﬁdence interval, the sample proportion ˆpcan be substituted for pto check the success-failure\ncondition and compute the standard error. In a hypothesis test, p0is substituted for p.\nConﬁdence intervals for a proportion\nWhen using the normal approximation to the sampling distribution of ˆp, a conﬁdence interval for\na proportion has the same structure as a conﬁdence interval for a mean; it is centered at the point\nestimate, with a margin of error calculated from the standard error and appropriate z?value. The\nformula for a 95% conﬁdence interval is\nˆp\u00061:96r\nˆp(1\u0000ˆp)\nn:\nEXAMPLE 8.2\nUsing the normal approximation, construct an approximate 95% conﬁdence interval for the re-\nsponse probability for patients with advanced melanoma who were administered the combination\nof nivolumab and ipilimumab.\nThe independence and success-failure assumptions should be checked ﬁrst. Since the outcome of\none patient is unlikely to inﬂuence that of other patients, the observations are independent. The\nsuccess-failure condition is satisﬁed since nˆp= (52)(:404) = 21>10 andnˆp(1\u0000ˆp) = (52)(:596) =\n31>10.\nThe point estimate for the response probability, based on a sample of size n= 52, is ˆp= 0:404. For a\n95% conﬁdence interval, z?= 1:96. The standard error is estimated as:q\nˆp(1\u0000ˆp)\nn=q\n(0:404)(1\u00000:404)\n52=\n0:068. The conﬁdence interval is\n0:404\u00061:96(0:068)!(0:27;0:54)\nThe approximate 95% conﬁdence interval for p, the population response probability of melanoma\npatients to the combination of these new drugs, is (0.27, 0.54) or (27%, 54%).\n390 CHAPTER 8. INFERENCE FOR CATEGORICAL DATA\nGUIDED PRACTICE 8.3\nIn New York City on October 23rd, 2014, a doctor who had recently been treating Ebola patients in\nGuinea went to the hospital with a slight fever and was subsequently diagnosed with Ebola. Soon\nafter, a survey conducted by the Marist Poll, an organization with a carefully designed methodology\nfor drawing random samples from identiﬁed populations, found that 82% of New Yorkers favored\na \"mandatory 21-day quarantine for anyone who has come in contact with an Ebola patient.\"3a)\nVerify that the sampling distribution of ˆpis nearly normal. b) Construct a 95% conﬁdence interval\nforp, the proportion of New York adults who supported a quarantine for anyone who has come\ninto contact with an Ebola patient.4\nDid the participants in the melanoma trial constitute a random sample? Patients who partici-\npate in clinical trials are unlikely to be a random sample of patients with the disease under study\nsince the patients or their physicians must be aware of the trial, and patients must be well enough\nto travel to a major medical center and be willing to receive an experimental therapy that may have\nserious side e ﬀects.\nInvestigators in the melanoma trial were aware that the observed proportion of patients re-\nsponding in a clinical trial may be di ﬀerent than the hypothetical response probability in the pop-\nulation of patients with advanced melanoma. Study teams try to minimize these systematic dif-\nferences by following strict speciﬁcations for deciding whether patients are eligible for a study.\nHowever, there is no guarantee that the results observed in a sample will be replicated in the gen-\neral population.\nSmall, initial studies in which there is no control group, like the one described here, are early\nsteps in exploring the value of a new therapy and are used to justify further study of a treatment\nwhen the results are substantially di ﬀerent than expected. The largest observed response rate in\nprevious trials of 30% was close to the lower bound of the conﬁdence interval from the study\n(27%, 54%), so the results were considered adequate justiﬁcation for continued research on this\ntreatment.\nHypothesis testing for a proportion\nJust as with inference for population means, conﬁdence intervals for population proportions can\nbe used when deciding whether to reject a null hypothesis. It is useful in most settings, however, to\ncalculate the p-value for a test as a measure of the strength of the evidence contradicting the null\nhypothesis.\nWhen using the normal approximation for the distribution of ˆpto conduct a hypothesis test,\none should always verify that ˆpis nearly normal under H0by checking the independence and\nsuccess-failure conditions. Since a hypothesis test is based on the distribution of the test statistic\nunder the null hypothesis, the success-failure condition is checked using the null proportion p0,\nnot the estimate ˆp.\nAccording to the normal approximation to the binomial distribution, the number of successes\ninntrials is normally distributed with mean np0and standard deviationp\nnp(1\u0000p0). This approx-\nimation is valid when np0andn(1\u0000p0) are both at least 10.5\n3Poll ID NY141026 on maristpoll.marist.edu.\n4a) The poll is based on a simple random sample and consists of fewer than 10% of the adult population of New York,\nwhich makes independence a reasonable assumption. The success-failure condition is satisﬁed since, 1042(0 :82)>5 and\n1042(1\u00000:82)>5. b) 0:82\u00061:96q\n0:82(1\u00000:82)\n1042!(0:796;0:844).\n5The normal approximation to the binomial distribution was discussed in Section 3.2 of Chapter 3.\n8.1. INFERENCE FOR A SINGLE PROPORTION 391\nUnder the null hypothesis, the sample proportion ˆp=X=n is approximately distributed as\nN0\nBBBBB@p0;r\np0(1\u0000p0)\nn1\nCCCCCA:\nThe test statistic zfor the null hypothesis H0:p=p0based on a sample of size nis\nz=point estimate - null value\nSE\n=ˆp\u0000p0q\n(p0)(1\u0000p0)\nn:\nEXAMPLE 8.4\nSuppose that out of a cohort of 120 patients with stage 1 lung cancer at the Dana-Farber Cancer\nInstitute (DFCI) treated with a new surgical approach, 80 of the patients survive at least 5 years,\nand suppose that National Cancer Institute statistics indicate that the 5-year survival probability\nfor stage 1 lung cancer patients nationally is 0.60. Do the data collected from 120 patients support\nthe claim that the DFCI population treated with this new form of surgery has a di ﬀerent 5-year\nsurvival probability than the national population? Let \u000b= 0:10, since this is an early study of the\nnew surgery.\nTest the hypothesis H0:p= 0:60 versus the alternative, HA:p,0:60, using\u000b= 0:10. If we assume\nthat the outcome of one patient at DFCI does not inﬂuence the outcome of other patients, the\nindependence condition is met, and the success-failure condition is satisﬁed since (120)(0 :60) =\n80>5 and (120)(1\u00000:60) = 40>5:The test statistic is the z-score of the point estimate:\nz=point estimate - null value\nSE=0:67\u00000:60q\n(0:60)(1\u00000:60)\n120= 1:57:\nThep-value is the probability that a standard normal variable is larger than 1.57 or smaller than\n-1.57,P(jZj>1:57) = 0:12 ; since the p-value is greater than 0.10, there is insu ﬃcient evidence to\nrejectH0in favor ofHA. There is not convincing evidence that the survival probability at DFCI\ndiﬀers from the national survival probability. Had a more traditional 0.05 signiﬁcance level been\nused, the data would be even less convincing.\nEXAMPLE 8.5\nUsing the data from the study in advanced melanoma, use the normal approximation to the sam-\npling distribution of ˆpto test the null hypothesis that the response probability to the novel com-\nbined therapy is 30% against a one-sided alternative that the response proportion is greater than\n30%. Let\u000b= 0:10.\nThe test statistic has value\nz= (0:404\u00000:30)=p\n(0:30)(0:70)=52 = 1:64:\nThe one-sided p-value isP(Z\u00151:64) = 0:05; there is su ﬃcient evidence to reject the null hypothesis\nat\u000b= 0:10. This is an example of where a two-sided test and a one-sided test yield di ﬀerent\nconclusions.\n392 CHAPTER 8. INFERENCE FOR CATEGORICAL DATA\nGUIDED PRACTICE 8.6\nOne of the questions on the National Health and Nutrition Examination Survey (introduced in\nChapter 5) asked participants whether they participated in moderate or vigorous intensity sports,\nﬁtness, or recreational activities. In a random sample of 135 adults, 76 answered \"Yes\" to the\nquestion. Based on this evidence, are a majority of American adults physically active?6\n8.1.2 Inference using exact methods\nWhen the normal approximation to the distribution of ˆpmay not be accurate, inference is\nbased on exact binomial probabilities. Calculating conﬁdence intervals and p-values based on the\nbinomial distribution can be done by hand, with tables of the binomial distribution, or (more easily\nand accurately) with statistical software. The logic behind computing a p-value is discussed here,\nbut the formulas for a conﬁdence interval are complicated and are not shown.\nThep-value for a hypothesis test corresponds to the sum of the probabilities of all events\nthat are as or more extreme than the sample result. Let Xbe a binomial random variable with\nparameters nandp0, where ˆp=x=nandxis the observed number of events. If ˆp\u0014p0, then the\none-tail probability equals P(X\u0014x); if ˆp>p 0, then the one-tail probability equals P(X\u0015x). These\nprobabilities are calculated using the approaches from Chapter 3. Two-tailed probabilities are\ncalculated by doubling the appropriate one-tailed value.\nEXAMPLE 8.7\nIn 2009, the FDA Oncology Drug Advisory Committee (ODAC) recommended that the drug\nAvastin be approved for use in glioblastoma, a form of brain cancer. Tumor shrinkage after tak-\ning a drug is called a response; out of 85 patients, 24 exhibited a response. Historically, response\nprobabilities for brain cancer drugs were approximately 0.05, or about 5%. Assess whether there\nis evidence that the response probability for Avastin is di ﬀerent from previous drugs.\nH0:p= 0:05;HA:p,0:05. Let\u000b= 0:05.\nThe independence condition is satisﬁed, but the success-failure condition is not, since np0=\n(85)(0:05) = 4:25<5, so this is a setting where exact binomial probabilities should be used to\ncalculate ap-value.\nThe sample proportion ˆpequalsx=n= 24=85 = 0:28. Since ˆp>p 0, calculate the two-sided p-value\nfrom 2\u0002P(X\u001524), whereX\u0018Binom(85;0:05).\nCalculating the p-value is best done in software; the Rcommand pbinom returns a value of 5 :3486\u0002\n10\u000012.7\nThep-value is highly signiﬁcant and suggests that the response probability for Avastin is higher\nthan for previous brain cancer drugs. The FDA sta ﬀconsidered this evidence su ﬃciently strong to\njustify approval for the use of the drug, even though the FDA normally requires evidence from two\nindependently conducted randomized trials.\n6The observations are independent. Check success-failure: np0=n(1\u0000p0) = 135(0:5)>10.H0:p= 0:5;HA:p>0:5.\nCalculate the z-score:z=0:56\u00000:50q\n0:5(1\u00000:5)\n135= 1:39. Thep-value is 0.08. Since the p-value is larger than 0.05, there is insu ﬃcient\nevidence to reject H0; there is not convincing evidence that a majority of Americans are physically active, although the data\nsuggest that may be the case.\n72*pbinom(q = 23, size = 85, p = 0.05, lower.tail = FALSE)\n8.1. INFERENCE FOR A SINGLE PROPORTION 393\nGUIDED PRACTICE 8.8\nMedical consultants assist patients with all aspects of an organ donation surgery, with the goal of\nreducing the possibility of complications during the medical procedure and recovery. To attract\ncustomers, one consultant noted that while the usual proportion of complications in liver donation\nsurgeries in the United States is about 10%, only 3 out of her 62 clients experienced complications\nwith liver donor surgeries. Is there evidence to suggest that the proportion of complications in her\npatients is lower than the national average?8\n8.1.3 Choosing a sample size when estimating a proportion\nWhenever possible, a sample size for a study should be estimated before data collection be-\ngins. Section 5.4 explored the calculation of sample sizes that allow a hypothesis test comparing\ntwo groups to have adequate power. When estimating a proportion, preliminary sample size cal-\nculations are often done to estimate a sample size large enough to make the margin of error min\na conﬁdence interval su ﬃciently small for the interval to be useful. Recall that the margin of error\nmis the term that is added to and subtracted from the point estimate. Statistically, this means\nestimating a sample size nso that the sample proportion is within some margin of error mof the\nactual proportion with a certain level of conﬁdence. When the normal approximation is used for a\nbinomial proportion, a sample size su ﬃciently large to have a margin of error of mwill satisfy\nm= (z?)(s.e.( ˆp)) =z?r\n(p)(1\u0000p)\nn:\nAlgebra can be used to show that the above equation implies\nn=(z?)2(p)(1\u0000p)\nm2:\nIn some settings a preliminary estimate for pcan be used to calculate n. When no estimate is\navailable, calculus can be used to show that p(1\u0000p) has its largest value when p= 0:50, and that\nconservative value for pis often used to ensure that nis suﬃciently large regardless of the value of\nthe unknown population proportion p. In that case, nsatisﬁes\nn\u0015(z?)2(0:50)(1\u00000:50)\nm2=(z?)2\n4m2:\n8Assume that the 62 patients in her dataset may be viewed as a random sample from patients receiving a donated liver.\nThe sample proportion ˆp= 3=62 = 0:048. Under the null hypothesis, the expected number of complications is 62(0 :10) = 6:2,\nso the normal approximation may not be accurate and it is best to use exact binomial probabilities. Since ˆp\u0014p0, ﬁnd thep-\nvalue by calculating P(X\u00143) whenXhas a binomial distribution with parameters n= 62;p= 0:10:P(X\u00143) = 0:121. There\nis not su ﬃcient evidence to suggest that the proportion of complications among her patients is lower than the national\naverage.\n394 CHAPTER 8. INFERENCE FOR CATEGORICAL DATA\nEXAMPLE 8.9\nDonor organs for organ transplant are scarce. Studies are conducted to explore whether the pop-\nulation of eligible organs can be expanded. Suppose a research team is studying the possibility\nof transplanting lungs from hepatitis C positive individuals; recipients can be treated with one of\nthe new drugs that cures hepatitis C. Preliminary studies in organ transplant are often designed\nto estimate the probability of a successful organ graft 6 months after the transplant. How large\nshould a study be so that the 95% conﬁdence interval for the probability of a successful graft at 6\nmonths is no wider than 20%?\nA conﬁdence interval no wider than 20% has a margin of error of 10%, or 0.10. Using the conser-\nvative value p = 0.50,\nn=(1:96)2\n(4)(0:102)= 96:04:\nSample sizes are always rounded up, so the study should have 97 patients.\nSince the study will likely yield a value ˆpdiﬀerent from 0.50, the ﬁnal margin of error will be\nsmaller than\u00060:10.\nWhen the conﬁdence coe ﬃcient is 95%, 1.96 can replaced by 2 and the sample size formula\nreduces to\nn= 1=m2:\nThis remarkably simple formula is often used by practitioners for a quick estimate of sample size.\nGUIDED PRACTICE 8.10\nA 2015 estimate of Congress’ approval rating was 19%.9Using this estimate, how large should an\nadditional survey be to produce a margin of error of 0.04 with 95% conﬁdence?10\n9www.gallup.com/poll/183128/ﬁve-months-gop-congress-approval-remains-low.aspx\n10Apply the formula\n1:96\u0002r\np(1\u0000p)\nn\u00191:96\u0002r\n0:19(1\u00000:19)\nn\u00140:04!n\u0015369:5:\nA sample size of 370 or more would be reasonable.\n8.2. INFERENCE FOR THE DIFFERENCE OF TWO PROPORTIONS 395\n8.2 Inference for the difference of two proportions\nJust as inference can be done for the di ﬀerence of two population means, conclusions can also\nbe drawn about the di ﬀerence of two population proportions: p1\u0000p2.\n8.2.1 Sampling distribution of the difference of two proportions\nThe normal model can be applied to ˆp1\u0000ˆp2if the sampling distribution for each sample\nproportion is nearly normal and if the samples are independent random samples from the relevant\npopulations.\nCONDITIONS FOR THE SAMPLING DISTRIBUTION OF ˆP1\u0000ˆP2TO BE APPROXIMATELY NORMAL\nThe diﬀerence ˆp1\u0000ˆp2tends to follow a normal model when\n– each of the two samples are random samples from a population,\n– the two samples are independent of each other, and\n– each sample proportion follows (approximately) a normal model. This condition is sat-\nisﬁed when n1p1;n1(1\u0000p1);n2p2andn2(1\u0000p2) are all\u001510.\nThe standard error of the di ﬀerence in sample proportions is\nSEˆp1\u0000ˆp2=q\nSE2\nˆp1+SE2\nˆp2=s\np1(1\u0000p1)\nn1+p2(1\u0000p2)\nn2; (8.11)\nwherep1andp2are the population proportions, and n1andn2are the two sample sizes.\n396 CHAPTER 8. INFERENCE FOR CATEGORICAL DATA\n8.2.2 Conﬁdence intervals for p1\u0000p2p1\u0000p2p1\u0000p2\nWhen calculating conﬁdence intervals for a di ﬀerence of two proportions using the normal\napproximation to the binomial, the two sample proportions are used to verify the success-failure\ncondition and to compute the standard error.\nEXAMPLE 8.12\nThe way a question is phrased can inﬂuence a person’s response. For example, Pew Research Center\nconducted a survey with the following question:11\nAs you may know, by 2014 nearly all Americans will be required to have health insur-\nance. [People who do not buy insurance will pay a penalty] while [People who cannot\naﬀord it will receive ﬁnancial help from the government]. Do you approve or disap-\nprove of this policy?\nFor each randomly sampled respondent, the statements in brackets were randomized: either they\nwere kept in the order given above, or the order of the two statements was reversed. Figure 8.1\nshows the results of this experiment. Calculate and interpret a 90% conﬁdence interval of the\ndiﬀerence in the probability of approval of the policy.\nFirst the conditions for the use of a normal model must be veriﬁed. The Pew Research Center uses\nsampling methods that produce random samples of the US population (at least approximately) and\nbecause each group was a simple random sample from less than 10% of the population, the obser-\nvations are independent, both within the samples and between the samples. The success-failure\ncondition also holds for each sample, so the normal model can be used for conﬁdence intervals for\nthe diﬀerence in approval proportions. The point estimate of the di ﬀerence in support, where ˆp1\ncorresponds to the original ordering and ˆp2to the reversed ordering:\nˆp1\u0000ˆp2= 0:47\u00000:34 = 0:13:\nThe standard error can be computed from Equation (8.11) using the sample proportions:\nSE\u0019r\n0:47(1\u00000:47)\n771+0:34(1\u00000:34)\n732= 0:025:\nFor a 90% conﬁdence interval, z?= 1:65:\npoint estimate\u0006z?\u0002SE! 0:13\u00061:65\u00020:025! (0:09;0:17):\nWith 90% conﬁdence, the proportion approving the 2010 health care law ranged between 9% and\n17% depending on the phrasing of the question. The Pew Research Center interpreted this mod-\nestly large di ﬀerence as an indication that for most of the public, opinions were still ﬂuid on the\nhealth insurance mandate. The law eventually passed as the A ﬀordable Health Care Act (ACA).\nSample size ( ni) Approve (%) Disapprove (%) Other\nOriginal ordering 771 47 49 3\nReversed ordering 732 34 63 3\nFigure 8.1: Results for a Pew Research Center poll where the ordering of two\nstatements in a question regarding healthcare were randomized.\n11www.people-press.org/2012/03/26/public-remains-split-on-health-care-bill-opposed-to-mandate. Sample sizes for\neach polling group are approximate.\n8.2. INFERENCE FOR THE DIFFERENCE OF TWO PROPORTIONS 397\n8.2.3 Hypothesis testing for p1\u0000p2p1\u0000p2p1\u0000p2\nHypothesis tests for p1\u0000p2are usually testing the null hypothesis of no di ﬀerence between p1\nandp2; i.e.H0:p1\u0000p2= 0. Under the null hypothesis, ˆp1\u0000ˆp2is normally distributed with mean 0\nand standard deviationq\np(1\u0000p)(1\nn1+1\nn2), where under the null hypothesis p=p1=p2.\nSincepis unknown, an estimate is used to compute the standard error of ˆp1\u0000ˆp2;pcan be\nestimated by ˆp, the weighted average of the sample proportions ˆp1and ˆp2:\nˆp=n1ˆp1+n2ˆp2\nn1+n2=x1+x2\nn1+n2;\nwherex1is the number of observed events in the ﬁrst sample and x2is the number of observed\nevents in the second sample. This pooled proportion ˆpis also used to check the success-failure\ncondition.\nThe test statistic zfor testingH0:p1=p2versusHA:p1,p2equals:\nz=ˆp1\u0000ˆp2q\nˆp(1\u0000ˆp)\u00101\nn1+1\nn2\u0011:\n398 CHAPTER 8. INFERENCE FOR CATEGORICAL DATA\nEXAMPLE 8.13\nThe use of screening mammograms for breast cancer has been controversial for decades because\nthe overall beneﬁt on breast cancer mortality is uncertain. Several large randomized studies have\nbeen conducted in an attempt to estimate the e ﬀect of mammogram screening. A 30-year study\nto investigate the e ﬀectiveness of mammograms versus a standard non-mammogram breast cancer\nexam was conducted in Canada with 89,835 female participants.12During a 5-year screening\nperiod, each woman was randomized to either receive annual mammograms or standard physical\nexams for breast cancer. During the 25 years following the screening period, each woman was\nscreened for breast cancer according to the standard of care at her health care center.\nAt the end of the 25 year follow-up period, 1,005 women died from breast cancer. The results by\nintervention are summarized in Figure 8.2.\nAssess whether the normal model can be used to analyze the study results.\nSince the participants were randomly assigned to each group, the groups can be treated as inde-\npendent, and it is reasonable to assume independence of patients within each group. Participants\nin randomized studies are rarely random samples from a population, but the investigators in the\nCanadian trial recruited participants using a general publicity campaign, by sending personal in-\nvitation letters to women identiﬁed from general population lists, and through contacting family\ndoctors. In this study, the participants can reasonably be thought of as a random sample.\nThe pooled proportion ˆpis\nˆp=x1+x2\nn1+n2=500 + 505\n500 + 44;425 + 505 + 44 ;405= 0:0112:\nChecking the success-failure condition for each group:\nˆp\u0002nmgm = 0:0112\u000244,925 = 503 (1 \u0000ˆp)\u0002nmgm = 0:9888\u000244,925 = 44,422\nˆp\u0002nctrl= 0:0112\u000244,910 = 503 (1 \u0000ˆp)\u0002nctrl= 0:9888\u000244,910 = 44,407\nAll values are at least 10.\nThe normal model can be used to analyze the study results.\nDeath from breast cancer?\nYes No\nMammogram 500 44,425\nControl 505 44,405\nFigure 8.2: Summary results for the mammogram study.\n12Miller AB. 2014. Twenty ﬁve year follow-up for breast cancer incidence and mortality of the Canadian National Breast\nScreening Study: randomised screening trial . BMJ 2014;348:g366 doi: 10.1136/bmj.g366\n8.2. INFERENCE FOR THE DIFFERENCE OF TWO PROPORTIONS 399\nEXAMPLE 8.14\nDo the results from the study provide convincing evidence of a di ﬀerence in the proportion of\nbreast cancer deaths between women who had annual mammograms during the screening period\nversus women who received annual screening with physical exams?\nThe null hypothesis is that the probability of a breast cancer death is the same for the women in\nthe two groups. If group 1 represents the mammogram group and group 2 the control group,\nH0:p1=p2andHA:p1,p2. Let\u000b= 0:05.\nCalculate the test statistic z:\nz=0:01113\u00000:01125q\n(0:0112)(1\u00000:0112)\u00101\n44;925+1\n44;910\u0011=\u00000:17:\nThe two-sided p-value isPjZj\u00150:17 = 0:8650, which is greater than 0.05. There is insu ﬃcient\nevidence to reject the null hypothesis; the observed di ﬀerence in breast cancer death rates is rea-\nsonably explained by chance.\nEvaluating medical treatments typically requires accounting for additional evidence that cannot\nbe evaluated from a statistical test. For example, if mammograms are much more expensive than a\nstandard screening and do not o ﬀer clear beneﬁts, there is reason to recommend standard screen-\nings over mammograms. This study also found that a higher proportion of diagnosed breast cancer\ncases in the mammogram screening arm (3250 in the mammogram group vs 3133 in the physical\nexam group), despite the nearly equal number of breast cancer deaths. The investigators inferred\nthat mammograms may cause over-diagnosis of breast cancer, a phenomenon in which a breast\ncancer diagnosed with mammogram and subsequent biopsy may never become symptomatic. The\npossibility of over-diagnosis is one of the reasons mammogram screening remains controversial.\n400 CHAPTER 8. INFERENCE FOR CATEGORICAL DATA\nEXAMPLE 8.15\nCalculate a 95% conﬁdence interval for the di ﬀerence in proportions of deaths from breast cancer\nfrom the Canadian study.\nThe independence and random sampling conditions have already been discussed. The success fail-\nure condition should be checked for each sample, since this is not a hypothesis testing context (i.e.,\nthere is no null hypothesis). For the mammogram group, ˆp1= 0:01113;n1ˆp1= (0:1113)(44;925) =\n500 andn1(1\u0000ˆp1) = 39;925:It is easy to show that the success failure condition is holds for the\ncontrol group as well.\nThe point estimate for the di ﬀerence in the probability of death is\nˆp1\u0000ˆp2= 0:01113\u00000:01125 =\u00000:00012;\nor 0.012%.\nThe standard error for the estimated di ﬀerence uses the individual estimates of the probability of\na death:\nSE\u0019r\n0:01113(1\u00000:01113)\n44;925+0:01125(1\u00000:01125)\n44;910= 0:0007:\nThe 95% conﬁdence interval is given by\n\u00000:00012\u0006(1:96)(0:0007) = (\u00000:0015;0:0013):\nWith 95% conﬁdence, the di ﬀerence in the probability of death is between -0.15% and 0.13%. As\nexpected from the large p-value, the conﬁdence interval contains the null value 0.\n8.3. INFERENCE FOR TWO OR MORE GROUPS 401\n8.3 Inference for two or more groups\nThe comparison of the proportion of breast cancer deaths between the two groups can also\nbe approached using a two-way contingency table, which contains counts for combinations of\noutcomes for two variables. The results for the mammogram study in this format are shown in\nFigure 8.3.\nPreviously, the main question of interest was stated as, \"Is there evidence of a di ﬀerence in the\nproportion of breast cancer deaths between the two screening groups?\" If the probability of a death\nfrom breast cancer does not depend the method of screening, then screening method and outcome\nare independent. Thus, the question can be re-phrased: \"Is there evidence that screening method\nis associated with outcome?\"\nHypothesis testing in a two-way table assesses whether the two variables of interest are associ-\nated (i.e., not independent). The approach can be applied to settings with two or more groups and\nfor responses that have two or more categories. The observed number of counts in each table cell\nare compared to the number of expected counts , where the expected counts are calculated under\nthe assumption that the null hypothesis of no association is true. A \u001f2test of signiﬁcance is based\non the di ﬀerences between observed and expected values in the cells.\nDeath from BC Yes No Total\nMammogram 500 44,425 44,925\nControl 505 44,405 44,910\nTotal 1,005 88,830 89,835\nFigure 8.3: Results of the mammogram study, as a contingency table with\nmarginal totals.\nGUIDED PRACTICE 8.16\nFormulate hypotheses for a contingency-table approach to analyzing the mammogram data.13\n8.3.1 Expected counts\nIf type of breast cancer screening had no e ﬀect on outcome in the mammogram data, what\nwould the expected results be?\nRecall that if two events AandBare independent, then P(A\\B) =P(A)P(B). LetArepre-\nsent assignment to the mammogram group and Bthe event of death from breast cancer. Under\nindependence, the number of individuals out of 89,835 that are expected to be in the mammogram\nscreening group and die from breast cancer equals:\n(89;835)P(A)P(B) = (89;835)\u001244;925\n89;835\u0013\u00121;005\n89;835\u0013\n= 502:6:\nNote that the quantities 44,925 and 1,005 are the row and column totals corresponding to the\nupper left cell of Figure 8.3, and 89,835 is the total number nof observations in the table. A general\nformula for computing expected counts for any cell can be written from the marginal totals and\nthe total number of observations.\n13H0: There is no association between type of breast cancer screening and death from breast cancer. HA: There is an\nassociation between type of breast cancer screening and death from breast cancer.\n402 CHAPTER 8. INFERENCE FOR CATEGORICAL DATA\nCOMPUTING EXPECTED COUNTS IN A TWO-WAY TABLE\nTo calculate the expected count for the ithrow andjthcolumn, compute\nExpected Countrowi;colj=(rowitotal)\u0002(columnjtotal)\ntable total:\nEXAMPLE 8.17\nCalculate expected counts for the data in Figure 8.3.\nE1;1=44;925\u00021;005\n89;835= 502:6E1;2=44;925\u000288;830\n89;835= 44;422:4\nE2;1=2;922\u00021;005\n89;835= 502:4E2;2=7;078\u000288;830\n89;835= 44;407:6\nDeath from BC Yes No Total\nMammogram 500 (502.6) 44,425 (44,422.4) 44,925\nControl 505 (502.4) 44,405 (44,407.6) 44,910\nTotal 1,005 88,830 89,835\nFigure 8.4: Results of the mammogram study, with (expected counts) . The ex-\npected counts should also sum to the row and column totals; this can be a useful\ncheck for accuracy.\nEXAMPLE 8.18\nIf a newborn is HIV+, should he or she be treated with nevirapine (NVP) or a more expensive\ndrug, lopinarvir (LPV)? In this setting, success means preventing virologic failure; i.e., growth of\nthe virus. A randomized study was conducted to assess whether there is an association between\ntreatment and outcome.14Of the 147 children administered NVP, about 41% experienced virologic\nfailure; of the 140 children administered LPV, about 19% experienced virologic failure. Construct\na table of observed counts and a table of expected counts.\nConvert the proportions to count data: 41% of 147 is approximately 60, and 19% of 140 is approx-\nimately 27. The observed results are given in Figure 8.5.\nCalculate the expected counts for each cell:\nE1;1=87\u0002147\n287= 44:6E1;2=87\u0002140\n287= 42:4\nE2;1=200\u0002147\n287= 102:4E2;2=200\u0002140\n287= 97:6\nThe expected counts are summarized in Figure 8.6.\n14Violari A, et al. N Engl J Med 2012; 366:2380-2389 DOI: 10.1056/NEJMoa1113249\n8.3. INFERENCE FOR TWO OR MORE GROUPS 403\nNVP LPV Total\nVirologic Failure 60 27 87\nStable Disease 87 113 200\nTotal 147 140 287\nFigure 8.5: Observed counts for the HIV study.\nNVP LPV Total\nVirologic Failure 44.6 42.4 87\nStable Disease 102.4 97.6 200\nTotal 147 140 287\nFigure 8.6: Expected counts for the HIV study.\n8.3.2 The \u001f2\u001f2\u001f2test statistic\nPreviously, test statistics have been constructed by calculating the di ﬀerence between a point\nestimate and a null value, then dividing by the standard error of the point estimate to standardize\nthe diﬀerence. The \u001f2statistic is based on a di ﬀerent idea. In each cell of a table, the di ﬀerence\nobserved -expected is a measure of the discrepancy between what was observed in the data and what\nshould have been observed under the null hypothesis of no association. If the row and column\nvariables are highly associated, that di ﬀerence will be large. Two adjustments are made to the\ndiﬀerences before the ﬁnal statistic is calculated. First, since both positive and negative di ﬀerences\nsuggest a lack of independence, the di ﬀerences are squared to remove the e ﬀect of the sign. Second,\ncells with larger counts may have larger discrepancies by chance alone, so the squared di ﬀerences\nin each cell are scaled by the number expected in the cell under the hypothesis of independence.\nThe ﬁnal\u001f2statistic is the sum of these standardized squared di ﬀerences, where the sum has one\nterm for each cell in the table.\nThe\u001f2test statistic is calculated as:\n\u001f2\nchi-square\ntest statistic\u001f2=X\nall cells(observed\u0000expected)2\nexpected:\nThe theory behind the \u001f2test and its sampling distribution relies on the same normal ap-\nproximation to the binomial distribution that was introduced earlier. The cases in the dataset must\nbe independent and each expected cell count should be at least 10. The second condition can be\nrelaxed in tables with more than 4 cells.\nCONDITIONS FOR THE \u001f2\u001f2\u001f2TEST\nTwo conditions that must be checked before performing a \u001f2test:\nIndependence. Each case that contributes a count to the table must be independent of all the\nother cases in the table.\nSample size. Each expected cell count must be greater than or equal to 10. For tables larger\nthan 2\u00022, it is appropriate to use the test if no more than 1/5 of the expected counts are\nless than 5, and all expected counts are greater than 1.\n404 CHAPTER 8. INFERENCE FOR CATEGORICAL DATA\nEXAMPLE 8.19\nFor the mammogram data, check the conditions for the \u001f2test and calculate the \u001f2test statistic.\nIndependence is a reasonable assumption, since individuals have been randomized to either the\ntreatment or control group. Each expected cell count is greater than 10.\n\u001f2=X\nall cells(observed\u0000expected)2\nexpected\n=(500\u0000502:6)2\n502:6+(44;425\u000044;422:4)2\n44;422:4+(505\u0000502:4)2\n502:4+(44;405\u000044;407:6)2\n44;407:6\n= 0:02:\nGUIDED PRACTICE 8.20\nFor the HIV data, check the conditions for the \u001f2test and calculate the \u001f2test statistic.15\n8.3.3 Calculating ppp-values for a \u001f2\u001f2\u001f2distribution\nThe chi-square distribution is often used with data and statistics that are positive and right-\nskewed. The distribution is characterized by a single parameter, the degrees of freedom. Figure 8.7\ndemonstrates three general properties of chi-square distributions as the degrees of freedom in-\ncreases: the distribution becomes more symmetric, the center moves to the right, and the variability\nincreases.\n0 5 10 15 20 25Degrees of Freedom\n2\n4\n9\nFigure 8.7: Three chi-square distributions with varying degrees of freedom.\n15Independence holds, since this is a randomized study. The expected counts are greater than 10. \u001f2=(60\u000044:6)2\n44:6+\n(27\u000042:4)2\n42:4+(87\u0000102:4)2\n102:4+(113\u000097:6)2\n97:6= 14:7:\n8.3. INFERENCE FOR TWO OR MORE GROUPS 405\nThe\u001f2statistic from a contingency table has a sampling distribution that approximately fol-\nlows a\u001f2distribution with degrees of freedom df= (r\u00001)(c\u00001), whereris the number of rows\nandcis the number of columns. Either statistical software or a table can be used to calculate p-\nvalues from the \u001f2distribution. The chi-square table is partially shown in Figure 8.8, and a more\ncomplete table is presented in Appendix B.3 on page 468. This table is very similar to the t-table:\neach row provides values for distributions with di ﬀerent degrees of freedom, and a cut-o ﬀvalue is\nprovided for speciﬁed tail areas. One important di ﬀerence from the t-table is that the \u001f2table only\nprovides upper tail values.\nUpper tail 0.3 0.2 0.1 0.05 0.02 0.01 0.005 0.001\ndf 1 1.07 1.64 2.71 3.84 5.41 6.63 7.88 10.83\n2 2.41 3.22 4.61 5.99 7.82 9.21 10.60 13.82\n3 3.66 4.64 6.25 7.81 9.84 11.34 12.84 16.27\n4 4.88 5.99 7.78 9.49 11.67 13.28 14.86 18.47\n5 6.06 7.29 9.24 11.07 13.39 15.09 16.75 20.52\n6 7.23 8.56 10.64 12.59 15.03 16.81 18.55 22.46\n7 8.38 9.80 12.02 14.07 16.62 18.48 20.28 24.32\nFigure 8.8: A section of the chi-square table. A complete table is in Appendix B.3\non page 468.\nEXAMPLE 8.21\nCalculate an approximate p-value for the mammogram data, given that the \u001f2statistic equals 0.02.\nAssess whether the data provides convincing evidence of an association between screening group\nand breast cancer death.\nThe degrees of freedom in a 2 \u00022 table is 1, so refer to the values in the ﬁrst column of the prob-\nability table. The value 0.02 is less than 1.07, so the p-value is greater than 0.3. The data do not\nprovide convincing evidence of an association between screening group and breast cancer death.\nThis supports the conclusions from Example 8.14, where the p-value was calculated to be 0.8650\nand is visualized in Figure 8.9.\n0 1 2 3 4 5\nFigure 8.9: The p-value for the mammogram data is shaded on the \u001f2distribution\nwithdf= 1.The shaded area is to the right of x = 0.02.\nGUIDED PRACTICE 8.22\nCalculate an approximate p-value for the HIV data. Assess whether the data provides convincing\nevidence of an association between treatment and outcome at the \u000b= 0:01 signiﬁcance level.16\n16The\u001f2statistic is 14.7. For degrees of freedom 1, the tail area beyond 14.7 is smaller than 0.001. There is evidence to\nsuggest that treatment is not independent of outcome.\n406 CHAPTER 8. INFERENCE FOR CATEGORICAL DATA\n8.3.4 Interpreting the results of a \u001f2\u001f2\u001f2test\nIf thep-value from a \u001f2test is small enough to provide evidence to reject the null hypothe-\nsis of no association, it is important to explore the results further to understand direction of the\nobserved association. This is done by examining the residuals, the standardized di ﬀerences of the\nobserved -expected , for each cell. Instead of using squared di ﬀerences, the residuals are based on the\ndiﬀerences themselves, and the standardizing or scaling factor isp\nexpected. Calculating residuals\ncan be particularly helpful for understanding the results from large tables.\nFor each cell in a table, the residual equals:\nobserved\u0000expectedp\nexpected:\nResiduals with a large magnitude contribute the most to the \u001f2statistic. If a residual is positive,\nthe observed value is greater than the expected value, and vice versa for a negative residual.\n8.3. INFERENCE FOR TWO OR MORE GROUPS 407\nEXAMPLE 8.23\nIn the FAMuSS study introduced in Chapter 1, researchers measured a variety of demographic\nand genetic characteristics for about 1,300 participants, including data on race and genotype at\na speciﬁc locus on the ACTN3 gene (Figure 8.10). Is there evidence of an association between\ngenotype and race?\nFirst, check the assumptions for applying a \u001f2test. It is reasonable to assume independence, since\nit is unlikely that any participants were related to each other. None of the expected counts, as\nshown in Figure 8.11, are less than 5.\nH0: Race and genotype are independent.\nHA: Race and genotype are not independent.\nLet\u000b= 0:05.\nCalculate the \u001f2statistic:\n\u001f2=X\nall cells(observed\u0000expected)2\nexpected\n=(16\u00007:85)2\n7:85+(6\u000011:84)2\n11:84+:::+(5\u00006:22)2\n6:22\n= 19:4:\nCalculate the p-value: for a table with 3 rows and 5 columns, the \u001f2statistic is distributed with\n(3\u00001)(5\u00001) = 8 degrees of freedom. From the table, a \u001f2value of 19.4 corresponds to a tail\narea between 0.01 and 0.02. Thus, there is su ﬃcient evidence to reject the null hypothesis of\nindependence between race and genotype.\nThep-value can be obtained using the Rfunction pchisq (pchisq(19.4, df = 8, lower.tail =\nFALSE) ), which returns a value of 0.012861.\nTo further explore the di ﬀerences in genotype distribution between races, calculate residuals for\neach cell (Figure 8.12). The largest residuals are in the ﬁrst row; there are many more African\nAmericans with the CC genotype than expected under independence, and fewer with the CT geno-\ntype than expected. The residuals in the second row indicate a similar trend for Asians, but with\na less pronounced di ﬀerence. These results suggest further directions for research; a future study\ncould enroll a larger number of African American and Asian participants to examine whether the\nobserved trend holds with a more representative sample. Geneticists might also be interested in ex-\nploring whether this genetic di ﬀerence between populations has an observable phenotypic e ﬀect.\nCC CT TT Sum\nAfrican American 16 6 5 27\nAsian 21 18 16 55\nCaucasian 125 216 126 467\nHispanic 4 10 9 23\nOther 7 11 5 23\nSum 173 261 161 595\nFigure 8.10: Observed counts for race and genotype data from the FAMuSS study.\n408 CHAPTER 8. INFERENCE FOR CATEGORICAL DATA\nCC CT TT Sum\nAfrican Am 7.85 11.84 7.31 27.00\nAsian 15.99 24.13 14.88 55.00\nCaucasian 135.78 204.85 126.36 467.00\nHispanic 6.69 10.09 6.22 23.00\nOther 6.69 10.09 6.22 23.00\nSum 173.00 261.00 161.00 595.00\nFigure 8.11: Expected counts for race and genotype data from the FAMuSS study.\nCC CT TT Sum\nAfrican Am 2.91 -1.70 -0.85 0.00\nAsian 1.25 -1.25 0.29 0.00\nCaucasian -0.93 0.78 -0.03 0.00\nHispanic -1.04 -0.03 1.11 0.00\nOther 0.12 0.29 -0.49 0.00\nSum 0.00 0.00 0.00 0.00\nFigure 8.12: Residuals for race and genotype data from the FAMuSS study.\nEXAMPLE 8.24\nIn Guided Practice 8.22, the p-value was found to be smaller than 0.001, suggesting that treatment\nis not independent of outcome. Does the evidence suggest that infants should be given nevirapine\nor lopinarvir?\nIn a 2\u00022 table, it is relatively easy to directly compare observed and expected counts. For nevi-\nrapine, more infants than expected experienced virologic failure (60 > 44.6), while fewer than\nexpected reached a stable disease state (87 < 102.4). For lopinarvir, fewer infants than expected\nexperienced virologic failure (27 < 42.4), and more infants than expected reached a stable disease\nstate (113 > 97.6) (Figure 8.13). The outcomes for infants on lopinarvir are better than for those\non nevirapine; combined with the results of the signiﬁcance test, the data suggest that lopinarvir is\nassociated with better treatment outcomes.\nNVP LPV Total\nVirologic Failure 6044.6 2742.4 87\nStable Disease 87102.4 113 97.6 200\nTotal 147 140 287\nFigure 8.13: Observed and (expected) counts for the HIV study.\nGUIDED PRACTICE 8.25\nConﬁrm the conclusions reached in Example 8.24 by analyzing the residuals.17\n17R1;1=(44:6\u000060)p\n44:6= 2:31;R1;2=(42:4\u000027)p\n27=\u00002:37;R2;1=(87\u0000102:4)p\n102:4=\u00001:53;R2;2=(113\u000097:6)p\n97:6= 1:56. The positive\nresiduals for the upper left and lower right cells indicate that more infants than expected experienced virologic failure on\nNVP and stable disease on LPV; vice versa for the upper right and lower left cells. The larger magnitude of the residuals\nfor the two NVP cells indicates that most of the discrepancy between observed and expected counts is for outcomes related\nto NVP .\n8.3. INFERENCE FOR TWO OR MORE GROUPS 409\nGUIDED PRACTICE 8.26\nChapter 1 started with the discussion of a study examining whether exposure to peanut products\nreduce the rate of a child developing peanut allergies. Children were randomized either to the\npeanut avoidance or the peanut consumption group; at 5 years of age, each child was tested for\npeanut allergy using an oral food challenge (OFC). The results of the OFC are reproduced in Fig-\nure 8.14; failing the food challenge indicates an allergic reaction. Assess whether there is evidence\nfor exposure to peanut allergy reducing the chance of developing peanut allergies.18\nFAIL OFC PASS OFC Sum\nPeanut Avoidance 36 227 263\nPeanut Consumption 5 262 267\nSum 41 489 530\nFigure 8.14: LEAP Study Results.\n8.3.5 Fisher’s exact test\nIf sample sizes are too small, the \u001f2distribution does not yield accurate p-values for assessing\nindependence of the row and column variables in a table. When expected counts in a table are less\nthan 10, Fisher’s exact test is often used to calculate exact levels of signiﬁcance. This test is usually\napplied to 2\u00022 tables. It can be applied to larger tables, but the logic behind the test is complex\nand the calculations involved are computationally intensive, so this section covers only 2 \u00022 tables.\nClostridium di ﬃcileis a bacterium that causes inﬂammation of the colon. Antibiotic treatment\nis typically not e ﬀective, particularly for patients who experience multiple recurrences of infection.\nInfusion of feces from healthy donors has been reported as an e ﬀective treatment for recurrent\ninfection. A randomized trial was conducted to compare the e ﬃcacy of donor-feces infusion versus\nvancomycin, the antibiotic typically prescribed to treat C. diﬃcileinfection. The results of the trial\nare shown in Figure 8.15.19A brief calculation shows that all of the expected cell counts are less\nthan 10, so the \u001f2test should not be used as a test for association.\nUnder the null hypothesis, the probabilities of cure in the fecal infusion and vancomycin\ngroups are equal; i.e., individuals in one group are just as likely to be cured as individuals in the\nother group. Suppose the probability that an individual is cured, given that he or she was assigned\nto the fecal infusion group, is p1and the probability an individual is cured in the vancomycin\ngroup isp2. Researchers were interested in testing the null hypothesis H0:p1=p2.\nCured Uncured Sum\nFecal Infusion 13 3 16\nVancomycin 4 9 13\nSum 17 12 29\nFigure 8.15: Fecal Infusion Study Results.\n18The assumptions for conducting a \u001f2test are satisﬁed. Calculate a \u001f2test statistic: 24.29. The associated p-value is\n8:3\u000210\u00007. There is evidence to suggest that treatment group is not independent of outcome. Speciﬁcally, a residual analysis\nshows that in the peanut avoidance group, more children than expected failed the OFC; in the peanut consumption group,\nmore children than expected passed the OFC.\n19These results correspond to the number of patients cured after the ﬁrst infusion of donor feces and the number of\npatients cured in the vancomycin-alone group.\n410 CHAPTER 8. INFERENCE FOR CATEGORICAL DATA\nThep-value is the probability of observing results as or more extreme than those observed in\nthe study under the assumption that the null hypothesis is true. Previously discussed methods for\nsigniﬁcance testing have relied on calculating a test statistic associated with a deﬁned sampling\ndistribution, then obtaining p-values from tail areas on the distribution. Fisher’s exact test uses a\nsimilar approach, but introduces a new sampling distribution.\nThep-value for Fisher’s exact test is calculated by adding together the individual conditional\nprobabilities of obtaining each table that is as or more extreme than the one observed, under the\nnull hypothesis and given that the marginal totals are considered ﬁxed.\n– When the row and column totals are held constant, the value of any one cell in the table\ndetermines the rest of the entries. For example, if the marginal sums in Figure 8.15 are\nknown, along with the value in one cell (e.g., the upper right equals 3), it is possible to\ncalculate the values in the other three cells. Thus, when marginal totals are considered ﬁxed,\neach table represents a unique set of results.\n– Extreme tables are those which contradict the null hypothesis of p1=p2. In the fecal infusion\ngroup, under the null hypothesis of no di ﬀerence in the population proportion cured, one\nwould expect16\u000217\n29= 9:38 cured individuals. The 13 observed cured individuals is extreme\nin the direction of more being cured than expected under the null hypothesis. An extreme\nresult in the other direction would be, for instance, 1 cured patient in the fecal infusion group\nand 16 in the vancomycin group.\nEXAMPLE 8.27\nOf the 17 patients cured, 13 were in the fecal infusion group and 4 were in the vancomycin group.\nAssume that the marginal totals are ﬁxed (i.e., 17 patients were cured, 12 were uncured, and 16\npatients were in the fecal infusion group, while 13 were in the vancomycin group). Enumerate all\npossible sets of results that are more extreme than what was observed, in the same direction.\nThe observed results show a case of ˆp1>ˆp2; results that are more extreme consist of cases where\nmore than 13 cured patients were in the fecal infusion group. Under the assumption that the total\nnumber of cured patients is constant at 17 and that only 16 patients were assigned to the fecal\ninfusion group (out of 29 patients total), more extreme results are represented by cases where 14,\n15, or 16 cured patients were in the fecal infusion group. The following tables illustrate the unique\ncombinations of values for the 4 table cells corresponding to those extreme results.\nCured Uncured Sum\nFecal Infusion 14 2 16\nVancomycin 3 10 13\nSum 17 12 29\nCured Uncured Sum\nFecal Infusion 15 1 16\nVancomycin 2 11 13\nSum 17 12 29\nCured Uncured Sum\nFecal Infusion 16 0 16\nVancomycin 1 12 13\nSum 17 12 29\n8.3. INFERENCE FOR TWO OR MORE GROUPS 411\nCalculating a one-sided ppp-value\nSuppose that researchers were interested in testing the null hypothesis against the one-sided al-\nternative,HA:p1> p 2. To calculate the one-sided p-value, sum the probabilities of each table\nrepresenting results as or more extreme than those observed; speciﬁcally, sum the probabilities of\nobserving Figure 8.15 and the tables in Example 8.27.\nCured Uncured Sum\nFecal Infusion a b a +b\nVancomycin c d c +d\nSuma+c b +d n\nFigure 8.16: General Layout of Data in Fecal Infusion Study.\nThe probability of observing a table with cells a;b;c;d given ﬁxed marginal totals a+b,c+d,\na+c, andb+dfollows the hypergeometric distribution. The hypergeometric distribution was\nintroduced in Section 3.5.3.\nP(a;b;c;d ) = HGeom( a+b;c+d;a+c) =\u0000a+b\na\u0001\u0000c+d\nc\u0001\n\u0000n\na+c\u0001=(a+b)! (c+d)! (a+c)! (b+d)!\na!b!c!d!n!:\nEXAMPLE 8.28\nCalculate the probability of observing Figure 8.15, assuming the margin totals are ﬁxed.\nP(13;3;4;9) =\u000016\n13\u0001\u000013\n4\u0001\n\u000029\n17\u0001=16! 13! 17! 12!\n13! 3! 4! 9! 29!= 7:71\u000210\u00003:\nThe value 0.0077 represents the probability of observing 13 cured patients out of 16 individuals\nin the fecal infusion group and 1 cured in the vancomycin group, given that there are a total of 29\npatients and 17 were cured overall.\n412 CHAPTER 8. INFERENCE FOR CATEGORICAL DATA\nEXAMPLE 8.29\nEvaluate the statistical signiﬁcance of the observed data in Figure 8.15 using the one-sided alter-\nnativeHA:p1>p2.\nCalculate the probability of the tables from Example 8.27. Generally, the formula for these tables\nis\nP(a;b;c;d ) =\u0000a+b\na\u0001\u0000c+d\nc\u0001\n\u0000n\na+c\u0001=\u000016\na\u0001\u000013\nc\u0001\n\u000029\n17\u0001;\nsince the marginal totals from Figure 8.15 are ﬁxed. The value aranges from 14, 15, 16, while c\nranges from 3, 2, 1.\nP(14;2;3;10) =\u000016\n14\u0001\u000013\n3\u0001\n\u000029\n17\u0001= 6:61\u000210\u00004\nP(15;1;2;11) =\u000016\n15\u0001\u000013\n2\u0001\n\u000029\n17\u0001= 2:40\u000210\u00005\nP(16;0;1;12) =\u000016\n16\u0001\u000013\n1\u0001\n\u000029\n17\u0001= 2:51\u000210\u00007\nThe probability of the observed table is 7 :71\u000210\u00003, as calculated in the previous example.\nThe one-sided p-value is the sum of these table probabilities: (7 :71\u000210\u00003) + (6:61\u000210\u00004) + (2:40\u0002\n10\u00005) + (2:51\u000210\u00007) = 0:0084:\nThe results are signiﬁcant at the \u000b= 0:05 signiﬁcance level. There is evidence to support the one-\nsided alternative that the proportion of cured patients in the fecal infusion group is higher than\nthe proportion of cured patients in the vancomycin group. However, it is important to note that\ntwo-sided alternatives are the standard in medical literature. Conducting a two-sided test would\nbe especially desirable when evaluating a treatment which lacks randomized trials supporting its\neﬃcacy, such as donor-feces infusion.\nCalculating a two-sided ppp-value\nThere are various methods for calculating a two-sided p-value in the Fisher’s exact test setting.\nWhen the test is calculated by hand, the most common way to calculate a two-sided p-value is to\ndouble the smaller of the one-sided p-values. One other common method used by various statisti-\ncal computing packages such as Ris to classify \"more extreme\" tables as all tables with probabilities\nless than that of the observed table, in both directions. The two-sided p-value is the sum of proba-\nbilities for the qualifying tables. That approach is illustrated in the next example.\n8.3. INFERENCE FOR TWO OR MORE GROUPS 413\nEXAMPLE 8.30\nEvaluate the statistical signiﬁcance of the observed data in Figure 8.15 using the two-sided alter-\nnativeHA:p1,p2.\nIdentify tables that are more extreme in the other direction of the observed result, i.e. where the\nproportion of cured patients in the vancomycin group are higher than in the fecal infusion group.\nStart with the most extreme cases and calculate probabilities until a table has a p-value higher than\n7:71\u000210\u00003, the probability of the observed table.\nThe most extreme result in the ˆp1<ˆp2direction would be if all patients in the vancomycin group\nwere cured; then 13 of the cured patients would be in the vancomycin group and 4 would be in the\nfecal transplant group. This table has probability 3 :5\u000210\u00005.\nCured Uncured Sum\nFecal Infusion 4 12 16\nVancomycin 13 0 13\nSum 17 12 29\nContinue enumerating tables by decreasing the number of cured patients in the vancomycin group.\nThe table with 5 cured patients in the fecal infusion group has probability 1 :09\u000210\u00003.\nCured Uncured Sum\nFecal Infusion 5 11 16\nVancomycin 12 1 13\nSum 17 12 29\nThe table with 6 cured patients in the fecal infusion group has probability 0.012. This value is\ngreater than 7 :71\u000210\u00003, so it will not be part of the sum to calculate the two-sided p-value.\nCured Uncured Sum\nFecal Infusion 6 10 16\nVancomycin 11 2 13\nSum 17 12 29\nAs calculated in the previous example, the one-sided p-value is 0:0084. Thus, the two-sided p-value\nfor these data equals 0 :0084 + (3:5\u000210\u00005) + (1:09\u000210\u00003) = 0:0095. The results are signiﬁcant at the\n\u000b= 0:01 signiﬁcance level, and there is evidence to support the e ﬃcacy of donor-feces infusion as\na treatment for recurrent C. diﬃcileinfection.\n414 CHAPTER 8. INFERENCE FOR CATEGORICAL DATA\n8.4 Chi-square tests for the ﬁt of a distribution\nThe\u001f2test can also be used to examine the appropriateness of hypothesized distribution\nfor a dataset, most commonly when a set of observations falls naturally into categories as in the\nexamples discussed in this section. As with testing in the two-way table setting, expected counts are\ncalculated based on the assumption that the hypothesized distribution is correct, and the statistic\nis based on the discrepancies between observed and expected counts. The \u001f2sampling distribution\nfor the test statistic is reasonably accurate when each expected count is at least 5 and follows a \u001f2\ndistribution with k\u00001 degrees of freedom, where kis the number of categories. Some guidelines\nrecommend that no more than 1/5 of the cells have expected counts less than 5, but the stricter\nrequirement that all cells have expected counts greater than 5 is safer.\nWhen used in this setting, the \u001f2test is often called a ‘goodness-of-ﬁt’ test , a term that is\noften misunderstood. Small p-values of the test suggest evidence that a hypothesized distribution\nis not a good model, but non-signiﬁcant p-values do not imply that the hypothesized distribution\nis the best model for the data, or even a good one. In the logic of hypothesis testing, failure to reject\na null hypothesis cannot be viewed as evidence that the null hypothesis is true.\nEXAMPLE 8.31\nThe participants in the FAMuSS study were volunteers at a university, and so did not come from\na random sample of the US population. The participants may not be representative of the general\nUnited States population. The \u001f2test can be used to test the null hypothesis that the participants\nare racially representative of the general population. Figure 8.17 shows the number observed by\nracial category in FAMuSS and the proportions of the US population in each of those categories.20\nUnder the null hypothesis, the sample proportions should equal the population proportions. For\nexample, since African Americans are 0.128 of the general proportion, (0 :128)(595) = 76 :16 African\nAmericans would be expected in the sample. The rest of the expected counts are shown in Fig-\nure 8.18.\nSince each expected count is greater than or equal to 5, the \u001f2distribution can be used to calculate\nap-value for the test.\n\u001f2=X\nall cells(observed\u0000expected)2\nexpected\n=(27\u000076:16)2\n76:16+(55\u00005:95)2\n5:95+(467\u0000478:38)2\n478:38+(46\u000034:51)2\n34:51\n= 440:18:\nThere are 3 degrees of freedom, since k= 4. The\u001f2statistic is extremely large, and the asso-\nciated tail area is smaller than 0.001. There is more than su ﬃcient evidence to reject the null\nhypothesis that the sample is representative of the general population. A comparison of the ob-\nserved and expected values (or the residuals) indicates that the largest discrepancy is with the\nover-representation of Asian participants.\n20The US Census Bureau considers Hispanic as a classiﬁcation separate from race, on the basis that Hispanic individuals\ncan be any race. In order to facilitate the comparison with the FAMuSS data, participants identiﬁed as \"Hispanic\" have been\nmerged with the \"Other\" category.\n8.4. CHI-SQUARE TESTS FOR THE FIT OF A DISTRIBUTION 415\nRace African American Asian Caucasian Other Total\nFAMuSS 27 55 467 46 595\nUS Census 0.128 0.01 0.804 0.058 1.00\nFigure 8.17: Representation by race in the FAMuSS study versus the general pop-\nulation.\nRace African American Asian Caucasian Other Total\nObserved 27 55 467 46 595\nExpected 76.16 5.95 478.38 34.51 595\nFigure 8.18: Actual and expected counts in the FAMuSS data.\nEXAMPLE 8.32\nAccording to Mendelian genetics, alleles segregate independently; if an individual is heterozygous\nfor a gene and has alleles AandB, then the alleles have an equal chance of being passed to an\noﬀspring. Under this framework, if two individuals with genotype ABmate, then their o ﬀspring\nare expected to exhibit a 1:2:1 genotypic ratio; 25% of the o ﬀspring will be AA, 50% will be AB, and\n50% will be BB. The term \"segregation distortion\" refers to a deviation from expected Mendelian\nfrequencies.\nAt a speciﬁc gene locus in the plant Arabidopsis thaliana , researchers have observed 84 AAindi-\nviduals, 233 ABindividuals, and 134 BBindividuals. Is there evidence of segregation disorder at\nthis locus? Conduct the test at \u000b= 0:0001 to account for multiple testing, since the original study\nexamined approximately 250 locations across the genome.\nThe Mendelian proportions are 25%, 50%, and 25%. Thus, the expected counts in a group of 451\nindividuals are: 112.75 AA, 225.50AB, and 112.75 BB. No expected count is less than 5.\n\u001f2=X\nall cells(observed\u0000expected)2\nexpected\n=(84\u0000112:75)2\n112:75+(233\u0000225:50)2\n225:50+(134\u0000112:75)2\n112:75\n= 11:59:\nThere are 2 degrees of freedom, since k= 3. Thep-value is between 0.005 and 0.001, which is\ngreater than \u000b= 0:0001. There is insu ﬃcient evidence to reject the null hypothesis that the o ﬀ-\nspring ratios correspond to expected Mendelian frequencies; i.e., there is not evidence of segrega-\ntion distortion at this locus.\n416 CHAPTER 8. INFERENCE FOR CATEGORICAL DATA\n8.5 Outcome-based sampling: case-control studies\n8.5.1 Introduction\nThe techniques so far in this chapter have often relied on the assumption that the data were\ncollected using random sampling from a population. When cases come from a random sample,\nthe sample proportion of observations with a particular outcome should accurately estimate the\npopulation proportion, given that the sample size is large enough. When studying rare outcomes,\nhowever, moderate sized samples may contain few or none of the outcomes. Persistent pulmonary\nhypertension of the newborn (PPHN) is a dangerous condition in which the blood vessels in the\nlungs of a newborn do not relax immediately after birth, leading to inadequate oxygenation. The\ncondition is rare, occurring in about 1.9 per 1,000 live births, so it is di ﬃcult to study using random\nsampling. In the early 2000s, anecdotal evidence began to accumulate that the risk of the condi-\ntion might be increased if the mother of the newborn had been taking a particular medication for\ndepression, a selective serotonin reuptake inhibitor (SSRI) during the third trimester of pregnancy\nor even as early as during week 20 of the pregnancy.\nOne design for studying the issue would enroll two cohorts of women, one in which women\nwere taking SSRIs for depression and one in which they were not. However, if the chance of PPHN\nwas 1.9/1,000 in newborns of a control cohort of 1,000 women, then the probability of observing\nno cases of PPHN is about 0.15. If the probability of PPHN is elevated among infants born to\nwomen taking SSRIs, such as to 3.0/1,000, the chance of observing no cases among 1,000 women\nis approximately 0.05. Precise measures of the probability of PPHN occurring would require very\nlarge cohorts.\nAn alternative design for studies like this reverses the sampling scheme so that the two cohorts\nare determined by outcome, rather than exposure; a cohort with the condition and a cohort without\nthe condition are sampled, then exposure to a possible cause is recorded. To apply this design\nfor studying PPHN, a registry of live births could be used to sort births by presence or absence\nof PPHN. The number in each group in which the mother had been taking SSRIs could then be\nrecorded (based on medical records). Such a design would have the advantage of su ﬃcient numbers\nof cases with and without PPHN, but it has other limitations which will be discussed later in this\nsection. Traditionally, these studies have been called case-control studies because of the original\nsampling of individuals with and without a condition. More generally, it is an example of outcome-\ndependent sampling.\n8.5. OUTCOME-BASED SAMPLING: CASE-CONTROL STUDIES 417\n8.5.2\u001f2\u001f2\u001f2tests of association in case-control studies\nIn 2006, Chambers, et. al reported a case-control study examining the association of SSRI\nuse and persistent pulmonary hypertension in newborns.21The study team enrolled 337 women\nwhose infants su ﬀered from PPHN and 836 women with similar characteristics but whose infants\ndid not have PPHN. Among the women whose infants had PPHN, 14 had taken an SSRI after week\n20 of the pregnancy. In the cohort of women whose infants did not have PPHN, 6 had been taking\nthe medication after week 20. In the subset of women who had been taking an SSRI, the infants are\nconsidered ‘exposed’ to the medication. The data from the study are summarized in Figure 8.19.\nPPHN present Yes No Total\nSSRI exposed 14 6 20\nSSRI unexposed 323 830 1153\nTotal 337 836 1173\nFigure 8.19: SSRI exposure vs observed number of PPHN cases in newborns.\nThe sample of women participating in the study are clearly not a random sample drawn from\nwomen who had recently given birth; they were identiﬁed according to the disease status of their\ninfants. In this sample, the proportion of newborns with PPHN (337/1173 = 28.7%) is much higher\nthan the disease prevalence in the general population.\nEven so, the concept of independence between rows and columns under a null hypothesis of\nno association still holds. If SSRI use had no e ﬀect on the occurrence of PPHN, then the proportions\nof mothers taking SSRIs among the PPHN and non-PPHN infants should be about the same. In\nother words, the null hypothesis of equal SSRI use among mothers with/without PPHN a ﬀected\ninfants is the hypothesis of no association between SSRI use and PPHN. The test of independence\ncan be conducted using the approach introduced earlier in the chapter.\nThe expected counts shown in Figure 8.20 suggest that the p-value from a \u001f2test may not\nbe accurate; under the null hypothesis, the expected number of PPHN cases in the SSRI exposed\ngroup is less than 10.\nPPHN present Yes No Total\nSSRI exposed 5.80 14.20 20\nSSRI unexposed 331.20 811.80 1153\nTotal 337 836 1173\nFigure 8.20: SSRI exposure vs expected number of PPHN cases in newborn.\nThep-value from Fisher’s exact test is <0:001 (0:00014, to be precise), so the evidence is\nstrong that SSRI exposure and PPHN are associated. Fisher’s exact test is often used in studies of\nrare conditions or exposures since one or more expected cell counts are typically less than 10.\n21N Engl J Med 2006;354:579-87.\n418 CHAPTER 8. INFERENCE FOR CATEGORICAL DATA\n8.5.3 Estimates of association in case-control studies\nFor data in a 2\u00022 table, correct point estimates of association depend on the mechanism used\nto gather the data. In the example of a clinical trial of nevirapine versus lopinarvir discussed in\nSection 8.3.1, the population proportion of children who would experience virologic failure after\ntreatment with one of the drugs can be estimated by the observed proportion of virologic failures\nwhile on that drug. For nevirapine, the proportion of children with virologic failure is 60/147 =\n0.41, while for lopinarvir the proportion is 27/140 = 0.19. The di ﬀerence in outcome between the\ntwo groups can be summarized by the di ﬀerence in these proportions. The proportion experiencing\nvirologic failure when treated with nevirapine was 0.12 larger in nevirapine (0.41 - 0.29), so if the\ntwo drugs were to be used in a large population, approximately 12% more children treated with\nnevirapine would experience virologic failure as compared to lopinarvir. The conﬁdence intervals\ndiscussed in Section 8.2.2 can be used to express the uncertainty in this estimate.\nSince the proportion of virologic failures can be estimated from the trial data, the relative risk\nof virologic failure can also be used to estimate the association between treatment and virologic\nfailure. Relative risk is the ratio of two proportions, and was introduced in Section 1.6.2. The\nrelative risk of virologic failure with nevirapine versus lopinarvir is 0 :41=0:19 = 2:16. Children\ntreated with nevirapine are estimated to be more than twice as likely to experience virologic failure.\nStatistically, the population parameter for the relative risk in the study of HIV+is a ratio of\nconditional probabilities:\nP(virologic failurejtreatment with nevirapine)\nP(virologic failurejtreatment with lopinarvir):\nIn a study like the PPHN case-control study, the natural population parameter of interest\nwould be the relative risk of PPHN for infants exposed to an SSRI during after week 20 of gesta-\ntion compared to those who were not exposed. However, in the design of this study, participat-\ning mothers were sampled and grouped according to whether their infants did or did not su ﬀer\nfrom PPHN, rather than assigned to either SSRI exposure or non-exposure. Relative risk of PPHN\nfrom exposure to SSRI cannot be estimated from the data because it is not possible to estimate\nP(PPHNjSSRI exposure) and P(PPHNjno SSRI exposure). In case-control studies, association is es-\ntimated using odds and odds ratios rather than relative risk.\nThe odds of SSRI exposure among the cases are given by the fraction\nodds cases =P(SSRI exposurejPPHN)\nP(no SSRI exposure jPPHN)=14=337\n323=337=14\n323:\nThe odds of SSRI exposure among the controls are given by the fraction\nodds controls =P(SSRI exposurejno PPHN)\nP(no SSRI exposure jno PPHN)=6=836\n830=836=6\n830:\nThe ratio of the odds, the odds ratio, compares the odds of exposure among the cases to the\nodds of exposure among the controls:\nOR exposure, cases vs. controls =odds cases\nodds controls=14=323\n6=830=(14)(830)\n(323)(6)= 6:00:\n8.5. OUTCOME-BASED SAMPLING: CASE-CONTROL STUDIES 419\nA population odds ratio of, for example, 1.5, implies that the odds of exposure in cases are 50%\nlarger than the odds of exposure in controls. For this study, the odds ratio of 6.00 implies that the\nodds of SSRI exposure in infants with PPHN are 6 times as large as the odds of exposure in infants\nwithout PPHN. Epidemiologists describe this odds ratio as the odds of exposure given presence of\nPPHN compared to the odds of exposure given absence of PPHN. An OR greater than 1 suggests\nthat the exposure may be a risk factor for the disease or condition under study. Epidemiologists\nalso use the term relative odds as a synonym for odds ratio.\nSurprisingly, the odds ratio of exposure comparing cases to controls is equivalent to the odds\nratio of disease comparing exposed to unexposed.22With a speciﬁc example, it is easy to see how\nthe fraction for the odds ratios are numerically equivalent:\nOR disease, exposed versus unexposed =odds exposed\nodds unexposed=14=6\n323=830=(14)(830)\n(6)(323)= 6:00:\nDespite the apparently restrictive nature of the case-control sampling design, the odds ratio\nof interest, the odds ratio for disease given exposure, can be estimated from case-control data.\nEpidemiologists rely on one additional result, called the rare disease assumption. When a\ndisease is rare, the odds ratio for the disease given exposure is approximately equal to the relative\nrisk of the disease given exposure. These identities are the reason case-control studies are widely\nused in settings in which a disease is rare: it allows for the relative risk of disease given exposure\nto be estimated, even if the study design is based on sampling cases and controls then measuring\nexposure.\nIn a general 2\u00022 table of exposure versus disease status (Figure 8.21) the odds ratio for disease\ngiven exposure status is the ad=bc .\nDisease Status Present Absent Total\nExposed a b a +b\nUnexposed c d c +d\nTotal a+c b +d n\nFigure 8.21: Exposure vs Disease Status.\nIn the PPHN case-control data, the odds ratio for PPHN given SSRI exposure status is (14)(830) =(6)(323) =\n6:00. Because PPHN is a rare condition, the risk of PPHN among infants exposed to an SSRI is es-\ntimated to be approximately 6 times that of the risk among unexposed infants. Infants exposed to\nan SSRI are 600% more likely to su ﬀer from PPHN.\nIt can be shown that the p-value used in a test of no association (between exposure and disease)\nis also thep-value for a test of the null hypothesis that the odds ratio is 1.\n22This result can be shown through Bayes’ rule.\n420 CHAPTER 8. INFERENCE FOR CATEGORICAL DATA\n8.6 Notes\nTwo-way tables are often used to summarize data from medical research studies, and entire\ntexts have been written about methods of analysis for these tables. This chapter covers only the\nmost basic of those methods.\nUntil recently, Fisher’s exact test could only be calculated for 2 \u00022 tables with small cell counts.\nResearch has produced faster algorithms for enumerating tables and calculating p-values, and the\ncomputational power of recent desktop and laptop computers now make it possible to calculate\nthe Fisher test on nearly any 2 \u00022 table. There are also versions of the test that can be calculated\non tables with more than 2 rows and/or columns. The practical result for data analysts is that the\nsample size condition for the validity of the \u001f2test can be made more restrictive. This chapter\nrecommends using the \u001f2test only when cell counts in a 2 \u00022 table are greater than 10; some\napproaches recommend cell counts larger than 10.\nFor many years, introductory textbooks recommended using a modiﬁed version of the \u001f2test,\ncalled the Fisher-Yates test, which adjusted the value of the statistic in small sample sizes to increase\nthe accuracy of the \u001f2sampling distribution in calculating p-values. The Fisher-Yates version of\nthe test is no longer used as often because of the widespread availability of the Fisher test.\nThe Fisher test is not without controversy, at least in the theoretical literature. Conditioning\non the row and column totals allows the calculation of a p-value from the hypergeometric distribu-\ntion, but in principle restricts inference to the set of tables with the same row and column values.\nIn practice, this is less serious than it may seem. For tables of moderate size, the p-values from the\n\u001f2and Fisher tests are nearly identical and for tables with small counts, the Fisher test guarantees\nthat the Type I error will be no larger than the speciﬁed value of \u000b. In small sample sizes, some\nstatisticians argue that the Fisher-Yates correction is preferable to the Fisher test because of the dis-\ncrete nature of the hypergeometric distribution. In small tables, for example, an observed p-value\nof 0.04 may be the largest value that is less than 0.05, such that the Type I error of the test in that\nsituation is 0.04, not 0.05.\nSection 8.5.3 does not show the derivation that the odds ratio estimated from a case-control\nis the same as that from a cohort study. It is long and algebraically more complex than other\nderivations shown in the text, but it is a direct application of Bayes’ rule, applied to each term in\nthe fraction that deﬁnes population odds ratio.\nThe two labs for this chapter examine methods of inference for the success probability in\nbinomial data then generalizes inference for binomial proportions to two-way contingency tables.\nLab 2 also discusses measures of association in two-by-two tables. The datasets in the labs are\nsimilar to datasets that arise frequently in medical statistics. Lab 1 assesses the evidence for a\ntreatment e ﬀect in a single uncontrolled trial of a new drug for melanoma and whether outcomes in\nstage 1 lung cancer are di ﬀerent among patients treated at Dana-Farber Cancer Institute compared\nto population based statistics. In Lab 2, students analyze a dataset from a published clinical trial\nexamining the beneﬁt of using a more expensive but potentially more e ﬀective drug to treat HIV-\npositive infants.\n8.7. EXERCISES 421\n8.7 Exercises\n8.7.1 Inference for a single proportion\n8.1 Vegetarian college students. Suppose that 8% of college students are vegetarians. Determine if the\nfollowing statements are true or false, and explain your reasoning.\n(a) The distribution of the sample proportions of vegetarians in random samples of size 60 is approximately\nnormal since n\u001530.\n(b) The distribution of the sample proportions of vegetarian college students in random samples of size 50 is\nright skewed.\n(c) A random sample of 125 college students where 12% are vegetarians would be considered unusual.\n(d) A random sample of 250 college students where 12% are vegetarians would be considered unusual.\n(e) The standard error would be reduced by one-half if we increased the sample size from 125 to 250.\n8.2 Young Americans, Part I. About 77% of young adults think they can achieve the American dream.\nDetermine if the following statements are true or false, and explain your reasoning.23\n(a) The distribution of sample proportions of young Americans who think they can achieve the American\ndream in samples of size 20 is left skewed.\n(b) The distribution of sample proportions of young Americans who think they can achieve the American\ndream in random samples of size 40 is approximately normal since n\u001530.\n(c) A random sample of 60 young Americans where 85% think they can achieve the American dream would\nbe considered unusual.\n(d) A random sample of 120 young Americans where 85% think they can achieve the American dream would\nbe considered unusual.\n8.3 Gender equality. The General Social Survey asked a random sample of 1,390 Americans the following\nquestion: “On the whole, do you think it should or should not be the government’s responsibility to promote\nequality between men and women?” 82% of the respondents said it “should be”. At a 95% conﬁdence level,\nthis sample has 2% margin of error. Based on this information, determine if the following statements are true\nor false, and explain your reasoning.24\n(a) We are 95% conﬁdent that between 80% and 84% of Americans in this sample think it’s the government’s\nresponsibility to promote equality between men and women.\n(b) We are 95% conﬁdent that between 80% and 84% of all Americans think it’s the government’s responsi-\nbility to promote equality between men and women.\n(c) If we considered many random samples of 1,390 Americans, and we calculated 95% conﬁdence intervals\nfor each, 95% of these intervals would include the true population proportion of Americans who think it’s\nthe government’s responsibility to promote equality between men and women.\n(d) In order to decrease the margin of error to 1%, we would need to quadruple (multiply by 4) the sample\nsize.\n(e) Based on this conﬁdence interval, there is su ﬃcient evidence to conclude that a majority of Americans\nthink it’s the government’s responsibility to promote equality between men and women.\n23A. Vaughn. “Poll ﬁnds young adults optimistic, but not about money”. In: Los Angeles Times (2011).\n24National Opinion Research Center, General Social Survey, 2018.\n422 CHAPTER 8. INFERENCE FOR CATEGORICAL DATA\n8.4 Elderly drivers. The Marist Poll published a report stating that 66% of adults nationally think licensed\ndrivers should be required to retake their road test once they reach 65 years of age. It was also reported\nthat interviews were conducted on 1,018 American adults, and that the margin of error was 3% using a 95%\nconﬁdence level.25\n(a) Verify the margin of error reported by The Marist Poll.\n(b) Based on a 95% conﬁdence interval, does the poll provide convincing evidence that more than 70% of the\npopulation think that licensed drivers should be required to retake their road test once they turn 65?\n8.5 Fireworks on July 4th.A local news outlet reported that 56% of 600 randomly sampled Kansas residents\nplanned to set o ﬀﬁreworks on July 4th. Determine the margin of error for the 56% point estimate using a\n95% conﬁdence level.26\n8.6 Life rating in Greece. Greece has faced a severe economic crisis since the end of 2009. A Gallup poll\nsurveyed 1,000 randomly sampled Greeks in 2011 and found that 25% of them said they would rate their\nlives poorly enough to be considered “su ﬀering”.27\n(a) Describe the population parameter of interest. What is the value of the point estimate of this parameter?\n(b) Check if the conditions required for constructing a conﬁdence interval based on these data are met.\n(c) Construct a 95% conﬁdence interval for the proportion of Greeks who are “su ﬀering\".\n(d) Without doing any calculations, describe what would happen to the conﬁdence interval if we decided to\nuse a higher conﬁdence level.\n(e) Without doing any calculations, describe what would happen to the conﬁdence interval if we used a larger\nsample.\n8.7 Study abroad. A survey on 1,509 high school seniors who took the SAT and who completed an optional\nweb survey shows that 55% of high school seniors are fairly certain that they will participate in a study abroad\nprogram in college.28\n(a) Is this sample a representative sample from the population of all high school seniors in the US? Explain\nyour reasoning.\n(b) Let’s suppose the conditions for inference are met. Even if your answer to part (a) indicated that this\napproach would not be reliable, this analysis may still be interesting to carry out (though not report).\nConstruct a 90% conﬁdence interval for the proportion of high school seniors (of those who took the SAT)\nwho are fairly certain they will participate in a study abroad program in college, and interpret this interval\nin context.\n(c) What does “90% conﬁdence\" mean?\n(d) Based on this interval, would it be appropriate to claim that the majority of high school seniors are fairly\ncertain that they will participate in a study abroad program in college?\n25Marist Poll, Road Rules: Re-Testing Drivers at Age 65?, March 4, 2011.\n26Survey USA, News Poll #19333, data collected on June 27, 2012.\n27Gallup World, More Than One in 10 “Su ﬀering\" Worldwide, data collected throughout 2011.\n28studentPOLL, College-Bound Students’ Interests in Study Abroad and Other International Learning Activities, January\n2008.\n8.7. EXERCISES 423\n8.8 Legalization of marijuana, Part I. The General Social Survey asked 1,578 US residents: “Do you think\nthe use of marijuana should be made legal, or not?” 61% of the respondents said it should be made legal.29\n(a) Is 61% a sample statistic or a population parameter? Explain.\n(b) Construct a 95% conﬁdence interval for the proportion of US residents who think marijuana should be\nmade legal, and interpret it in the context of the data.\n(c) A critic points out that this 95% conﬁdence interval is only accurate if the statistic follows a normal\ndistribution, or if the normal model is a good approximation. Is this true for these data? Explain.\n(d) A news piece on this survey’s ﬁndings states, “Majority of Americans think marijuana should be legal-\nized.” Based on your conﬁdence interval, is this news piece’s statement justiﬁed?\n8.9 National Health Plan, Part I. AKaiser Family Foundation poll for US adults in 2019 found that 79% of\nDemocrats, 55% of Independents, and 24% of Republicans supported a generic “National Health Plan”. There\nwere 347 Democrats, 298 Republicans, and 617 Independents surveyed.30\n(a) A political pundit on TV claims that a majority of Independents support a National Health Plan. Do these\ndata provide strong evidence to support this type of statement?\n(b) Would you expect a conﬁdence interval for the proportion of Independents who oppose the public option\nplan to include 0.5? Explain.\n8.10 Legalize Marijuana, Part II. As discussed in Exercise 8.8, the General Social Survey reported a sample\nwhere about 61% of US residents thought marijuana should be made legal. If we wanted to limit the margin\nof error of a 95% conﬁdence interval to 2%, about how many Americans would we need to survey?\n8.11 National Health Plan, Part II. Exercise 8.9 presents the results of a poll evaluating support for a generic\n“National Health Plan” in the US in 2019, reporting that 55% of Independents are supportive. If we wanted\nto estimate this number to within 1% with 90% conﬁdence, what would be an appropriate sample size?\n8.12 Acetaminophen and liver damage. It is believed that large doses of acetaminophen (the active ingre-\ndient in over the counter pain relievers like Tylenol) may cause damage to the liver. A researcher wants to\nconduct a study to estimate the proportion of acetaminophen users who have liver damage. For participating\nin this study, he will pay each subject $20 and provide a free medical consultation if the patient has liver\ndamage.\n(a) If he wants to limit the margin of error of his 98% conﬁdence interval to 2%, what is the minimum amount\nof money he needs to set aside to pay his subjects?\n(b) The amount you calculated in part (a) is substantially over his budget so he decides to use fewer subjects.\nHow will this a ﬀect the width of his conﬁdence interval?\n8.13 College smokers. We are interested in estimating the proportion of students at a university who\nsmoke. Out of a random sample of 200 students from this university, 40 students smoke.\n(a) Calculate a 95% conﬁdence interval for the proportion of students at this university who smoke, and\ninterpret this interval in context. (Reminder: Check conditions.)\n(b) If we wanted the margin of error to be no larger than 2% at a 95% conﬁdence level for the proportion of\nstudents who smoke, how big of a sample would we need?\n29National Opinion Research Center, General Social Survey, 2018.\n30Kaiser Family Foundation, The Public On Next Steps For The ACA And Proposals To Expand Coverage, data collected\nbetween Jan 9-14, 2019.\n424 CHAPTER 8. INFERENCE FOR CATEGORICAL DATA\n8.14 2010 Healthcare Law. On June 28, 2012 the U.S. Supreme Court upheld the much debated 2010\nhealthcare law, declaring it constitutional. A Gallup poll released the day after this decision indicates that\n46% of 1,012 Americans agree with this decision. At a 95% conﬁdence level, this sample has a 3% margin\nof error. Based on this information, determine if the following statements are true or false, and explain your\nreasoning.31\n(a) We are 95% conﬁdent that between 43% and 49% of Americans in this sample support the decision of the\nU.S. Supreme Court on the 2010 healthcare law.\n(b) We are 95% conﬁdent that between 43% and 49% of Americans support the decision of the U.S. Supreme\nCourt on the 2010 healthcare law.\n(c) If we considered many random samples of 1,012 Americans, and we calculated the sample proportions\nof those who support the decision of the U.S. Supreme Court, 95% of those sample proportions will be\nbetween 43% and 49%.\n(d) The margin of error at a 90% conﬁdence level would be higher than 3%.\n8.15 Oral contraceptive use, Part I. In a study of 100 randomly sampled 18 year-old women in an inner\ncity neighborhood, 15 reported that they were taking birth control pills.\n(a) Can the normal approximation to the binomial distribution be used to calculate a conﬁdence interval for\nproportion of women using birth control pills in this neighborhood? Explain your answer.\n(b) Compute an approximate 95% conﬁdence interval for the population proportion of women age 18 in this\nneighborhood taking birth control pills.\n(c) Does the interval from part (b) support the claim that, for the young women in this neighborhood, the\npercentage who use birth control is not signiﬁcantly di ﬀerent from the national average of 5%? Justify\nyour answer.\n8.16 Oral contraceptive use, Part II. Suppose that the study were repeated in a di ﬀerent inner city neigh-\nborhood and that out of 50 randomly sampled 18-year-old women, 6 reported that they were taking birth\ncontrol pills. The researchers would like to assess the evidence that the proportion of 18-year-old women\nusing birth control pills in this neighborhood is greater than the national average of 5%.\n(a) Can the normal approximation to the binomial distribution be used to conduct a hypothesis test of the\nnull hypothesis that the proportion of women using birth control pills in this neighborhood is equal to\n0.05? Explain your answer.\n(b) State the hypotheses for the analysis of interest and compute the p-value.\n(c) Interpret the results from part (b) in the context of the data.\n31Gallup, Americans Issue Split Decision on Healthcare Ruling, data collected June 28, 2012.\n8.7. EXERCISES 425\n8.7.2 Inference for the difference of two proportions\n8.17 Social experiment, Part I. A “social experiment\" conducted by a TV program questioned what people\ndo when they see a very obviously bruised woman getting picked on by her boyfriend. On two di ﬀerent\noccasions at the same restaurant, the same couple was depicted. In one scenario the woman was dressed\n“provocatively” and in the other scenario the woman was dressed “conservatively”. The table below shows\nhow many restaurant diners were present under each scenario, and whether or not they intervened.\nScenario\nProvocative Conservative Total\nInterveneYes 5 15 20\nNo 15 10 25\nTotal 20 25 45\nExplain why the sampling distribution of the di ﬀerence between the proportions of interventions under\nprovocative and conservative scenarios does not follow an approximately normal distribution.\n8.18 Heart transplant success. The Stanford University Heart Transplant Study was conducted to de-\ntermine whether an experimental heart transplant program increased lifespan. Each patient entering the\nprogram was o ﬃcially designated a heart transplant candidate, meaning that he was gravely ill and might\nbeneﬁt from a new heart. Patients were randomly assigned into treatment and control groups. Patients in the\ntreatment group received a transplant, and those in the control group did not. The table below displays how\nmany patients survived and died in each group.32\ncontrol treatment\nalive 4 24\ndead 30 45\nSuppose we are interested in estimating the di ﬀerence in survival rate between the control and treatment\ngroups using a conﬁdence interval. Explain why we cannot construct such an interval using the normal\napproximation. What might go wrong if we constructed the conﬁdence interval despite this problem?\n8.19 National Health Plan, Part III. Exercise 8.9 presents the results of a poll evaluating support for a\ngenerically branded “National Health Plan” in the United States. 79% of 347 Democrats and 55% of 617\nIndependents support a National Health Plan.\n(a) Calculate a 95% conﬁdence interval for the di ﬀerence between the proportion of Democrats and Inde-\npendents who support a National Health Plan ( pD\u0000pI), and interpret it in this context. We have already\nchecked conditions for you.\n(b) True or false: If we had picked a random Democrat and a random Independent at the time of this poll, it\nis more likely that the Democrat would support the National Health Plan than the Independent.\n8.20 Sleep deprivation, CA vs. OR, Part I. According to a report on sleep deprivation by the Centers\nfor Disease Control and Prevention, the proportion of California residents who reported insu ﬃcient rest or\nsleep during each of the preceding 30 days is 8.0%, while this proportion is 8.8% for Oregon residents. These\ndata are based on simple random samples of 11,545 California and 4,691 Oregon residents. Calculate a 95%\nconﬁdence interval for the di ﬀerence between the proportions of Californians and Oregonians who are sleep\ndeprived and interpret it in context of the data.33\n32B. Turnbull et al. “Survivorship of Heart Transplant Data”. In: Journal of the American Statistical Association 69 (1974),\npp. 74–80.\n33CDC, Perceived Insu ﬃcient Rest or Sleep Among Adults — United States, 2008.\n426 CHAPTER 8. INFERENCE FOR CATEGORICAL DATA\n8.21 Remdesivir in COVID-19. Remdesivir is an antiviral drug previously tested in animal models infected\nwith coronaviruses like SARS and MERS. As of May 2020, remdesivir has temporary approval from the FDA\nfor use in severely ill COVID-10 patients. A randomized controlled trial conducted in China enrolled 236\npatients with severe COVID-19; 158 were assigned to receive remdesivir and 78 to receive a placebo. In\nthe remdesivir group, 103 patients showed clinical improvement; in the placebo group, 45 patients showed\nclinical improvement.\n(a) Conduct a formal comparison of the clinical improvement rates and summarize your ﬁndings.\n(b) Report and interpret an appropriate conﬁdence interval.\n8.22 Sleep deprivation, CA vs. OR, Part II. Exercise 8.20 provides data on sleep deprivation rates of\nCalifornians and Oregonians. The proportion of California residents who reported insu ﬃcient rest or sleep\nduring each of the preceding 30 days is 8.0%, while this proportion is 8.8% for Oregon residents. These data\nare based on simple random samples of 11,545 California and 4,691 Oregon residents.\n(a) Conduct a hypothesis test to determine if these data provide strong evidence the rate of sleep deprivation\nis diﬀerent for the two states. (Reminder: Check conditions)\n(b) It is possible the conclusion of the test in part (a) is incorrect. If this is the case, what type of error was\nmade?\n8.23 Gender and color preference. A study asked 1,924 male and 3,666 female undergraduate college\nstudents their favorite color. A 95% conﬁdence interval for the di ﬀerence between the proportions of males\nand females whose favorite color is black ( pmale\u0000pfemale ) was calculated to be (0.02, 0.06). Based on this\ninformation, determine if the following statements are true or false, and explain your reasoning for each\nstatement you identify as false.34\n(a) We are 95% conﬁdent that the true proportion of males whose favorite color is black is 2% lower to 6%\nhigher than the true proportion of females whose favorite color is black.\n(b) We are 95% conﬁdent that the true proportion of males whose favorite color is black is 2% to 6% higher\nthan the true proportion of females whose favorite color is black.\n(c) 95% of random samples will produce 95% conﬁdence intervals that include the true di ﬀerence between\nthe population proportions of males and females whose favorite color is black.\n(d) We can conclude that there is a signiﬁcant di ﬀerence between the proportions of males and females whose\nfavorite color is black and that the di ﬀerence between the two sample proportions is too large to plausibly\nbe due to chance.\n(e) The 95% conﬁdence interval for ( pfemale\u0000pmale) cannot be calculated with only the information given in\nthis exercise.\n34L Ellis and C Ficek. “Color preferences according to gender and sexual orientation”. In: Personality and Individual\nDiﬀerences 31.8 (2001), pp. 1375–1379.\n8.7. EXERCISES 427\n8.24 Prenatal vitamins and Autism. Researchers studying the link between prenatal vitamin use and\nautism surveyed the mothers of a random sample of children aged 24 - 60 months with autism and con-\nducted another separate random sample for children with typical development. The table below shows the\nnumber of mothers in each group who did and did not use prenatal vitamins during the three months before\npregnancy (periconceptional period).35\nAutism\nAutism Typical development Total\nPericonceptional No vitamin 111 70 181\nprenatal vitamin Vitamin 143 159 302\nTotal 254 229 483\n(a) State appropriate hypotheses to test for independence of use of prenatal vitamins during the three months\nbefore pregnancy and autism.\n(b) Complete the hypothesis test and state an appropriate conclusion. (Reminder: Verify any necessary con-\nditions for the test.)\n(c) A New York Times article reporting on this study was titled “Prenatal Vitamins May Ward O ﬀAutism\".\nDo you ﬁnd the title of this article to be appropriate? Explain your answer. Additionally, propose an\nalternative title.36\n8.25 Sleep deprived transportation workers. The National Sleep Foundation conducted a survey on the\nsleep habits of randomly sampled transportation workers and a control sample of non-transportation workers.\nThe results of the survey are shown below.37\nTransportation Professionals\nTruck Train Bus/Taxi/Limo\nControl Pilots Drivers Operators Drivers\nLess than 6 hours of sleep 35 19 35 29 21\n6 to 8 hours of sleep 193 132 117 119 131\nMore than 8 hours 64 51 51 32 58\nTotal 292 202 203 180 210\nConduct a hypothesis test to evaluate if these data provide evidence of a di ﬀerence between the proportions\nof truck drivers and non-transportation workers (the control group) who get less than 6 hours of sleep per\nday, i.e. are considered sleep deprived.\n8.26 An apple a day keeps the doctor away. A physical education teacher at a high school wanting to in-\ncrease awareness on issues of nutrition and health asked her students at the beginning of the semester whether\nthey believed the expression “an apple a day keeps the doctor away”, and 40% of the students responded yes.\nThroughout the semester she started each class with a brief discussion of a study highlighting positive e ﬀects\nof eating more fruits and vegetables. She conducted the same apple-a-day survey at the end of the semester,\nand this time 60% of the students responded yes. Can she used a two-proportion method from this section\nfor this analysis? Explain your reasoning.\n35R.J. Schmidt et al. “Prenatal vitamins, one-carbon metabolism gene variants, and risk for autism”. In: Epidemiology\n22.4 (2011), p. 476.\n36R.C. Rabin. “Patterns: Prenatal Vitamins May Ward O ﬀAutism”. In: New York Times (2011).\n37National Sleep Foundation, 2012 Sleep in America Poll: Transportation Workers’ Sleep, 2012.\n428 CHAPTER 8. INFERENCE FOR CATEGORICAL DATA\n8.7.3 Inference for two or more groups\n8.27 True or false, Part I. Determine if the statements below are true or false. For each false statement,\nsuggest an alternative wording to make it a true statement.\n(a) The chi-square distribution, just like the normal distribution, has two parameters, mean and standard\ndeviation.\n(b) The chi-square distribution is always right skewed, regardless of the value of the degrees of freedom\nparameter.\n(c) The chi-square statistic is always positive.\n(d) As the degrees of freedom increases, the shape of the chi-square distribution becomes more skewed.\n8.28 True or false, Part II. Determine if the statements below are true or false. For each false statement,\nsuggest an alternative wording to make it a true statement.\n(a) As the degrees of freedom increases, the mean of the chi-square distribution increases.\n(b) If you found \u001f2= 10 withdf= 5 you would fail to reject H0at the 5% signiﬁcance level.\n(c) When ﬁnding the p-value of a chi-square test, we always shade the tail areas in both tails.\n(d) As the degrees of freedom increases, the variability of the chi-square distribution decreases.\n8.29 Quitters. Does being part of a support group a ﬀect the ability of people to quit smoking? A county\nhealth department enrolled 300 smokers in a randomized experiment. 150 participants were assigned to a\ngroup that used a nicotine patch and met weekly with a support group; the other 150 received the patch and\ndid not meet with a support group. At the end of the study, 40 of the participants in the patch plus support\ngroup had quit smoking while only 30 smokers had quit in the other group.\n(a) Create a two-way table presenting the results of this study.\n(b) Answer each of the following questions under the null hypothesis that being part of a support group does\nnot aﬀect the ability of people to quit smoking, and indicate whether the expected values are higher or\nlower than the observed values.\ni. How many subjects in the “patch + support\" group would you expect to quit?\nii. How many subjects in the “patch only\" group would you expect to not quit?\n8.30 Parasitic worm. Lymphatic ﬁlariasis is a disease caused by a parasitic worm. Complications of the\ndisease can lead to extreme swelling and other complications. Here we consider results from a randomized\nexperiment that compared three di ﬀerent drug treatment options to clear people of the this parasite, which\npeople are working to eliminate entirely. The results for the second year of the study are given below:38\nClear at Year 2 Not Clear at Year 2\nThree drugs 52 2\nTwo drugs 31 24\nTwo drugs annually 42 14\n(a) Set up hypotheses for evaluating whether there is any di ﬀerence in the performance of the treatments,\nand also check conditions.\n(b) Statistical software was used to run a chi-square test, which output:\nX2= 23:7 df= 2 p-value = 7.2e-6\nUse these results to evaluate the hypotheses from part (a), and provide a conclusion in the context of the\nproblem.\n38Christopher King et al. “A Trial of a Triple-Drug Treatment for Lymphatic Filariasis”. In: New England Journal of\nMedicine 379 (2018), pp. 1801–1810.\n8.7. EXERCISES 429\n8.31 PREVEND, Part IV. In the PREVEND data, researchers measured various features of study participants,\nincluding data on statin use and highest level of education attained. A two-way table of education level and\nstatin use is shown below.\nPrimary LowerSec UpperSec Univ Sum\nNonUser 31 111 107 136 385\nUser 20 46 27 22 115\nSum 51 157 134 158 500\n(a) Set up hypotheses for evaluating whether there is an association between statin use and educational level.\n(b) Check assumptions required for an analysis of these data.\n(c) Statistical software was used to conduct a \u001f2test: the test statistic is 19.054, with p-value 0.0027. Sum-\nmarize the conclusions in context of the data, and be sure to comment on the direction of association.\n8.32 Diabetes and unemployment. A Gallup poll surveyed Americans about their employment status and\nwhether or not they have diabetes. The survey results indicate that 1.5% of the 47,774 employed (full or part\ntime) and 2.5% of the 5,855 unemployed 18-29 year olds have diabetes.39\n(a) Create a two-way table presenting the results of this study.\n(b) State appropriate hypotheses to test for di ﬀerence in proportions of diabetes between employed and un-\nemployed Americans.\n(c) The sample di ﬀerence is about 1%. If we completed the hypothesis test, we would ﬁnd that the p-value\nis very small (about 0), meaning the di ﬀerence is statistically signiﬁcant. Use this result to explain the\ndiﬀerence between statistically signiﬁcant and practically signiﬁcant ﬁndings.\n8.33 TB Treatment. Tuberculosis (TB) is an infectious disease caused by the Mycobacterium tuberculosis bac-\nteria. Active TB can be cured by adhering to a treatment regimen of several drugs for 6-9 months. A major bar-\nrier to eliminating TB worldwide is failure to adhere to treatment; this is known as defaulting from treatment.\nA study was conducted in Thailand to identify factors associated with default from treatment. The study\nresults indicate that out of 54 diabetic participants, 0 defaulted from treatment; out of 1,180 non-diabetic\nparticipants, 54 defaulted from treatment. Participants were recruited at health centers upon diagnosis of TB.\n(a) Create a two-way table presenting the results of this study.\n(b) State appropriate hypotheses to test for di ﬀerence in proportions of treatment default between diabetics\nand non-diabetics.\n(c) Check assumptions. You may use a less stringent version of the success-failure condition: the expected\nnumber of successes per group should be greater than or equal to 5 (rather than 10).\n(d) Formally test whether the proportion of patients who default from treatment di ﬀers between diabetics\nand non-diabetics. Summarize your ﬁndings.\n39Gallup Wellbeing, Employed Americans in Better Health Than the Unemployed, data collected Jan. 2, 2011 - May 21,\n2012.\n430 CHAPTER 8. INFERENCE FOR CATEGORICAL DATA\n8.34 Coffee and Depression. Researchers conducted a study investigating the relationship between caf-\nfeinated co ﬀee consumption and risk of depression in women. They collected data on 50,739 women free of\ndepression symptoms at the start of the study in the year 1996, and these women were followed through 2006.\nThe researchers used questionnaires to collect data on ca ﬀeinated co ﬀee consumption, asked each individual\nabout physician- diagnosed depression, and also asked about the use of antidepressants. The table below\nshows the distribution of incidences of depression by amount of ca ﬀeinated co ﬀee consumption.40\nCaﬀeinated co ﬀee consumption\n\u00141 2-6 1 2-3 \u00154\ncup/week cups/week cup/day cups/day cups/day Total\nClinical Yes 670 373 905 564 95 2,607\ndepression No 11,545 6,244 16,329 11,726 2,288 48,132\nTotal 12,215 6,617 17,234 12,290 2,383 50,739\n(a) What type of test is appropriate for evaluating if there is an association between co ﬀee intake and depres-\nsion?\n(b) Write the hypotheses for the test you identiﬁed in part (a).\n(c) Calculate the overall proportion of women who do and do not su ﬀer from depression.\n(d) Identify the expected count for the highlighted cell, and calculate the contribution of this cell to the test\nstatistic.\n(e) The test statistic is \u001f2= 20:93. What is the p-value?\n(f) What is the conclusion of the hypothesis test?\n(g) One of the authors of this study was quoted on the NYTimes as saying it was “too early to recommend\nthat women load up on extra co ﬀee\" based on just this study.41Do you agree with this statement? Explain\nyour reasoning.\n8.35 Mosquito nets and malaria. This problem examines a hypothetical prospective study about an impor-\ntant problem in the developing world: the use of mosquito nets to prevent malaria in children. The nets are\ntypically used to protect children from mosquitoes while sleeping.\nSuppose that in a large region of an African country, 100 households with one child are randomized to\nreceive free mosquito nets for the child in the household and 100 households with one child are randomized\nto a control group where families do not receive the nets.\nYou are given the following information:\n– In the 100 households receiving the nets, 22 children became infected with malaria.\n– In the 100 households without the nets, 30 children became infected with malaria.\n– The 200 families selected to participate in the study may be regarded as a random sample from the\nfamilies in the region, so the 100 families in each group may be regarded as random samples from the\npopulation.\n– Malaria among children is common in this region, with a prevalence of approximately 25%.\n(a) Write down the 2 \u00022 contingency table that corresponds to the data from the trial, labeling the table\nclearly and including the row and column totals.\n(b) Under the hypothesis of no association between use of a mosquito net and malaria infection, calculate the\nexpected number of infected children among 100 families who did receive a net.\n(c) The\u001f2statistic for this 2 \u00022 table is 1.66. Use this information to conduct a test of the null hypothesis of\nno eﬀect of the use of a mosquito net on malaria infection in children.\n(d) Compute and interpret the estimated relative risk of malaria infection, comparing the households without\na net to those with a net.\n40M. Lucas et al. “Co ﬀee, caﬀeine, and risk of depression among women”. In: Archives of internal medicine 171.17 (2011),\np. 1571.\n41A. O’Connor. “Co ﬀee Drinking Linked to Less Depression in Women”. In: New York Times (2011).\n8.7. EXERCISES 431\n8.36 Health care fraud. Most errors in billing insurance providers for health care services involve honest\nmistakes by patients, physicians, or others involved in the health care system. However, fraud is a serious\nproblem. The National Health Care Anti-Fraud Association estimates that approximately $68 billion is lost\nto health care fraud each year. Often when fraud is suspected, an audit of randomly selected billings is\nconducted. The selected claims are reviewed by experts and each claim is classiﬁed as allowed or not allowed.\nThe claims not allowed are considered to be potentially fraudulent.\nIn general, the distribution of claims is highly skewed such that the majority of claims ﬁled are small\nclaims and only a few are large claims. Since simple random sampling would likely be overwhelmed by\nsmall claims, claims chosen for auditing are sampled in a stratiﬁed way: a set number of claims are sampled\nfrom each category of claim size: small, medium, and large. Here are data from an audit that used stratiﬁed\nsampling from three strata based on the claim size (i.e., monetary amount of the claim).\nStratum Sampled Claims Not Allowed\nSmall 100 10\nMedium 50 17\nLarge 20 4\n(a) Can these data be used to estimate the proportion of large claims for which fraud might be expected?\n(b) Can these data be used to estimate the proportion of possibly fraudulent claims that are large claims?\n(c) Construct a 2 \u00023 contingency table of counts for these data and include the marginal totals, with the rows\nbeing the classiﬁcation of claims and the columns being the size of the claim.\n(d) Calculate the expected number of claims that would not be allowed among the large claims, under the\nhypothesis of no association of between size of claim and the claim not being allowed.\n(e) Is the use of the chi-square statistic justiﬁed for these data?\n(f) A chi-square test of no association between size of claim and whether it was allowed has value 12.93.\nHow many degrees of freedom does the chi-square statistic have and what is the p-value for a test of no\nassociation?\n(g) Compute the \u001f2residuals. Based on the residuals, interpret the ﬁndings in the context of the data.\n8.37 Anxiety. Psychologists conducted an experiment to investigate the e ﬀect of anxiety on a person’s desire\nto be alone or in the company of others (Schacter 1959; Lehmann 1975). A group of 30 individuals were\nrandomly assigned into two groups; one group was designated the \"high anxiety\" group and the other the\n\"low anxiety\" group. Those in the high-anxiety group were told that in the \"upcoming experiment\", they\nwould be subjected to painful electric shocks, while those in the low-anxiety group were told that the shocks\nwould be mild and painless.42All individuals were informed that there would be a 10 minute wait before the\nexperiment began, and that they could choose whether to wait alone or with other participants.\nThe following table summarizes the results:\nWait Together Wait Alone Sum\nHigh-Anxiety 12 5 17\nLow-Anxiety 4 9 13\nSum 16 14 30\n(a) Under the null hypothesis of no association, what are the expected cell counts?\n(b) Under the assumption that the marginal totals are ﬁxed and the null hypothesis is true, what is the prob-\nability of the observed set of results?\n(c) Enumerate the tables that are more extreme than what was observed, in the same direction.\n(d) Conduct a formal test of association for the results and summarize your ﬁndings. Let \u000b= 0:05.\n42Individuals were not actually subjected to electric shocks of any kind\n432 CHAPTER 8. INFERENCE FOR CATEGORICAL DATA\n8.38 Salt intake and CVD. Suppose we are interested in investigating the relationship between high salt\nintake and death from cardiovascular disease (CVD). One possible study design is to identify a group of high-\nand low-salt users then follow them over time to compare the relative frequency of CVD death in the two\ngroups. In contrast, a less expensive study design is to look at death records, identify CVD deaths from non-\nCVD deaths, collect information about the dietary habits of the deceased, then compare salt intake between\nindividuals who died of CVD versus those who died of other causes. This design is called a retrospective\ndesign.\nSuppose a retrospective study is done in a speciﬁc county of Massachusetts; data are collected on men\nages 50-54 who died over a 1-month period. Of 35 men who died from CVD, 5 had a diet with high salt intake\nbefore they died, while of the 25 men who died from other causes, 2 had a diet with high salt intake. These\ndata are summarized in the following table.\nCVD Death Non-CVD Death Total\nHigh Salt Diet 5 2 7\nLow Salt Diet 30 23 53\nTotal 35 25 60\n(a) Under the null hypothesis of no association, what are the expected cell counts?\n(b) Of the 35 CVD deaths, 5 were in the high salt diet group and 30 were in the low salt diet group. Under the\nassumption that the marginal totals are ﬁxed, enumerate all possible sets of results (i.e., the table counts)\nthat are more extreme than what was observed, in the same direction.\n(c) Calculate the probability of observing each set of results from part (b).\n(d) Evaluate the statistical signiﬁcance of the observed data with a two-sided alternative. Let \u000b= 0:05. Sum-\nmarize your results.\n8.7.4 Chi-square tests for the ﬁt of a distribution\n8.39 Open source textbook. A professor using an open source introductory statistics book predicts that\n60% of the students will purchase a hard copy of the book, 25% will print it out from the web, and 15% will\nread it online. At the end of the semester he asks his students to complete a survey where they indicate what\nformat of the book they used. Of the 126 students, 71 said they bought a hard copy of the book, 30 said they\nprinted it out from the web, and 25 said they read it online.\n(a) State the hypotheses for testing if the professor’s predictions were inaccurate.\n(b) How many students did the professor expect to buy the book, print the book, and read the book exclusively\nonline?\n(c) This is an appropriate setting for a chi-square test. List the conditions required for a test and verify they\nare satisﬁed.\n(d) Calculate the chi-squared statistic, the degrees of freedom associated with it, and the p-value.\n(e) Based on the p-value calculated in part (d), what is the conclusion of the hypothesis test? Interpret your\nconclusion in this context.\n8.40 Barking Deer. Microhabitat factors associated with foraging sites of barking deer in Hainan Island,\nChina were examined. In this region, woods make up 4.8% of the land, cultivated grass plots make up 14.7%,\nand deciduous forests make up 39.6%. Of the 426 sites where the deer forage, 4 were categorized as woods,\n16 as cultivated grass plots, and 61 as deciduous forests. The table below summarizes these data.43\nwoods cultivated grassplot deciduous forests other total\n4 16 61 345 426\n(a) Write the hypotheses for testing if barking deer prefer to forage in certain habitats over others.\n(b) Check if the assumptions and conditions required for testing these hypotheses are reasonably met.\n(c) Do these data provide convincing evidence that barking deer prefer to forage in certain habitats over\nothers? Conduct an analysis and summarize your ﬁndings.\n43Liwei Teng et al. “Forage and bed sites characteristics of Indian muntjac (Muntiacus muntjak) in Hainan Island,\nChina”. In: Ecological Research 19.6 (2004), pp. 675–681.\n8.7. EXERCISES 433\n8.7.5 Outcome-based sampling: case-control studies\n8.41 CVD and Diabetes. An investigator asked for the records of patients diagnosed with diabetes in his\npractice, then sampled 20 patients with cardiovascular disease (CVD) and 80 patients without CVD. For the\nsampled patients, he then recorded whether or not the age of onset of diabetes was at age 50 or younger. Of\nthe 40 patients whose age of onset of diabetes was 50 years of age or earlier, 15 had cardiovascular disease. In\nthe remaining 60 patients, 5 had cardiovascular disease.\n(a) Write the contingency table that summarizes the result of this study.\n(b) What is the relative odds of cardiovascular disease, comparing the older patients to those less than 50\nyears old at onset of diabetes?\n(c) Interpret the relative odds of cardiovascular disease and comment on whether the relative odds cohere\nwith what you might expect.\n(d) In statistical terms, state the null hypothesis of no association between the presence of cardiovascular\ndisease and age of onset of diabetes.\n(e) What test can be used to test the null hypothesis? Are the assumptions for the test reasonably satisﬁed?\n(f) The value of the chi-square test statistic for this table is 11. Identify the logical ﬂaw in the following\nstatement: \"In this retrospective study of cardiovascular disease and diabetes, our study has demonstrated\nstatistically signiﬁcant evidence that diabetes increases the risk of cardiovascular disease.\"\n8.42 Blood thinners. Cardiopulmonary resuscitation (CPR) is a procedure commonly used on individuals\nsuﬀering a heart attack when other emergency resources are not available. This procedure is helpful in main-\ntaining some blood circulation, but the chest compressions involved can also cause internal injuries. Internal\nbleeding and other injuries complicate additional treatment e ﬀorts following arrival at a hospital. For in-\nstance, while blood thinners may be used to help release a clot that is causing a heart attack, the blood thinner\nwould have negative repercussions on any internal injuries.\nThis problem uses data from a study in which patients who underwent CPR for a heart attack and were\nsubsequently admitted to a hospital. These patients were randomly divided into a treatment group where\nthey received a blood thinner or the control group where they did not receive the blood thinner. The outcome\nvariable of interest was whether the patients survived for at least 24 hours.\nThe study results are shown in the table below:\nTreatment Control Total\nSurvived 14 11 25\nDied 26 39 65\nTotal 40 50 90\n(a) For this table, calculate the odds ratio for survival, comparing treatment to control, and the relative risk\nof survival, comparing treatment to control.\n(b) What is the interpretation of each of these two statistics?\n(c) In this study, which of the two summary statistics in part (a) is the better description of the treatment\neﬀect? Why?\n434 CHAPTER 8. INFERENCE FOR CATEGORICAL DATA\n8.43 CNS disorder. Suppose an investigator has studied the possible association between the use of a weight\nloss drug and a rare central nervous system (CNS) disorder. He samples from a group of volunteers with and\nwithout the disorder, and records whether they have used the weight loss drug. The data are summarized in\nthe following table:\nDrug Use\nCNS disorder Yes No\nYes 10 2000\nNo 7 4000\n(a) Can these data be used to estimate the probability of a CNS disorder for someone taking the weight loss\ndrug?\n(b) For this study, what is an appropriate measure of association between the weight-loss drug and the pres-\nence of CNS disorder?\n(c) Calculate the measure of association speciﬁed in part (b).\n(d) Interpret the calculation from part (c).\n(e) What test of signiﬁcance is the best choice for analyzing the hypothesis of no association for these data?\n8.44 Asthma risk. Asthma is a chronic lung disease characterized as hypersensitivity to a variety of stimuli,\nsuch as tobacco smoke, mold, and pollen. The prevalence of asthma has been increasing in recent decades,\nespecially in children. Some studies suggest that children who either live in a farm environment or have pets\nbecome less likely to develop asthma later in life, due to early exposure to elevated amounts of microorgan-\nisms. A large study was conducted in Norway to investigate the association between early exposure to animals\nand subsequent risk for asthma.\nUsing data from national registers, researchers identiﬁed 11,585 children known to have asthma at age 6\nyears out of the 276,281 children born in Norway between January 1, 2006 and December 31, 2009. Children\nwhose parents were registered as \"animal producers and related workers\" during the child’s ﬁrst year of life\nwere deﬁned as being exposed to farm animals. Of the 958 children exposed to farm animals, 19 had an\nasthma diagnosis at age 6.\n(a) Do these data support previous ﬁndings that living in a farm environment is associated with lower risk\nof childhood asthma? Conduct a formal analysis and summarize your ﬁndings. Be sure to check any\nnecessary assumptions.\n(b) Is the relative risk an appropriate measure of association for these data? Brieﬂy explain your answer.\n(c) In language accessible to someone who has not taken a statistics course, explain whether these results\nrepresent evidence that exposure to farm animals reduces the risk of developing asthma. Limit your\nanswer to no more than seven sentences.\n8.45 Tea consumption and carcinoma. In a study examining the association between green tea consump-\ntion and esophageal carcinoma, researchers recruited 300 patients with carcinoma and 571 without carcinoma\nand administered a questionnaire about tea drinking habits. Out of the 47 individuals who reported that they\nregularly drink green tea, 17 had carcinoma. Out of the 824 individuals who reported they never drink green\ntea, 283 had carcinoma.\n(a) Analyze the data to assess evidence for an association between green tea consumption and esophageal\ncarcinoma from these data. Summarize your results.\n(b) Report and interpret an appropriate measure of association.\n435\nAppendix A\nEnd of chapter exercise solutions\n1 Introduction to data\n1.1 (a) Treatment: 10 =43 = 0:23!23%.\n(b) Control: 2 =46 = 0:04!4%. (c) A higher percentage of patients in the treatment group were pain free\n24 hours after receiving acupuncture. (d) It is possible that the observed di ﬀerence between the two group\npercentages is due to chance.\n1.3 (a) “Is there an association between air pollution exposure and preterm births?\" (b) 143,196 births in\nSouthern California between 1989 and 1993. (c) Measurements of carbon monoxide, nitrogen dioxide, ozone,\nand particulate matter less than 10 \u0016g=m3(PM 10) collected at air-quality-monitoring stations as well as length\nof gestation. Continuous numerical variables.\n1.5 (a) “Does explicitly telling children not to cheat a ﬀect their likelihood to cheat?\". (b) 160 children between\nthe ages of 5 and 15. (c) Four variables: (1) age (numerical, continuous), (2) sex (categorical), (3) whether they\nwere an only child or not (categorical), (4) whether they cheated or not (categorical).\n1.7 (a) Control: the group of 16 female birds that received no treatment. Treatment: the group of 16 female\nbirds that were given supplementary diets.\n(b) \"Does egg coloration indicate the health of female collared ﬂycatchers?\"\n(c) Darkness of blue color in female birds’ eggs. Continuous numerical variable.\n1.9 (a) Each row represents a participant.\n(b) The response variable is colon cancer stage. The explanatory variables are the abundance levels of the ﬁve\nbacterial species.\n(c) Colon cancer stage: ordinal categorical variable. Abundance levels of bacterial species: continuous numer-\nical variable.\n1.11 (a) The population of interest consists of babies born in Southern California. The sample consists of the\n143,196 babies born between 1989 and 1993 in Southern California.\n(b) Assuming that the sample is representative of the population of interest, the results of the study can be\ngeneralized to the population. The ﬁndings cannot be used to establish causal relationships because the study\nwas an observational study, not an experiment.\n1.13 (a) The population of interest consists of asthma patients who rely on medication for asthma treatment.\nThe sample consists of the 600 asthma patients ages 18-69 who participated in the study.\n(b) The sample may not be representative of the population because study participants were recruited, an\nexample of a convenience sample. Thus, the results of the study may not be generalizable to the population.\nThe ﬁndings can be used to establish causal relationships because the study is an experiment conducted with\ncontrol, randomization, and a reasonably large sample size.\n436 APPENDIX A. END OF CHAPTER EXERCISE SOLUTIONS\n1.15 (a) Experiment.\n(b) The experimental group consists of the chicks that received vitamin supplements. The control group\nconsists of the chicks that did not receive vitamin supplements.\n(c) Randomization ensures that there are not systematic di ﬀerences between the control and treatment groups.\nEven if chicks may vary in ways that a ﬀect body mass and corticosterone levels, random allocation essentially\nevens out such di ﬀerences, on average, between the two groups. This is essential for a causal interpretation of\nthe results to be valid.\n1.17 (a) Observational study.\n(b) Answers may vary. One possible confounding variable is the wealth of a country. A wealthy country’s\ncitizens tend to have a higher life expectancy due to a higher quality of life, and the country tends to have a\nhigher percentage of internet users because there is enough money for the required infrastructure and citizens\ncan aﬀord computers. Wealth of a country is associated with both estimated life expectancy and percentage of\ninternet users. Omitting the confounder from the analysis distorts the relationship between the two variables,\nsuch that there may seem to be a direct relationship when there is not.\n1.19 (a) Simple random sampling is reasonable if 500 students is a large enough sample size relative to the\ntotal student population of the university.\n(b) Since student habits may vary by ﬁeld of study, stratifying by ﬁeld of study would be a reasonable decision.\n(c) Students in the same class year may have more similar habits. Since clusters should be diverse with respect\nto the outcome of interest, this would not be a good approach.\n1.21 (a) Non-responders may have a di ﬀerent response to this question, e.g. parents who returned the sur-\nveys likely don’t have di ﬃculty spending time with their children.\n(b) It is unlikely that the women who were reached at the same address 3 years later are a random sample.\nThese missing responders are probably renters (as opposed to homeowners) which means that they might be\nin a lower socio-economic class than the respondents.\n(c) This is an observational study, not an experiment, so it is not advisable to draw conclusions about causal\nrelationships. The relationship may be in the other direction; i.e., that these people go running precisely be-\ncause they do not have joint problems. Additionally, the data are not even su ﬃcient to provide evidence of an\nassociation between running and joint problems because data have only been collected from individuals who\ngo running regularly. Instead, a sample of individuals should be collected that includes both people who do\nand do not regularly go running; the number of individuals in each group with joint problems can then be\ncompared for evidence of an association.\n1.23 The lead author’s statements are not accurate because he or she drew conclusions about causation (that\nincreased alcohol sales taxes lower rates of sexually transmitted infections) from an observational study. In\naddition, although the study observed that there was a decline in gonorrhea rate, the lead author generalized\nthe observation to all sexually transmitted infections.\n1.25 (a) Randomized controlled experiment. (b) Explanatory: treatment group (categorical, with 3 levels).\nResponse variable: Psychological well-being. (c) No, because the participants were volunteers. (d) Yes, because\nit was an experiment. (e) The statement should say “evidence” instead of “proof”.\n437\n1.27 (a) The two distributions have the same median since they have the same middle number when ordered\nfrom least to greatest. Distribution 2 has a higher IQR because its ﬁrst and third quartiles are farther apart\nthan in Distribution 2.\n(b) Distribution 2 has a higher median since it has a higher middle number when ordered from least to great-\nest. Distribution 2 has a higher IQR because its ﬁrst and third quartiles are farther apart than in Distribution 1.\n(c) Distribution 2 has a higher median since all values in this distribution are higher than in Distribution 1.\nThe two distributions have the same IQR since the distance between the ﬁrst and third quartiles in each dis-\ntribution is the same.\n(d) Distribution 2 has a higher median since most values in this distribution are higher than those in Distri-\nbution 1. Distribution 2 has a higher IQR because its ﬁrst and third quartiles are farther apart than those of\nDistribution 1.\n1.29 (a) The distribution is bimodal, with peaks between 15-20 and 25-30. Values range from 0 to 65.\n(b) The median AQI is about 30.\n(c) I would expect the mean to be higher than the median, since there is some right skewing.\n1.31 (a) The median is a much better measure of the typical amount earned by these 42 people. The mean\nis much higher than the income of 40 of the 42 people. This is because the mean is an arithmetic average and\ngets aﬀected by the two extreme observations. The median does not get e ﬀected as much since it is robust to\noutliers. (b) The IQR is a much better measure of variability in the amounts earned by nearly all of the 42\npeople. The standard deviation gets a ﬀected greatly by the two high salaries, but the IQR is robust to these\nextreme observations.\n1.33 (a) These data are categorical. They can be summarized numerically in either a frequency table or\nrelative frequency table, and summarized graphically in a bar plot of either counts or proportions.\n(b) The results of these studies cannot be generalized to the larger population. Individuals taking the survey\nrepresent a speciﬁc subset of the population that are conscious about dental health, since they are at the\ndentist’s o ﬃce for an appointment. Additionally, there may be response bias; even though the surveys are\nanonymous, it is likely that respondents will feel some pressure to give a \"correct\" answer in such a setting,\nand claim to ﬂoss more often than they actually do.\n1.35 (a) Yes, there seems to be a positive association between lifespan and length of gestation. Generally, as\ngestation increases, so does life span.\n(b) Positive association. Reversal of the plot axes does not change the nature of an association.\n1.37 (a) 75% of the countries have an adolescent fertility rate less than or equal to 75.73 births per 1,000\nadolescents.\n(b) It is likely that the observations are missing due to the Iraq War and general instability in the region during\nthis time period. It is unlikely that the ﬁve-number summary would have been a ﬀected very much, even if the\nvalues were extreme; the median and IQR are robust estimates, and the dataset is relatively large, with data\nfrom 188 other countries.\n(c) The median and IQR decreases each year, with Q1 and Q3 also decreasing.\n1.39 (a) 4,371/8,474 = 0.56 !56%\n(b) 110/190 = 0.58 !58%\n(c) 27/633 = 0.04 !4%\n(d) 53/3,110 = 0.02 !2%\n(e) Relative risk:27=633\n53=3;110= 2:50. Yes, since the relative risk is greater than 1. A relative risk of 2.50 indicates\nthat individuals with high trait anger are 2.5 times more likely to experience a CHD event than individuals\nwith low trait anger.\n(f) Side-by-side boxplots, since blood cholesterol level is a numerical variable and anger group is categorical.\n438 APPENDIX A. END OF CHAPTER EXERCISE SOLUTIONS\n2 Probability\n2.1 (a) False. These are independent trials.\n(b) False. There are red face cards.\n(c) True. A card cannot be both a face card and an ace.\n2.3 (a)1\n4.\nSolution 1: A colorblind male has genotype X\u0000Y. He must have inherited X\u0000from his mother (probability of\n1\n2) andYfrom his father (probability of1\n2). Since these are two independent events, the probability of both\noccuring is (1\n2)(1\n2) =1\n4.\nSolution 2: Determine the possibilities using a Punnett square. There are 4 equally likely possibilities, one of\nwhich is a colorblind male. Thus, the probability is1\n4.\nX+Y\nX+X+X+X+Y\nX\u0000X+X\u0000X\u0000Y\n(b) True. An o ﬀspring of this couple cannot be both female and colorblind.\n2.5 (a) 0.25. Let Hrepresent the event of being a high school graduate and Frepresent the event of being a\nwoman.P(H) =P(HandW) +P(HandWC) =P(HjW)P(W) +P(HjWC)P(WC) = (0:20)(0:50) + (0:30)(0:50) =\n0:25.\n(b) 0.91.(AC) =P(ACandW) +P(ACandWC) = (1\u00000:09) + (1\u00000:09) = 0:91.\n(c) 0.25. Let Xrepresent the event of having at least a Bachelor’s degree, where Brepresents the event of\nattaining at most a Bachelor’s degree and Gthe event of attaining at most a graduate or professional degree.\nP(XjWC) =P(BjWC) +P(GjWC) = 0:16 + 0:09 = 0:25.\n(d) 0.26.P(XjW) =P(BjW) +P(GjW) = 0:17 + 0:09 = 0:26.\n(e) 0.065. Let XWbe the event that a woman has at least a Bachelor’s degree, and XMbe the event that a man\nhas at least a Bachelor’s degree. Assuming that the education levels of the husband and wife are independent,\nP(XWandXM) =P(XW)\u0002P(XM) = (0:25)(0:26) = 0:065. This assumption is probably not reasonable, because\npeople tend to marry someone with a comparable level of education.\n2.7 (a) LetCrepresent the event that one urgent care center sees 300-449 patients in a week. Assuming that\nthe number of patient visits are independent between urgent care centers in a given county for a given week,\nthe probability that three random urgent care centers see 300-449 patients in a week is [ P(C)]3= (0:288)3=\n0:024. This assumption is not reasonable because a county is a small area with relatively few urgent care\ncenters; if one urgent care center takes in more patients than usual during a given week, so might other\nurgent care centers in the same county (e.g., this could occur during ﬂu season).\n(b) 2:32\u000210\u00007. LetDrepresent the event that one urgent care center sees 450 or more patients in a week.\nAssuming independence, the probability that 10 urgent care centers throughout a state all see 450 or more\npatients in a week is [ P(D)]10= (0:217)10= 2:32\u000210\u00007. This assumption is reasonable because a state is a\nlarge area that contains many urgent care centers; the number of patients one urgent care center takes in is\nlikely independent of the number of patients another urgent care center in the state takes in.\n(c) No, it is not possible, because it is not reasonable to assume that the patient visits for a given week are\nindependent of those for the following week.\n2.9 (a) If the class is not graded on a curve, they are independent. If graded on a curve, then neither indepen-\ndent nor disjoint – unless the instructor will only give one A, which is a situation we will ignore in parts (b)\nand (c). (b) They are probably not independent: if you study together, your study habits would be related,\nwhich suggests your course performances are also related. (c) No. See the answer to part (a) when the course\nis not graded on a curve. More generally: if two things are unrelated (independent), then one occurring does\nnot preclude the other from occurring.\n439\n2.11 (a) 0:60 + 0:20\u00000:18 = 0:62\n(b) 0:18=0:20 = 0:90\n(c) 0:11=0:33 = 0:33\n(d) No, because the answers to parts (c) and (d) are not equal. If global warming belief were independent of\npolitical party, then among liberal Democrats and conservative Republicans, there would be equal proportions\nof people who believe the earth is warming.\n(e) 0:06=0:34 = 0:18\n2.13 (a) 375;264=436;968 = 0:859\n(b) 229;246=255;980 = 0:896\n(c) 0.896. This is equivalent to (b).\n(d) 146;018=180;988 = 0:807\n(e) 4;719=7;394 = 0:638\n(f) No, because the answers to (c) and (d) are not equal. If gender and seat belt usage were independent, then\namong males and females, there would be the same proportion of people who always wear seat belts.\n2.15 The PPV is 0.8248. The NPV is 0.9728.\nP(DjT+) =P(T+jD)P(D)\nP(T+jD)P(D)+P(T+jDC)P(DC)=(0:997)(0:0259)\n(0:997)(0:0259)+(1\u00000:926)(1\u00000:259)= 0:8248.\nP(DCjT\u0000) =P(T\u0000jDC)P(DC)\nP(T\u0000jDC)P(DC)+P(T\u0000jD)P(D)=(0:926)(1\u00000:259)\n(0:926)(1\u00000:259)+(1\u00000:997)(0:259)= 0:9728.\nHIV? Result\nyes,  0.259positive,  0.9970.259*0.997 = 0.2582\nnegative,  0.0030.259*0.003 = 0.0008\nno,  0.741positive,  0.0740.741*0.074 = 0.0548\nnegative,  0.9260.741*0.926 = 0.6862\n2.17 0.0714. Even when a patient tests positive for lupus, there is only a 7.14% chance that he actually has\nlupus. House may be right.\nLupus? Result\nyes,  0.02positive,  0.980.02*0.98 = 0.0196\nnegative,  0.020.02*0.02 = 0.0004\nno,  0.98positive,  0.260.98*0.26 = 0.2548\nnegative,  0.740.98*0.74 = 0.7252\n2.19 (a) LetErepresent the event of agreeing with the idea of evolution and Dbe the event of being a\nDemocrat. From the problem statement, P(EjD) = 0:67.P(ECjD) = 1\u0000P(EjD) = 1\u00000:67 = 0:33.\n(b) LetIrepresent the event of being an independent. P(EjI) = 0:65, as stated in the problem.\n(c) LetRrepresent the event of being a Republican. P(EjR) = 1\u0000P(ECjR) = 1\u00000:48 = 0:52.\n(d) 0.35.P(RjE) =P(EandR)\nP(E)=P(R)P(EjR)\nP(E)=(0:40)(0:52)\n0:60= 0:35.\n2.21 Mumps is the most likely disease state, since P(B3jA) = 0:563,P(B1jA) = 0:023, andP(B2jA) =:415.\nP(BijA) =P(AjBi)P(Bi)\nP(A).P(A) =P(AandB1)+P(AandB2)+P(AandB3) =P(AjB1)P(B1)+P(AjB2)P(B2)+P(AjB3)P(B3).\n2.23 (a) LetAbe the event of knowing the answer and Bbe the event of answering it correctly. Assume that\nif a participant knows the correct answer, they answer correctly with probability 1: P(BjA) = 1. If they guess\nrandomly, they have 1 out of mchances to answer correctly, thus P(BjAC) = 1=m.P(AjB) =1\u0001p\n(1\u0001p)+(1\nm\u0001(1\u0000p))=\np\np+1\u0000p\nm.\n(b) 0.524. Let Abe the event of having an IQ over 150 and Bbe the event of receiving a score indicating an IQ\nover 150. From the problem statement, P(BjA) = 1 andP(BjAC) = 0:001.P(ACjB) =0:001\u0001(1\u00001\n1;100)\n(1\u0001(1\n1;100))+(0:001\u0001(1\u00001\n1;100))=\n0:524.\n440 APPENDIX A. END OF CHAPTER EXERCISE SOLUTIONS\n2.25 (a) In descending order on the table, the PPV for each age group is 0.003, 0.064, 0.175, 0.270; the NPV\nfor each age group is 0.999, 0.983, 0.948, 0.914.\n(b) As prevalence of prostate cancer increases by age group, PPV also increases. However, with rising preva-\nlence, NPV decreases.\n(c) The probability that a man has prostate cancer, given a positive test, necessarily increases as the overall\nprobability of having prostate cancer increases. If more men have the disease, the chance of a positive test\nresult being a true positive increases (and the chances of the result being a false positive decreases). The de-\ncreasing NPV values follow similar logic: if more men have the disease, the chance of a negative test being a\ntrue negative decreases (and the chances of the result being a false negative increases).\n(d) Lowering the cuto ﬀfor a positive test would result in more men testing positive, since men with PSA val-\nues 2.5 ng/ml to 4.1 ng/ml were not previously classiﬁed as testing positive. Since the sensitivity of a test is\nthe proportion who test positive among those who have disease, and the number with disease does not change,\nthe proportion will increase, except in the rare and unlikely situation where the additional positive tests are\namong only men without the disease.\n2.27 (a) Frequency of X+X+: 0.863. Frequency of X+X\u0000: 0.132. Frequency of X\u0000X\u0000: 0.005. Frequency of\nX\u0000Y: 0.07. Frequency of X+Y: 0.93. From frequency of X\u0000X\u0000, frequency of X\u0000allele isp\n0:005 = 0:071; thus,\nfrequency of X+allele is 1\u00000:071 = 0:929. Frequency of X+Yis 1\u00000:093 = 0:07.\n(b) 0.033. Let Abe the event that two parents are not colorblind, and Brepresent the event of having a\ncolorblind child. On the tree, \u0002represents a mating between two genotypes. P(BjA) = [P(X+X+\u0002X+YjA)\u0001\nP(BjX+X+\u0002X+Y)] + [P(X+X\u0000\u0002X+YjA)\u0001P(BjX+X\u0000\u0002X+Y)] = (0:867)(0) + (0:133)(1=4) = 0:033.\nA\nX+X+\u0002X+Y\nBBCX+X\u0000\u0002X+Y\nBBC\n2.29 (a) Calculate P(M\\B), the probability a dog has a facial mask and a black coat. Note that the event M\nconsists of having either a unilateral mask or a bilateral mask.\nP(M\\B) =P(M1\\B) +P(M2\\B)\n=P(M1jB)P(B) +P(M2jB)P(B)\n=(0:25)(0:40) + (0:35)(0:40)\n=0:24\nThe probability an Australian cattle dog has a facial mask and a black coat is 0.31.\n(b) Calculate P(M2), the prevalence of bilateral masks. The event of having a bilateral mask can be partitioned\ninto either having a bilateral mask and a red coat or having a bilateral mask and a black coat.\nP(M2) =P(R\\M2) +P(B\\M2)\n=P(M2jR)P(R) +P(M2jB)P(B)\n=(0:10)(0:60) + (0:35)(0:40)\n=0:20\nThe prevalence of bilateral masks in Australian cattle dogs is 0.20.\n(c) Calculate P(RjM2), the probability of having a red coat given having a bilateral mask. Apply the deﬁnition\nof conditional probability.\n441\nP(RjM2) =P(R\\M2)\nP(M2)\n=P(M2jR)P(R)\nP(M2)\n=(0:10)(0:60)\n0:20\n=0:30\nThe probability of being a Red Heeler among Australian cattle dogs with bilateral facial masks is 0.30.\n(d) The following new information has been introduced:\n-P(D1jR;M 0) =P(D1jR;M 1) = 0:15,P(DCjR;M 0) =P(DCjR;M 1) = 0:60.\n-P(M2\\D2\\R) = 0:012,P(M2\\D1\\R) = 0:045\n-P(M2\\D2\\B) = 0:012,P(M2\\D1\\B) = 0:045\n-P(D1jM0;B) =P(D1jM1;B) = 0:05,P(D2jM0;B) =P(D2jM1;B) = 0:01\ni. Calculate P(M2\\DC\\R).\nP(M2\\DC\\R) =P(DCjM2;R)P(M2jR)P(R)\n=P(DCjM2;R)(0:10)(0:60)\nTo calculate P(DCjM2;R), ﬁrst calculate P(D1jM2;R) andP(D2jM2;R) from the joint probabilities given\nin the problem, then apply the complement rule.\nP(D1jM2;R) =P(M2\\D1\\R)\nP(M2\\R)=0:045\n(0:10)(0:60)= 0:75\nP(D2jM2;R) =P(M2\\D2\\R)\nP(M2\\R)=0:012\n(0:10)(0:60)= 0:20\nBack to the original question...\nP(M2\\DC\\R) =P(DCjM2;R)P(M2jR)P(R)\n=P(DCjM2;R)(0:10)(0:60)\n=[1\u0000(0:75 + 0:20)](0:10)(0:60)\n=(0:05)(0:10)(0:60)\n=0:003\nThe probability that an Australian cattle dog has a bilateral mask, no hearing deﬁcits, and a red coat is\n0.003.\nii. Calculate P(DCjM2;B).\nP(DCjM2;B) =1\u0000[P(D1jM2;B) +P(D2jM2;B)]\n=1\u0000\"P(D1\\M2\\B)\nP(M2\\B)+P(D2\\M2\\B)\nP(M2\\B)#\n=1\u0000\"0:045\nP(M2jB)P(B)+0:012\nP(M2jB)P(B)#\n=1\u0000\"0:045\n(0:35)(0:40)+0:012\n(0:35)(0:40)#\n=0:593\nThe proportion of bilaterally masked Blue Heelers without hearing deﬁcits is 0.593.\niii. Calculate P(DjR) andP(DjB).\n442 APPENDIX A. END OF CHAPTER EXERCISE SOLUTIONS\nP(DjR) =P(D\\M0jR) +P(D\\M1jR) +P(D\\M2jR)\n=[1\u0000P(DCjR;M 0)](P(M0jR) + [1\u0000P(DCjR;M 1)](P(M1jR) + [1\u0000P(DCjM2;R)](P(M2jR)\n=(1\u00000:60)(0:50) + (1\u00000:60)(0:40) + (1\u00000:05)(0:10)\n=0:455\nP(DjB) =P(D\\M0jB) +P(D\\M1jB) +P(D\\M2jB)\n=[P(D1jB;M 0) +P(D2jB;M 0)](P(M0jB) + [P(D1jB;M 0) +P(D2jB;M 0)](P(M1jB)\n+ [1\u0000P(DCjM2;B)](P(M2jB)\n=(0:05 + 0:01)(0:40) + (0:05 + 0:01)(0:25) + (1\u00000:593)(0:35)\n=0:181\nThe prevalence of deafness among Red Heelers is higher, at 0.455 versus 0.181 in Blue Heelers.\niv. Calculate P(BjDC).\nP(BjDC) =P(B\\DC)\nP(DC)\n=P(DCjB)P(B)\nP(DC\\B) +P(DC\\R)\n=[1\u0000P(DjB)]P(B)\n[1\u0000P(DjB)]P(B) + [1\u0000P(DjR)]P(R)\n=(1\u00000:181)(0:40)\n(1\u00000:181)(0:40) + (1\u00000:455)(0:60)\n=0:50\nThe probability that a dog is a Blue Heeler given that it is known to have no hearing deﬁcits is 0.50.\n3 Distributions of random variables\n3.1 (a) 13. (b) No, these 27 students are not a random sample from the university’s student population. For\nexample, it might be argued that the proportion of smokers among students who go to the gym at 9 am on a\nSaturday morning would be lower than the proportion of smokers in the university as a whole.\n3.3 (a) The probability of drawing three hearts equals (13 =52)(12=51)(11=50) = 0:0129, and the probability of\ndrawing three black cards equals (26 =52)(25=51)(24=50) = 0:1176; thus, the probability of any other draw is\n1\u00000:0129\u00000:1176 = 0:8694.E(X) = 0:0129(50)+0 :1176(25)+0 :8694(0) = 3:589:Var (X) = 0:0129(50\u00003:589)2+\n0:1176(25\u00003:589)2+ 0:8694(0\u00003:589)2= 93:007.SD(X) =p\nVar(X) = 9:644:\n(b) LetYrepresent the net proﬁt/loss, where Y=X\u00005.E(Y) =E(X\u00005) =E(X)\u00005 =\u00001:412. Standard deviation\ndoes not change from a shift of the distribution; SD(Y) =SD(X) = 9:644.\n(c) It is not advantageous to play, since the expected winnings are lower than $5.\n3.5 (a) 215 eggs. Let Xrepresent the number of eggs laid by one gull. E(X) = 0:25(1) + 0:40(2) + 0:30(3) +\n0:05(4) = 2:15:E(100X) = 100E(X) = 215:\n(b) 85.29 eggs. Var(X) = 0:25(1\u00002:15)2+0:40(2\u00002:15)2+0:30(3\u00002:15)2+0:05(4\u00002:15)2= 0:7275:Var (100X) =\n1002Var(X) = 7275!p\n7275 = 85:29:\n443\n3.7 (a) Binomial conditions are met: (1) Independent trials: In a random sample across the US, it is reasonable\nto assume that whether or not one 18-20 year old has consumed alcohol does not depend on whether or not\nanother one has. (2) Fixed number of trials: n= 10. (3) Only two outcomes at each trial: Consumed or did not\nconsume alcohol. (4) Probability of a success is the same for each trial: p= 0:697.\n(b) LetXbe the number of 18-20 year olds who have consumed alcohol; X\u0018Bin(10;0:697).P(X= 6) = 0:203.\n(c) LetYbe the number of 18-20 year olds who have not consumed alcohol; Y\u0018Bin(10;1\u00000:697).P(Y= 4) =\nP(X= 6) = 0:203.\n(d)X\u0018Bin(5;0:697).P(X\u00142) = 0:167.\n(e)X\u0018Bin(5;0:697).P(X\u00151) = 1\u0000P(X= 0) = 0:997.\n3.9 (a)\u001634:85,\u001b= 3:25 (b)Z=45\u000034:85\n3:25= 3:12. 45 is more than 3 standard deviations away from the mean,\nwe can assume that it is an unusual observation. Therefore yes, we would be surprised. (c) Using the normal\napproximation, 0.0009. With 0.5 correction, 0.0015.\n3.11 (a) Both O+ and O- individuals can donate blood to a Type O+ patient; n= 15,p= 0:45.\u0016=np= 6:75.\n\u001b=p\nnp(1\u0000p) = 1:93.\n(b) Only O- individuals can donate blood to a Type O- patient; n= 15,p= 0:08.P(X\u00153) = 0:113.\n3.13 0.132. LetXbe the number of IV drug users who contract Hepatitis C within a month; X\u0018Bin(5;0:30),\nP(X= 3) = 0:132.\n3.15 (a) LetXrepresent the number of infected stocks in the sample; X\u0018Bin(250;0:30).P(X= 60) = 0:006.\n(b)P(X\u001460) = 0:021.\n(c)P(X\u001580) = 0:735.\n(D) 40% of 250 is 100. P(X\u0014100) = 0:997. Yes, this seems reasonable; it is essentially guaranteed that within\na sample of 250, no more than 40% will be infected.\n3.17 (a) (200)(0:12) = 24 cases of hyponatremia are expected during the marathon.\n(b) LetXrepresent the number of cases of hyponatremia during the marathon. P(X>30) = 0:082.\n3.19 (a) 8.85%. (b) 6.94%. (c) 58.86%. (d) 4.56%.\n(a)−1.350\n(b)01.48\n(c)0\n(d)−2 02\n3.21 (a) 0.005. (b) 0.911. (c) 0.954. (d) 1.036. (e) -0.842\n3.23 (a) Verbal:N(\u0016= 151;\u001b= 7), Quant: N(\u0016= 153;\u001b= 7:67).ZVR= 1:29,ZQR= 0:52. She did better on\nthe Verbal Reasoning section since her Z-score on that section was higher.\nVR\nZ = 1.29QR\nZ = 0.52\n(b)PercVR= 0:9007\u001990%,PercQR= 0:6990\u001970%. 100%\u000090% = 10% did better than her on VR, and\n100%\u000070% = 30% did better than her on QR.\n(c) 159. (d) 147.\n3.25 (a) 0.115. (b) The coldest 10% of days are colder than 70.59\u000eF.\n3.27 (a) 0.023. (b) 72.66 mg/dL.\n444 APPENDIX A. END OF CHAPTER EXERCISE SOLUTIONS\n3.29 (a) 82.4%. (b) About 38 years of age.\n3.31 (a)n= 50, andp= 0:70.\u0016=np= 35.\u001b=p\nnp(1\u0000p) = 3:24.\n(b) Bothnpandn(1\u0000p) are greater than 10. Thus, it is valid to approximate the distribution as X\u0018N(35;3:24),\nwhereXis the number of 18-20 year olds who have consumed alcohol. P(X\u001545) = 0:001.\n3.33 LetXrepresent the number of students who accept the o ﬀer;X\u0018Bin(2500;0:70). This distribution\ncan be approximated by a N(1750;22:91). The approximate probability that the school does not have enough\ndorm room spots equals P(X\u00151;786) = 0:06.\n3.35 The data appear to follow a normal distribution, since the points closely follow the line on the normal\nprobability plot. There are some small deviations, but this is to be expected for such a small sample size.\n3.37 (a)P(X= 2) =exp\u00002(22)\n2!= 0:271. (b)P(X\u00142) =P(X= 0) +P(X= 1) +P(X= 2) = 0:677. (c)P(X\u00153) =\n1\u0000P(X\u00142) = 0:323.\n3.39 (a)\u0016=\u0015= 75,\u001b=p\n\u0015= 8:66. (b)Z=\u00001:73. Since 60 is within 2 standard deviations of the mean, it\nwould not generally be considered unusual. Note that we often use this rule of thumb even when the normal\nmodel does not apply. (c) Using Poisson with \u0015= 75: 0.0402.\n3.41 (a) The expected number of cases of osteosarcoma in NYC in a given year is 11.2. (b) Let Xrepresent\nthe number of osteosarcoma cases diagnosed. The probability that 15 or more cases will be diagnosed in a\ngiven year is the quantity P(X\u001515) = 1\u0000P(X<15) = 1\u0000P(X\u001414) = 0:161:(c) First, calculate \u0015Bgiven that\nn= 450;000 for Brooklyn: 3.6. The probability of observing 10 or more cases in Brooklyn in a given year is\nthe quantity P(XB\u001510) = 1\u0000P(XB<10) = 1\u0000P(XB\u00149) = 0:004:(d) No, he is not correct. The probability\ncalculated in c) deals only with Brooklyn: the probability that there are 10 or more cases in Brooklyn for a\nsingle year. It does not say anything about cases in other boroughs. If we assume independence between\nboroughs, the probability that the o ﬃcial is referring to is:\nP(X= 0 in other boroughs) \u0002P(X\u001510 in Brooklyn).\nThere is no reason to expect that P(X= 0 in other boroughs) should equal 1, so this probability is di ﬀer-\nent from the one in part c). (e) o, this probability is not equal to the probability calculated in part c). Over\nﬁve years, there are ﬁve opportunities for the event of 10 or more cases in Brooklyn in a single year to occur.\nLetYrepresent the event that in a single year, 10 or more cases of osteosarcoma are observed in Brooklyn. If\nwe assume independence between years, then Yfollows a binomial distribution with n= 5 andpof success as\ncaculated in part c); P(Y= 1) = 0:020.\n3.43 (a)\u0015for a population of 2,000,000 male births is 400. The probability of at most 380 newborn males\nwith hemophilia is P(X\u0014380), where X\u0018Pois(400): 0.165.\n(b)P(X\u0015450) = 0:0075.\n(c) The number of male births is (1/2)(1,500,000) = 750,000. The rate \u0015for one year is 150. Over 5 years, the\nrate\u0015is 750. The expected number of hemophilia births over 5 years is 750 and the standard deviation isp\n750 = 27:39.\n3.45 (a) On average, 2 women would need to be sampled in order to select a married woman ( \u0016= 1=p=\n2:123), with standard deviation 1.544 ( \u001b=r\n(1\u0000p)\np2).\n(b)\u0016= 3:33.\u001b= 2:79.\n(c) Decreasing the probability increases both the mean and the standard deviation.\n3.47 (a) LetXrepresent the number of stocks that must be sampled to ﬁnd an infected stock; X\u0018Geom(0:30).\nP(X\u00145) = 0:832.\n(b)P(X\u00146) = 0:882.\n(c)P(X\u00153) = 1\u0000P(X\u00142) = 0:49.\n445\n3.49 (a) 0:8752\u00020:125 = 0:096. (b)\u0016= 8,\u001b= 7:48.\n3.51 (a) 0.0804. (b) 0.0322. (c) 0.0193.\n3.53 (a) 0.102, geometric with p= 1994=14;604 = 0:137.\n(b) 0.854, binomial with n= 10,p= 0:137.\n(c) 0.109, binomial with n= 10,p= 0:137.\n(d) The mean and standard deviation of a negative binomial random variable with r= 4 andp= 0:137 are\n29.30 and 13.61, respectively.\n3.55 (a)\u0016= 2:05;\u001b2= 1:77.\n(b) LetXrepresent the number of soapy-taste detectors; X\u0018HGeom(1994 ;14604\u00001994;15).P(X= 4) =\n0:09435.\n(c)P(X\u00142) = 0:663.\n(d) 0.09437, from the binomial distribution. With a large sample size, sampling with replacement is highly\nunlikely to result in any particular individual being sampled again. In this case, the hypergeometric and\nbinomial distributions will produce equal probabilities.\n3.57 (a) The marginal distributions for Xis obtained by summing across the two rows, and for Yby summing\nthe columns. The marginal probabilities for X= 0 andX= 1 are 0.60 and 0.40, and for Y=\u00001 andY= 1 are\nboth 0.50; i.e., pX(0) = 0:60,pX(1) = 0:40,pY(\u00001) =pY(1) = 0:50 (b) The mean and variance of Xare calculated\nusing the formulas in Section 3.1.2 and 3.1.3 and are\n\u0016X= (0)(0:60) + (1)(0:40) = 0:40\n\u001b2\nX= (0\u00000:40)2(0:60) + (1\u00000:40)2(0:40) = 0:24\nThe standard deviation of Xisp\n0:24 = 0:49. (c) The two standardized values of Xare obtained by subtracting\nthe mean of Xfrom each value and dividing by the standard deviation. The two standardized values are -0.82\nand 1.23. (d) The correlation between XandYadds the 4 products of the standardized values, weighted by\nthe values in the joint distribution:\n\u001aX;Y= (\u00000:82)(\u00001)(0:20) + (\u00000:83)(1)(0:40) + (1:23)(\u00001)(0:30) + (1:23)(1)(0:10) =\u0000:41\n(e) No. The correlation between XandYis not zero.\n3.59 (a) Sum over the margins to calculate the marginal distributions.\npY(\u00001) = 0:25pY(0) = 0:20pY(1) = 0:55\npX(\u00001) = 0:45pX(0) = 0:20pX(1) = 0:35\n(b) The expected value of Xis calculated as follows:\nE(X) =X\nixiP(X=xi) = (\u00001)(0:45) + (0)(0:20) + (1)(0:35) =\u00000:10\n(c) The variance of Yis calculated by ﬁrst calculating E(Y), then using that in the formula for a variance of a\nrandom variable.\nE(Y) =X\niyiP(Y=yi) = (\u00001)(0:25) + (0)(0:20) + (1)(0:55) = 0:30\nVar(Y) =X\ni(yi\u0000E(Y))2P(Y=yi) = (\u00001\u00000:30)2(0:25) + (0\u00000:30)2(0:20) + (1\u00000:30)2(0:55) = 0:71\n(d)P(X=\u00001jY= 0) = 0=0:20 = 0;P(X= 0jY= 0) = 0:10=0:20 = 0:50;P(X= 1jY= 0) = 0:10=0:20 = 0:5.\n446 APPENDIX A. END OF CHAPTER EXERCISE SOLUTIONS\n3.61 (a) No. The new marginal distributions for the costs for the two members of the couple are shown in the\nfollowing table. The values and the marginal distribution for the partner’s cost do not change, so the expected\nvalue and standard deviation will not change. The previous values for the mean and standard deviation were\n$980 and $9.80.\nPartner Costs, Y\nEmployee costs, X $968 $988 Marg. Dist., X\n$968 0.18 0.12 0.30\n$1,008 0.15 0.25 0.40\n$1,028 0.07 0.23 0.30\nMarg. Dist., Y 0.40 0.60 1.00\n(b) The expected value and standard deviation of the employee’s costs are calculated as in Example 3.6, but\nusing the new marginal distribution. The new values for the mean and standard deviation are $1,002 and\n$23.75. (c) The expected total cost is $1,002 + $980 = $1,982. (d) The calculation correlation depends on the\nstandardized costs for each member of the couple and the joint probabilities. The new standardized values for\nthe employee costs are -1.43, 0.25, and 1.09; the corresponding values for the partner are -1.22 and 0.82. The\ncorrelation is the weighted sum of the 6 products, weighted by the joint probabilities: \u001aX;Y= 0:29. (e) The\nnew variance for the total cost will be (23 :80)2+ (9:80)2+ (2)(23:8)(9:80)(0:29) = 796:00 The new standard\ndeviation isp\n796:00 = $28:21.\n4 Foundations for inference\n4.1 (a)x= 0:6052:\n(b)s= 0:0131.\n(c)Z0:63=0:63\u00000:6052\n0:0131= 1:893. No, this level of BGC is within 2 SD of the mean.\n(d) The standard error of the sample mean is given byspn=0:0131p\n70= 0:00157.\n4.3 (a) This is the sampling distribution of the sample mean.\n(b) The sampling distribution will be normal and symmetric, centered around the theoretical population mean\n\u0016of the number of eggs laid by this hen species during a breeding period.\n(c) The variability of the distribution is the standard error of the sample mean:spn=18:2p\n45= 2:71.\n(d) The variability of the new distribution will be greater than the variability of the original distribution.\nConceptually, a smaller sample is less informative, which leads to a more uncertain estimate. This can be\nshown concretely with a calculation:18:2p\n10= 5:76 is larger than 2.71.\n4.5 (a) We are 95% conﬁdent that the mean number of hours that U.S. residents have to relax or pursue\nactivities that they enjoy is between 3.53 and 3.83 hours.\n(b) A larger margin of error with the same sample occurs with a higher conﬁdence level (i.e., larger critical\nvalue).\n(c) The margin of error of the new 95% conﬁdence interval will be smaller, since a larger sample size results\nin a smaller standard error. (d) A 90% conﬁdence interval will be smaller than the original 95% interval, since\nthe critical value is smaller and results in a smaller margin of error. The interval will provide a more precise\nestimate, but have an associated lower conﬁdence of capturing \u0016.\n447\n4.7 (a) False. Provided the data distribution is not very strongly skewed ( n= 64 in this sample, so we can be\nslightly lenient with the skew), the distribution of the sample mean will be nearly normal, allowing for the\nnormal approximation.\n(b) False. Inference is made on the population parameter, not the point estimate. The point estimate is always\nin the conﬁdence interval.\n(c) True.\n(d) False. The conﬁdence interval is not about a sample mean.\n(e) False. A wider interval is required to be more conﬁdent about capturing the parameter.\n(f) True. The margin of error is half the width of the interval, and the sample mean is the midpoint of the\ninterval.\n(g) False. To halve the margin of error requires sampling 22= 4 times the number of people in the initial\nsample.\n4.9 (a) i. False. There is a 5% chance that any 95% conﬁdence interval does not contain the true population\nmean days out of the past 30 days that U.S. adults experienced poor mental health. ii. False. The population\nparameter\u0016is either inside or outside the interval; there is no probability associated with whether the ﬁxed\nvalue\u0016is in a certain calculated interval. The randomness is associated with the interval (and the method\nfor calculating it), not the parameter \u0016. Thus, it would not be reasonable to say there is a 95% chance that\nthe particular interval (3.40, 4.24) contains \u0016; this interpretation is coherent with the statement in part iii. of\nthis question. iii. True. This is the deﬁnition of what it means to be 95% conﬁdent. iv. True. The interval\ncorresponds to a two-sided test, with H0:\u0016= 4:5 days andHA:\u0016,4:5 days and\u000b= 1\u00000:95 = 0:05. Since\n\u00160of 4.5 days is outside the interval, the sample provides su ﬃcient evidence to reject the null hypothesis and\naccept the alternative hypothesis. v. False. We can only be conﬁdent that 95% of the time, the entire interval\ncalculated contains \u0016. It is not possible to make this statement about xor any other point within the interval.\nvi. False. The conﬁdence interval is a statement about the population parameter \u0016, the mean days out of the\npast 30 days that all US adults experienced poor mental health. The sample mean xis a known quantity.\n(b) The 90% conﬁdence interval will be smaller than the 95% conﬁdence interval. If we are less conﬁdent\nthat an interval contains \u0016, this implies that the interval is less wide; if we are more conﬁdent, the interval is\nwider. Think about a theoretical \"100%\" conﬁdence interval—to be 100% conﬁdent of capturing \u0016, then the\nrange must be all possible numbers that \u0016could be. (c) (3.47, 4.17) days\n4.11 (a) The null hypothesis is that New Yorkers sleep an average of 8 hours of night ( H0:\u0016= 8 hours). The\nalternative hypothesis is that New Yorkers sleep less than 8 hours a night on average ( HA:\u0016<8 hours).\n(b) The null hypothesis is employees spend on average 15 minutes on non-business activities in a day ( H0:\n\u0016= 15 minutes). The alternative hypothesis is that employees spend on average more than 15 minutes on\nnon-business activities in a day ( HA:\u0016>15 minutes).\n4.13 Hypotheses are always made about the population parameter \u0016, not the sample mean x. The correct\nvalue of\u00160is 10 hours, as based on the previous evidence; both hypotheses should include \u00160. The correct\nhypotheses are H0:\u0016= 10 hours and HA:\u0016>10 hours.\n4.15 (a) This claim is not supported by the conﬁdence interval. 3 hours corresponds to a time of 180 minutes;\nthere is evidence that the average waiting time is lower than 3 hours.\n(b) 2.2 hours corresponds to 132 minutes, which is within the interval. It is plausible that \u0016is 132 minutes,\nsince we are 95% conﬁdent that the interval (128 minutes, 147 minutes) contains the average wait time.\n(c) Yes, the claim would be supported based on a 99% interval, since the 99% interval is wider than the 95%\ninterval.\n4.17H0:\u0016= 130 grams, HA:\u0016,130 grams. Test the hypothesis by calculating the test statistic: t=x\u0000\u00160\ns=pn=\n130\u0000134\n17=sqrt 35= 1:39. This results in a p-value of 0.17. There is insu ﬃcient evidence to reject the null hypothesis.\nThere is no evidence that the nutrition label does not provide an accurate measure of calories.\n448 APPENDIX A. END OF CHAPTER EXERCISE SOLUTIONS\n4.19 (a) The 95% conﬁdence interval is 3 ;150\u0006(1:96\u0002250=p\n50) = (3080:7;3219:3) grams.\n(b) She will conduct a test of the null against the two-sided alternative HA:\u0016,3250 grams. Calculate the\ntest statistic: t=x\u0000\u00160\ns=pn=3150\u00003250\n250=p\n50=\u00002:83. Thep-value is 0.007. There is su ﬃcient evidence to reject the null\nhypothesis and conclude that the mean birthweight of babies from inner-city teaching hospitals is lower than\n3,260 grams.\n4.21 (a)H0: Anti-depressants do not help symptoms of ﬁbromyalgia. HA: Anti- depressants do treat symp-\ntoms of ﬁbromyalgia. (b) Concluding that anti-depressants work for the treatment of ﬁbromyalgia symptoms\nwhen they actually do not. (c) Concluding that anti-depressants do not work for the treatment of ﬁbromyalgia\nsymptoms when they actually do. (d) If she makes a Type 1 error, she will continue taking medication that\ndoes not actually treat her disorder. If she makes a Type 2 error, she will stop taking medication that could\ntreat her disorder.\n4.23 (a) The standard error is larger under scenario I; standard error is larger for smaller values of n.\n(b) The margin of error is larger under scenario I; to be more conﬁdence of capturing the population parameter\nrequires a larger conﬁdence interval.\n(c) Thep-value from a Z-statistic only depends on the value of the Z-statistic; the value is equal under the\nscenarios.\n(d) The probability of making a Type II error and falsely rejecting the alternative is higher under scenario I; it\nis easier to reject the alternative with a high \u000b.\n5 Inference for numerical data\n5.1 (a)df= 6\u00001 = 5,t?\n5= 2:02 (column with two tails of 0.10, row with df= 5). (b)df= 21\u00001 = 20,\nt?\n20= 2:53 (column with two tails of 0.02, row with df= 20). (c)df= 28,t?\n28= 2:05. (d)df= 11,t?\n11= 3:11.\n5.3 On az-distribution, the cuto ﬀvalue for the upper 5% of values is 1.96. A t-distribution has wider tails\nthan a normal distribution but approaches the shape of a standard normal as degrees of freedom increases.\nThus, 1.98 corresponds to the cuto ﬀfor at-distribution with 100 degrees of freedom, 2.01 the cuto ﬀfor 50\ndegrees of freedom, and 2.23 the cuto ﬀfor 10 degrees of freedom.\n5.5 The mean is the midpoint: ¯x= 20. Identify the margin of error: ME = 1:015, then use t?\n35= 2:03 and\nSE=s=pnin the formula for margin of error to identify s= 3.\n5.7 (a)H0:\u0016= 8 (New Yorkers sleep 8 hrs per night on average.) HA:\u0016,8 (New Yorkers sleep less or more\nthan 8 hrs per night on average.) (b) Independence: The sample is random. The min/max suggest there are\nno concerning outliers. T=\u00001:75.df= 25\u00001 = 24. (c) p-value = 0 :093. If in fact the true population mean of\nthe amount New Yorkers sleep per night was 8 hours, the probability of getting a random sample of 25 New\nYorkers where the average amount of sleep is 7.73 hours per night or less (or 8.27 hours or more) is 0.093.\n(d) Since p-value >0.05, do not reject H0. The data do not provide strong evidence that New Yorkers sleep\nmore or less than 8 hours per night on average. (e) No, since the p-value is smaller than 1 \u00000:90 = 0:10.\n5.9Tis either -2.09 or 2.09. Then ¯xis one of the following:\n\u00002:09 =¯x\u000060\n8p\n20!¯x= 56:26\n2:09 =¯x\u000060\n8p\n20!¯x= 63:74\n449\n5.11 (a) We will conduct a 1-sample t-test.H0:\u0016= 5.HA:\u0016,5. We’ll use \u000b= 0:05. This is a random sample,\nso the observations are independent. To proceed, we assume the distribution of years of piano lessons is\napproximately normal. SE= 2:2=p\n20 = 0:4919. The test statistic is T= (4:6\u00005)=SE=\u00000:81.df= 20\u00001 = 19.\nThe one-tail area is about 0.21, so the p-value is about 0.42, which is bigger than \u000b= 0:05 and we do not\nrejectH0. That is, we do not have su ﬃciently strong evidence to reject the notion that the average is 5 years.\n(b) UsingSE= 0:4919 andt?\ndf=19= 2:093, the conﬁdence interval is (3.57, 5.63). We are 95% conﬁdent that\nthe average number of years a child takes piano lessons in this city is 3.57 to 5.63 years. (c) They agree, since\nwe did not reject the null hypothesis and the null value of 5 was in the t-interval.\n5.13 If the sample is large, then the margin of error will be about 1 :96\u0002100=pn. We want this value to be\nless than 10, which leads to n\u0015384:16, meaning we need a sample size of at least 385 (round up for sample\nsize calculations!).\n5.15 (a) Since it’s the same students at the beginning and the end of the semester, there is a pairing between\nthe datasets; for a given student their beginning and end of semester grades are dependent. (b) Since the\nsubjects were sampled randomly, each observation in the men’s group does not have a special correspondence\nwith exactly one observation in the other (women’s) group. (c) Since it’s the same subjects at the beginning\nand the end of the study, there is a pairing between the datasets; for a subject their beginning and end of\nsemester artery thickness are dependent. (d) Since it’s the same subjects at the beginning and the end of the\nstudy, there is a pairing between the datasets; for a subject their beginning and end of semester weights are\ndependent.\n5.17 (a) For each observation in one data set, there is exactly one specially corresponding observation in the\nother data set for the same geographic location. The data are paired. (b) H0:\u0016diﬀ= 0 (There is no di ﬀerence\nin average number of days exceeding 90°F in 1948 and 2018 for NOAA stations.) HA:\u0016diﬀ,0 (There is a\ndiﬀerence.) (c) Locations were randomly sampled, so independence is reasonable. The sample size is at least\n30, so we’re just looking for particularly extreme outliers: none are present (the observation o ﬀleft in the\nhistogram would be considered a clear outlier, but not a particularly extreme one). Therefore, the conditions\nare satisﬁed. (d) SE= 17:2=p\n197 = 1:23.T=2:9\u00000\n1:23= 2:36 with degrees of freedom df= 197\u00001 = 196. This\nleads to a one-tail area of 0.0096 and a p-value of about 0.019. (e) Since the p-value is less than 0.05, we reject\nH0. The data provide strong evidence that NOAA stations observed more 90°F days in 2018 than in 1948.\n(f) Type 1 Error, since we may have incorrectly rejected H0. This error would mean that NOAA stations did\nnot actually observe a decrease, but the sample we took just so happened to make it appear that this was the\ncase. (g) No, since we rejected H0, which had a null value of 0.\n5.19 (a)SE= 1:23 andt?= 1:65. 2:9\u00061:65\u00021:23!(0:87;4:93).\n(b) We are 90% conﬁdent that there was an increase of 0.87 to 4.93 in the average number of days that hit 90°F\nin 2018 relative to 1948 for NOAA stations.\n(c) Yes, since the interval lies entirely above 0.\n5.21 (a) Each of the 36 mothers is related to exactly one of the 36 fathers (and vice-versa), so there is a special\ncorrespondence between the mothers and fathers. (b) H0:\u0016diff = 0.HA:\u0016diff,0. Independence: random\nsample from less than 10% of population. Sample size of at least 30. The skew of the di ﬀerences is, at worst,\nslight.Z= 2:72!p-value = 0:0066. Since p-value <0.05, rejectH0. The data provide strong evidence that the\naverage IQ scores of mothers and fathers of gifted children are di ﬀerent, and the data indicate that mothers’\nscores are higher than fathers’ scores for the parents of gifted children.\n450 APPENDIX A. END OF CHAPTER EXERCISE SOLUTIONS\n5.23 (a) Sincep <0:05, there is statistically signiﬁcant evidence that the population di ﬀerence in BGC is\nnot 0. Since the observed mean BGC is higher in the food supplemented group, these data suggest that food\nsupplemented birds have higher BGC on average than birds that are not food supplemented. (b) The 95%\nconﬁdence interval is d\u0006t?sdpn. Since the mean of the di ﬀerences is equal to the di ﬀerence of the means,\nd= 1:70\u00000:586 = 1:114. The test statistic is t=d\nsd=pn, so the standard error ( sd=pn) can be solved for:\nsd=pn=d=t= 1:114=2:64 = 0:422:The critical t-value for a 95% conﬁdence interval on a t-distribution with\n16\u00001 = 15 degrees of freedom is 2.13. Thus, the 95% conﬁdence interval is 1 :114\u0006(2:13\u00020:422)!(0:215;2:01)\ngrams. With 95% conﬁdence, the interval (0.215, 2.01) grams contains the population mean di ﬀerence in egg\nmass between food supplemented birds and non supplemented birds.\n5.25 (a) These data are paired. For example, the Friday the 13th in say, September 1991, would probably be\nmore similar to the Friday the 6th in September 1991 than to Friday the 6th in another month or year.\n(b) Let\u0016diﬀ=\u0016sixth\u0000\u0016thirteenth .H0:\u0016diﬀ= 0.HA:\u0016diﬀ,0.\n(c) Independence: The months selected are not random. However, if we think these dates are roughly equiv-\nalent to a simple random sample of all such Friday 6th/13th date pairs, then independence is reasonable.\nTo proceed, we must make this strong assumption, though we should note this assumption in any reported\nresults. Normality: With fewer than 10 observations, we would need to see clear outliers to be concerned.\nThere is a borderline outlier on the right of the histogram of the di ﬀerences, so we would want to report this\nin formal analysis results.\n(d)T= 4:93 fordf= 10\u00001 = 9!p-value = 0.001.\n(e) Since p-value <0.05, reject H0. The data provide strong evidence that the average number of cars at the\nintersection is higher on Friday the 6ththan on Friday the 13th. (We should exercise caution about generaliz-\ning the interpretation to all intersections or roads.)\n(f) If the average number of cars passing the intersection actually was the same on Friday the 6thand 13th,\nthen the probability that we would observe a test statistic so far from zero is less than 0.01.\n(g) We might have made a Type 1 Error, i.e. incorrectly rejected the null hypothesis.\n5.27 (a)H0:\u0016diff = 0.HA:\u0016diff,0.T=\u00002:71.df= 5. p-value = 0 :042. Since p-value <0.05, reject\nH0. The data provide strong evidence that the average number of tra ﬃc accident related emergency room\nadmissions are di ﬀerent between Friday the 6thand Friday the 13th. Furthermore, the data indicate that the\ndirection of that di ﬀerence is that accidents are lower on Friday the 6threlative to Friday the 13th.\n(b) (-6.49, -0.17).\n(c) This is an observational study, not an experiment, so we cannot so easily infer a causal intervention implied\nby this statement. It is true that there is a di ﬀerence. However, for example, this does not mean that a\nresponsible adult going out on Friday the 13thhas a higher chance of harm than on any other night.\n5.29 (a) Chicken fed linseed weighed an average of 218.75 grams while those fed horsebean weighed an\naverage of 160.20 grams. Both distributions are relatively symmetric with no apparent outliers. There is more\nvariability in the weights of chicken fed linseed. (b) H0:\u0016ls=\u0016hb.HA:\u0016ls,\u0016hb. We leave the conditions to\nyou to consider. T= 3:02,df=min(11;9) = 9!0:01<p-value<0:02. Since p-value <0.05, reject H0. The\ndata provide strong evidence that there is a signiﬁcant di ﬀerence between the average weights of chickens\nthat were fed linseed and horsebean. (c) Type 1 Error, since we rejected H0. (d) Yes, since p-value >0.01, we\nwould have failed to reject H0.\n451\n5.31H0:\u0016C=\u0016S.HA:\u0016C,\u0016S.T= 3:27,df= 11!p-value<0:01. Since p-value <0:05, rejectH0.\nThe data provide strong evidence that the average weight of chickens that were fed casein is di ﬀerent than\nthe average weight of chickens that were fed soybean (with weights from casein being higher). Since this is a\nrandomized experiment, the observed di ﬀerence can be attributed to the diet.\n5.33H0:\u0016T=\u0016C.HA:\u0016T,\u0016C.T= 2:24,df= 21!0:02<p-value<0:05. Since p-value <0.05, reject\nH0. The data provide strong evidence that the average food consumption by the patients in the treatment\nand control groups are di ﬀerent. Furthermore, the data indicate patients in the distracted eating (treatment)\ngroup consume more food than patients in the control group.\n5.35 Let\u0016diff =\u0016pre\u0000\u0016post.H0:\u0016diff = 0: Treatment has no e ﬀect.HA:\u0016diff,0: Treatment has an e ﬀect\non P .D.T. scores, either positive or negative. Conditions: The subjects are randomly assigned to treatments,\nso independence within and between groups is satisﬁed. All three sample sizes are smaller than 30, so we\nlook for clear outliers. There is a borderline outlier in the ﬁrst treatment group. Since it is borderline, we will\nproceed, but we should report this caveat with any results. For all three groups: df= 13.T1= 1:89!p-value\n= 0.081,T2= 1:35!p-value = 0.200), T3=\u00001:40!(p-value = 0.185). We do not reject the null hypothesis\nfor any of these groups. As earlier noted, there is some uncertainty about if the method applied is reasonable\nfor the ﬁrst group.\n5.37 Diﬀerence we care about: 40. Single tail of 90%: 1 :28\u0002SE. Rejection region bounds: \u00061:96\u0002SE(if 5%\nsigniﬁcance level). Setting 3 :24\u0002SE= 40, subbing in SE=q\n942\nn+942\nn, and solving for the sample size ngives\n116 plots of land for each fertilizer.\n5.39H0:\u00161=\u00162=\u0001\u0001\u0001=\u00166.HA: The average weight varies across some (or all) groups. Independence: Chicks\nare randomly assigned to feed types (presumably kept separate from one another), therefore independence of\nobservations is reasonable. Approx. normal: the distributions of weights within each feed type appear to be\nfairly symmetric. Constant variance: Based on the side-by-side box plots, the constant variance assumption\nappears to be reasonable. There are di ﬀerences in the actual computed standard deviations, but these might\nbe due to chance as these are quite small samples. F5;65= 15:36 and the p-value is approximately 0. With\nsuch a small p-value, we reject H0. The data provide convincing evidence that the average weight of chicks\nvaries across some (or all) feed supplement groups.\n5.41 (a)H0: The population mean of MET for each group is equal to the others. HA: At least one pair of\nmeans is di ﬀerent. (b) Independence: We don’t have any information on how the data were collected, so we\ncannot assess independence. To proceed, we must assume the subjects in each group are independent. In\npractice, we would inquire for more details. Normality: The data are bound below by zero and the standard\ndeviations are larger than the means, indicating very strong skew. However, since the sample sizes are ex-\ntremely large, even extreme skew is acceptable. Constant variance: This condition is su ﬃciently met, as the\nstandard deviations are reasonably consistent across groups. (c) See below, with the last column omitted:\nDf Sum Sq Mean Sq F value\ncoﬀee 4 10508 2627 5.2\nResiduals 50734 25564819 504\nTotal 50738 25575327\n(d) Since p-value is very small, reject H0. The data provide convincing evidence that the average MET di ﬀers\nbetween at least one pair of groups.\n5.43 (a)H0: Average GPA is the same for all majors. HA: At least one pair of means are di ﬀerent. (b) Since\np-value>0.05, fail to reject H0. The data do not provide convincing evidence of a di ﬀerence between the\naverage GPAs across three groups of majors. (c) The total degrees of freedom is 195 + 2 = 197, so the sample\nsize is 197 + 1 = 198.\n452 APPENDIX A. END OF CHAPTER EXERCISE SOLUTIONS\n5.45 (a) False. As the number of groups increases, so does the number of comparisons and hence the modiﬁed\nsigniﬁcance level decreases. (b) True. (c) True. (d) False. We need observations to be independent regardless\nof sample size.\n5.47 (a)H0: Average score di ﬀerence is the same for all treatments. HA: At least one pair of means are\ndiﬀerent. (b) We should check conditions. If we look back to the earlier exercise, we will see that the patients\nwere randomized, so independence is satisﬁed. There are some minor concerns about skew, especially with\nthe third group, though this may be acceptable. The standard deviations across the groups are reasonably\nsimilar. Since the p-value is less than 0.05, reject H0. The data provide convincing evidence of a di ﬀerence\nbetween the average reduction in score among treatments. (c) We determined that at least two means are\ndiﬀerent in part (b), so we now conduct K= 3\u00022=2 = 3 pairwise t-tests that each use \u000b= 0:05=3 = 0:0167 for a\nsigniﬁcance level. Use the following hypotheses for each pairwise test. H0: The two means are equal. HA: The\ntwo means are di ﬀerent. The sample sizes are equal and we use the pooled SD, so we can compute SE= 3:7\nwith the pooled df= 39. The p-value for Trmt 1 vs. Trmt 3 is the only one under 0.05: p-value = 0.035 (or\n0.024 if using spooled in place ofs1ands3, though this won’t a ﬀect the ﬁnal conclusion). The p-value is larger\nthan 0:05=3 = 1:67, so we do not have strong evidence to conclude that it is this particular pair of groups that\nare diﬀerent. That is, we cannot identify if which particular pair of groups are actually di ﬀerent, even though\nwe’ve rejected the notion that they are all the same!\n6 Simple linear regression\n6.1 (a) Strong relationship, but a straight line would not ﬁt the data. (b) Strong relationship, and a linear\nﬁt would be reasonable. (c) Weak relationship, and trying a linear ﬁt would be reasonable. (d) Moderate\nrelationship, but a straight line would not ﬁt the data. (e) Strong relationship, and a linear ﬁt would be\nreasonable. (f) Weak relationship, and trying a linear ﬁt would be reasonable.\n6.3 (a) There is a moderate, positive, and linear relationship between shoulder girth and height. (b) Changing\nthe units, even if just for one of the variables, will not change the form, direction or strength of the relationship\nbetween the two variables.\n6.5 Over-estimate. Since the residual is calculated as observed\u0000predicted , a negative residual means that\nthe predicted value is higher than the observed value.\n6.7 (a)murder =\u000029:901+2:559\u0002poverty %. (b) Expected murder rate in metropolitan areas with no poverty\nis -29. 901 per million. This is obviously not a meaningful value, it just serves to adjust the height of the\nregression line. (c) For each additional percentage increase in poverty, we expect murders per million to be\nhigher on average by 2.559. (e)p\n0:7052 = 0:8398.\n6.9 (a) The slope of -1.26 indicates that on average, an increase in age of 1 year is associated with a lower\nRFFT score by 1.26 points. The intercept of 137.55 represents the predicted mean RFFT score for an individual\nof age 0 years; this does not have interpretive meaning since the RFFT cannot be reasonably administered to a\nnewborn. (b) RFFT score di ﬀers on average by 10( \u00001:26) = 12:6 points between an individual who is 60 years\nold versus 50 years old, with the older individual having the lower score. (c) According to the model, average\nRFFT score for a 70-year-old is 137 :55\u00001:26(70) = 49:3 points. (d) No, it is not valid to use the linear model\nto estimate RFFT score for a 20-year-old. As indicated in the plot, data are only available for individuals as\nyoung as about 40 years old.\n6.11 (a) The residual plot will show randomly distributed residuals around 0. The variance is also approxi-\nmately constant. (b) The residuals will show a fan shape, with higher variability for smaller x. There will also\nbe many points on the right above the line. There is trouble with the model being ﬁt here.\n453\n6.13 (a) The points with the lowest and highest values for height have relatively high leverage. They do not\nseem particularly inﬂuential because they are not outliers; the one with a low x-value has a low y-value and\nthe one with a high x-value has a high y-value, which follows the positive trend visible in the data. (b) Yes,\nsince the data show a linear trend, it is appropriate to use R2as a metric for describing the strength of the\nmodel ﬁt. (c) Height explains about 72% of the observed variability in length.\n6.15 There is an upwards trend. However, the variability is higher for higher calorie counts, and it looks like\nthere might be two clusters of observations above and below the line on the right, so we should be cautious\nabout ﬁtting a linear model to these data.\n6.17 (a) There is an outlier in the bottom right. Since it is far from the center of the data, it is a point with\nhigh leverage. It is also an inﬂuential point since, without that observation, the regression line would have a\nvery di ﬀerent slope.\n(b) There is an outlier in the bottom right. Since it is far from the center of the data, it is a point with high\nleverage. However, it does not appear to be a ﬀecting the line much, so it is not an inﬂuential point.\n(c) The observation is in the center of the data (in the x-axis direction), so this point does nothave high\nleverage. This means the point won’t have much e ﬀect on the slope of the line and so is not an inﬂuential\npoint.\n6.19 (a) Linearity is satisﬁed; the data scatter about the horizontal line with no apparent pattern. The vari-\nability seems constant across the predicted length values. (b) The ﬁsh were randomly sampled from a river, so\nwithout additional details about the life cycle of the ﬁsh, it seems reasonable to assume the height and length\nof any one ﬁsh does not provide information about the height and length of another ﬁsh. This could be vio-\nlated, if, for example, the ﬁsh in a river tend to be closely related and height and length are highly heritable.\n(c) The residuals are approximately normally distributed, with some small deviations from normality in the\ntails. There are more outliers in both tails than expected under a normal distribution.\n6.21 One possible equation is price = 44:51 + 12:3(carat 1:00), where the explanatory variable is a binary\nvariable taking on value 1if the diamond is 1 carat.\n6.23 (a) The relationship is positive, moderate-to-strong, and linear. There are a few outliers but no points\nthat appear to be inﬂuential.\n(b)weight =\u0000105:0113 + 1:0176\u0002height .\nSlope: For each additional centimeter in height, the model predicts the average weight to be 1.0176 additional\nkilograms (about 2.2 pounds).\nIntercept: People who are 0 centimeters tall are expected to weigh - 105.0113 kilograms. This is obviously not\npossible. Here, the y- intercept serves only to adjust the height of the line and is meaningless by itself.\n(c)H0: The true slope coe ﬃcient of height is zero ( \f1= 0).\nHA: The true slope coe ﬃcient of height is di ﬀerent than zero ( \f1,0).\nThe p-value for the two-sided alternative hypothesis ( \f1,0) is incredibly small, so we reject H0. The data\nprovide convincing evidence that height and weight are positively correlated. The true slope parameter is\nindeed greater than 0.\n(d)R2= 0:722= 0:52. Approximately 52% of the variability in weight can be explained by the height of\nindividuals.\n454 APPENDIX A. END OF CHAPTER EXERCISE SOLUTIONS\n6.25 (a)H0:\f1= 0.HA:\f1,0. The p-value, as reported in the table, is incredibly small and is smaller than\n0.05, so we reject H0. The data provide convincing evidence that wives’ and husbands’ heights are positively\ncorrelated.\n(b)heightW= 43:5755 + 0:2863\u0002heightH.\n(c) Slope: For each additional inch in husband’s height, the average wife’s height is expected to be an additional\n0.2863 inches on average. Intercept: Men who are 0 inches tall are expected to have wives who are, on average,\n43.5755 inches tall. The intercept here is meaningless, and it serves only to adjust the height of the line.\n(d) The slope is positive, so rmust also be positive. r=p\n0:09 = 0:30.\n(e) 63.33. Since R2is low, the prediction based on this regression model is not very reliable.\n(f) No, we should avoid extrapolating.\n(g) Yes, the p-value for the slope parameter is less than \u000b= 0:05. There is su ﬃcient evidence to accept the\nalternative hypothesis, HA:\f1,0. These data suggest that wife height and husband height are positively\nassociated at the population level.\n(h) No, a 95% conﬁdence interval for \f1would not be expected to contain the null value 0, since the p-value\nis less than 0.05.\n6.27 (a) The point estimate and standard error are b1= 0:9112 andSE= 0:0259. We can compute a T-\nscore:T= (0:9112\u00001)=0:0259 =\u00003:43. Usingdf= 168, the p-value is about 0.001, which is less than \u000b=\n0:05. That is, the data provide strong evidence that the average di ﬀerence between husbands’ and wives’\nages has actually changed over time. (b) dageW= 1:5740 + 0:9112\u0002ageH. (c) Slope: For each additional year\nin husband’s age, the model predicts an additional 0.9112 years in wife’s age. This means that wives’ ages\ntend to be lower for later ages, suggesting the average gap of husband and wife age is larger for older people.\nIntercept: Men who are 0 years old are expected to have wives who are on average 1.5740 years old. The\nintercept here is meaningless and serves only to adjust the height of the line. (d) R=p\n0:88 = 0:94. The\nregression of wives’ ages on husbands’ ages has a positive slope, so the correlation coe ﬃcient will be positive.\n(e)dageW= 1:5740 + 0:9112\u000255 = 51:69. SinceR2is pretty high, the prediction based on this regression\nmodel is reliable. (f) No, we shouldn’t use the same model to predict an 85 year old man’s wife’s age. This\nwould require extrapolation. The scatterplot from an earlier exercise shows that husbands in this data set are\napproximately 20 to 65 years old. The regression model may not be reasonable outside of this range.\n6.29 (a) Yes, since p<0:01.H0:\f1= 0,HA:\f1,0, where\f1represents the population average change\nin RFFT score associated with a change in 1 year of age. There is statistically signiﬁcant evidence that age is\nnegatively associated with RFFT score. (b) With 99% conﬁdence, the interval (-1.49, -1.03) points contains\nthe population average di ﬀerence in RFFT score between individuals who di ﬀer in age by 1 year; the older\nindividual is predicted to have a lower RFFT score.\n6.31 (a) First, compute the standard error: s.e.(E(agewifejagehusband = 55)) = 3:95r\n1\n170+(55\u000042:92)2\n(170\u00001)11:762=\n0:435:The critical value is t?\n0:975;df=169= 1:97. Thus, the 95% conﬁdence interval is 51 :69\u0006(1:97)(0:435) =\n(50:83;52:55) years. (b) First, compute the standard error: s.e.( agewifejagehusband = 55) = 3:95r\n1 +1\n170+(55\u000042:92)2\n(170\u00001)11:762=\n3:97. The 95% prediction interval is 51 :69\u0006(1:97)(3:97) = (43:85;59:54) years. (c) For the approximate 95%\nconﬁdence interval, use s=pn= 3:95=p\n170 = 0:303 as the approximate standard error: (51 :09;52:29) years. For\nthe approximate 95% prediction interval, use sp\n1 + 1=n= 3:95p\n1 + 1=170 = 4:25 as the approximate standard\nerror: (43:30;60:09) years.\n455\n7 Multiple linear regression\n7.1 Although the use of statins appeared to be associated with lower RFFT score when no adjustment was\nmade for possible confounders, statin use is not signiﬁcantly associated with RFFT score in a model that\nadjusts for age. After adjusting for age, the estimated di ﬀerence in mean RFFT score between statin users and\nnon-users is 0.85 points; there is a 74% chance of observing such a di ﬀerence if there is no di ﬀerence between\nmean RFFT score in the population of statin users and non-users.\n7.3 (a)baby _weight = 123:57\u00008:96(smoke )\u00001:98(parity ) (b) A child born to a mother who smokes has a\nbirth weight about 9 ounces less, on average, than one born to a mother who does not smoke, holding birth\norder constant. A child who is the ﬁrst born has birth weight about 2 ounces less, on average, than one who is\nnot ﬁrst born, when comparing children whose mothers were either both smokers or both nonsmokers. The\nintercept represents the predicted mean birth weight for a child whose mother is not a smoker and who was\nnot the ﬁrst born. (c) The estimated di ﬀerence in mean birth weight for two infants born to non-smoking\nmothers, where one is ﬁrst born and the other is not, is -1.98. (d) This is the same value as in part (c).\n(e) 123:57\u00008:96(0)\u00001:98(1) = 121:59 ounces.\n7.5 (a)baby _weight =\u000080:41 + 0:44\u0002gestation\u00003:33\u0002parity\u00000:01\u0002age+ 1:15\u0002height + 0:05\u0002weight\u0000\n8:40\u0002smoke . (b)\fgestation : The model predicts a 0.44 ounce increase in the birth weight of the baby for each\nadditional day of pregnancy, all else held constant. \fage: The model predicts a 0.01 ounce decrease in the birth\nweight of the baby for each additional year in mother’s age, all else held constant. (c) Parity might be correlated\nwith one of the other variables in the model, which complicates model estimation. (d) baby _weight = 120:58.\ne= 120\u0000120:58 =\u00000:58. The model over-predicts this baby’s birth weight. (e) R2= 0:2504.R2\nadj= 0:2468.\n7.7 Nearly normal residuals: With so many observations in the data set, we look for particularly extreme\noutliers in the histogram and do not see any. Variability of residuals: The scatterplot of the residuals versus\nthe ﬁtted values does not show any overall structure. However, values that have very low or very high ﬁtted\nvalues appear to also have somewhat larger outliers. In addition, the residuals do appear to have constant\nvariability between the two parity and smoking status groups, though these items are relatively minor.\nIndependent residuals: The scatterplot of residuals versus the order of data collection shows a random scatter,\nsuggesting that there is no apparent structures related to the order the data were collected.\nLinear relationships between the response variable and numerical explanatory variables: The residuals vs.\nheight and weight of mother are randomly distributed around 0. The residuals vs. length of gestation plot\nalso does not show any clear or strong remaining structures, with the possible exception of very short or long\ngestations. The rest of the residuals do appear to be randomly distributed around 0.\nAll concerns raised here are relatively mild. There are some outliers, but there is so much data that the\ninﬂuence of such observations will be minor.\n7.9 (b) True. (c) False. This would only be the case if the data was from an experiment and x1was one of\nthe variables set by the researchers. (Multiple regression can be useful for forming hypotheses about causal\nrelationships, but it o ﬀers zero guarantees.) (d) False. We should check normality like we would for inference\nfor a single mean: we look for particularly extreme outliers if n\u001530 or for clear outliers if n<30.\n456 APPENDIX A. END OF CHAPTER EXERCISE SOLUTIONS\n7.11 (a) (-0.32, 0.16). We are 95% conﬁdent that male students on average have GPAs 0.32 points lower\nto 0.16 points higher than females when controlling for the other variables in the model. (b) Yes, since the\np-value is larger than 0.05 in all cases (not including the intercept).\n7.13 (a)eggs:laid =\u000017:88 + 4:28(wolbachia ) + 0:272(tibia ) (b) An increase in Wolbachia density of one unit\nis associated with on average 4.28 more eggs laid over a lifetime, assuming body size is held constant. (c) In a\nmultiple regression model adjusting for body size as a potential confounder, increase in Wolbachia density was\nsigniﬁcantly positively associated with realized ﬁtness, measured as the number of eggs laid over a female’s\nfull lifetime ( p= 0:002). These data are consistent with the scientiﬁc hypothesis that Wolbachia is beneﬁcial\nfor its host in nature. (d) (1 :85;7:05) eggs (e) As a group, the predictors Wolbachia density and tibia length are\nuseful for predicting the number of eggs laid over a lifetime.\n7.15 (a) Since the di ﬀerence is taken in the direction (pre - post), a positive value for trt.effect indicates\nthat the post-intervention score is lower than the pre-intervention score, which represents e ﬃcacy of the\nintervention. A negative value would represent a patient’s deviant T scores increasing after the interven-\ntion. (b) Let Ybe the change in MMPI score for a participant in this study, Xneutral a variable with value\n1 for participants assigned to the neutral tape and 0 otherwise, and Xtherapeutic a variable with value 1\nfor participants in the emotional neutral group and 0 otherwise. The population-level equation is E(Y) =\n\f0+\fneutralXneutral +\ftherapeuticXtherapeutic . For these data, the estimated model equation is by=\u00003:21 +\n6:07Xneutral + 9:43Xtherapeutic . (c) The predicted di ﬀerence scores byfor a patient receiving the neutral tape\nwill be by=b0+bneutralXneutral +btherapeuticXtherapeutic =\u00003:21 + 6:07 + 0 = 2:86:(d) Yes. The intercept is the\naverage of the score di ﬀerence for the group that did not hear a taped message. (e) The two slopes represent\nthe change in average MMPI score di ﬀerence from the average for the group that did not receive a tape. The\nAbsent category is the reference group. (f) The p\u0000value for the intercept corresponds to a test of the null\nhypothesis that the average di ﬀerence score was 0 in the group that did not hear a taped message. The slope\np-values correspond to tests of the null hypotheses of (on average) no change in di ﬀerence scores between the\nintervention with no tape and each of the other two interventions.\n7.17 (a) Letpreandpost denote the pre- and post-intervention scores, respectively. The estimated equation\nfor the model is dpost = 28:41 + 0:66(pre)\u00005:73Xneutral\u00009:75Xtherapeutic . (b) Since the coe ﬃcient of the pre-\nintervention score is positive, post-intervention scores tend to increase as the pre-intervention score increases.\n(c) Yes. The t-statistic for the coe ﬃcient of preis 4.05 and is statistically signiﬁcant. (d) In this model,\ntreatment is a factor variable with three levels and the intervention with no tape is the baseline treatment\nthat does not appear in the model. For a participant with pre= 70 and no tape, the predicted value of post\nis 28:41 + 0:66(73)\u00005:73(1) = 70:86 (e) For a given value of pre, the coe ﬃcient of treatmentNeutral is the\npredicted change in post between an participant without a tape and one with the emotionally neutral tape.\nThe model implies that post will be 5.7 points lower with the emotionally neutral tape. The evidence for\na treatment e ﬀect of the emotionally neutral tape is weak; the coe ﬃcient is not statistically signiﬁcant at\n\u000b= 0:05.\n457\n7.19 (a)dpost =\u000017:58+1:28(pre)+67:75(neutral )+64:42(therapeutic )\u00000:99(pre\u0002neutral )\u00001:01(pre\u0002therapeutic )\n(b) The coe ﬃcient for preis the predicted increase in post score associated with a 1 unit increase in pre-score\nfor individuals in the absent arm, while the coe ﬃcients of the interaction terms for neutral and therapeutic\nrepresent the di ﬀerence in association between pre and post scores for individuals in those groups. For exam-\nple, an individual in the neutral group is expected to have a 1 :28\u00000:99 = 0:29 point increase in post score,\non average, per 1 point increase in pre-score. The coe ﬃcients of the slopes for neutral and therapeutic are\ndiﬀerences in intercept values relative to the intercept for the model, which is for the baseline group (absent).\n(c) Absent: dpost =\u000017:58 + 1:28(pre) Neutral: dpost =\u000017:58 + 67:75 + 1:28(pre)\u00000:99(pre) = 50:17 + 0:29(pre)\nTherapeutic: dpost =\u000017:58 + 64:42 + 1:28(pre)\u00001:01(pre) = 46:84 + 0:27(pre) (d) These data suggest there is a\nstatistically signiﬁcant di ﬀerence in association between pre- and post-intervention scores by treatment group\nrelative to the group that did not receive any treatment. The coe ﬃcients of both interaction terms are statis-\ntically signiﬁcant at \u000b= 0:05. Since the slopes are smaller than the slope for the treatment absent group, the\ndata demonstrate that individuals in either treatment group show less increase in MMPI score than occurs\nwhen no treatment is applied.\n7.21 (a)RFFT = 140:20\u000013:97(Statin )\u00001:31(Age)+0:25(Statin\u0002Age) (b) The model intercept represents the\npredicted mean RFFT score for a statin non-user of age 0 years; the intercept does not have a meaningful in-\nterpretation. The slope coe ﬃcient for age represents the predicted change in RFFT score for a statin non-user;\nfor non-users, a one year increase in age is associated with a 1.32 decrease in RFFT score. The slope coe ﬃcient\nfor statin use represents the di ﬀerence in intercept between the regression line for users and the regression\nline for non-users; the intercept for users is -13.97 points lower than that of non-users. The interaction term\ncoeﬃcient represents the di ﬀerence in the magnitude of association between RFFT score and age between\nusers and non-users; in users, the slope coe ﬃcient representing predicted change in RFFT score per 1 year\nchange in age is higher by 0.25 points. (c) No, there is not evidence that the association between RFFT score\nand age di ﬀers by statin use. The p-value of the interaction coe ﬃcient is 0.32, which is higher than \u000b= 0:05.\n7.23 Age should be the ﬁrst variable removed from the model. It has the highest p-value, and its removal\nresults in an adjusted R2of 0.255, which is higher than the current adjusted R2.\n7.25 (a) The strongest predictor of birth weight appears to be gestational age; these two variables show a\nstrong positive association. Both parity and smoker status show a slight association with gestational age; the\nﬁrst born child tends to be a lower birth weight and children from mothers who smoke tend to have lower birth\nweight. While there does not appear to be an association between birth weight and age of the mother, there\nmay be a slight positive association between both birth weight and height and birth weight and weight. All\npredictor variables with exception of age seem potentially useful for inclusion in an initial model. (b) Height\nand weight appear to be positively associated.\n7.27 (a) TheF-statistic for the model corresponds to a test of H0:\fneutral =\ftherapeutic = 0. (b) The inter-\ncept coe ﬃcient is the estimated mean di ﬀerence score for the no intervention group, and the estimated mean\ndiﬀerence score for the other two groups can be calculated by adding each of the slope estimates to the inter-\ncept. (c) Under the null hypothesis that the two slope coe ﬃcients are 0, all three interventions would have\nthe same mean di ﬀerence in MMPI scores. This is the same as the null hypothesis for an ANOV A with three\ngroups (H0:\u00161=\u00162=\u00163), which states that all three population means are the same. (d) The assumptions for\nmultiple regression and ANOV A are outlined in Sections 7.3.1 and 5.5, respectively. The assumptions for the\ntwo models are the same, though they may be phrased di ﬀerently. The ﬁrst assumption in multiple regression\nis linear change of the mean response variable when one predictor changes and the others do not change.\nSince each of the two predictor variables in this model can only change from 0 to 1, this assumption is simply\nthat the means in the three groups are possibly di ﬀerent, which is true in ANOV A. The second assumption in\nregression is that the variance of the residuals is approximately constant. Since the predicted response for an\nintervention group is its mean, the constant variance assumption in regression is the equivalent assumption\nin ANOV A that the three groups have approximately constant variance. Both models assume that the obser-\nvations are independent and that the residuals follow a normal distribution. This is a very long way of saying\nthat the two models are identical!\n458 APPENDIX A. END OF CHAPTER EXERCISE SOLUTIONS\n8 Inference for categorical data\n8.1 (a) False. Doesn’t satisfy success-failure condition. (b) True. The success-failure condition is not satisﬁed.\nIn most samples we would expect ˆpto be close to 0.08, the true population proportion. While ˆpcan be much\nabove 0.08, it is bound below by 0, suggesting it would take on a right skewed shape. Plotting the sampling\ndistribution would conﬁrm this suspicion. (c) False. SEˆp= 0:0243, and ˆp= 0:12 is only0:12\u00000:08\n0:0243= 1:65 SEs\naway from the mean, which would not be considered unusual. (d) True. ˆp= 0:12 is 2.32 standard errors away\nfrom the mean, which is often considered unusual. (e) False. Decreases the SE by a factor of 1 =p\n2.\n8.3 (a) False. A conﬁdence interval is constructed to estimate the population proportion, not the sample\nproportion. (b) True. 95% CI: 82% \u00062%. (c) True. By the deﬁnition of the conﬁdence level. (d) True.\nQuadrupling the sample size decreases the SE and ME by a factor of 1 =p\n4. (e) True. The 95% CI is entirely\nabove 50%.\n8.5 With a random sample, independence is satisﬁed. The success-failure condition is also satisﬁed. ME =\nz?q\nˆp(1\u0000ˆp)\nn= 1:96q\n0:56\u00020:44\n600= 0:0397\u00194%\n8.7 (a) No. The sample only represents students who took the SAT, and this was also an online survey.\n(b) (0.5289, 0.5711). We are 90% conﬁdent that 53% to 57% of high school seniors who took the SAT are fairly\ncertain that they will participate in a study abroad program in college. (c) 90% of such random samples would\nproduce a 90% conﬁdence interval that includes the true proportion. (d) Yes. The interval lies entirely above\n50%.\n8.9 (a) We want to check for a majority (or minority), so we use the following hypotheses:\nH0:p= 0:5 HA:p,0:5\nWe have a sample proportion of ˆp= 0:55 and a sample size of n= 617 independents.\nSince this is a random sample, independence is satisﬁed. The success-failure condition is also satisﬁed: 617 \u0002\n0:5 and 617\u0002(1\u00000:5) are both at least 10 (we use the null proportion p0= 0:5 for this check in a one-proportion\nhypothesis test).\nTherefore, we can model ˆpusing a normal distribution with a standard error of\nSE=r\np(1\u0000p)\nn= 0:02\n(We use the null proportion p0= 0:5 to compute the standard error for a one-proportion hypothesis test.)\nNext, we compute the test statistic:\nZ=0:55\u00000:5\n0:02= 2:5\nThis yields a one-tail area of 0.0062, and a p-value of 2 \u00020:0062 = 0:0124.\nBecause the p-value is smaller than 0.05, we reject the null hypothesis. We have strong evidence that the\nsupport is di ﬀerent from 0.5, and since the data provide a point estimate above 0.5, we have strong evidence\nto support this claim by the TV pundit.\n(b) No. Generally we expect a hypothesis test and a conﬁdence interval to align, so we would expect the\nconﬁdence interval to show a range of plausible values entirely above 0.5. However, if the conﬁdence level\nis misaligned (e.g. a 99% conﬁdence level and a \u000b= 0:05 signiﬁcance level), then this is no longer generally\ntrue.\n459\n8.11 Since a sample proportion ( ˆp= 0:55) is available, we use this for the sample size calculations. The\nmargin of error for a 90% conﬁdence interval is 1 :65\u0002SE= 1:65\u0002q\np(1\u0000p)\nn. We want this to be less than 0.01,\nwhere we use ˆpin place ofp:\n1:65\u0002r\n0:55(1\u00000:55)\nn\u00140:01\n1:6520:55(1\u00000:55)\n0:012\u0014n\nFrom this, we get that nmust be at least 6739.\n8.13 (a)H0:p= 0:5.HA:p,0:5. Independence (random sample) is satisﬁed, as is the success-failure\nconditions (using p0= 0:5, we expect 40 successes and 40 failures). Z= 2:91!the one tail area is 0.0018,\nso the p-value is 0.0036. Since the p-value <0:05, we reject the null hypothesis. Since we rejected H0and\nthe point estimate suggests people are better than random guessing, we can conclude the rate of correctly\nidentifying a soda for these people is signiﬁcantly better than just by random guessing. (b) If in fact people\ncannot tell the di ﬀerence between diet and regular soda and they were randomly guessing, the probability of\ngetting a random sample of 80 people where 53 or more identify a soda correctly (or 53 or more identify a\nsoda incorrectly) would be 0.0036.\n8.15 (a) Yes, it is reasonable to use the normal approximation to the binomial distribution. The sam-\nple observations are independent and the expected numbers of successes and failures are greater than 10:\nnˆp= (100)(:15) = 15 and n(1\u0000ˆp) = (100)(0:85) = 85. (b) An approximate 95% conﬁdence interval is ˆp\u0006\n1:96q\nˆp(1\u0000ˆp)\nn!(0:08;0:22). (c) The interval does not support the claim. Since the interval does not contain\n0.05, there is statistically signiﬁcant evidence at \u000b= 0:05 that the proportion of young women in the neighbor-\nhood who use birth control is di ﬀerent than 0.05. The interval is above 0.05, which is indicative of evidence\nthat more than 5% of young women in the neighborhood use birth control.\n8.17 This is not a randomized experiment, and it is unclear whether people would be a ﬀected by the behavior\nof their peers. That is, independence may not hold. Additionally, there are only 5 interventions under the\nprovocative scenario, so the success-failure condition does not hold. Even if we consider a hypothesis test\nwhere we pool the proportions, the success-failure condition will not be satisﬁed. Since one condition is\nquestionable and the other is not satisﬁed, the di ﬀerence in sample proportions will not follow a nearly normal\ndistribution.\n8.19 (a) Standard error:\nSE=r\n0:79(1\u00000:79)\n347+0:55(1\u00000:55)\n617= 0:03\nUsingz?= 1:96, we get:\n0:79\u00000:55\u00061:96\u00020:03!(0:181;0:299)\nWe are 95% conﬁdent that the proportion of Democrats who support the plan is 18.1% to 29.9% higher than\nthe proportion of Independents who support the plan. (b) True.\n460 APPENDIX A. END OF CHAPTER EXERCISE SOLUTIONS\n8.21 (a) TestH0:p1=p2againstHA:p1,p2, wherep1represents the population proportion of clinical\nimprovement in COVID-19 patients treated with remdesivir and p2represents the population proportion of\nclinical improvement in COVID-19 patients treated with placebo. Let \u000b= 0:05. Thep-value is 0.328, which\nis greater than \u000b; there is insu ﬃcient evidence to reject the null hypothesis of no di ﬀerence. Even though\nthe proportion of patients who experienced clinical improvement about 7% higher in the remdesivir group,\nthis diﬀerence is not extreme enough to represent su ﬃcient evidence that remdesivir is more e ﬀective than\nplacebo. (b) The 95% conﬁdence interval is (-0.067, 0.217); with 95% conﬁdence, this interval captures the\ndiﬀerence in population proportion of clinical mortality between COVID-19 patients treated with remdesivir\nand those treated with placebo. The interval contains 0, which is consistent with no statistically signiﬁcant ev-\nidence of a di ﬀerence. The interval reﬂects the lack of precision around the e ﬀect estimate that is characteristic\nof an insu ﬃciently large sample size.\n8.23 (a) False. The entire conﬁdence interval is above 0. (b) True. (c) True. (d) True. (e) False. It is simply the\nnegated and reordered values: (-0.06,-0.02).\n8.25 Subscript Cmeans control group. Subscript Tmeans truck drivers. H0:pC=pT.HA:pC,pT.\nIndependence is satisﬁed (random samples), as is the success-failure condition, which we would check using\nthe pooled proportion ( ˆppool= 70=495 = 0:141).Z=\u00001:65!p-value = 0 :0989. Since the p-value is high\n(default to alpha = 0.05), we fail to reject H0. The data do not provide strong evidence that the rates of sleep\ndeprivation are di ﬀerent for non-transportation workers and truck drivers.\n8.27 (a) False. The chi-square distribution has one parameter called degrees of freedom. (b) True. (c) True.\n(d) False. As the degrees of freedom increases, the shape of the chi-square distribution becomes more symmetric.\n8.29 (a) Two-way table:\nQuit\nTreatment Yes No Total\nPatch + support group 40 110 150\nOnly patch 30 120 150\nTotal 70 230 300\n(b-i)Erow 1;col1=(row 1total )\u0002(col1total )\ntabletotal= 35. This is lower than the observed value.\n(b-ii)Erow 2;col2=(row 2total )\u0002(col2total )\ntabletotal= 115. This is lower than the observed value.\n8.31 (a)H0: There is no association between statin use and educational level. HA: There is an association\nbetween statin use and educational level\n(b) It is reasonable to assume the counts are independent. The smallest expected value in the table is 39.27,\nso the success-failure condition is reasonably met. (c) There is statistically signiﬁcant evidence at \u000b= 0:05 of\nan association between educational level and statin use. Individuals with a higher educational level are less\nlikely to be statin users.\n8.33 (a)\nNo Default Default Sum\nNon-Diabetic 1053 127 1180\nDiabetic 54 0 54\nSum 1107 127 1234\n(b)H0:p1=p2versusHA:p1,p2, wherep1represents the population proportion of treatment default\nin diabetics and p2represents the population proportion of treatment default in non-diabetics. (c) It is\nreasonable to assume the counts are independent. The smallest expected value is 5.56, which is not smaller\nthan 5. (d) The \u001f2test statistic is 5.37, with 1 degree of freedom. The p-value of the test statistic is 0.02. There\nis suﬃcient evidence to conclude that the proportion of treatment default is higher in non-diabetics than in\ndiabetics.\n461\n8.35 (a) One possible 2 \u00022 contingency table:\nMosquito Nets\nNo Yes Total\nMalaria 30 22 52\nNo Malaria 70 78 148\nTotal 100 100 200\n(b) Expected number of infected children among 100 families who did receive a net:52\u0002100\n200= 26.\n(c) The null hypothesis is H0: Using a mosquito net and being infected with malaria are not associated.\nThe alternative is HA: using a net and being infected with malaria are associated. The \u001f2statistic (1.66) has\n1 degree of freedom and the table A3 can be used to show that p>0:10. There is not statistically signiﬁcant\nevidence of an association between malaria infection and use of a net in children.\n(d) Because this is a prospective study, the relative risk can be calculated directly from the table. Let\npNo Nets be the probability that a child without a net will be infected with malaria: ˆpNo Nets =30\n100= 0:30.\nLetpNets be the probability that a child with a net will be infected with malaria: ˆpNets =22\n100= 0:22. The\nestimated relative risk: cRR=ˆpNo Nets\nˆpNets=0:30\n0:22= 1:36. The risk of malaria infection for children in the control\ngroup is 36% higher than risk for children in the treatment group.\n8.37 (a) Under the null hypothesis of no association, the expected cell counts are 9.07 and 7.93 in the wait\ntogether and wait alone groups, respectively, for those considered \"high anxiety\" and 6.93 and 6.07 in the wait\ntogether and wait alone groups, respectively, for those considered \"low anxiety\". (b) Use the hypergeometric\ndistribution with parameters N= 30,m= 16, andn= 17; calculate P(X= 12). Consider the \"successes\" to\nbe the individuals who wait together, and the \"number sampled\" to be the people randomized to the high-\nanxiety group. The probability of the observed set of results, assuming the marginal totals are ﬁxed and the\nnull hypothesis is true, is 0.0304. (c) More individuals than expected in the high-anxiety group were observed\nto wait together; thus, tables that are more extreme in the same direction also consist of those where more\npeople in the high-anxiety group wait together than observed. These are tables in which 13, 14, 15, or 16\nindividuals in the high-anxiety group wait together.\nWait Together Wait Alone Sum\nHigh-Anxiety 13 4 17\nLow-Anxiety 3 10 13\nSum 16 14 30\nWait Together Wait Alone Sum\nHigh-Anxiety 14 3 17\nLow-Anxiety 2 11 13\nSum 16 14 30\nWait Together Wait Alone Sum\nHigh-Anxiety 15 2 17\nLow-Anxiety 1 12 13\nSum 16 14 30\nWait Together Wait Alone Sum\nHigh-Anxiety 16 1 17\nLow-Anxiety 0 13 13\nSum 16 14 30\n(d) Letp1represent the population proportion of individuals waiting together in the high-anxiety group\nandp2represent the population proportion of individuals waiting together in the low-anxiety group. Test\nH0:p1=p2againstHA:p1,p2. Let\u000b= 0:05. The two-sided p-value is 0.063. There is insu ﬃcient evidence\nto reject the null hypothesis; the data do not suggest there is an association between high anxiety and a person’s\ndesire to be in the company of others.\n462 APPENDIX A. END OF CHAPTER EXERCISE SOLUTIONS\n8.39 (a)H0: The distribution of the format of the book used by the students follows the professor’s predic-\ntions.HA: The distribution of the format of the book used by the students does not follow the professor’s\npredictions. (b) Ehard copy = 126\u00020:60 = 75:6.Eprint = 126\u00020:25 = 31:5.Eonline = 126\u00020:15 = 18:9. (c) In-\ndependence: The sample is not random. However, if the professor has reason to believe that the proportions\nare stable from one term to the next and students are not a ﬀecting each other’s study habits, independence is\nprobably reasonable. Sample size: All expected counts are at least 5. (d) \u001f2= 2:32,df= 2, p-value = 0.313.\n(e) Since the p-value is large, we fail to reject H0. The data do not provide strong evidence indicating the\nprofessor’s predictions were statistically inaccurate.\n8.41 (a)\nCVD No CVD\nAge Onset\u001450 Years 15 25\nAge Onset>50 Years 5 55\n(b) The odds of CVD for patients older than 50 years when diagnosed with diabetes is 5/55 = 0.09. The\nodds of CVD for the patients younger than 50 years at diabetes onset is 15/25 = 0.60. The relative odds (or\nodds ratio, OR) is 0.09/0.60 = 0.15.\n(c) The odds of CVD for someone with late onset diabetes is less than 1/5 that of people with earlier\nonset diabetes. This can be explained by the fact that people with diabetes tend to build up plaque in their\narteries; with early onset diabetes, plaque has longer time to accumulate, eventually causing CVD.\n(d)H0:OR= 1.\n(e) The chi-square test can be used to test H0as long as the conditions for the test have been met. The\nobservations are likely independent; knowing one person’s age of diabetes onset and CVD status is unlikely\nto provide information about another person’s age of diabetes onset and CVD status. Under H0, the expected\ncell count for the lower left cell is (60)(20)/100 = 12, which is bigger than 5; all other expected cell counts will\nbe larger.\n(f) Since the study is not a randomized experiment, it cannot demonstrate causality. It may be the case,\nfor example, that CVD presence causes earlier onset of diabetes. The study only demonstrates an association\nbetween cardiovascular disease and diabetes.\n8.43 (a) No. This is an example of outcome dependent sampling. Subjects were ﬁrst identiﬁed according to\npresence or absence of the CNS disorder, then queried about use of the drug. It is only possible to estimate\nthe probability that someone had used the drug, given they either did or did not have a CNS disorder.\n(b) The appropriate measure of association is the odds ratio.\n(c) The easiest way of calculating the OR for the table is the cross-product of the diagonal elements of\nthe table: [(10)(4000) ]=[(2000)(7) ]= 2:86. Using the deﬁnition, it can be calculated as:\nˆOR=ˆP(CNSjUsage)\n1\u0000ˆP(CNSjUsage)\nˆP(CNSjNo Usage)\n1\u0000ˆP(CNSjNo Usage)=ad\nbc=(10)(4000)\n(2000)(7)= 2:86\n(d) The odds ratio has the interpretation of the relative odds of presence of a CNS disorder, comparing\npeople who have used the weight loss drug to those who have not. People who have used the weight loss drug\nhave odds of CNS that are almost three times as large as those for people who have not used the drug.\n(e) Fisher’s exact test is better than the chi-square test. The independence assumption is met, but the\nexpected cell count corresponding the presence of a CNS disorder and the use of the drug is 5.68, so not all\nthe expected cell counts are less than 10.\n8.45 (a) Thep-value is 0.92; there is insu ﬃcient evidence to reject the null hypothesis of no association.\nThese data are plausible with the null hypothesis that green tea consumption is independent of esophageal\ncarcinoma. (b) Since the study uses outcome-dependent sampling, the odds ratio should be used as a measure\nof association rather than relative risk. The odds ratio of esophageal carcinoma, comparing green tea drinkers\nto non-drinkers, is 1.08; the odds of carcinoma for those who regularly drink green tea are 8% larger than the\nodds for those who never drink green tea.\n463\nAppendix B\nDistribution tables\nB.1 Normal Probability Table\nThe area to the left of Zrepresents the percentile of the observation. The normal probability table\nalways lists percentiles.\nnegative Z\nY\npositive Z\nTo ﬁnd the area to the right, calculate 1 minus the area to the left.\n1.0000 0.6664 0.3336 = \nFor additional details about working with the normal distribution and the normal probability table,\nsee Section 3.3, which starts on page 152.\n464 APPENDIX B. DISTRIBUTION TABLES\nnegative Z\nSecond decimal place of Z\n0.09 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.01 0.00 Z\n0.0002 0.0003 0.0003 0.0003 0.0003 0.0003 0.0003 0.0003 0.0003 0.0003 \u00003:4\n0.0003 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0005 0.0005 0.0005 \u00003:3\n0.0005 0.0005 0.0005 0.0006 0.0006 0.0006 0.0006 0.0006 0.0007 0.0007 \u00003:2\n0.0007 0.0007 0.0008 0.0008 0.0008 0.0008 0.0009 0.0009 0.0009 0.0010 \u00003:1\n0.0010 0.0010 0.0011 0.0011 0.0011 0.0012 0.0012 0.0013 0.0013 0.0013 \u00003:0\n0.0014 0.0014 0.0015 0.0015 0.0016 0.0016 0.0017 0.0018 0.0018 0.0019 \u00002:9\n0.0019 0.0020 0.0021 0.0021 0.0022 0.0023 0.0023 0.0024 0.0025 0.0026 \u00002:8\n0.0026 0.0027 0.0028 0.0029 0.0030 0.0031 0.0032 0.0033 0.0034 0.0035 \u00002:7\n0.0036 0.0037 0.0038 0.0039 0.0040 0.0041 0.0043 0.0044 0.0045 0.0047 \u00002:6\n0.0048 0.0049 0.0051 0.0052 0.0054 0.0055 0.0057 0.0059 0.0060 0.0062 \u00002:5\n0.0064 0.0066 0.0068 0.0069 0.0071 0.0073 0.0075 0.0078 0.0080 0.0082 \u00002:4\n0.0084 0.0087 0.0089 0.0091 0.0094 0.0096 0.0099 0.0102 0.0104 0.0107 \u00002:3\n0.0110 0.0113 0.0116 0.0119 0.0122 0.0125 0.0129 0.0132 0.0136 0.0139 \u00002:2\n0.0143 0.0146 0.0150 0.0154 0.0158 0.0162 0.0166 0.0170 0.0174 0.0179 \u00002:1\n0.0183 0.0188 0.0192 0.0197 0.0202 0.0207 0.0212 0.0217 0.0222 0.0228 \u00002:0\n0.0233 0.0239 0.0244 0.0250 0.0256 0.0262 0.0268 0.0274 0.0281 0.0287 \u00001:9\n0.0294 0.0301 0.0307 0.0314 0.0322 0.0329 0.0336 0.0344 0.0351 0.0359 \u00001:8\n0.0367 0.0375 0.0384 0.0392 0.0401 0.0409 0.0418 0.0427 0.0436 0.0446 \u00001:7\n0.0455 0.0465 0.0475 0.0485 0.0495 0.0505 0.0516 0.0526 0.0537 0.0548 \u00001:6\n0.0559 0.0571 0.0582 0.0594 0.0606 0.0618 0.0630 0.0643 0.0655 0.0668 \u00001:5\n0.0681 0.0694 0.0708 0.0721 0.0735 0.0749 0.0764 0.0778 0.0793 0.0808 \u00001:4\n0.0823 0.0838 0.0853 0.0869 0.0885 0.0901 0.0918 0.0934 0.0951 0.0968 \u00001:3\n0.0985 0.1003 0.1020 0.1038 0.1056 0.1075 0.1093 0.1112 0.1131 0.1151 \u00001:2\n0.1170 0.1190 0.1210 0.1230 0.1251 0.1271 0.1292 0.1314 0.1335 0.1357 \u00001:1\n0.1379 0.1401 0.1423 0.1446 0.1469 0.1492 0.1515 0.1539 0.1562 0.1587 \u00001:0\n0.1611 0.1635 0.1660 0.1685 0.1711 0.1736 0.1762 0.1788 0.1814 0.1841 \u00000:9\n0.1867 0.1894 0.1922 0.1949 0.1977 0.2005 0.2033 0.2061 0.2090 0.2119 \u00000:8\n0.2148 0.2177 0.2206 0.2236 0.2266 0.2296 0.2327 0.2358 0.2389 0.2420 \u00000:7\n0.2451 0.2483 0.2514 0.2546 0.2578 0.2611 0.2643 0.2676 0.2709 0.2743 \u00000:6\n0.2776 0.2810 0.2843 0.2877 0.2912 0.2946 0.2981 0.3015 0.3050 0.3085 \u00000:5\n0.3121 0.3156 0.3192 0.3228 0.3264 0.3300 0.3336 0.3372 0.3409 0.3446 \u00000:4\n0.3483 0.3520 0.3557 0.3594 0.3632 0.3669 0.3707 0.3745 0.3783 0.3821 \u00000:3\n0.3859 0.3897 0.3936 0.3974 0.4013 0.4052 0.4090 0.4129 0.4168 0.4207 \u00000:2\n0.4247 0.4286 0.4325 0.4364 0.4404 0.4443 0.4483 0.4522 0.4562 0.4602 \u00000:1\n0.4641 0.4681 0.4721 0.4761 0.4801 0.4840 0.4880 0.4920 0.4960 0.5000 \u00000:0\n\u0003ForZ\u0014\u00003:50, the probability is less than or equal to 0 :0002.\n465\nY\npositive Z\nSecond decimal place of Z\nZ 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09\n0.0 0.5000 0.5040 0.5080 0.5120 0.5160 0.5199 0.5239 0.5279 0.5319 0.5359\n0.1 0.5398 0.5438 0.5478 0.5517 0.5557 0.5596 0.5636 0.5675 0.5714 0.5753\n0.2 0.5793 0.5832 0.5871 0.5910 0.5948 0.5987 0.6026 0.6064 0.6103 0.6141\n0.3 0.6179 0.6217 0.6255 0.6293 0.6331 0.6368 0.6406 0.6443 0.6480 0.6517\n0.4 0.6554 0.6591 0.6628 0.6664 0.6700 0.6736 0.6772 0.6808 0.6844 0.6879\n0.5 0.6915 0.6950 0.6985 0.7019 0.7054 0.7088 0.7123 0.7157 0.7190 0.7224\n0.6 0.7257 0.7291 0.7324 0.7357 0.7389 0.7422 0.7454 0.7486 0.7517 0.7549\n0.7 0.7580 0.7611 0.7642 0.7673 0.7704 0.7734 0.7764 0.7794 0.7823 0.7852\n0.8 0.7881 0.7910 0.7939 0.7967 0.7995 0.8023 0.8051 0.8078 0.8106 0.8133\n0.9 0.8159 0.8186 0.8212 0.8238 0.8264 0.8289 0.8315 0.8340 0.8365 0.8389\n1.0 0.8413 0.8438 0.8461 0.8485 0.8508 0.8531 0.8554 0.8577 0.8599 0.8621\n1.1 0.8643 0.8665 0.8686 0.8708 0.8729 0.8749 0.8770 0.8790 0.8810 0.8830\n1.2 0.8849 0.8869 0.8888 0.8907 0.8925 0.8944 0.8962 0.8980 0.8997 0.9015\n1.3 0.9032 0.9049 0.9066 0.9082 0.9099 0.9115 0.9131 0.9147 0.9162 0.9177\n1.4 0.9192 0.9207 0.9222 0.9236 0.9251 0.9265 0.9279 0.9292 0.9306 0.9319\n1.5 0.9332 0.9345 0.9357 0.9370 0.9382 0.9394 0.9406 0.9418 0.9429 0.9441\n1.6 0.9452 0.9463 0.9474 0.9484 0.9495 0.9505 0.9515 0.9525 0.9535 0.9545\n1.7 0.9554 0.9564 0.9573 0.9582 0.9591 0.9599 0.9608 0.9616 0.9625 0.9633\n1.8 0.9641 0.9649 0.9656 0.9664 0.9671 0.9678 0.9686 0.9693 0.9699 0.9706\n1.9 0.9713 0.9719 0.9726 0.9732 0.9738 0.9744 0.9750 0.9756 0.9761 0.9767\n2.0 0.9772 0.9778 0.9783 0.9788 0.9793 0.9798 0.9803 0.9808 0.9812 0.9817\n2.1 0.9821 0.9826 0.9830 0.9834 0.9838 0.9842 0.9846 0.9850 0.9854 0.9857\n2.2 0.9861 0.9864 0.9868 0.9871 0.9875 0.9878 0.9881 0.9884 0.9887 0.9890\n2.3 0.9893 0.9896 0.9898 0.9901 0.9904 0.9906 0.9909 0.9911 0.9913 0.9916\n2.4 0.9918 0.9920 0.9922 0.9925 0.9927 0.9929 0.9931 0.9932 0.9934 0.9936\n2.5 0.9938 0.9940 0.9941 0.9943 0.9945 0.9946 0.9948 0.9949 0.9951 0.9952\n2.6 0.9953 0.9955 0.9956 0.9957 0.9959 0.9960 0.9961 0.9962 0.9963 0.9964\n2.7 0.9965 0.9966 0.9967 0.9968 0.9969 0.9970 0.9971 0.9972 0.9973 0.9974\n2.8 0.9974 0.9975 0.9976 0.9977 0.9977 0.9978 0.9979 0.9979 0.9980 0.9981\n2.9 0.9981 0.9982 0.9982 0.9983 0.9984 0.9984 0.9985 0.9985 0.9986 0.9986\n3.0 0.9987 0.9987 0.9987 0.9988 0.9988 0.9989 0.9989 0.9989 0.9990 0.9990\n3.1 0.9990 0.9991 0.9991 0.9991 0.9992 0.9992 0.9992 0.9992 0.9993 0.9993\n3.2 0.9993 0.9993 0.9994 0.9994 0.9994 0.9994 0.9994 0.9995 0.9995 0.9995\n3.3 0.9995 0.9995 0.9995 0.9996 0.9996 0.9996 0.9996 0.9996 0.9996 0.9997\n3.4 0.9997 0.9997 0.9997 0.9997 0.9997 0.9997 0.9997 0.9997 0.9997 0.9998\n\u0003ForZ\u00153:50, the probability is greater than or equal to 0 :9998.\n466 APPENDIX B. DISTRIBUTION TABLES\nB.2 t-Probability Table\n−3−2−10123\nOne tail−3−2−10123\nOne tail−3−2−10123\nTwo tails\nFigure B.1: Tails for the t-distribution.\none tail 0.100 0.050 0.025 0.010 0.005\ntwo tails 0.200 0.100 0.050 0.020 0.010\ndf 1 3.08 6.31 12.71 31.82 63.66\n2 1.89 2.92 4.30 6.96 9.92\n3 1.64 2.35 3.18 4.54 5.84\n4 1.53 2.13 2.78 3.75 4.60\n5 1.48 2.02 2.57 3.36 4.03\n6 1.44 1.94 2.45 3.14 3.71\n7 1.41 1.89 2.36 3.00 3.50\n8 1.40 1.86 2.31 2.90 3.36\n9 1.38 1.83 2.26 2.82 3.25\n10 1.37 1.81 2.23 2.76 3.17\n11 1.36 1.80 2.20 2.72 3.11\n12 1.36 1.78 2.18 2.68 3.05\n13 1.35 1.77 2.16 2.65 3.01\n14 1.35 1.76 2.14 2.62 2.98\n15 1.34 1.75 2.13 2.60 2.95\n16 1.34 1.75 2.12 2.58 2.92\n17 1.33 1.74 2.11 2.57 2.90\n18 1.33 1.73 2.10 2.55 2.88\n19 1.33 1.73 2.09 2.54 2.86\n20 1.33 1.72 2.09 2.53 2.85\n21 1.32 1.72 2.08 2.52 2.83\n22 1.32 1.72 2.07 2.51 2.82\n23 1.32 1.71 2.07 2.50 2.81\n24 1.32 1.71 2.06 2.49 2.80\n25 1.32 1.71 2.06 2.49 2.79\n26 1.31 1.71 2.06 2.48 2.78\n27 1.31 1.70 2.05 2.47 2.77\n28 1.31 1.70 2.05 2.47 2.76\n29 1.31 1.70 2.05 2.46 2.76\n30 1.31 1.70 2.04 2.46 2.75\n467\none tail 0.100 0.050 0.025 0.010 0.005\ntwo tails 0.200 0.100 0.050 0.020 0.010\ndf 31 1.31 1.70 2.04 2.45 2.74\n32 1.31 1.69 2.04 2.45 2.74\n33 1.31 1.69 2.03 2.44 2.73\n34 1.31 1.69 2.03 2.44 2.73\n35 1.31 1.69 2.03 2.44 2.72\n36 1.31 1.69 2.03 2.43 2.72\n37 1.30 1.69 2.03 2.43 2.72\n38 1.30 1.69 2.02 2.43 2.71\n39 1.30 1.68 2.02 2.43 2.71\n40 1.30 1.68 2.02 2.42 2.70\n41 1.30 1.68 2.02 2.42 2.70\n42 1.30 1.68 2.02 2.42 2.70\n43 1.30 1.68 2.02 2.42 2.70\n44 1.30 1.68 2.02 2.41 2.69\n45 1.30 1.68 2.01 2.41 2.69\n46 1.30 1.68 2.01 2.41 2.69\n47 1.30 1.68 2.01 2.41 2.68\n48 1.30 1.68 2.01 2.41 2.68\n49 1.30 1.68 2.01 2.40 2.68\n50 1.30 1.68 2.01 2.40 2.68\n60 1.30 1.67 2.00 2.39 2.66\n70 1.29 1.67 1.99 2.38 2.65\n80 1.29 1.66 1.99 2.37 2.64\n90 1.29 1.66 1.99 2.37 2.63\n100 1.29 1.66 1.98 2.36 2.63\n150 1.29 1.66 1.98 2.35 2.61\n200 1.29 1.65 1.97 2.35 2.60\n300 1.28 1.65 1.97 2.34 2.59\n400 1.28 1.65 1.97 2.34 2.59\n500 1.28 1.65 1.96 2.33 2.59\n1 1.28 1.65 1.96 2.33 2.58\n468 APPENDIX B. DISTRIBUTION TABLES\nB.3 Chi-Square Probability Table\n0 5 10 15\nFigure B.2: Areas in the chi-square table always refer to the right tail.\nUpper tail 0.3 0.2 0.1 0.05 0.02 0.01 0.005 0.001\ndf 1 1.07 1.64 2.71 3.84 5.41 6.63 7.88 10.83\n2 2.41 3.22 4.61 5.99 7.82 9.21 10.60 13.82\n3 3.66 4.64 6.25 7.81 9.84 11.34 12.84 16.27\n4 4.88 5.99 7.78 9.49 11.67 13.28 14.86 18.47\n5 6.06 7.29 9.24 11.07 13.39 15.09 16.75 20.52\n6 7.23 8.56 10.64 12.59 15.03 16.81 18.55 22.46\n7 8.38 9.80 12.02 14.07 16.62 18.48 20.28 24.32\n8 9.52 11.03 13.36 15.51 18.17 20.09 21.95 26.12\n9 10.66 12.24 14.68 16.92 19.68 21.67 23.59 27.88\n10 11.78 13.44 15.99 18.31 21.16 23.21 25.19 29.59\n11 12.90 14.63 17.28 19.68 22.62 24.72 26.76 31.26\n12 14.01 15.81 18.55 21.03 24.05 26.22 28.30 32.91\n13 15.12 16.98 19.81 22.36 25.47 27.69 29.82 34.53\n14 16.22 18.15 21.06 23.68 26.87 29.14 31.32 36.12\n15 17.32 19.31 22.31 25.00 28.26 30.58 32.80 37.70\n16 18.42 20.47 23.54 26.30 29.63 32.00 34.27 39.25\n17 19.51 21.61 24.77 27.59 31.00 33.41 35.72 40.79\n18 20.60 22.76 25.99 28.87 32.35 34.81 37.16 42.31\n19 21.69 23.90 27.20 30.14 33.69 36.19 38.58 43.82\n20 22.77 25.04 28.41 31.41 35.02 37.57 40.00 45.31\n25 28.17 30.68 34.38 37.65 41.57 44.31 46.93 52.62\n30 33.53 36.25 40.26 43.77 47.96 50.89 53.67 59.70\n40 44.16 47.27 51.81 55.76 60.44 63.69 66.77 73.40\n50 54.72 58.16 63.17 67.50 72.61 76.15 79.49 86.66\n469\nIndex\nAc, 100\nadjustedR2(R2\nadj),341, 341\nadjusted R-squared, 341\nalternative hypothesis ( HA),212\nanalysis of variance (ANOV A), 264, 264–270\nassociation, 38\nbar plot, 37\nsegmented bar plot, 45\nBayes’ Theorem, 114, 111–116\nblocking, 26\nblocks, 26\nBonferroni correction, 269\nboxplot, 34\ncase-control studies, 416–419\ntests for association, 417\ncategorical variable, 16\nlevels, 16\nnominal, 16\nordinal, 16\nCentral Limit Theorem, 202\nchi-square distribution, 404\nchi-square statistic, 403\nchi-square table, 405\ncohort, 24\ncollections, 94\ncolumn totals, 43\ncomplement, 100\nconditional distribution, 178, 179\nconditional distribution for random\nvariables, 177–183\nconditional probability, 108–116\nconﬁdence interval, 205, 210\nconﬁdence level, 207–208\ndiﬀerence of two means, 248–249\ndiﬀerence of two proportions, 396\ninterpretation, 209–210regression coe ﬃcient, 309\nsingle proportion, 389\nconﬁdent, 205\nconfounder, 28,332,350\nconfounding factor, 28\nconfounding variable, 28\ncontingency table, 43\ncontinuous probability distribution, 98\ncontinuous random variable, 141\ncontrol, 26\ncorrelated random variables, 180\ncorrelation, 40\ncorrelation coe ﬃcient, 40\ndata, 11\nArabidopsis thaliana, 415\nbirths, 249–251\nbreast cancer, 397–400\ncdc, 200\nCongress approval rating, 394\ndevelopmental disability support,\n253–255, 339–340\ndolphins and mercury, 241–242\nfamuss, 15, 37–48, 406–407, 414–415\nFCID, 98–99\nfecal infusion, 409–413\nforest birds, 359–367\nfrog, 14–15, 30–38, 48\nglioblastoma, 392\nGolub, 59–67\nhealth care, 396\nhiv, 402, 407\nLEAP, 12–13, 26, 408\nlife.expectancy, 40\nmammography, 397–400\nnhanes, 38–39, 297, 391\n470 INDEX\npersistent pulmonary hypertension in\nnewborns (PPHN), 417\nPREVEND, 293–295, 332–337, 350–351\nstem cells, heart function, 247–249\nswim suit velocities, 244\nwhite ﬁsh and mercury, 243\ndata density, 33\ndata ﬁshing, 266\ndata matrix, 15\ndeck of cards, 95\ndeviation, 31\ndiscrete probability distributions, 98\ndiscrete random variable, 141\ndisjoint events, 93\ndistribution, 30\nt, 238–240\nBernoulli, 147, 147–148\nbinomial, 149, 147–151\nnormal approximation, 161–162\ngeometric, 171, 170–171\nhypergeometric, 175–176\nnegative binomial, 172, 172–174\nnormal, 152, 152–167\nPoisson, 168, 168–169\ndot plot, 32\neﬀect size, 235\nempirical rule, 36\nestimate, 199\nevent, 94, 94–95\nexpectation, 142\nexpected counts, 401\nexpected value, 142\nexperiment, 24\nexplanatory variable, 17,291\nexponentially, 170\nF-statistic, 267\nfactor variables, 16\nfactorial, 149\nfailure, 147\nfalse negative, 112\nfalse positive, 112\nFisher’s exact test, 409\nfrequency table, 37\nGeneral Addition Rule, 96\nGeneral Multiplication Rule, 110\ngoodness-of-ﬁt test, 414\nGreek\nlambda (\u0015), 168\nmu (\u0016), 142sigma (\u001b), 143\nhigh leverage, 305\nhistogram, 33,48\nhypothesis testing, 212–222\ndecision errors, 221\nsigniﬁcance level, 221–222\nsingle proportion, 390\nindependent, 38,101\nindependent random variables, 180\ninﬂuential, 306\ninteraction, 352\ninterquartile range, 32\njoint distribution, 177,181\njoint distribution for random variables,\n177–183\njoint probabilities, 107\njoint probability, 107, 106–107\nLaw of Large Numbers, 92\nleast squares regression, 295–297\nR-squared (R2),302\nleast squares regression line, 295, 296\nlinear association, 38\nlinear model, 295\nlurking variable, 28\nmargin of error, 205,393, 393–394\nmarginal distribution, 178\nmarginal distribution for random variables,\n177–183\nmarginal probabilities, 107\nmarginal probability, 107, 106–107\nmarginal totals, 43\nmean, 30\naverage, 30\nmean square between groups ( MSG ),266\nmean square error ( MSE ),267,345\nmedian, 30\nMilgram, Stanley, 147\nmodality\nbimodal, 34\nmultimodal, 34\nunimodal, 34\nmode, 34\nmodel sum of squares (MSM), 345\nmoderate relationships, 291\nmultiple linear regression, 331\nF-statistic, 345\nANOV A, connection with, 368–369\n471\nassumptions, 338\ncategorical predictors, 347–348\nconﬁdence interval for the mean, 346\nconﬁdence intervals, 344\ngeneral model, 342\nhypothesis tests, 344\ninteraction, 352–356\nmodel selection, 358\nprediction, 336\nprediction interval, 346\nresidual plots, 338\nresiduals, 338, 343\nMultiplication Rule, 103\nmutually exclusive events, 93\nn choose k, 149\nnegative predictive value, 113\nnegatively associated, 38\nnon-response, 21\nnon-response bias, 21\nnormal probability plot, 163, 163–167, 301,\n339\nnormal probability table, 156\nnull hypothesis ( H0),212\nnumerical variable, 15\ncontinuous, 15\ndiscrete, 15\nobservational study, 24\nodds, 418\nodds ratio, 46,418\ncase-control studies, 418\noutlier, 35\noutlier in regression, 306\np-value, 214\npaired data, 244, 244\nparameter, 148\npercentile, 156\nplacebo, 24\npoint estimate, 201, 201–204\ndiﬀerence of two means, 247–248\ndiﬀerence of two proportions, 395\npopulation mean, 201\nsingle proportion, 389\npooled standard deviation, 256\npopulation, 18, 18–21\npopulation e ﬀect size, 259\npopulation parameter, 199\npopulation regression model, 296\npositive predictive value, 111,113\npositively associated, 38power of a test, 257,259\nprediction interval, 314\npredictor, 291\nprevalence, 113\nprobability, 92, 89–116\nprobability density function, 98\nprobability distribution, 97\nprobability of a success, 147\nprospective study, 29\nquantile-quantile plot, 163\nrandom phenomena, 93\nrandom variable, 139, 139–146\nrejection region, 217,258\nrelative frequency table, 37\nrelative odds, 419\nrelative risk, 46\nreplication, 26\nresidual, 295\nresidual confounders, 351\nresiduals, 298–301\ncontingency table, 406\nregression, 295, 338\nresponse variable, 17,291\nretrospective study, 29\nrobust estimates, 32\nrow totals, 43\nS, 100\ns, 31\nsample, 18\ncluster, 24\ncluster sample, 24\ncluster sampling, 25\nconvenience sample, 20\nmultistage sample, 24\nmultistage sampling, 25\nnon-response, 21\nnon-response bias, 21\noutcome-dependent, 416\nrandom sample, 20–21\nrepresentative sample, 20\nsimple random, 22\nsimple random sampling, 23\nstrata, 22\nstratiﬁed sampling, 22, 23\nsample proportion, 147,388\nsample size\nestimating a proportion, 393–394\nsample space, 100\nsampling distribution\n472 INDEX\ndiﬀerence of two proportions, 395\nregression coe ﬃcient, 308, 344\nsample mean, 202\nsample proportion, 389\nsampling variation, 201\nscatter plots, 294\nscatterplot, 38\nscatterplot matrix, 362\nscatterplots, 293\nSE, 203\nsensitivity, 113\nsets, 94\nshape, 33\nside-by-side boxplots, 48\nsigniﬁcance level, 213, 221–222\nmultiple comparisons, 269–270\nsimple linear regression, 291\nassumptions, 294\ncategorical predictors, 303\ninterpretation, 298\noutliers, 305\nprediction intervals, 314\nR-squared (R2), 302\nsimple random sample, 20\nSimpson’s paradox, 57\nskew\nexample: strong, 250\nexample: very strong, 165\nleft skewed, 33\nright skewed, 33\nspeciﬁcity, 113\nstandard deviation, 31, 143\nstandard error (SE), 203\ndiﬀerence in means, 248\ndiﬀerence in proportions, 395\nregression coe ﬃcient, 309\nsingle proportion, 389standard normal distribution, 152\nstrata, 22\nstratiﬁcation, 26\nstrong relationships, 291\nsuccess, 147\nsuccess-failure condition, 389\nsum of squared errors, 267\nsum of squares between groups, 266\nsymmetric, 33\nt-distribution, 238–240\nt-table, 239\nt-test\none-sample, 241–243\npaired data, 244–245\ntwo independent groups, 249–251\ntime series, 294\ntransformation, 36\ntree diagram, 112, 116\ntrial, 147\ntwo-by-two tables, 46\ntwo-sided alternative, 213\ntwo-sided conﬁdence intervals, 205\ntwo-way tables, 43\nType I error, 220\nType II error, 220\nuncorrelated, 38\nvariables, 14\nvariance, 31, 143\nVenn diagrams, 95\nweak relationship, 291\nwhiskers, 35\nZ, 153\nZ-score, 153",
  "normalized_text": "introductory statistics for the life and biomedical sciences first edition julie vu preceptor in statistics harvard university david harrington professor of biostatistics (emeritus) harvard t.h. chan school of public health dana-farber cancer institute copyright © 2020. first edition. version date: august 8th, 2021. this textbook and its supplements, including slides and labs, may be downloaded for free at openintro.org/book/biostat . this textbook is a derivative of openintro statistics 3rd edition by diez, barr, and çetinkayarundel, and it is available under a creative commons attribution-sharealike 3.0 unported united states license. license details are available at the creative commons website: creativecommons.org . source files for this book may be found on github at github.com/oi-biostat/oi_biostat_text . 3 table of contents 1 introduction to data 10 1.1 case study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 1.2 data basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 1.3 data collection principles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 1.4 numerical data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 1.5 categorical data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 1.6 relationships between two variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 1.7 exploratory data analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 1.8 notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 74 1.9 exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 75 2 probability 88 2.1 defining probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90 2.2 conditional probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106 2.3 extended example . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117 2.4 notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 126 2.5 exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 127 3 distributions of random variables 138 3.1 random variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140 3.2 binomial distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 147 3.3 normal distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152 3.4 poisson distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 168 3.5 distributions related to bernoulli trials . . . . . . . . . . . . . . . . . . . . . . . . . . . 170 3.6 distributions for pairs of random variables . . . . . . . . . . . . . . . . . . . . . . . . 177 3.7 notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 184 3.8 exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185 4 foundations for inference 198 4.1 variability in estimates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201 4.2 confidence intervals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205 4.3 hypothesis testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212 4.4 notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 225 4.5 exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227 4 table of contents 5 inference for numerical data 236 5.1 single-sample inference with the t-distribution . . . . . . . . . . . . . . . . . . . . . . 238 5.2 two-sample test for paired data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244 5.3 two-sample test for independent data . . . . . . . . . . . . . . . . . . . . . . . . . . . 247 5.4 power calculations for a di fference of means . . . . . . . . . . . . . . . . . . . . . . . . 257 5.5 comparing means with anov a . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 264 5.6 notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272 5.7 exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274 6 simple linear regression 290 6.1 examining scatterplots . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 293 6.2 estimating a regression line using least squares . . . . . . . . . . . . . . . . . . . . . . 295 6.3 interpreting a linear model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298 6.4 statistical inference with regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 308 6.5 interval estimates with regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 312 6.6 notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 316 6.7 exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 317 7 multiple linear regression 330 7.1 introduction to multiple linear regression . . . . . . . . . . . . . . . . . . . . . . . . . 332 7.2 simple versus multiple regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 334 7.3 evaluating the fit of a multiple regression model . . . . . . . . . . . . . . . . . . . . . 338 7.4 the general multiple linear regression model . . . . . . . . . . . . . . . . . . . . . . . 342 7.5 categorical predictors with several levels . . . . . . . . . . . . . . . . . . . . . . . . . . 347 7.6 reanalyzing the prevend data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 350 7.7 interaction in regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 352 7.8 model selection for explanatory models . . . . . . . . . . . . . . . . . . . . . . . . . . 358 7.9 the connection between anov a and regression . . . . . . . . . . . . . . . . . . . . . 368 7.10 notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 370 7.11 exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 372 8 inference for categorical data 386 8.1 inference for a single proportion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 388 8.2 inference for the di fference of two proportions . . . . . . . . . . . . . . . . . . . . . . 395 8.3 inference for two or more groups . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 401 8.4 chi-square tests for the fit of a distribution . . . . . . . . . . . . . . . . . . . . . . . . 414 8.5 outcome-based sampling: case-control studies . . . . . . . . . . . . . . . . . . . . . . 416 8.6 notes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 420 8.7 exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 421 a end of chapter exercise solutions 435 b distribution tables 463 index 469 5 foreword the past year has been challenging for the health sciences in ways that we could not have imagined when we started writing 5 years ago. the rapid spread of the sars coronavirus (sars-cov-2) worldwide has upended the scientific research process and highlighted the need for maintaining a balance between speed and reliability. major medical journals have dramatically increased the pace of publication; the urgency of the situation necessitates that data and research findings be made available as quickly as possible to inform public policy and clinical practice. yet it remains essential that studies undergo rigorous review; the retraction of two high-profile coronavirus studies1, 2 sparked widespread concerns about data integrity, reproducibility, and the editorial process. in parallel, deepening public awareness of structural racism has caused a re-examination of the role of race in published studies in health and medicine. a recent review of algorithms used to direct treatment in areas such as cardiology, obstetrics and oncology uncovered examples of race used in ways that may lead to substandard care for people of color.3the sars-cov-2 pandemic has reminded us once again that marginalized populations are disproportionately at risk for bad health outcomes. data on 17 million patients in england4suggest that blacks and south asians have a death rate that is approximately 50% higher than white members of the population. understanding the sars coronavirus and tackling racial disparities in health outcomes are but two of the many areas in which biostatistics will play an important role in the coming decades. much of that work will be done by those now beginning their study of biostatistics. we hope this book provides an accessible point of entry for students planning to begin work in biology, medicine, or public health. while the material presented in this book is essential for understanding the foundations of the discipline, we advise readers to remember that a mastery of technical details is secondary to choosing important scientific questions, examining data without bias, and reporting results that transparently display the strengths and weaknesses of a study. 1mandeep r. mehra et al. “retraction: cardiovascular disease, drug therapy, and mortality in covid-19. n engl j med. doi: 10.1056/nejmoa2007621.” in: new england journal of medicine 382.26 (2020), pp. 2582–2582. doi:10.1056/ nejmc2021225 . 2mandeep r mehra et al. “retracted:hydroxychloroquine or chloroquine with or without a macrolide for treatment of covid-19: a multinational registry analysis”. in: the lancet (2020). doi:https://doi.org/10.1016/s0140-6736(20) 31180-6 . 3darshali a. vyas et al. “hidden in plain sight — reconsidering the use of race correction in clinical algorithms”. in:new england journal of medicine (2020). doi:10.1056/nejmms2004740 . 4elizabeth j. williamson et al. “opensafely: factors associated with covid-19 death in 17 million patients”. in: nature (2020). issn: 1476-4687. 6 preface this text introduces statistics and its applications in the life sciences and biomedical research. it is based on the freely available openintro statistics , and, like openintro , it may be downloaded at no cost.5in writing introduction to statistics for the life and biomedical sciences , we have added substantial new material, but also retained some examples and exercises from openintro that illustrate important ideas even if they do not relate directly to medicine or the life sciences. because of its link to the original openintro project, this text is often referred to as openintro biostatistics in the supplementary materials. this text is intended for undergraduate and graduate students interested in careers in biology or medicine, and may also be profitably read by students of public health or medicine. it covers many of the traditional introductory topics in statistics, in addition to discussing some newer methods being used in molecular biology. statistics has become an integral part of research in medicine and biology, and the tools for summarizing data and drawing inferences from data are essential both for understanding the outcomes of studies and for incorporating measures of uncertainty into that understanding. an introductory text in statistics for students who will work in medicine, public health, or the life sciences should be more than simply the usual introduction, supplemented with an occasional example from biology or medical science. by drawing the majority of examples and exercises in this text from published data, we hope to convey the value of statistics in medical and biological research. in cases where examples draw on important material in biology or medicine, the problem statement contains the necessary background information. computing is an essential part of the practice of statistics. nearly everyone entering the biomedical sciences will need to interpret the results of analyses conducted in software; many will also need to be capable of conducting such analyses. the text and associated materials separate those two activities to allow students and instructors to emphasize either or both skills. the text discusses the important features of figures and tables used to support an interpretation, rather than the process of generating such material from data. this allows students whose main focus is understanding statistical concepts not to be distracted by the details of a particular software package. in our experience, however, we have found that many students enter a research setting after only a single course in statistics. these students benefit from a practical introduction to data analysis that incorporates the use of a statistical computing language. the‘ self-paced learning labs associated with the text provide such an introduction; these are described in more detail later in this preface. the datasets used in this book are available via the ropenintro package available on cran6and the roibiostat package available via github. 5pdf available at https://www.openintro.org/book/biostat/ and source available at https://github.com/ oi-biostat/oi_biostat_text . 6diez dm, barr cd, çetinkaya-rundel m. 2012. openintro : openintro data sets and supplement functions. http: //cran.r-project.org/web/packages/openintro. 7 textbook overview the chapters of this book are as follows: 1. introduction to data. data structures, basic data collection principles, numerical and graphical summaries, and exploratory data analysis. 2. probability. the basic principles of probability. 3. distributions of random variables. introduction to random variables, distributions of discrete and continuous random variables, and distributions for pairs of random variables. 4. foundations for inference. general ideas for statistical inference in the context of estimating a population mean. 5. inference for numerical data. inference for one-sample and two-sample means with the t-distribution, power calculations for a di fference of means, and anov a. 6. simple linear regression. an introduction to linear regression with a single explanatory variable, evaluating model assumptions, and inference in a regression context. 7. multiple linear regression. general multiple regression model, categorical predictors with more than two values, interaction, and model selection. 8. inference for categorical data. inference for single proportions, inference for two or more groups, and outcome-based sampling. examples, exercises, and appendices examples in the text help with an understanding of how to apply methods: example 0.1 this is an example. when a question is asked here, where can the answer be found? the answer can be found here, in the solution section of the example. when we think the reader would benefit from working out the solution to an example, we frame it as guided practice. guided practice 0.2 the reader may check or learn the answer to any guided practice problem by reviewing the full solution in a footnote.7 there are exercises at the end of each chapter that are useful for practice or homework assignments. solutions to odd numbered problems can be found in appendix a. readers will notice that there are fewer end of chapter exercises in the last three chapters. the more complicated methods, such as multiple regression, do not always lend themselves to hand calculation, and computing is increasingly important both to gain practical experience with these methods and to explore complex datasets. for students more interested in concepts than computing, however, we have included useful end of chapter exercises that emphasize the interpretation of output from statistical software. probability tables for the normal, t, and chi-square distributions are in appendix b, and pdf copies of these tables are also available from openintro.org for anyone to download, print, share, or modify. the labs and the text also illustrate the use of simple rcommands to calculate probabilities from common distributions. 7guided practice problems are intended to stretch your thinking, and you can check yourself by reviewing the footnote solution for any guided practice. 8 chapter 0. preface self-paced learning labs the labs associated with the text can be downloaded from github.com/oi-biostat/oi_biostat_ labs . they provide guidance on conducting data analysis and visualization with the rstatistical language and the computing environment rstudio, while building understanding of statistical concepts. the labs begin from first principles and require no previous experience with statistical software. both rand rstudio are freely available for all major computing operating systems, and the unit 0 labs ( 00_getting_started ) provide information on downloading and installing them. information on downloading and installing the packages may also be found at openintro.org . the labs for each chapter all have the same structure. each lab consists of a set of three documents: a handout with the problem statements, a template to be used for working through the lab, and a solution set with the problem solutions. the handout and solution set are most easily read in pdf format (although rmd files are also provided), while the template is an rmd file that can be loaded into rstudio. each chapter of labs is accompanied by a set of \"lab notes\", which provides a reference guide of all new rfunctions discussed in the labs. learning is best done, of course, if a student attempts the lab exercises before reading the solutions. the \"lab notes\" may be a useful resource to refer to while working through problems. openintro, online resources, and getting involved openintro is an organization focused on developing free and a ffordable education materials. the first project, openintro statistics , is intended for introductory statistics courses at the high school through university levels. other projects examine the use of randomization methods for learning about statistics and conducting analyses ( introductory statistics with randomization and simulation ) and advanced statistics that may be taught at the high school level ( advanced high school statistics ). we encourage anyone learning or teaching statistics to visit openintro.org and get involved by using the many online resources, which are all free, or by creating new material. students can test their knowledge with practice quizzes, or try an application of concepts learned in each chapter using real data and the free statistical software r. teachers can download the source for course materials, labs, slides, datasets, rfigures, or create their own custom quizzes and problem sets for students to take on the website. everyone is also welcome to download the book’s source files to create a custom version of this textbook or to simply share a pdf copy with a friend or on a website. all of these products are free, and anyone is welcome to use these online tools and resources with or without this textbook as a companion. acknowledgements the openintro project would not have been possible without the dedication of many people, including the authors of openintro statistics , the openintro team and the many faculty, students, and readers who commented on all the editions of openintro statistics . this text has benefited from feedback from andrea foulkes, raji balasubramanian, curry hilton, michael parzen, kevin rader, and the many excellent teaching fellows at harvard college who assisted in courses using the book. the cover design was provided by pierre baduel. 9 10 chapter 1 introduction to data 1.1 case study 1.2 data basics 1.3 data collection principles 1.4 numerical data 1.5 categorical data 1.6 relationships between two variables 1.7 exploratory data analysis 1.8 notes 1.9 exercises 11 making observations and recording data form the backbone of empirical research, and represent the beginning of a systematic approach to investigating scientific questions. as a discipline, statistics focuses on addressing the following three questions in a rigorous and e fficient manner: how can data best be collected? how should data be analyzed? what can be inferred from data? this chapter provides a brief discussion on the principles of data collection, and introduces basic methods for summarizing and exploring data. for labs, slides, and other resources, please visit www.openintro.org/book/biostat 12 chapter 1. introduction to data 1.1 case study: preventing peanut allergies the proportion of young children in western countries with peanut allergies has doubled in the last 10 years. previous research suggests that exposing infants to peanut-based foods, rather than excluding such foods from their diets, may be an e ffective strategy for preventing the development of peanut allergies. the \"learning early about peanut allergy\" (leap) study was conducted to investigate whether early exposure to peanut products reduces the probability that a child will develop peanut allergies.1 the study team enrolled children in the united kingdom between 2006 and 2009, selecting 640 infants with eczema, egg allergy, or both. each child was randomly assigned to either the peanut consumption (treatment) group or the peanut avoidance (control) group. children in the treatment group were fed at least 6 grams of peanut protein daily until 5 years of age, while children in the control group avoided consuming peanut protein until 5 years of age. at 5 years of age, each child was tested for peanut allergy using an oral food challenge (ofc): 5 grams of peanut protein in a single dose. a child was recorded as passing the oral food challenge if no allergic reaction was detected, and failing the oral food challenge if an allergic reaction occurred. these children had previously been tested for peanut allergy through a skin test, conducted at the time of study entry; the main analysis presented in the paper was based on data from 530 children with an earlier negative skin test.2 individual-level data from the study are shown in figure 1.1 for 5 of the 530 children—each row represents a participant and shows the participant’s study id number, treatment group assignment, and ofc outcome.3 participant.id treatment.group overall.v60.outcome leap_100522 peanut consumption pass ofc leap_103358 peanut consumption pass ofc leap_105069 peanut avoidance pass ofc leap_994047 peanut avoidance pass ofc leap_997608 peanut consumption pass ofc figure 1.1: individual-level leap results, for five children. the data can be organized in the form of a two-way summary table; figure 1.2 shows the results categorized by treatment group and ofc outcome. fail ofc pass ofc sum peanut avoidance 36 227 263 peanut consumption 5 262 267 sum 41 489 530 figure 1.2: summary of leap results, organized by treatment group (either peanut avoidance or consumption) and result of the oral food challenge at 5 years of age (either pass or fail). 1du toit, george, et al. randomized trial of peanut consumption in infants at risk for peanut allergy. new england journal of medicine 372.9 (2015): 803-813. 2although a total of 542 children had an earlier negative skin test, data collection did not occur for 12 children. 3the data are available as leap in the rpackage oibiostat . 1.1. case study 13 the summary table makes it easier to identify patterns in the data. recall that the question of interest is whether children in the peanut consumption group are more or less likely to develop peanut allergies than those in the peanut avoidance group. in the avoidance group, the proportion of children failing the ofc is 36 =263 = 0:137 (13.7%); in the consumption group, the proportion of children failing the ofc is 5 =267 = 0:019 (1.9%). figure 1.3 shows a graphical method of displaying the study results, using either the number of individuals per category from figure 1.2 or the proportion of individuals with a specific ofc outcome in a group. peanut avoidance peanut consumption050100150200250fail ofc pass ofc (a) peanut avoidance peanut consumption0.00.20.40.60.81.0 fail ofc pass ofc (b) figure 1.3: (a) a bar plot displaying the number of individuals who failed or passed the ofc in each treatment group. (b) a bar plot displaying the proportions of individuals in each group that failed or passed the ofc. the proportion of participants failing the ofc is 11.8% higher in the peanut avoidance group than the peanut consumption group. another way to summarize the data is to compute the ratio of the two proportions (0.137/0.019 = 7.31), and conclude that the proportion of participants failing the ofc in the avoidance group is more than 7 times as large as in the consumption group; i.e., the risk of failing the ofc was more than 7 times as great for participants in the avoidance group relative to the consumption group. based on the results of the study, it seems that early exposure to peanut products may be an effective strategy for reducing the chances of developing peanut allergies later in life. it is important to note that this study was conducted in the united kingdom at a single site of pediatric care; it is not clear that these results can be generalized to other countries or cultures. the results also raise an important statistical issue: does the study provide definitive evidence that peanut consumption is beneficial? in other words, is the 11.8% di fference between the two groups larger than one would expect by chance variation alone? the material on inference in later chapters will provide the statistical tools to evaluate this question. 14 chapter 1. introduction to data 1.2 data basics effective organization and description of data is a first step in most analyses. this section introduces a structure for organizing data and basic terminology used to describe data. 1.2.1 observations, variables, and data matrices in evolutionary biology, parental investment refers to the amount of time, energy, or other resources devoted towards raising o ffspring. this section introduces the frog dataset, which originates from a 2013 study about maternal investment in a frog species.4reproduction is a costly process for female frogs, necessitating a trade-o ffbetween individual egg size and total number of eggs produced. researchers were interested in investigating how maternal investment varies with altitude and collected measurements on egg clutches found at breeding ponds across 11 study sites; for 5 sites, the body size of individual female frogs was also recorded. altitude latitude egg.size clutch.size clutch.volume body.size 1 3,462.00 34.82 1.95 181.97 177.83 3.63 2 3,462.00 34.82 1.95 269.15 257.04 3.63 3 3,462.00 34.82 1.95 158.49 151.36 3.72 150 2,597.00 34.05 2.24 537.03 776.25 na figure 1.4: data matrix for the frog dataset. figure 1.4 displays rows 1, 2, 3, and 150 of the data from the 431 clutches observed as part of the study.5each row in the table corresponds to a single clutch, indicating where the clutch was collected ( altitude and latitude ),egg.size ,clutch.size ,clutch.volume , and body.size of the mother when available. \"na\" corresponds to a missing value, indicating that information on an individual female was not collected for that particular clutch. the recorded characteristics are referred to as variables ; in this table, each column represents a variable. variable description altitude altitude of the study site in meters above sea level latitude latitude of the study site measured in degrees egg.size average diameter of an individual egg to the 0.01 mm clutch.size estimated number of eggs in clutch clutch.volume volume of egg clutch in mm3 body.size length of mother frog in cm figure 1.5: variables and their descriptions for the frog dataset. it is important to check the definitions of variables, as they are not always obvious. for example, why has clutch.size not been recorded as whole numbers? for a given clutch, researchers counted approximately 5 grams’ worth of eggs and then estimated the total number of eggs based on the mass of the entire clutch. definitions of the variables are given in figure 1.5.6 4chen, w., et al. maternal investment increases with altitude in a frog on the tibetan plateau. journal of evolutionary biology 26.12 (2013): 2710-2715. 5the frog dataset is available in the rpackage oibiostat . 6the data discussed here are in the original scale; in the published paper, some values have undergone a natural log transformation. 1.2. data basics 15 the data in figure 1.4 are organized as a data matrix . each row of a data matrix corresponds to an observational unit, and each column corresponds to a variable. a piece of the data matrix for the leap study introduced in section 1.1 is shown in figure 1.1; the rows are study participants and three variables are shown for each participant. data matrices are a convenient way to record and store data. if the data are collected for another individual, another row can easily be added; similarly, another column can be added for a new variable. 1.2.2 types of variables the functional polymorphisms associated with human muscle size and strength study (famuss) measured a variety of demographic, phenotypic, and genetic characteristics for about 1,300 participants.7data from the study have been used in a number of subsequent studies,8such as one examining the relationship between muscle strength and genotype at a location on the actn3 gene.9 the famuss dataset is a subset of the data for 595 participants.10four rows of the famuss dataset are shown in figure 1.6, and the variables are described in figure 1.7. sex age race height weight actn3.r577x ndrm.ch 1 female 27 caucasian 65.0 199.0 cc 40.0 2 male 36 caucasian 71.7 189.0 ct 25.0 3 female 24 caucasian 65.0 134.0 ct 40.0 595 female 30 caucasian 64.0 134.0 cc 43.8 figure 1.6: four rows from the famuss data matrix. variable description sex sex of the participant age age in years race race, recorded as african am (african american), caucasian ,asian , hispanic orother height height in inches weight weight in pounds actn3.r577x genotype at the location r577x in the actn3 gene. ndrm.ch percent change in strength in the non-dominant arm, comparing strength after to before training figure 1.7: variables and their descriptions for the famuss dataset. the variables age,height ,weight , and ndrm.ch arenumerical variables . they take on numerical values, and it is reasonable to add, subtract, or take averages with these values. in contrast, a variable reporting telephone numbers would not be classified as numerical, since sums, di fferences, and averages in this context have no meaning. age measured in years is said to be discrete , since it can only take on numerical values with jumps; i.e., positive integer values. percent change in strength in the non-dominant arm ( ndrm.ch ) iscontinuous , and can take on any value within a specified range. 7thompson pd, moyna m, seip, r, et al., 2004. functional polymorphisms associated with human muscle size and strength. medicine and science in sports and exercise 36:1132 - 1139. 8pescatello l, et al. highlights from the functional single nucleotide polymorphisms associated with human muscle size and strength or famuss study, biomed research international 2013. 9clarkson p, et al., journal of applied physiology 99: 154-163, 2005. 10the subset is from foulkes, andrea s. applied statistical genetics with r: for population-based association studies. springer science & business media, 2009. the full version of the data is available at http://people.umass.edu/foulkes/ asg/data.html . 16 chapter 1. introduction to data figure 1.8: breakdown of variables into their respective types. the variables sex,race , and actn3.r577x arecategorical variables , which take on values that are names or labels. the possible values of a categorical variable are called the variable’s levels .11 for example, the levels of actn3.r577x are the three possible genotypes at this particular locus: cc, ct, or tt. categorical variables without a natural ordering are called nominal categorical variables ;sex,race , and actn3.r577x are all nominal categorical variables. categorical variables with levels that have a natural ordering are referred to as ordinal categorical variables . for example, age of the participants grouped into 5-year intervals (15-20, 21-25, 26-30, etc.) is an ordinal categorical variable. example 1.1 classify the variables in the frog dataset: altitude ,latitude ,egg.size ,clutch.size , clutch.volume , and body.size . the variables egg.size ,clutch.size ,clutch.volume , and body.size are continuous numerical variables, and can take on all positive values. in the context of this study, the variables altitude and latitude are best described as categorical variables, since the numerical values of the variables correspond to the 11 specific study sites where data were collected. researchers were interested in exploring the relationship between altitude and maternal investment; it would be reasonable to consider altitude an ordinal categorical variable. guided practice 1.2 characterize the variables treatment.group and overall.v60.outcome from the leap study (discussed in section 1.1).12 guided practice 1.3 suppose that on a given day, a research assistant collected data on the first 20 individuals visiting a walk-in clinic: age (measured as less than 21, 21 - 65, and greater than 65 years of age), sex, height, weight, and reason for the visit. classify each of the variables.13 11categorical variables are sometimes called factor variables . 12these variables measure non-numerical quantities, and thus are categorical variables with two levels. 13height and weight are continuous numerical variables. age as measured by the research assistant is ordinal categorical. sex and the reason for the visit are nominal categorical variables. 1.2. data basics 17 1.2.3 relationships between variables many studies are motivated by a researcher examining how two or more variables are related. for example, do the values of one variable increase as the values of another decrease? do the values of one variable tend to di ffer by the levels of another variable? one study used the famuss data to investigate whether actn3 genotype at a particular location (residue 577) is associated with change in muscle strength. the actn3 gene codes for a protein involved in muscle function. a common mutation in the gene at a specific location changes the cytosine (c) nucleotide to a thymine (t) nucleotide; individuals with the tt genotype are unable to produce any actn3 protein. researchers hypothesized that genotype at this location might influence muscle function. as a measure of muscle function, they recorded the percent change in non-dominant arm strength after strength training; this variable, ndrm.ch , is the response variable in the study. a response variable is defined by the particular research question a study seeks to address, and measures the outcome of interest in the study. a study will typically examine whether the values of a response variable di ffer as values of an explanatory variable change, and if so, how the two variables are related. a given study may examine several explanatory variables for a single response variable.14 the explanatory variable examined in relation to ndrm.ch in the study is actn3.r557x , actn3 genotype at location 577. example 1.4 in the maternal investment study conducted on frogs, researchers collected measurements on egg clutches and female frogs at 11 study sites, located at di ffering altitudes, in order to investigate how maternal investment varies with altitude. identify the response and explanatory variables in the study. the variables egg.size ,clutch.size , and clutch.volume are response variables indicative of maternal investment. the explanatory variable examined in the study is altitude . while latitude is an environmental factor that might potentially influence features of the egg clutches, it is not a variable of interest in this particular study. female body size ( body.size ) is neither an explanatory nor response variable. guided practice 1.5 refer to the variables from the famuss dataset described in figure 1.7 to formulate a question about the relationships between these variables, and identify the response and explanatory variables in the context of the question.15 14response variables are sometimes called dependent variables and explanatory variables are often called independent variables or predictors. 15two sample questions: (1) does change in participant arm strength after training seem associated with race? the response variable is ndrm.ch and the explanatory variable is race . (2) do male participants appear to respond di fferently to strength training than females? the response variable is ndrm.ch and the explanatory variable is sex. 18 chapter 1. introduction to data 1.3 data collection principles the first step in research is to identify questions to investigate. a clearly articulated research question is essential for selecting subjects to be studied, identifying relevant variables, and determining how data should be collected. 1.3.1 populations and samples consider the following research questions: 1. do bluefin tuna from the atlantic ocean have particularly high levels of mercury, such that they are unsafe for human consumption? 2. for infants predisposed to developing a peanut allergy, is there evidence that introducing peanut products early in life is an e ffective strategy for reducing the risk of developing a peanut allergy? 3. does a recently developed drug designed to treat glioblastoma, a form of brain cancer, appear more e ffective at inducing tumor shrinkage than the drug currently on the market? each of these questions refers to a specific target population . for example, in the first question, the target population consists of all bluefin tuna from the atlantic ocean; each individual bluefin tuna represents a case. it is almost always either too expensive or logistically impossible to collect data for every case in a population. as a result, nearly all research is based on information obtained about a sample from the population. a sample represents a small fraction of the population. researchers interested in evaluating the mercury content of bluefin tuna from the atlantic ocean could collect a sample of 500 bluefin tuna (or some other quantity), measure the mercury content, and use the observed information to formulate an answer to the research question. guided practice 1.6 identify the target populations for the remaining two research questions.16 16in question 2, the target population consists of infants predisposed to developing a peanut allergy. in question 3, the target population consists of patients with glioblastoma. 1.3. data collection principles 19 1.3.2 anecdotal evidence anecdotal evidence typically refers to unusual observations that are easily recalled because of their striking characteristics. physicians may be more likely to remember the characteristics of a single patient with an unusually good response to a drug instead of the many patients who did not respond. the dangers of drawing general conclusions from anecdotal information are obvious; no single observation should be used to draw conclusions about a population. while it is incorrect to generalize from individual observations, unusual observations can sometimes be valuable. e.c. heyde was a general practitioner from vancouver who noticed that a few of his elderly patients with aortic-valve stenosis (an abnormal narrowing) caused by an accumulation of calcium had also su ffered massive gastrointestinal bleeding. in 1958, he published his observation.17further research led to the identification of the underlying cause of the association, now called heyde’s syndrome.18 an anecdotal observation can never be the basis for a conclusion, but may well inspire the design of a more systematic study that could be definitive. 17heyde ec. gastrointestinal bleeding in aortic stenosis. n engl j med 1958;259:196. 18greenstein rj, mcelhinney aj, reuben d, greenstein aj. co-lonic vascular ectasias and aortic stenosis: coincidence or causal relationship? am j surg 1986;151:347-51. 20 chapter 1. introduction to data 1.3.3 sampling from a population sampling from a population, when done correctly, provides reliable information about the characteristics of a large population. the us centers for disease control (us cdc) conducts several surveys to obtain information about the us population, including the behavior risk factor surveillance system (brfss).19the brfss was established in 1984 to collect data about healthrelated risk behaviors, and now collects data from more than 400,000 telephone interviews conducted each year. data from a recent brfss survey are used in chapter 4. the cdc conducts similar surveys for diabetes, health care access, and immunization. likewise, the world health organization (who) conducts the world health survey in partnership with approximately 70 countries to learn about the health of adult populations and the health systems in those countries.20 the general principle of sampling is straightforward: a sample from a population is useful for learning about a population only when the sample is representative of the population. in other words, the characteristics of the sample should correspond to the characteristics of the population. suppose that the quality improvement team at an integrated health care system, such as harvard pilgrim health care, is interested in learning about how members of the health plan perceive the quality of the services o ffered under the plan. a common pitfall in conducting a survey is to use a convenience sample , in which individuals who are easily accessible are more likely to be included in the sample than other individuals. if a sample were collected by approaching plan members visiting an outpatient clinic during a particular week, the sample would fail to enroll generally healthy members who typically do not use outpatient services or schedule routine physical examinations; this method would produce an unrepresentative sample (figure 1.9). figure 1.9: instead of sampling from all members equally, approaching members visiting a clinic during a particular week disproportionately selects members who frequently use outpatient services. random sampling is the best way to ensure that a sample reflects a population. in a simple random sample , each member of a population has the same chance of being sampled. one way to achieve a simple random sample of the health plan members is to randomly select a certain number of names from the complete membership roster, and contact those individuals for an interview (figure 1.10). 19https://www.cdc.gov/brfss/index.html 20http://www.who.int/healthinfo/survey/en/ 1.3. data collection principles 21 figure 1.10: five members are randomly selected from the population to be interviewed. even when a simple random sample is taken, it is not guaranteed that the sample is representative of the population. if the non-response rate for a survey is high, that may be indicative of a biased sample. perhaps a majority of participants did not respond to the survey because only a certain group within the population is being reached; for example, if questions assume that participants are fluent in english, then a high non-response rate would be expected if the population largely consists of individuals who are not fluent in english (figure 1.11). such non-response bias can skew results; generalizing from an unrepresentative sample may likely lead to incorrect conclusions about a population. figure 1.11: surveys may only reach a certain group within the population, which leads to non-response bias. for example, a survey written in english may only result in responses from health plan members fluent in english. guided practice 1.7 it is increasingly common for health care facilities to follow-up a patient visit with an email providing a link to a website where patients can rate their experience. typically, less than 50% of patients visit the website. if half of those who respond indicate a negative experience, do you think that this implies that at least 25% of patient visits are unsatisfactory?21 21it is unlikely that the patients who respond constitute a representative sample from the larger population of patients. this is not a random sample, because individuals are selecting themselves into a group, and it is unclear that each person has an equal chance of answering the survey. if our experience is any guide, dissatisfied people are more likely to respond to these informal surveys than satisfied patients. 22 chapter 1. introduction to data 1.3.4 sampling methods almost all statistical methods are based on the notion of implied randomness. if data are not sampled from a population at random, these statistical methods – calculating estimates and errors associated with estimates – are not reliable. four random sampling methods are discussed in this section: simple, stratified, cluster, and multistage sampling. in a simple random sample , each case in the population has an equal chance of being included in the sample (figure 1.12). under simple random sampling, each case is sampled independently of the other cases; i.e., knowing that a certain case is included in the sample provides no information about which other cases have also been sampled. instratified sampling , the population is first divided into groups called strata before cases are selected within each stratum (typically through simple random sampling) (figure 1.12). the strata are chosen such that similar cases are grouped together. stratified sampling is especially useful when the cases in each stratum are very similar with respect to the outcome of interest, but cases between strata might be quite di fferent. suppose that the health care provider has facilities in di fferent cities. if the range of services offered di ffer by city, but all locations in a given city will o ffer similar services, it would be e ffective for the quality improvement team to use stratified sampling to identify participants for their study, where each city represents a stratum and plan members are randomly sampled from each city. 1.3. data collection principles 23 index● ● ●● ●●● ● ●●● ●●●● ● ●●● ●● ●● ●● ●●● ● ●● ●●● ●● ●●●● ● ● ● ●●● ●● ●●●●●● ● ● ● ● ●●●● ●● ●● ● ●● ●●● ●● ●● ● ●● ●●● ●●● ●●●● ● ● ●●● ● ● ● ●●●● ● ●●● ●● ●●● ● ●● ●● ●● ● ●● ●● ● ●● ●●● ● ●● ●● ●● ● ●● ●● ● ●● ● ● ●●●● ●●● ●●● ●● ● ● ●● ●● ● ●●● ●●● ●● ● ●● ●● ● ● ●● ● ●● ● ●● ●●●● ● ● ●● ●● ● ●● ● ●●● ●● ● ● ●● ●● ●●● ●●● ● ●● ● ●●● ●●● ●●● ●●●●●●● ●●● ●● ●●● ●● ●● ●● ●● ●●● ●● ● ●● ●●● ● ●● ●●● ● ●●● ● ●● ●●● ●●●●●● ● ●●● ●● ●● ●● ● ●●●● ●●● ●● ●● ● ●● ●●● stratum 1stratum 2 stratum 3stratum 4 stratum 5stratum 6 figure 1.12: examples of simple random and stratified sampling. in the top panel, simple random sampling is used to randomly select 18 cases (circled orange dots) out of the total population (all dots). the bottom panel illustrates stratified sampling: cases are grouped into six strata, then simple random sampling is employed within each stratum. 24 chapter 1. introduction to data in a cluster sample , the population is first divided into many groups, called clusters . then, a fixed number of clusters is sampled and all observations from each of those clusters are included in the sample (figure 1.13). a multistage sample is similar to a cluster sample, but rather than keeping all observations in each cluster, a random sample is collected within each selected cluster (figure 1.13). unlike with stratified sampling, cluster and multistage sampling are most helpful when there is high case-to-case variability within a cluster, but the clusters themselves are similar to one another. for example, if neighborhoods in a city represent clusters, cluster and multistage sampling work best when the population within each neighborhood is very diverse, but neighborhoods are relatively similar. applying stratified, cluster, or multistage sampling can often be more economical than only drawing random samples. however, analysis of data collected using such methods is more complicated than when using data from a simple random sample; this text will only discuss analysis methods for simple random samples. example 1.8 suppose researchers are interested in estimating the malaria rate in a densely tropical portion of rural indonesia. there are 30 villages in the area, each more or less similar to the others. the goal is to test 150 individuals for malaria. evaluate which sampling method should be employed. a simple random sample would likely draw individuals from all 30 villages, which could make data collection extremely expensive. stratified sampling is not advisable, since there is not enough information to determine how strata of similar individuals could be built. however, cluster sampling or multistage sampling are both reasonable options. for example, with multistage sampling, half of the villages could be randomly selected, and then 10 people selected from each village. this strategy is more e fficient than a simple random sample, and can still provide a sample representative of the population of interest. 1.3.5 introducing experiments and observational studies the two primary types of study designs used to collect data are experiments and observational studies. in an experiment , researchers directly influence how data arise, such as by assigning groups of individuals to di fferent treatments and assessing how the outcome varies across treatment groups. the leap study is an example of an experiment with two groups, an experimental group that received the intervention (peanut consumption) and a control group that received a standard approach (peanut avoidance). in studies assessing e ffectiveness of a new drug, individuals in the control group typically receive a placebo , an inert substance with the appearance of the experimental intervention. the study is designed such that on average, the only di fference between the individuals in the treatment groups is whether or not they consumed peanut protein. this allows for observed di fferences in experimental outcome to be directly attributed to the intervention and constitute evidence of a causal relationship between intervention and outcome. in an observational study , researchers merely observe and record data, without interfering with how the data arise. for example, to investigate why certain diseases develop, researchers might collect data by conducting surveys, reviewing medical records, or following a cohort of many similar individuals. observational studies can provide evidence of an association between variables, but cannot by themselves show a causal connection. however, there are many instances where randomized experiments are unethical, such as to explore whether lead exposure in young children is associated with cognitive impairment. 1.3. data collection principles 25 index●● ●● ●●● ●●● ● ●●● ● ●●●●● ●●● ●● ●●● ●● ● ●● ● ●● ● ●● ● ●● ●●● ●●● ●●● ●● ●●● ●●● ●●● ● ● ● ● ●● ● ●● ●●● ●● ● ●●●●● ● ●●● ● ● ●● ●●● ●● ● ●●● ● ● ●● ●●● ●●● ●● ● ● ● ●●● ● ● ● ●● ● ●●● ●●● ● ●●●●●● ●●● ●●● ● ●● ●● ●● ● ●● ● ●●●● ● ●●● ●● ●●● ● ●●● ● ●● ● ●●●● ●●●● ● ●●● ● ●● ● ●●●● ●●● ●● ●●● ●● ●● ● cluster 1cluster 2 cluster 3 cluster 4cluster 5 cluster 6cluster 7 cluster 8cluster 9 ● ●●● ●●●● ●● ● ●● ●● ●● ●●●● ● ●●● ●● ●● ● ●● ●● ●●● ● ●● ● ●● ●●● ● ●● ●●● ● ● ●●●● ●● ●●●●● ●● ●● ●●● ● ●● ●●●●● ●● ●● ●●●● ●●● ●●● ●● ●● ●● ● ●● ●● ●● ●●● ●●● ●●●● ●●● ●●● ● ●● ●● ●● ●●● ●● ●●● ● ●● ● ●● ● ●● ● ●●●● ●●● ● ●●●● ●● ● cluster 1cluster 2 cluster 3 cluster 4cluster 5 cluster 6cluster 7 cluster 8cluster 9 figure 1.13: examples of cluster and multistage sampling. the top panel illustrates cluster sampling: data are binned into nine clusters, three of which are sampled, and all observations within these clusters are sampled. the bottom panel illustrates multistage sampling, which di ffers from cluster sampling in that only a subset from each of the three selected clusters are sampled. 26 chapter 1. introduction to data 1.3.6 experiments experimental design is based on three principles: control, randomization, and replication. control. when selecting participants for a study, researchers work to control for extraneous variables and choose a sample of participants that is representative of the population of interest. for example, participation in a study might be restricted to individuals who have a condition that suggests they may benefit from the intervention being tested. infants enrolled in the leap study were required to be between 4 and 11 months of age, with severe eczema and/or allergies to eggs. randomization. randomly assigning patients to treatment groups ensures that groups are balanced with respect to both variables that can and cannot be controlled. for example, randomization in the leap study ensures that the proportion of males to females is approximately the same in both groups. additionally, perhaps some infants were more susceptible to peanut allergy because of an undetected genetic condition; under randomization, it is reasonable to assume that such infants were present in equal numbers in both groups. randomization allows di fferences in outcome between the groups to be reasonably attributed to the treatment rather than inherent variability in patient characteristics, since the treatment represents the only systematic di fference between the two groups. in situations where researchers suspect that variables other than the intervention may influence the response, individuals can be first grouped into blocks according to a certain attribute and then randomized to treatment group within each block; this technique is referred to as blocking orstratification . the team behind the leap study stratified infants into two cohorts based on whether or not the child developed a red, swollen mark (a wheal) after a skin test at the time of enrollment; afterwards, infants were randomized between peanut consumption and avoidance groups. figure 1.14 illustrates the blocking scheme used in the study. replication. the results of a study conducted on a larger number of cases are generally more reliable than smaller studies; observations made from a large sample are more likely to be representative of the population of interest. in a single study, replication is accomplished by collecting a su fficiently large sample. the leap study randomized a total of 640 infants. randomized experiments are an essential tool in research. the us food and drug administration typically requires that a new drug can only be marketed after two independently conducted randomized trials confirm its safety and e fficacy; the european medicines agency has a similar policy. large randomized experiments in medicine have provided the basis for major public health initiatives. in 1954, approximately 750,000 children participated in a randomized study comparing polio vaccine with a placebo.22in the united states, the results of the study quickly led to the widespread and successful use of the vaccine for polio prevention. 22meier, paul. \"the biggest public health experiment ever: the 1954 field trial of the salk poliomyelitis vaccine.\" statistics: a guide to the unknown . san francisco: holden-day (1972): 2-13. 1.3. data collection principles 27 figure 1.14: a simplified schematic of the blocking scheme used in the leap study, depicting 640 patients that underwent randomization. patients are first divided into blocks based on response to the initial skin test, then each block is randomized between the avoidance and consumption groups. this strategy ensures an even representation of patients in each group who had positive and negative skin tests. 28 chapter 1. introduction to data 1.3.7 observational studies in observational studies, researchers simply observe selected potential explanatory and response variables. participants who di ffer in important explanatory variables may also di ffer in other ways that influence response; as a result, it is not advisable to make causal conclusions about the relationship between explanatory and response variables based on observational data. for example, while observational studies of obesity have shown that obese individuals tend to die sooner than individuals with normal weight, it would be misleading to conclude that obesity causes shorter life expectancy. instead, underlying factors are probably involved; obese individuals typically exhibit other health behaviors that influence life expectancy, such as reduced exercise or unhealthy diet. suppose that an observational study tracked sunscreen use and incidence of skin cancer, and found that the more sunscreen a person uses, the more likely they are to have skin cancer. these results do not mean that sunscreen causes skin cancer. one important piece of missing information is sun exposure – if someone is often exposed to sun, they are both more likely to use sunscreen and to contract skin cancer. sun exposure is a confounding variable : a variable associated with both the explanatory and response variables.23there is no guarantee that all confounding variables can be examined or measured; as a result, it is not advisable to draw causal conclusions from observational studies. confounding is not limited to observational studies. for example, consider a randomized study comparing two treatments (varenicline and buproprion) against a placebo as therapies for aiding smoking cessation.24at the beginning of the study, participants were randomized into groups: 352 to varenicline, 329 to buproprion, and 344 to placebo. not all participants successfully completed the assigned therapy: 259, 225, and 215 patients in each group did so, respectively. if an analysis were based only on the participants who completed therapy, this could introduce confounding; it is possible that there are underlying di fferences between individuals who complete the therapy and those who do not. including all randomized participants in the final analysis maintains the original randomization scheme and controls for di fferences between the groups.25 guided practice 1.9 as stated in example 1.4, female body size ( body.size ) in the parental investment study is neither an explanatory nor a response variable. previous research has shown that larger females tend to produce larger eggs and egg clutches; however, large body size can be costly at high altitudes. discuss a possible reason for why the study team chose to measure female body size when it is not directly related to their main research question.26 23also called a lurking variable ,confounding factor , or a confounder . 24jorenby, douglas e., et al. \"e fficacy of varenicline, an 4 2 nicotinic acetylcholine receptor partial agonist, vs placebo or sustained-release bupropion for smoking cessation: a randomized controlled trial.\" jama 296.1 (2006): 56-63. 25this strategy, commonly used for analyzing clinical trial data, is referred to as an intention-to-treat analysis. 26female body size is a potential confounding variable, since it may be associated with both the explanatory variable (altitude) and response variables (measures of maternal investment). if the study team observes, for example, that clutch size tends to decrease at higher altitudes, they should check whether the apparent association is not simply due to frogs at higher altitudes having smaller body size and thus, laying smaller clutches. 1.3. data collection principles 29 observational studies may reveal interesting patterns or associations that can be further investigated with follow-up experiments. several observational studies based on dietary data from different countries showed a strong association between dietary fat and breast cancer in women. these observations led to the launch of the women’s health initiative (whi), a large randomized trial sponsored by the us national institutes of health (nih). in the whi, women were randomized to standard versus low fat diets, and the previously observed association was not confirmed. observational studies can be either prospective or retrospective. a prospective study identifies participants and collects information at scheduled times or as events unfold. for example, in the nurses’ health study, researchers recruited registered nurses beginning in 1976 and collected data through administering biennial surveys; data from the study have been used to investigate risk factors for major chronic diseases in women.27retrospective studies collect data after events have taken place, such as from medical records. some datasets may contain both retrospectively- and prospectively-collected variables. the cancer care outcomes research and surveillance consortium (cancors) enrolled participants with lung or colorectal cancer, collected information about diagnosis, treatment, and previous health behavior, but also maintained contact with participants to gather data about long-term outcomes.28 27www.channing.harvard.edu/nhs 28ayanian, john z., et al. \"understanding cancer treatment and outcomes: the cancer care outcomes research and surveillance consortium.\" journal of clinical oncology 22.15 (2004): 2992-2996 30 chapter 1. introduction to data 1.4 numerical data this section discusses techniques for exploring and summarizing numerical variables, using thefrog data from the parental investment study introduced in section 1.2. 1.4.1 measures of center: mean and median the mean , sometimes called the average, is a measure of center for a distribution of data. to find the average clutch volume for the observed egg clutches, add all the clutch volumes and divide by the total number of clutches.29 x=177:8 + 257:0 +\u0001\u0001\u0001+ 933:3 431= 882:5 mm3: the sample mean is often labeled x, to distinguish it from \u0016, the mean of the entire population x sample mean \u0016 population meanfrom which the sample is drawn. the letter xis being used as a generic placeholder for the variable of interest, clutch.volume . mean the sample mean of a numerical variable is the sum of the values of all observations divided by the number of observations: x=x1+x2+\u0001\u0001\u0001+xn n; (1.10) wherex1;x2;:::;xnrepresent the nobserved values. the median is another measure of center; it is the middle number in a distribution after the values have been ordered from smallest to largest. if the distribution contains an even number of observations, the median is the average of the middle two observations. there are 431 clutches in the dataset, so the median is the clutch volume of the 216thobservation in the sorted values of clutch.volume : 831:8 mm3. 29for computational convenience, the volumes are rounded to the first decimal. 1.4. numerical data 31 1.4.2 measures of spread: standard deviation and interquartile range the spread of a distribution refers to how similar or varied the values in the distribution are to each other; i.e., whether the values are tightly clustered or spread over a wide range. the standard deviation for a set of data describes the typical distance between an observation and the mean. the distance of a single observation from the mean is its deviation . below are the deviations for the 1st, 2nd, 3rd, and 431stobservations in the clutch.volume variable. x1\u0000x= 177:8\u0000882:5 =\u0000704:7 x2\u0000x= 257:0\u0000882:5 =\u0000625:5 x3\u0000x= 151:4\u0000882:5 =\u0000731:1 ::: x431\u0000x= 933:2\u0000882:5 = 50:7 the sample variance , the average of the squares of these deviations, is denoted by s2: s2 sample variance s2=(\u0000704:7)2+ (\u0000625:5)2+ (\u0000731:1)2+\u0001\u0001\u0001+ (50:7)2 431\u00001 =496;602:09 + 391;250:25 + 534;507:21 +\u0001\u0001\u0001+ 2570:49 430 = 143;680:9: the denominator is n\u00001 rather than n; this mathematical nuance accounts for the fact that sample mean has been used to estimate the population mean in the calculation. details on the statistical theory can be found in more advanced texts. the sample standard deviation sis the square root of the variance: s=p 143;680:9 = 379:05mm3: s sample standard deviationlike the mean, the population values for variance and standard deviation are denoted by greek letters: \u001b2for the variance and \u001bfor the standard deviation. \u001b2 population variance \u001b population standard deviationstandard deviation the sample standard deviation of a numerical variable is computed as the square root of the variance, which is the sum of squared deviations divided by the number of observations minus 1. s=s (x1\u0000x)2+ (x2\u0000x)2+\u0001\u0001\u0001+ (xn\u0000x)2 n\u00001; (1.11) wherex1;x2;:::;xnrepresent the nobserved values. 32 chapter 1. introduction to data variability can also be measured using the interquartile range (iqr). the iqr for a distribution is the di fference between the first and third quartiles: q3\u0000q1. the first quartile ( q1) is equivalent to the 25thpercentile; i.e., 25% of the data fall below this value. the third quartile (q3) is equivalent to the 75thpercentile. by definition, the median represents the second quartile, with half the values falling below it and half falling above. the iqr for clutch.volume is 1096:0\u0000609:6 = 486:4 mm3. measures of center and spread are ways to summarize a distribution numerically. using numerical summaries allows for a distribution to be e fficiently described with only a few numbers.30 for example, the calculations for clutch.volume indicate that the typical egg clutch has volume of about 880 mm3, while the middle 50% of egg clutches have volumes between approximately 600 mm3and 1100:0 mm3. 1.4.3 robust estimates figure 1.15 shows the values of clutch.volume as points on a single axis. there are a few values that seem extreme relative to the other observations: the four largest values, which appear distinct from the rest of the distribution. how do these extreme values a ffect the value of the numerical summaries? clutch volumes100300500700900110013001500170019002100230025002700 figure 1.15: dot plot of clutch volumes from the frog data. figure 1.16 shows the summary statistics calculated under two scenarios, one with and one without the four largest observations. for these data, the median does not change, while the iqr differs by only about 6 mm3. in contrast, the mean and standard deviation are much more a ffected, particularly the standard deviation. robust not robust scenario median iqr x s original data (with extreme observations) 831.8 486.9 882.5 379.1 data without four largest observations 831.8 493.9 867.9 349.2 figure 1.16: a comparison of how the median, iqr, mean ( x), and standard deviation (s) change when extreme observations are present. the median and iqr are referred to as robust estimates because extreme observations have little effect on their values. for distributions that contain extreme values, the median and iqr will provide a more accurate sense of the center and spread than the mean and standard deviation. 30numerical summaries are also known as summary statistics. 1.4. numerical data 33 1.4.4 visualizing distributions of data: histograms and boxplots graphs show important features of a distribution that are not evident from numerical summaries, such as asymmetry or extreme values. while dot plots show the exact value of each observation, histograms and boxplots graphically summarize distributions. in a histogram , observations are grouped into bins and plotted as bars. figure 1.17 shows the number of clutches with volume between 0 and 200 mm3, 200 and 400 mm3, etc. up until 2,600 and 2,800 mm3.31these binned counts are plotted in figure 1.18. clutch volumes 0-200 200-400 400-600 600-800 \u0001\u0001\u0001 2400-2600 2600-2800 count 4 29 69 99 \u0001\u0001\u0001 2 1 figure 1.17: the counts for the binned clutch.volume data. clutch volumefrequency 0 500 1000 1500 2000 2500020406080100 figure 1.18: a histogram of clutch.volume . histograms provide a view of the data density . higher bars indicate more frequent observations, while lower bars represent relatively rare observations. figure 1.18 shows that most of the egg clutches have volumes between 500-1,000 mm3, and there are many more clutches with volumes smaller than 1,000 mm3than clutches with larger volumes. histograms show the shape of a distribution. the tails of a symmetric distribution are roughly equal, with data trailing o fffrom the center roughly equally in both directions. asymmetry arises when one tail of the distribution is longer than the other. a distribution is said to be right skewed when data trail o ffto the right, and left skewed when data trail o ffto the left.32figure 1.18 shows that the distribution of clutch volume is right skewed; most clutches have relatively small volumes, and only a few clutches have high volumes. 31by default in r, the bins are left-open and right-closed; i.e., the intervals are of the form (a, b]. thus, an observation with value 200 would fall into the 0-200 bin instead of the 200-400 bin. 32other ways to describe data that are skewed to the right/left: skewed to the right/left orskewed to the positive/negative end . 34 chapter 1. introduction to data amode is represented by a prominent peak in the distribution.33figure 1.19 shows histograms that have one, two, or three major peaks. such distributions are called unimodal ,bimodal , and multimodal , respectively. any distribution with more than two prominent peaks is called multimodal. note that the less prominent peak in the unimodal distribution was not counted since it only di ffers from its neighboring bins by a few observations. prominent is a subjective term, but it is usually clear in a histogram where the major peaks are. 051015051015 05101520051015 0510152005101520 figure 1.19: from left to right: unimodal, bimodal, and multimodal distributions. aboxplot indicates the positions of the first, second, and third quartiles of a distribution in addition to extreme observations.34figure 1.20 shows a boxplot of clutch.volume alongside a vertical dot plot. clutch volume 5001000150020002500 lower whiskerq1 (first quartile)median (second quartile)q3 (third quartile)upper whiskeroutliers −−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−− figure 1.20: a boxplot and dot plot of clutch.volume . the horizontal dashes indicate the bottom 50% of the data and the open circles represent the top 50%. 33another definition of mode, which is not typically used in statistics, is the value with the most occurrences. it is common that a dataset contains noobservations with the same value, which makes this other definition impractical for many datasets. 34boxplots are also known as box-and-whisker plots. 1.4. numerical data 35 in a boxplot, the interquartile range is represented by a rectangle extending from the first quartile to the third quartile, and the rectangle is split by the median (second quartile). extending outwards from the box, the whiskers capture the data that fall between q1\u00001:5\u0002iqr andq3+1:5\u0002 iqr . the whiskers must end at data points; the values given by adding or subtracting 1 :5\u0002iqr define the maximum reach of the whiskers. for example, with the clutch.volume variable,q3+ 1:5\u0002iqr = 1;096:5+1:5\u0002486:4 = 1;826:1 mm3. however, there was no clutch with volume 1,826.1 mm3; thus, the upper whisker extends to 1,819.7 mm3, the largest observation that is smaller than q3+ 1:5\u0002iqr . any observation that lies beyond the whiskers is shown with a dot; these observations are called outliers. an outlier is a value that appears extreme relative to the rest of the data. for theclutch.volume variable, there are several large outliers and no small outliers, indicating the presence of some unusually large egg clutches. the high outliers in figure 1.20 reflect the right-skewed nature of the data. the right skew is also observable from the position of the median relative to the first and third quartiles; the median is slightly closer to the first quartile. in a symmetric distribution, the median will be halfway between the first and third quartiles. guided practice 1.12 use the histogram and boxplot in figure 1.21 to describe the distribution of height in the famuss data, where height is measured in inches.35 heightfrequency 60 65 70 75050100150 (a) height 556065707580 (b) figure 1.21: a histogram and boxplot of height in the famuss data. 35the data are roughly symmetric (the left tail is slightly longer than the right tail), and the distribution is unimodal with one prominent peak at about 67 inches. the middle 50% of individuals are between 5.5 feet and just under 6 feet tall. there is one low outlier and one high outlier, representing individuals that are unusually short/tall relative to the other individuals. 36 chapter 1. introduction to data 1.4.5 transforming data when working with strongly skewed data, it can be useful to apply a transformation , and rescale the data using a function. a natural log transformation is commonly used to clarify the features of a variable when there are many values clustered near zero and all observations are positive. income (usd)frequency $0 $20k $40k $60k $80k $100k $120k020406080100 (a) income (log usd)frequency 56789101112051015202530 (b) figure 1.22: (a) histogram of per capita income. (b) histogram of the logtransformed per capita income. for example, income data are often skewed right; there are typically large clusters of low to moderate income, with a few large incomes that are outliers. figure 1.22(a) shows a histogram of average yearly per capita income measured in us dollars for 165 countries in 2011.36the data are heavily right skewed, with the majority of countries having average yearly per capita income lower than $10,000. once the data are log-transformed, the distribution becomes roughly symmetric (figure 1.22(b)).37 for symmetric distributions, the mean and standard deviation are particularly informative summaries. if a distribution is symmetric, approximately 70% of the data are within one standard deviation of the mean and 95% of the data are within two standard deviations of the mean; this guideline is known as the empirical rule . example 1.13 on the log-transformed scale, mean log income is 8.50, with standard deviation 1.54. apply the empirical rule to describe the distribution of average yearly per capita income among the 165 countries. according to the empirical rule, the middle 70% of the data are within one standard deviation of the mean, in the range (8.50 - 1.54, 8.50 + 1.54) = (6.96, 10.04) log(usd). 95% of the data are within two standard deviations of the mean, in the range (8.50 - 2(1.54), 8.50 + 2(1.54)) = (5.42, 11.58) log(usd). undo the log transformation. the middle 70% of the data are within the range ( e6:96,e10:04) = ($1,054, $22,925). the middle 95% of the data are within the range ( e5:42,e11:58) = ($226, $106,937). functions other than the natural log can also be used to transform data, such as the square root and inverse. 36the data are available as wdi.2011 in the rpackage oibiostat . 37in statistics, the natural logarithm is usually written log. in other settings it is sometimes written as ln. 1.5. categorical data 37 1.5 categorical data this section introduces tables and plots for summarizing categorical data, using the famuss dataset introduced in section 1.2.2. a table for a single variable is called a frequency table . figure 1.23 is a frequency table for theactn3.r577x variable, showing the distribution of genotype at location r577x on the actn3 gene for the famuss study participants. in a relative frequency table like figure 1.24, the proportions per each category are shown instead of the counts. cc ct tt sum counts 173 261 161 595 figure 1.23: a frequency table for the actn3.r577x variable. cc ct tt sum proportions 0.291 0.439 0.271 1.000 figure 1.24: a relative frequency table for the actn3.r577x variable. a bar plot is a common way to display a single categorical variable. the left panel of figure 1.25 shows a bar plot of the counts per genotype for the actn3.r577x variable. the plot in the right panel shows the proportion of observations that are in each level (i.e. in each genotype). cc ct ttcount 050100150200250300 genotypecc ct tt0.000.250.50proportion genotype figure 1.25: two bar plots of actn3.r577x . the left panel shows the counts, and the right panel shows the proportions for each genotype. 38 chapter 1. introduction to data 1.6 relationships between two variables this section introduces numerical and graphical methods for exploring and summarizing relationships between two variables. approaches vary depending on whether the two variables are both numerical, both categorical, or whether one is numerical and one is categorical. 1.6.1 two numerical variables scatterplots in the frog parental investment study, researchers used clutch volume as a primary variable of interest rather than egg size because clutch volume represents both the eggs and the protective gelatinous matrix surrounding the eggs. the larger the clutch volume, the higher the energy required to produce it; thus, higher clutch volume is indicative of increased maternal investment. previous research has reported that larger body size allows females to produce larger clutches; is this idea supported by the frog data? ascatterplot provides a case-by-case view of the relationship between two numerical variables. figure 1.26 shows clutch volume plotted against body size, with clutch volume on the y-axis and body size on the x-axis. each point represents a single case. for this example, each case is one egg clutch for which both volume and body size (of the female that produced the clutch) have been recorded. 4.0 4.5 5.0 5.5 6.05001000150020002500clutch volume (mm3) ●●●●●●●●●●● ●●●●● ●● ● ●● ●● ●●●●●●●●● ●●● ●● ●●● ●● ●●●● ●●● ●● ●● ● ●●● ●●● ●●●●● ●●●●●● ●● ●●●●● ●●● ●●● ●●●● ●● ●● ●● ●● ● ●●●●● ●●●● ●●● ● ●●● ●●● ●●● ●●●● ●● ●●● ● female body size (cm) figure 1.26: a scatterplot showing clutch.volume (vertical axis) vs. body.size (horizontal axis). the plot shows a discernible pattern, which suggests an association , or relationship, between clutch volume and body size; the points tend to lie in a straight line, which is indicative of a linear association . two variables are positively associated if increasing values of one tend to occur with increasing values of the other; two variables are negatively associated if increasing values of one variable occurs with decreasing values of the other. if there is no evident relationship between two variables, they are said to be uncorrelated orindependent . as expected, clutch volume and body size are positively associated; larger frogs tend to produce egg clutches with larger volumes. these observations suggest that larger females are capable of investing more energy into o ffspring production relative to smaller females. 1.6. relationships between two v ariables 39 the national health and nutrition examination survey (nhanes) consists of a set of surveys and measurements conducted by the us cdc to assess the health and nutritional status of adults and children in the united states. the following example uses data from a sample of 500 adults (individuals ages 21 and older) from the nhanes dataset.38 example 1.14 body mass index (bmi) is a measure of weight commonly used by health agencies to assess whether someone is overweight, and is calculated from height and weight.39describe the relationships shown in figure 1.27. why is it helpful to use bmi as a measure of obesity, rather than weight? figure 1.27(a) shows a positive association between height and weight; taller individuals tend to be heavier. figure 1.27(b) shows that height and bmi do not seem to be associated; the range of bmi values observed is roughly consistent across height. weight itself is not a good measure of whether someone is overweight; instead, it is more reasonable to consider whether someone’s weight is unusual relative to other individuals of a comparable height. an individual weighing 200 pounds who is 6 ft tall is not necessarily an unhealthy weight; however, someone who weighs 200 pounds and is 5 ft tall is likely overweight. it is not reasonable to classify individuals as overweight or obese based only on weight. bmi acts as a relative measure of weight that accounts for height. specifically, bmi is used as an estimate of body fat. according to us national institutes of health (us nih) and the world health organization (who), a bmi between 25.0 - 29.9 is considered overweight and a bmi over 30 is considered obese.40 150 160 170 180 19050100150200weight (kg) ● ●●● ●● ●● ● ● ●● ●● ●● ●● ●● ●●● ● ●● ●●●● ●●● ● ●● ●● ●● ●●●●● ●● ●●● ●● ●●● ● ●●● ●● ●●●● ● ●●● ● ●●● ●● ●●● ● ●● ●● ●●● ● ● ● ●●● ●●● ●●● ●●● ●●●●●●● ●● ●● ● ●●●● ● ●●● ● ●●●● ● ●● ●● ● ● ●● ●● ●● ●● ●● ● ●● ● ●●● ●●●●● ● ●●● ● ●●●● ●● ●● ●●● ●● ● ●●● ● ● ●●● ● ●● ●●● ● ●● ●●● ● ● ● ●● ●● ● ●● ●●●● ●●●● ●● ●● ● ● ●● ●●● ● ●●●●● ●●● ● ●●● ●● ●● ●●● ●● ●●● ●●● ●●● ●● ●●● ●● ●● ● ●● ● ●●●● ●●● ●●●● ●● ●●●● ●●● ●● ● ● ●●● ● ●●● ●●● ●● ●● ●● ●● ● ●●● ●● ● ●● ●● ●●●●●● ●●● ● ●● ●● ●●●● ●● ●● ●● ●● ●● ● ● ●● ●● ● ●● ●● ●●● ● ●● ●●● ●●● ● ●● ● ●●●●●● ● ●● ●●● ●●● ●● ●● ●●● ●● ●● ● ●● ● ●●● ● ●● ●●●● ●●● ● ●●●● ● ●● ●● ● ●●●●● ●●● ●● ●● ●●●● ●● ● ●● ●●● ●● ● ●● ●● ● ●● ●●● ●●● ● ● ●●● ●●● ●●● ● ●●●● ● ●● ● ●● ●● ●● ●● height (cm) (a) 150 160 170 180 190203040506070bmi ●● ● ●●● ●●● ● ●● ●● ● ●●● ●●●● ● ● ●● ●●●● ● ●●● ●● ●●● ● ●●● ●● ●● ●●● ●● ●●● ● ●●● ● ● ●●●● ● ●●● ● ●●● ●●● ●● ● ● ●●● ●●● ● ●● ●●●●●● ●●● ● ●● ●● ●●●●● ●● ●● ● ●●●● ● ●● ●● ●●●● ● ●● ●●● ● ●● ●● ●● ●● ●● ● ●● ● ● ●● ●●●●● ● ●●● ● ●●●● ●● ●● ●●● ●● ●●●● ●●●●● ● ●● ●●● ● ●●●● ● ● ● ● ●● ●●● ●● ●●● ● ●●●● ●● ●● ●● ●●● ●● ● ●● ●●●●●● ● ●●● ●●●●●● ● ●● ● ●● ●●● ●●● ●● ●● ●●● ●● ● ●●● ● ●● ●● ●● ●● ●● ●●● ●●● ● ●● ●●● ● ●●● ● ●●● ●● ● ●●●● ●● ●● ●● ●● ●●● ●● ●● ●● ● ●●● ●●● ●●● ●● ●●●● ●● ●● ●● ●● ●● ● ●●● ●●● ●● ●● ●●● ● ● ●●●● ●●● ●●● ● ●●●● ●● ● ●● ●●● ●● ● ●● ●● ● ●● ●● ●●● ●● ● ● ●● ● ●● ●●● ●●● ● ●● ●●● ● ●● ●● ● ●●●●● ●●● ●● ●●●●●● ●● ● ●● ●● ● ●● ● ●● ● ●● ●● ●●●●●● ● ● ●●● ●●● ●●● ● ●●● ●● ●● ●●● ● ●●● ●● height (cm) (b) figure 1.27: (a) a scatterplot showing height versus weight from the 500 individuals in the sample from nhanes . one participant 163.9 cm tall (about 5 ft, 4 in) and weighing 144.6 kg (about 319 lb) is highlighted. (b) a scatterplot showing height versus bmi from the 500 individuals in the sample from nhanes . the same individual highlighted in (a) is marked here, with bmi 53.83. 38the sample is available as nhanes.samp.adult.500 in the roibiostat package. 39bmi =weightkg height2m=weightlb height2 in\u0002703. 40https://www.nhlbi.nih.gov/health/educational/lose_wt/risk.htm 40 chapter 1. introduction to data example 1.15 figure 1.28 is a scatterplot of life expectancy versus annual per capita income for 165 countries in 2011. life expectancy is measured as the expected lifespan for children born in 2011 and income is adjusted for purchasing power in a country. describe the relationship between life expectancy and annual per capita income; do they seem to be linearly associated? life expectancy and annual per capita income are positively associated; higher per capita income is associated with longer life expectancy. however, the two variables are not linearly associated. when income is low, small increases in per capita income are associated with relatively large increases in life expectancy. however, once per capita income exceeds approximately $20,000 per year, increases in income are associated with smaller gains in life expectancy. in a linear association, change in the y-variable for every unit of the x-variable is consistent across the range of the x-variable; for example, a linear association would be present if an increase in income of $10,000 corresponded to an increase in life expectancy of 5 years, across the range of income. life expectancy (years)●● ● ●●●●● ●● ●●● ●●●● ●●● ● ●● ●● ●● ●● ● ● ● ●●● ●●● ● ●● ●●● ●● ●●● ●●●● ● ●●● ●● ● ● ●● ●●● ●●● ●●●● ●● ● ● ●● ● ● ●● ●● ●●●●● ● ● ●● ●●●● ● ●● ● ●●●● ● ● ● ●●● ● ●● ●●● ●●● ● ● ● ●● ●● ●● ●● ● ●● ●● ● ●● ●●● ● ●● ● ●●●● ● ●●●● ● ● ●●● ● ● ● ● $0 $20k $40k $60k $80k $100k50556065707580 figure 1.28: a scatterplot of life expectancy (years) versus annual per capita income (us dollars) in the wdi.2011 dataset. correlation correlation is a numerical summary statistic that measures the strength of a linear relationship between two variables. it is denoted by r, the correlation coe fficient , which takes on values between r correlation coefficient-1 and 1. if the paired values of two variables lie exactly on a line, r=\u00061; the closer the correlation coefficient is to\u00061, the stronger the linear association. when two variables are positively associated, with paired values that tend to lie on a line with positive slope, r>0. if two variables are negatively associated,r<0. a value of rthat is 0 or approximately 0 indicates no apparent association between two variables.41 41if paired values lie perfectly on either a horizontal or vertical line, there is no association and ris mathematically undefined. 1.6. relationships between two v ariables 41 r = 0.33 y r = 0.69 y r = 0.98 y r = 1.00 r = −0.08 y r = −0.64 y r = −0.92 y r = −1.00 figure 1.29: scatterplots and their correlation coe fficients. the first row shows positive associations and the second row shows negative associations. from left to right, strength of the linear association between xandyincreases. the correlation coe fficient quantifies the strength of a linear trend. prior to calculating a correlation, it is advisable to confirm that the data exhibit a linear relationship. although it is mathematically possible to calculate correlation for any set of paired observations, such as the life expectancy versus income data in figure 1.28, correlation cannot be used to assess the strength of a nonlinear relationship. correlation the correlation between two variables xandyis given by: r=1 n\u00001nx i=1 xi\u0000x sx! yi\u0000y sy! ; (1.16) where (x1;y1);(x2;y2);:::;(xn;yn) are thenpaired values of xandy, andsxandsyare the sample standard deviations of the xandyvariables, respectively. 42 chapter 1. introduction to data example 1.17 calculate the correlation coe fficient ofxandy, plotted in figure 1.30. calculate the mean and standard deviation for xandy:x= 2,y= 3,sx= 1, andsy= 2:65. r=1 n\u00001nx i=1 xi\u0000x sx! yi\u0000y sy! =1 3\u00001\u0014\u00121\u00002 1\u0013\u00125\u00003 2:65\u0013 +\u00122\u00002 1\u0013\u00124\u00003 2:65\u0013 +\u00123\u00002 1\u0013\u00120\u00003 2:65\u0013\u0015 =\u00000:94: the correlation is -0.94, which reflects the negative association visible from the scatterplot in figure 1.30. 1.0 1.5 2.0 2.5 3.0012345 xy● ● ● figure 1.30: a scatterplot showing three points: (1, 5), (2, 4), and (3, 0). 1.6. relationships between two v ariables 43 example 1.18 is it appropriate to use correlation as a numerical summary for the relationship between life expectancy and income after a log transformation is applied to both variables? refer to figure 1.31. figure 1.31 shows an approximately linear relationship; a correlation coe fficient is a reasonable numerical summary of the relationship. as calculated from statistical software, r= 0:79, which is indicative of a strong linear relationship. 6 7 8 910 113.94.04.14.24.34.4life expectancy (log years)●● ● ●●●●● ●● ● ●● ●●●● ●●● ● ●● ●● ●● ●● ●● ● ●●● ●●● ●●● ●●● ●● ●●● ●●●● ● ●●● ●● ● ● ●● ●●● ●●● ●●●● ●● ● ● ●● ● ● ●● ●● ●●●●● ● ● ●● ●●●● ● ●● ● ●●●●● ● ● ●●● ● ●● ●●● ●●● ● ● ● ●● ●● ●● ●● ● ●● ●● ● ●● ●●● ● ●● ● ●●● ● ● ●●●● ● ● ●●● ● ● ● ● per capita income (log usd) figure 1.31: a scatterplot showing log(income) (horizontal axis) vs. log(life.expectancy) (vertical axis). 1.6.2 two categorical variables contingency tables acontingency table summarizes data for two categorical variables, with each value in the table representing the number of times a particular combination of outcomes occurs.42figure 1.32 summarizes the relationship between race and genotype in the famuss data. the row totals provide the total counts across each row and the column totals are the total counts for each column; collectively, these are the marginal totals . cc ct tt sum african am 16 6 5 27 asian 21 18 16 55 caucasian 125 216 126 467 hispanic 4 10 9 23 other 7 11 5 23 sum 173 261 161 595 figure 1.32: a contingency table for race and actn3.r577x . 42contingency tables are also known as two-way tables . 44 chapter 1. introduction to data like relative frequency tables for the distribution of one categorical variable, contingency tables can also be converted to show proportions. since there are two variables, it is necessary to specify whether the proportions are calculated according to the row variable or the column variable. figure 1.33 shows the row proportions for figure 1.32; these proportions indicate how genotypes are distributed within each race. for example, the value of 0.593 in the upper left corner indicates that of the african americans in the study, 59.3% have the cc genotype. cc ct tt sum african am 0.593 0.222 0.185 1.000 asian 0.382 0.327 0.291 1.000 caucasian 0.268 0.463 0.270 1.000 hispanic 0.174 0.435 0.391 1.000 other 0.304 0.478 0.217 1.000 figure 1.33: a contingency table with row proportions for the race and actn3.r577x variables. figure 1.34 shows the column proportions for figure 1.32; these proportions indicate the distribution of races within each genotype category. for example, the value of 0.092 indicates that of the cc individuals in the study, 9.2% are african american. cc ct tt african am 0.092 0.023 0.031 asian 0.121 0.069 0.099 caucasian 0.723 0.828 0.783 hispanic 0.023 0.038 0.056 other 0.040 0.042 0.031 sum 1.000 1.000 1.000 figure 1.34: a contingency table with column proportions for the race and actn3.r577x variables. example 1.19 for african americans in the study, cc is the most common genotype and tt is the least common genotype. does this pattern hold for the other races in the study? do the observations from the study suggest that distribution of genotypes at r577x vary between populations? the pattern holds for asians, but not for other races. for the caucasian individuals sampled in the study, ct is the most common genotype at 46.3%. cc is the most common genotype for asians, but in this population, genotypes are more evenly distributed: 38.2% of asians sampled are cc, 32.7% are ct, and 29.1% are tt. the distribution of genotypes at r577x seems to vary by population. guided practice 1.20 as shown in figure 1.34, 72.3% of cc individuals in the study are caucasian. do these data suggest that in the general population, people of cc genotype are highly likely to be caucasian?43 43no, this is not a reasonable conclusion to draw from the data. the high proportion of caucasians among cc individuals primarily reflects the large number of caucasians sampled in the study – 78.5% of the people sampled are caucasian. the uneven representation of di fferent races is one limitation of the famuss data. 1.6. relationships between two v ariables 45 segmented bar plots asegmented bar plot is a way of visualizing the information from a contingency table. figure 1.35 graphically displays the data from figure 1.32; each bar represents a level of actn3.r577x and is divided by the levels of race . figure 1.35(b) uses the row proportions to create a standardized segmented bar plot. cc ct tt050100150200250300 african am asian caucasian hispanic other (a) cc ct tt0.00.20.40.60.81.0 african am asian caucasian hispanic other (b) figure 1.35: (a) segmented bar plot for individuals by genotype, with bars divided by race. (b) standardized version of figure (a). alternatively, the data can be organized as shown in figure 1.36, with each bar representing a level of race . the standardized plot is particularly useful in this case, presenting the distribution of genotypes within each race more clearly than in figure 1.36(a). african am asian caucasian hispanic other0100200300400500 cc ct tt (a) african am asiancaucasian hispanic other0.00.20.40.60.81.0 cc ct tt (b) figure 1.36: (a) segmented bar plot for individuals by race, with bars divided by genotype. (b) standardized version of figure (a). 46 chapter 1. introduction to data two-by-two tables: relative risk the results from medical studies are often presented in two-by-two tables (2\u00022 tables), contingency tables for categorical variables that have two levels. one of the variables defines two groups of participants, while the other represents the two possible outcomes. figure 1.37 shows a hypothetical two-by-two table of outcome by group. outcome a outcome b sum group 1 a b a +b group 2 c d c +d sum a+c b +d a +b+c+d=n figure 1.37: a hypothetical two-by-two table of outcome by group. in the leap study, participants are divided into two groups based on treatment (peanut avoidance versus peanut consumption), while the outcome variable records whether an individual passed or failed the oral food challenge (ofc). the results of the leap study as shown in figure 1.2 are in the form of a 2 \u00022 table; the table is reproduced below as figure 1.38. a statistic called the relative risk (rr) can be used to summarize the data in a 2 \u00022 table; the relative risk is a measure of the risk of a certain event occurring in one group relative to the risk of the event occurring in another group.44 fail ofc pass ofc sum peanut avoidance 36 227 263 peanut consumption 5 262 267 sum 41 489 530 figure 1.38: results of the leap study, described in section 1.1. the question of interest in the leap study is whether the risk of developing peanut allergy (i.e., failing the ofc) di ffers between the peanut avoidance and consumption groups. the relative risk of failing the ofc equals the ratio of the proportion of individuals in the avoidance group who failed the ofc to the proportion of individuals in the consumption group who failed the ofc. example 1.21 using the results from the leap study, calculate and interpret the relative risk of failing the oral food challenge, comparing individuals in the avoidance group to individuals in the consumption group. rrfailing ofc =proportion in avoidance group who failed ofc proportion in consumption group who failed ofc=36=263 5=267= 7:31: the relative risk is 7.31. the risk of failing the oral food challenge was more than 7 times greater for participants in the peanut avoidance group than for those in the peanut consumption group. 44chapter 8 discusses another numerical summary for 2 \u00022 tables, the odds ratio . 1.6. relationships between two v ariables 47 example 1.22 an observational study is conducted to assess the association between smoking and cardiovascular disease (cvd), in which researchers identified a cohort of individuals and categorized them according to smoking and disease status. if the relative risk of cvd is calculated as the ratio of the proportion of smokers with cvd to the proportion of non-smokers with cvd, interpret the results of the study if the relative risk equals 1, is less than 1, or greater than 1. a relative risk of 1 indicates that the risk of cvd is equal for smokers and non-smokers. a relative risk less than 1 indicates that smokers are at a lower risk of cvd than non-smokers; i.e., the proportion of individuals with cvd among smokers is lower than the proportion among non-smokers. a relative risk greater than 1 indicates that smokers are at a higher risk of cvd than non-smokers; i.e., the proportion of individuals with cvd among smokers is higher than the proportion among non-smokers. guided practice 1.23 for the study described in example 1.22, suppose that of the 231 individuals, 111 are smokers. 40 smokers and 32 non-smokers have cardiovascular disease. calculate and interpret the relative risk of cvd.45 relative risk relies on the assumption that the observed proportions of an event occurring in each group are representative of the risk, or incidence, of the event occurring within the populations from which the groups are sampled. for example, in the leap data, the relative risk assumes that the proportions 33 =263 and 5=267 are estimates of the proportion of individuals who would fail the ofc among the larger population of infants who avoid or consume peanut products. example 1.24 suppose another study to examine the association between smoking and cardiovascular disease is conducted, but researchers use a di fferent study design than described in example 1.22. for the new study, 90 individuals with cvd and 110 individuals without cvd are recruited. 40 of the individuals with cvd are smokers, and 80 of the individuals without cvd are non-smokers. should relative risk be used to summarize the observations from the new study? relative risk should not be calculated for these observations. since the number of individuals with and without cvd is fixed by the study design, the proportion of individuals with cvd within a certain group (smokers or non-smokers) as calculated from the data is not a measure of cvd risk for that population. 45the relative risk of cvd, comparing smokers to non-smokers, is (40 =111)=(32=120) = 1:35. smoking is associated with a 35% increase in the probability of cvd; in other words, the risk of cvd is 35% greater in smokers compared to non-smokers. 48 chapter 1. introduction to data guided practice 1.25 for a study examining the association between tea consumption and esophageal carcinoma, researchers recruited 300 patients with carcinoma and 571 without carcinoma and administered a questionnaire about tea drinking habits.46of the 47 individuals who reported that they regularly drink green tea, 17 had carcinoma. of the 824 individuals who reported that they never, or very rarely, drink green tea, 283 had carcinoma. evaluate whether the proportions 17 =47 and 283=824 are representative of the incidence rate of carcinoma among individuals who drink green tea regularly and those who do not.47 relative risk the relative risk of outcome a in the hypothetical two-by-two table (figure 1.37) can be calculated using either group 1 or group 2 as the reference group: rra, comparing group 1 to group 2 =a=(a+b) c=(c+d) rra, comparing group 2 to group 1 =c=(c+d) a=(a+b) the relative risk should only be calculated for data where the proportions a=(a+b) andc=(c+d) represent the incidence of outcome a within the populations from which groups 1 and 2 are sampled. 1.6.3 a numerical variable and a categorical variable methods for comparing numerical data across groups are based on the approaches introduced in section 1.4. side-by-side boxplots and hollow histograms are useful for directly comparing how the distribution of a numerical variable di ffers by category. recall the question introduced in section 1.2.3: is actn3 genotype associated with variation in muscle function? figure 1.39 visually shows the relationship between muscle function (measured as percent change in non-dominant arm strength) and actn3 genotype in the famuss data with side-by-side boxplots and hollow histograms. the hollow histograms highlight how the shapes of the distributions of ndrm.ch for each genotype are essentially similar, although the distribution for the cc genotype has less right skewing. the side-by-side boxplots are especially useful for comparing center and spread, and reveal that the t allele appears to be associated with greater muscle function; median percent change in non-dominant arm strength increases across the levels from cc to tt. guided practice 1.26 using figure 1.40, assess how maternal investment varies with altitude.48 46tea drinking habits and oesophageal cancer in a high risk area in northern iran: population based casecontrol study, islami f, et al., bmj (2009), doi 10.1136/bmj.b929 47the proportions calculated from the study data should not be used as estimates of the incidence rate of esophageal carcinoma among individuals who drink green tea regularly and those who do not, since the study selected participants based on carcinoma status. 48as a general rule, clutches found at higher altitudes have greater volume; median clutch volume tends to increase as altitude increases. this suggests that increased altitude is associated with a higher level of maternal investment. 1.6. relationships between two v ariables 49 genotypepercent change in non−dominant arm strength cc ct tt050100150200250 percent change in non−dominant arm strength050100150200250cc ct tt figure 1.39: side-by-side boxplot and hollow histograms for ndrm.ch , split by levels of actn3.r577x . altitudeclutch volume 2,035.00 2,926.00 3,080.00 3,189.00 3,479.00 3,493.00050010001500200025003000 figure 1.40: side-by-side boxplot comparing the distribution of clutch.volume for different altitudes. 50 chapter 1. introduction to data 1.7 exploratory data analysis the simple techniques for summarizing and visualizing data that have been introduced in this chapter may not seem especially powerful, but when applied in practice, they can be instrumental for gaining insight into the interesting features of a dataset. this section provides three examples of data-driven research questions that can be investigated through exploratory data analysis. 1.7.1 case study: discrimination in developmental disability support in the united states, individuals with developmental disabilities typically receive services and support from state governments. the state of california allocates funds to developmentallydisabled residents through the california department of developmental services (dds); individuals receiving dds funds are referred to as ’consumers’. the dataset dds.discr represents a sample of 1,000 dds consumers (out of a total population of approximately 250,000), and includes information about age, gender, ethnicity, and the amount of financial support per consumer provided by the dds.49figure 1.41 shows the first five rows of the dataset, and the variables are described in figure 1.42. a team of researchers examined the mean annual expenditures on consumers by ethnicity, and found that the mean annual expenditures on hispanic consumers was approximately onethird of the mean expenditures on white non-hispanic consumers. as a result, an allegation of ethnic discrimination was brought against the california dds. does this finding represent su fficient evidence of ethnic discrimination, or might there be more to the story? this section will illustrate the process behind conducting an exploratory analysis that not only investigates the relationship between two variables of interest, but also considers whether other variables might be influencing that relationship. id age.cohort age gender expenditures ethnicity 1 10210 13-17 17 female 2113 white not hispanic 2 10409 22-50 37 male 41924 white not hispanic 3 10486 0-5 3 male 1454 hispanic 4 10538 18-21 19 female 6400 hispanic 5 10568 13-17 13 male 4412 white not hispanic figure 1.41: five rows from the dds.discr data matrix. variable description id unique identification code for each resident age.cohort age as sorted into six groups, 0-5 years, 6-12 years, 13-17 years, 18-21 years, 22-50 years, and 51+ years age age, measured in years gender gender, either female ormale expenditures amount of expenditures spent by the state on an individual annually, measured in usd ethnicity ethnic group, recorded as either american indian ,asian ,black ,hispanic , multi race ,native hawaiian ,other , orwhite not hispanic figure 1.42: variables and their descriptions for the dds.discr dataset. 49the dataset is based on actual attributes of consumers, but has been altered to maintain consumer privacy. 1.7. exploratory data analysis 51 distributions of single variables to begin understanding a dataset, start by examining the distributions of single variables using numerical and graphical summaries. this process is essential for developing a sense of context; in this case, examining variables individually addresses questions such as \"what is the range of annual expenditures?\", \"do consumers tend to be older or younger?\", and \"are there more consumers from one ethnic group versus another?\". figure 1.43 illustrates the right skew of expenditures , indicating that for the majority of consumers, expenditures are relatively low; most are within the $0 - $5,000 range. there are some consumers for which expenditures are much higher, such as within the $60,000 - $80,000 range. precise numerical summaries can be calculated using statistical software: the quartiles for expenditures are $2,899, $7,026, and $37,710. expenditures (usd)frequency 0 20000 40000 60000 800000100200300400 figure 1.43: a histogram of expenditures . a consumer’s age is directly recorded as the variable age; in the age.cohort variable, consumers are assigned to one of six age cohorts. the cohorts are indicative of particular life phases. in the first three cohorts, consumers are still living with their parents as they move through preschool age, elementary/middle school age, and high school age. in the 18-21 cohort, consumers are transitioning from their parents’ homes to living on their own or in supportive group homes. from ages 22-50, individuals are mostly no longer living with their parents but may still receive some support from family. in the 51+ cohort, consumers often have no living parents and typically require the most amount of support. 52 chapter 1. introduction to data figure 1.44 reveals the right-skewing of age. most consumers are younger than 30. the plot in figure 1.44(b) graphically shows the number of individuals in each age cohort. there are approximately 200 individuals in each of the middle four cohorts, while there are about 100 individuals in the other two cohorts. age (years)frequency 0 20 40 60 80 1000100200300400 (a) 0−5 6−12 13−17 18−21 22−50 51+ age (years)frequency 050100150200 (b) figure 1.44: (a) histogram of age. (b) plot of age.cohort . there are eight ethnic groups represented in dds.discr . the two largest groups, hispanic and white non-hispanic, together represent about 80% of the consumers. american indian asian black hispanic multi race native hawaiian other white not hispanic ethnicityfrequency 0100200300400 figure 1.45: a plot of ethnicity . guided practice 1.27 using figure 1.46, does gender appear to be balanced in the dds.discr dataset?50 female male genderfrequency 0100200300400500 figure 1.46: a plot of gender . 50yes, approximately half of the individuals are female and half are male. 1.7. exploratory data analysis 53 relationships between two variables after examining variables individually, explore how variables are related to each other. while there exist methods for summarizing more than two variables simultaneously, focusing on two variables at a time can be surprisingly e ffective for making sense of a dataset. it is useful to begin by investigating the relationships between the primary response variable of interest and the exploratory variables. in this case study, the response variable is expenditures , the amount of funds the dds allocates annually to each consumer. how does expenditures vary by age, ethnicity, and gender? figure 1.47 shows a side-by-side boxplot of expenditures by age cohort. there is a clear upward trend, in which older individuals tend to receive more dds funds. this reflects the underlying context of the data. the purpose of providing funds to developmentally disabled individuals is to help them maintain a quality of life similar to those without disabilities; as individuals age, it is expected their financial needs will increase. some of the observed variation in expenditures can be attributed to the fact that the dataset includes a wide range of ages. if the data included only individuals in one cohort, such as the 22-50 cohort, the distribution of expenditures would be less variable, and range between $30,000 and $60,000 instead of from $0 and $80,000. age (years)expenditures (usd) 0−5 6−12 13−17 18−21 22−50 51+020000400006000080000 figure 1.47: a plot of expenditures byage.cohort . 54 chapter 1. introduction to data how does the distribution of expenditures vary by ethnic group? does there seem to be a difference in the amount of funding that a person receives, on average, between di fferent ethnicities? a side-by-side boxplot of expenditures byethnicity (figure 1.48) reveals that the distribution of expenditures is quite di fferent between ethnic groups. for example, there is very little variation inexpenditures for the multi race, native hawaiian, and other groups. additionally, the median expenditures are not the same between groups; the medians for american indian and native hawaiian individuals are about $40,000, as compared to medians of approximately $10,000 for asian and black consumers. ethnicityexpenditures (usd) american indian asian black hispanic multi race native hawaiian otherwhite not hispanic020000400006000080000 figure 1.48: a plot of expenditures byethnicity . the trend visible in figure 1.48 seems potentially indicative of ethnic discrimination. before proceeding with the analysis, however, it is important to take into account the fact that two of the groups, hispanic and white non-hispanic, comprise the majority of the data; some ethnic groups represent less than 10% of the observations (figure 1.45). for ethnic groups with relatively small sample sizes, it is possible that the observed samples are not representative of the larger populations. the rest of this analysis will focus on comparing how expenditures varies between the two largest groups, white non-hispanic and hispanic. guided practice 1.28 using figure 1.49, do annual expenditures seem to vary by gender?51 expenditures (usd) female male020000400006000080000 gender figure 1.49: a plot of expenditures bygender . 51no, the distribution of expenditures within males and females is very similar; both are right skewed, with approximately equal median and interquartile range. 1.7. exploratory data analysis 55 figure 1.50 compares the distribution of expenditures between hispanic and white nonhispanic consumers. most hispanic consumers receive between about $0 to $20,000 from the california dds; individuals receiving amounts higher than this are upper outliers. however, for white non-hispanic consumers, median expenditures is at $20,000, and the middle 50% of consumers receive between $5,000 and $40,000. the precise summary statistics can be calculated from computing software, as shown in the corresponding rlab. the mean expenditures for hispanic consumers is $11,066, while the mean expenditures for white non-hispanic consumers is over twice as large at $24,698. on average, a hispanic consumer receives less financial support from the california dds than a white non-hispanic consumer. does this represent evidence of discrimination? ethnicityexpenditures (usd) hispanic white not hispanic020000400006000080000 figure 1.50: a plot of expenditures byethnicity , showing only hispanics and white non-hispanics. recall that expenditures is strongly associated with age—older individuals tend to receive more financial support. is there also an association between age and ethnicity, for these two ethnic groups? when using data to investigate a question, it is important to explore not only how explanatory variables are related to the response variable(s), but also how explanatory variables influence each other. figures 1.51 and 1.52 show the distribution of age within hispanics and white non-hispanics. hispanics tend to be younger, with most hispanic consumers falling into the 6-12, 13-17, and 1821 age cohorts. in contrast, white non-hispanics tend to be older; most consumers in this group are in the 22-50 age cohort, and relatively more white non-hispanic consumers are in the 51+ age cohort as compared to hispanics. 0−5 6−12 13−17 18−21 22−50 51+ age (years)frequency 020406080100 (a) 0−5 6−12 13−17 18−21 22−50 51+ age (years)frequency 020406080100120 (b) figure 1.51: (a) plot of age.cohort within hispanics. (b) plot of age.cohort within white non-hispanics. 56 chapter 1. introduction to data age cohort hispanic white non-hispanic 0-5 44/376 = 12% 20/401 = 5% 6-12 91/376 = 24% 46/401 = 11% 13-17 103/376 = 27% 67/401 = 17% 18-21 78/376 = 21% 69/401 = 17% 22-50 43/376 = 11% 133/401 = 33% 51+ 17/376 = 5% 66/401 = 16% sum 376/376 = 100% 401/401 = 100% figure 1.52: consumers by ethnicity and age cohort, shown both as counts and proportions. recall that a confounding variable is a variable that is associated with the response variable and the explanatory variable under consideration; confounding was initially introduced in the context of sunscreen use and incidence of skin cancer, where sun exposure is a confounder. in this setting, age is a confounder for the relationship between expenditures and ethnicity . just as it would be incorrect to claim that sunscreen causes skin cancer, it is essential here to recognize that there is more to the story than the apparent association between expenditures and ethnicity . for a closer look at the relationship between age, ethnicity, and expenditures, subset the data further to compare how expenditures differs by ethnicity within each age cohort. if age is indeed the primary source of the observed variation in expenditures , then there should be little di fference in average expenditures between individuals in di fferent ethnic groups but the same age cohort. figure 1.53 shows the average expenditures within each age cohort, for hispanics versus white non-hispanics. the last column contains the di fference between the two averages (calculated as white non-hispanics average - hispanics average). age cohort hispanics white non-hispanics difference 0-5 1,393 1,367 -26 6-12 2,312 2,052 -260 13-17 3,955 3,904 -51 18-21 9,960 10,133 173 22-50 40,924 40,188 -736 51+ 55,585 52,670 -2915 average 11,066 24,698 13,632 figure 1.53: average expenditures by ethnicity and age cohort, in usd ($). for all age cohorts except 18-21 years, average expenditures for white non-hispanics is lower than for hispanics. when expenditures is compared within age cohorts, there are not large di fferences between mean expenditures for white non-hispanics versus hispanics. comparing individuals of similar ages reveals that the association between ethnicity and expenditures is not nearly as strong as it seemed from the initial comparison of overall averages. 1.7. exploratory data analysis 57 instead, it is the di fference in age distributions of the two populations that is driving the observed discrepancy in expenditures . the overall average of expenditures for the hispanic consumers is lower because the population of hispanic consumers is relatively young compared to the population of white non-hispanic consumers, and the amount of expenditures for younger consumers tends to be lower than for older consumers. based on an exploratory analysis that accounts for age as a confounding variable, there does not seem to be evidence of ethnic discrimination. identifying confounding variables is essential for understanding data. confounders are often context-specific; for example, age is not necessarily a confounder for the relationship between ethnicity and expenditures in a di fferent population. additionally, it is rarely immediately obvious which variables in a dataset are confounders; looking for confounding variables is an integral part of exploring a dataset. chapter 7 introduces multiple linear regression, a method that can directly summarize the relationship between ethnicity, expenditures, and age, in addition to the tools for evaluating whether the observed discrepancies within age cohorts are greater than would be expected by chance variation alone. simpson’s paradox these data represent an extreme example of confounding known as simpson’s paradox , in which an association observed in several groups may disappear or reverse direction once the groups are combined. in other words, an association between two variables xandymay disappear or reverse direction once data are partitioned into subpopulations based on a third variable z(i.e., a confounding variable). figure 1.53 shows how mean expenditures is higher for hispanics than white non-hispanics in all age cohorts except one. yet, once all the data are aggregated, the average expenditures for white non-hispanics is over twice as large as the average for hispanics. the paradox can be explored from a mathematical perspective by using weighted averages, where the average expenditure for each cohort is weighted by the proportion of the population in that cohort. example 1.29 using the proportions in figure 1.52 and the average expenditures for each cohort in figure 1.53, calculate the overall weighted average expenditures for hispanics and for white non-hispanics.52 for hispanics: 1;393(:12) + 2;312(:24) + 3;955(:27) + 9;960(:21) + 40;924(:11) + 55;585(:05) = $11;162: for white non-hispanics: 1;367(0:05) + 2;052(:11) + 3;904(:17) + 10;133(:17) + 40;188(:33) + 52;760(:16) = $24;384: the weights for the youngest four cohorts, which have lower expenditures, are higher for the hispanic population than the white non-hispanic population; additionally, the weights for the oldest two cohorts, which have higher expenditures, are higher for the white non-hispanic population. this leads to overall average expenditures for the white non-hispanics being higher than for hispanics. 52due to rounding, the overall averages calculated via this method will not exactly equal $11,066 and $24,698. 58 chapter 1. introduction to data 1.7.2 case study: molecular cancer classification the genetic code stored in dna contains the necessary information for producing the proteins that ultimately determine an organism’s observable traits (phenotype). although nearly every cell in an organism contains the same genes, cells may exhibit di fferent patterns of gene expression. not only can genes be switched on or o ffin certain tissues, but they can also be expressed at varying levels. these variations in gene expression underlie the wide range of physical, biochemical, and developmental di fferences that characterize specific cells and tissues. originally, scientists were limited to monitoring the expression of only a single gene at a time. the development of microarray technology in the 1990’s made it possible to examine the expression of thousands of genes simultaneously. while newer genomic technologies have started to replace microarrays for gene expression studies, microarrays continue to remain clinically relevant as a tool for genetic diagnosis. for example, a 2002 study examined the e ffectiveness of gene expression profiling as a tool for predicting disease outcome in breast cancer patients, reporting that the expression data from 70 genes constituted a more powerful predictor of survival than standard systems based on clinical criteria.53 this section introduces the principles behind dna microarrays and discusses the 1999 golub leukemia study, which represents one of the earliest applications of microarray technology for diagnostic purposes. dna microarrays microarray technology is based on hybridization, a basic property of nucleic acids in which complementary nucleotide sequences specifically bind together. each microarray consists of a glass or silicon slide dotted with a grid of short (25-40 base pairs long), single-stranded dna fragments, known as probes. the probes in a single spot are present in millions of copies, and optimized to uniquely correspond to a gene. to measure the gene expression profile of a sample, mrna is extracted from the sample and converted into complementary-dna (cdna). the cdna is then labeled with a fluorescent dye and added to a microarray. when cdna from the sample encounters complementary dna probes, the two strands will hybridize, allowing the cdna to adhere to specific spots on the slide. once the chip is illuminated (to activate the fluorescence) and scanned, the intensity of fluorescence detected at each spot corresponds to the amount of bound cdna. microarrays are commonly used to compare gene expression between an experimental sample and a reference sample. suppose that the reference sample is taken from healthy cells and the experimental sample from cancer cells. first, the cdna from the samples are di fferentially labeled, such as green dye for the healthy cells and red dye for the cancer cells. the samples are then mixed together and allowed to bind to the slide. if the expression of a particular gene is higher in the experimental sample than in the reference sample, then the corresponding spot on the microarray will appear red. in contrast, the spot will appear green if expression in the experimental sample is lower than in the reference sample. equal expression levels result in a yellow spot, while no expression in either sample shows as a black dot. the fluorescence intensity data provide a relative measure of gene expression, showing which genes on the chip seem to be more or less active in relation to each other. 53van de vijver mj, he yd, van’t veer lj, et al. a gene-expression sign as a predictor of survival in breast cancer. new england journal of medicine 2002;347:1999-2009. 1.7. exploratory data analysis 59 the raw data produced by a microarray is messy, due to factors such as imperfections during chip manufacturing or unpredictable probe behavior. it is also possible for inaccuracies to be introduced from cdna binding to probes that are not precise sequence matches; this nonspecific binding will contribute to observed intensity, but not reflect the expression level of a gene. methods to improve microarray accuracy by reducing the frequency of nonspecific binding include using longer probes or multiple probes per gene that correspond to di fferent regions of the gene sequence.54the affymetrix company developed a di fferent strategy involving the use of probe pairs; one set of probes are a perfect match to the gene sequence (pm probes), while the mismatch probes contain a single base di fference in the middle of the sequence (mm probes). the mm probes act as a control for any cdna that exhibit nonspecific binding; subtracting the mm probe intensity from the pm intensity (pm - mm) provides a more accurate measure of fluorescence produced by specific hybridization. considerable research has been done to develop methods for pre-processing microarray data to adjust for various errors and produce data that can be analyzed. when analyzing \"cleaned\" data from any experiment, it is important to be aware that the reliability of any conclusions drawn from the data depends, to a large extent, on the care that has been taken in collecting and processing the data. golub leukemia study accurate cancer classification is critical for determining an appropriate course of therapy. the chemotherapy regimens for acute leukemias di ffers based on whether the leukemia a ffects bloodforming cells (acute myeloid leukemia, aml) or white blood cells (acute lymphoblastic leukemia, all). at the time of the golub study, no single diagnostic test was su fficient for distinguishing between aml and all. to investigate whether gene expression profiling could be a tool for classifying acute leukemia type, golub and co-authors used a ffymetrix dna microarrays to measure the expression level of 7,129 genes from children known to have either aml or all.55 the original data (after some initial pre-processing) are available from the broad institute.56 the version of the data presented in this text have undergone further processing; the expression levels have been normalized to adjust for the variability between the separate arrays used for each sampled individual.57figure 1.54 describes the variables in the first six columns of the golub data. the last 7,129 columns of the dataset contain the expression data for the genes examined in the study; each column is named after the probe corresponding to a specific gene. variable description samples sample number; unique to each patient. bm.pb type of patient material. bm for bone marrow; pb for peripheral blood. gender f for female, m for male. source hospital where the patient was treated. tissue.mf combination of bm.pb and gender cancer leukemia type; aml is acute myeloid leukemia, allb is acute lymphoblastic leukemia with b-cell origin, and allt is acute lymphoblastic leukemia with t-cell origin. figure 1.54: variables and their descriptions for the patient descriptors in golub dataset. 54chou, c.c. et al. optimization of probe length and the number of probes per gene for optimal microarray analysis of gene expression. nucleic acids research 2004; 32: e99. 55golub, todd r., et al. molecular classification of cancer: class discovery and class prediction by gene expression monitoring. science 286 (1999): 531-537. 56http://www-genome.wi.mit.edu/mpr/data_set_all_aml.html 57john maindonald, w. john braun. data analysis and graphics using r: an example-based approach. 60 chapter 1. introduction to data figure 1.55 shows five rows and seven columns from the dataset. each row corresponds to a patient. these five patients were all treated at the dana farber cancer institute (dfci) ( source ) for all with b-cell origin ( cancer ), and samples were taken from bone marrow ( bm.pb ). four of the patients were female and one was male ( gender ). the last row in the table shows the normalized gene expression level for the gene corresponding to the probe affx.biob.5.at. samples bm.pb gender source tissue.mf cancer affx-biob-5_at 39 bm f dfci bm:f allb -1363.28 40 bm f dfci bm:f allb -796.29 42 bm f dfci bm:f allb -679.14 47 bm m dfci bm:m allb -1164.40 48 bm f dfci bm:f allb -1299.65 figure 1.55: five rows and seven columns from the golub data. the goal of the golub study was to develop a procedure for distinguishing between aml and all based only on the gene expression levels of a patient. there are two major issues to be addressed: 1.which genes are the most informative for making a prediction? if a gene is di fferentially expressed between individuals with aml versus all, then measuring the expression level of that gene may be informative for diagnosing leukemia type. for example, if a gene tends to be highly expressed in aml individuals, but only expressed at low levels in all individuals, it is more likely to be a good predictor of leukemia type than a gene that is expressed at similar levels in both aml and all patients. 2.how can leukemia type be predicted from expression data? suppose that a patient’s expression profile is measured for a group of genes. in an ideal scenario, all the genes measured would exhibit aml-like expression, or all-like expression, making a prediction obvious. in reality, however, a patient’s expression profile will not follow an idealized pattern. some of the genes may have expression levels more typical of aml, while others may suggest all. it is necessary to clearly define a strategy for translating raw expression data into a prediction of leukemia type. even though the golub dataset is relatively small by modern standards, it is already too large to feasibly analyze without the use of statistical computing software. in this section, conceptual details will be demonstrated with a small version of the dataset ( golub.small ) that contains only the data for 10 patients and 10 genes. figure 1.56 shows the cancer type and expression data in golub.small ; the expression values have been rounded to the nearest whole number, and the gene probes are labeled a-j for convenience. cancer a b c d e f g h i j allb 39308 35232 41171 35793 -593 -1053 -513 -537 1702 1120 allt 32282 41432 59329 49608 -123 -511 265 -272 3567 -489 allb 47430 35569 56075 42858 -208 -712 32 -313 433 400 allb 25534 16984 28057 32694 89 -534 -24 195 3355 990 allb 35961 24192 27638 22241 -274 -632 -488 20 2259 348 aml 46178 6189 12557 34485 -331 -776 -551 -48 4074 -578 aml 43791 33662 38380 29758 -47 124 1118 3425 7018 1133 aml 53420 26109 31427 23810 396 108 1040 1915 4095 -709 aml 41242 37590 47326 30099 15 -429 784 -532 1085 -1912 aml 41301 49198 66026 56249 -418 -948 -340 -905 877 745 figure 1.56: leukemia type and expression data from golub.small . 1.7. exploratory data analysis 61 to start understanding how gene expression di ffers by leukemia type, summarize the data separately for aml patients and for all patients, then make comparisons. for example, how does the expression of gene a di ffer between individuals with aml versus all? among the 5 individuals with aml, the mean expression for gene a is 45,186; among the 5 all individuals, mean expression for gene a is 36,103. figure 1.57 shows mean expression values for each gene among aml patients and figure 1.58 among all patients. aml a b c d e f g h i j 46178 6189 12557 34485 -331 -776 -551 -48 4074 -578 43791 33662 38380 29758 -47 124 1118 3425 7018 1133 53420 26109 31427 23810 396 108 1040 1915 4095 -709 41242 37590 47326 30099 15 -429 784 -532 1085 -1912 41301 49198 66026 56249 -418 -948 -340 -905 877 745 mean 45186 30550 39143 34880 -77 -384 410 771 3430 -264 figure 1.57: expression data for aml patients, where the last row contains mean expression value for each gene among the 5 aml patients. the first five rows are duplicated from the last five rows in figure 1.56. all a b c d e f g h i j 39308 35232 41171 35793 -593 -1053 -513 -537 1702 1120 32282 41432 59329 49608 -123 -511 265 -272 3567 -489 47430 35569 56075 42858 -208 -712 32 -313 433 400 25534 16984 28057 32694 89 -534 -24 195 3355 990 35961 24192 27638 22241 -274 -632 -488 20 2259 348 mean 36103 30682 42454 36639 -222 -689 -146 -181 2263 474 figure 1.58: expression data for all patients, where the last row contains mean expression value for each gene among the 5 all patients. the first five rows are duplicated from the first five rows in figure 1.56. example 1.30 on average, which genes are more highly expressed in aml patients? which genes are more highly expressed in all patients? for each gene, compare the mean expression value among all patients to the mean among aml patients. for example, the di fference in mean expression levels for gene a is xaml\u0000xall= 45186\u000036103 = 9083 : the differences in means for each gene are shown in figure 1.59. due to the order of subtraction used, genes with a positive di fference value are more highly expressed in aml patients: a, e, f, g, h, and i. genes b, c, d, and j are more highly expressed in all patients. a b c d e f g h i j aml mean 45186 30550 39143 34880 -77 -384 410 771 3430 -264 all mean 36103 30682 42454 36639 -222 -689 -146 -181 2263 474 difference 9083 -132 -3310 -1758 145 304 556 952 1167 -738 figure 1.59: the di fference in mean expression levels by leukemia type for each gene in golub.small . 62 chapter 1. introduction to data the most informative genes for predicting leukemia type are ones for which the di fference in means seems relatively large, compared to the entire distribution of di fferences. figure 1.60 visually displays the distribution of di fferences; the boxplot indicates that there is one large outlier and one small outlier. difference in mean expression (aml−all)frequency −4000−2000 02000400060008000100000123456 (a) difference in mean expression (aml−all)−4000−20000200040006000800010000 (b) figure 1.60: a histogram and boxplot of the di fferences in mean expression level between aml and all in the golub.small data. it is possible to identify the outliers from simply looking at the list of di fferences, since the list is short: genes a and c, with di fferences of 9,083 and -3,310, respectively.58it is important to remember that genes a and c are only outliers out of the specific 10 genes in golub.small , where mean expression has been calculated using data from 10 patients; these genes do not necessarily show outlier levels of expression relative to the complete dataset. with the use of computing software, the same process of calculating means, di fferences of means, and identifying outliers can easily be applied to the complete version of the data. figure 1.61 shows the distribution of di fferences in mean expression level between aml and all patients for all 7,129 genes in the dataset, from 62 patients. the vast majority of genes are expressed at similar levels in aml and all patients; most genes have a di fference in mean expression within -5,000 to 5,000. however, there are many genes that show extreme di fferences, as much as higher by 20,000 in aml or lower by 30,000 in all. these genes may be useful for di fferentiating between aml and all. the corresponding rlab illustrates the details of using rto identify these genes.59 note how figure 1.61 uses data from only 62 patients out of the 72 in the golub dataset; this subset is called golub.train . the remaining 10 patients have been set aside as a \"test\" dataset (golub.test ). based on what has been learned about expression patterns from the 62 patients in golub.train , how well can the leukemia type of the 10 patients in golub.test be predicted?60 58for a numerical approach, calculate the outlier boundaries defined by 1 :5\u0002iqr . 59lab 3, chapter 1. 60the original analysis used data from 38 patients to identify informative genes, then tested predictions on an independent collection of data from 34 patients. 1.7. exploratory data analysis 63 difference in mean expression (aml−all)frequency −10000 −5000 05000 10000 15000 200000100020003000 (a) difference in mean expression (aml−all) −10000−500005000100001500020000 (b) figure 1.61: a histogram and boxplot of the di fferences in mean expression level between aml and all, using information from 7,129 genes and 62 patients in thegolub data ( golub.train ). 64 chapter 1. introduction to data figure 1.62: schematic of the prediction strategy used by the golub team, reproduced with modifications from fig. 1b of the original paper. figure 1.62 illustrates the main ideas behind the strategy developed by the golub team to predict leukemia type from expression data. the vertical orange bars represent the gene expression levels of a patient for each gene, relative to the mean expression for aml patients and all patients from the training dataset (vertical blue bars). a gene will \"vote\" for either aml or all, depending on whether the patient’s expression level is closer to \u0016aml or\u0016all. in the example shown, three of the genes are considered to have all-like expression, versus the other two that are more aml-like. the votes are also weighted to account for how far an observation is from the midpoint between the two means (horizontal dotted blue line), i.e. the length of the dotted line shows the deviation from the midpoint. for example, the observed expression value for gene 2 is not as strong an indicator of all as the expression value for gene 1. the magnitude of the deviations ( v1,v2, ...) are summed to obtain vaml andvall, and a higher value indicates a prediction of either aml or all, respectively. the published analysis chose to use 50 informative genes; a decision about how many genes to use in a diagnostic panel typically involves considering factors such as the number of genes practical for a clinical setting. for simplicity, a smaller number of genes will be used in the analysis shown here. suppose that 10 genes are selected as predictors—the 5 largest outliers and 5 smallest outliers for the di fference in mean expression between aml and all. figure 1.63 shows expression data for these 10 genes from the 10 patients in golub.test , while figure 1.64 contains the mean expression value for each gene among aml and all patients in golub.train . 1.7. exploratory data analysis 65 m19507_at m27891_at m11147_at m96326_rna1_at y00787_s_at m14483_rna1_s_at x82240_rna1_at x58529_at m33680_at u05259_rna1_at 1 4481 47532 56261 1785 -77 7824 -231 9520 7181 2757 2 11513 2839 42469 5018 20831 27407 -1116 -221 6978 -187 3 21294 6439 30239 61951 -187 19692 -540 216 1741 -84 4 -399 26023 40910 1271 26842 30092 -1247 19033 13117 -188 5 -147 29609 37606 20053 12745 26985 -1104 -273 8701 -168 6 -1229 -1206 16932 2250 360 38058 20951 12406 9927 8378 7 -238 -610 21798 -991 -348 23986 6500 20451 8500 7005 8 -1021 -792 17732 730 5102 17893 158 9287 7924 9221 9 432 -1099 9683 -576 -804 14386 7097 5556 9915 5594 10 -518 -862 26386 -2971 -1032 30100 32706 21007 23932 14841 figure 1.63: expression data from the 10 patients in golub.test , for the 10 genes selected as predictors. each row represents a patient; the five right-most columns are the 5 largest outliers and the five left-most columns are the 5 smallest outliers. probe aml mean all mean midpoint m19507_at 20143 322 10232 m27891_at 17395 -262 8567 m11147_at 32554 16318 24436 m96326_rna1_at 16745 830 8787 y00787_s_at 16847 1002 8924 m14483_rna1_s_at 22268 33561 27914 x82240_rna1_at -917 9499 4291 x58529_at 598 10227 5413 m33680_at 4151 13447 8799 u05259_rna1_at 74 8458 4266 table 1.64: mean expression value for each gene among aml patients and all patients in golub.train , and the midpoint between the means. example 1.31 consider the expression data for the patient in the first row of figure 1.63. for each gene, identify whether the expression level is more aml-like or more all-like. for the gene represented by the m19507_at probe, the patient has a recorded expression level of 4,481, which is closer to the all mean of 322 than the aml mean of 20,143. however, for the gene represented by the m27891_at probe, the expression level of 47,532 is closer to the aml mean of 17,395 than the all mean of -262. expression at genes represented by m19507_at, m96326_rna1_at, y00787_s_at, and x58529_at are more all-like than aml-like. all other expression levels are closer to \u0016aml. example 1.32 use the information in figures 1.63 and 1.64 to calculate the magnitude of the deviations v1and v10for the first patient. for the gene represented by the m19507_at probe, the magnitude of the deviation is v1=j4;481\u0000 10;232j= 5;751. for the gene represented by the u05259_rna1_at probe, the magnitude of the deviation is v10= j2;757\u00004;266j= 1;509. 66 chapter 1. introduction to data m19507_at m27891_at m11147_at m96326_rna1_at y00787_s_at m14483_rna1_s_at x82240_rna1_at x58529_at m33680_at u05259_rna1_at 1 5751 38966 31825 7003 9001 20090 4522 4108 1618 1509 2 1281 5727 18032 3769 11906 507 5408 5634 1821 4453 3 11061 2128 5803 53164 9111 8222 4831 5196 7058 4350 4 10632 17457 16474 7516 17918 2178 5538 13621 4318 4454 5 10379 21042 13169 11265 3820 929 5395 5685 98 4434 6 11461 9773 7504 6537 8564 10144 16660 6994 1128 4112 7 10470 9176 2638 9778 9272 3928 2209 15038 300 2739 8 11254 9358 6704 8057 3823 10021 4133 3875 875 4955 9 9800 9666 14754 9363 9728 13529 2806 144 1116 1328 10 10750 9428 1949 11759 9956 2186 28415 15594 15133 10575 table 1.65: the magnitude of deviations from the midpoints. cells for which the expression level is more all-like (closer to \u0016allthan\u0016aml) are highlighted in blue. example 1.33 using the information in figure 1.65, make a prediction for the leukemia status of patient 1. calculate the total weighted votes for each category: vaml = 38;966 + 31;825 + 20;090 + 4;522 + 1;618 + 1;509 = 98;530 vall= 5;571 + 7;003 + 9;001 + 4;108 = 25;863 sincevaml>vall, patient 1 is predicted to have aml. guided practice 1.34 make a prediction for the leukemia status of patient 10.61 figure 1.66 shows the comparison between actual leukemia status and predicted leukemia status based on the described prediction strategy. the prediction matches patient leukemia status for all patients. actual prediction 1 aml aml 2 aml aml 3 aml aml 4 aml aml 5 aml aml 6 allb all 7 allb all 8 allb all 9 allb all 10 allb all figure 1.66: actual leukemia status versus predicted leukemia status for the patients in golub.test the analysis presented here is meant to illustrate how basic statistical concepts such as the definition of an outlier can be leveraged to address a relatively complex scientific question. there are entirely di fferent approaches possible for analyzing these data, and many other considerations that have not been discussed. for example, this method of summing the weighted votes for each gene assumes that each gene is equally informative; the analysis in the published paper incorporates an additional weighting factor when calculating vaml andvallthat accounts for how correlated each gene is with leukemia type. the published analysis also calculates prediction strength based on the values of vaml andvaml in order to provide a measure of how reliable each prediction is. 61sincevaml = 1;949 andvall= 113;796, patient 10 is predicted to have all. 1.7. exploratory data analysis 67 finally, it is important to remember that the golub analysis represented one of the earliest investigations into the use of gene expression data for diagnostic purposes. while the overall logical goals remain the same—identifying informative genes and developing a prediction strategy—the means of accomplishing them have become far more sophisticated. a modern study would have the benefit of referencing established, well-defined techniques for analyzing microarray data. 1.7.3 case study: cold-responsive genes in the plant arabidopsis arenosa in contrast to hybridization-based approaches, rna sequencing (rna-seq) allows for the entire transcriptome to be surveyed in a high-throughput, quantitative manner.62microarrays require gene-specific probes, which limits microarray experiments to detecting transcripts that correspond to known gene sequences. in contrast, rna-seq can still be used when genome sequence information is not available, such as for non-model organisms. rna-seq is an especially powerful tool for researchers interested in studying small-scale genetic variation, such as single nucleotide polymorphisms, which microarrays are not capable of detecting.63compared to microarrays, rna-seq technology o ffers increased sensitivity for detecting genes expressed at either low or very high levels. this section introduces the concepts behind rna-seq technology and discusses a study that used rna-seq to explore the genetic basis of cold response in the plant arabidopsis arenosa . rna sequencing (rna-seq) the first step in an rna-seq experiment is to prepare cdna sequence libraries for each rna sample being sequenced. rna is converted into cdna and sheared into short fragments; sequencing adapters and barcodes are added to each fragment that initiate the sequencing reaction and identify sequences that originate from di fferent samples. once all the cdna fragments are sequenced, the resulting short sequence reads must be re-constructed to produce the transcriptome. at this point, even the simplest rna-seq experiment has generated a relatively large amount of data; the complexity involved in processing and analyzing rna-seq data represents a significant challenge to widespread adoption of rna-seq technology. while a number of programs are available to help researchers process rna-seq data, improving computational methods for working with rna-seq data remains an active area of research. a transcriptome can be assembled from the short sequence reads by either de novo assembly or genome mapping. in de novo assembly, sequencing data are run through computer algorithms that identify overlapping regions in the short sequence reads to gradually piece together longer stretches of continuous sequence. alternatively, the reads can be aligned to a reference genome, a genome sequence which functions as a representative template for a given species; in cases where a species has not been sequenced, the genome of a close relative can also function as a reference genome. by mapping reads against a genome, it is possible to identify the position (and thus, the gene) from which a given rna transcript originated. it is also possible to use a combination of these two strategies, an approach that is especially advantageous when genomes have experienced major rearrangements, such as in the case of cancer cells.64once the transcripts have been assembled, information stored in sequence databases such as those hosted by the national center for biotechnology (ncbi) can be used to identify gene sequences (i.e., annotate the transcripts). 62wang, et al. rna-seq: a revolutionary tool for transcriptomics. nature genetics 2009; 10: 57-63. 63a single nucleotide polymorphism (snp) represents variation at a single position in dna sequence among individuals. 64garber, et al. computational methods for transcriptome annotation and quantification using rna-seq. nature methods 2011; 8: 469-477. 68 chapter 1. introduction to data quantifying gene expression levels from rna-seq data is based on counting the number of sequence reads per gene. if a particular gene is highly expressed, there will be a relatively high number of rna transcripts originating from that gene; thus, the probability that transcripts from this gene are sequenced multiple times is also relatively high, and the gene will have a high number of sequencing reads associated with it. the number of read counts for a given gene provides a measure of gene expression level, when normalized for transcript length. if a short transcript and long transcript are present in equal amounts, the long transcript will have more sequencing reads associated with it due to the fragmentation step in library construction. additional normalization steps are necessary when comparing data between samples to account for factors such as di fferences in the starting amount of rna or the total number of sequencing reads generated (sequencing depth, in the language of genomics). a variety of strategies have been developed to carry out such normalization procedures. cold-responsive genes in a. arenosa arabidopsis arenosa populations exist in di fferent habitats, and exhibit a range of di fferences in flowering time, cold sensitivity, and perenniality. sensitivity to cold is an important trait for perennials, plants that live longer than one year. it is common for perennials to require a period of prolonged cold in order to flower. this mechanism, known as vernalization, allows perennials to synchronize their life cycle with the seasons such that they flower only once winter is over. plant response to low temperatures is under genetic control, and mediated by a specific set of cold-responsive genes. in a recent study, researchers used rna-seq to investigate how cold responsiveness di ffers in two populations of a. arenosa : tbg (collected from triberg, germany) and ka (collected from kasparstein, austria).65tbg grows in and around railway tracks, while ka is found on shaded limestone outcrops in wooded forests. as an annual, tbg has lost the vernalization response and does not required extended cold in order to flower; in the wild, tbg plants usually die before the onset of winter. in contrast, ka is a perennial plant, in which vernalization is known to greatly accelerate the onset of flowering. winter conditions can be simulated by incubating plants at 4\u000ec for several weeks; a plant that has undergone cold treatment is considered vernalized, while plants that have not been exposed to cold treatment are non-vernalized. expression data were collected for 1,088 genes known to be cold-responsive in tbg and ka plants that were either vernalized or non-vernalized. figure 1.67 shows the data collected for the ka plants analyzed in the study, while figure 1.68 shows the tbg expression data. each row corresponds to a gene; the first column indicates gene name, while the rest correspond to expression measured in a plant sample. three individuals of each population were exposed to cold (vernalized, denoted by v), and three were not (nonvernalized, denoted by nv). expression was measured in gene counts (i.e. the number of rna transcripts present in a sample); the data were then normalized between samples to allow for comparisons between gene counts. for example, a value of 288.20 for the pux4 gene in ka nv 1 indicates that in one of the non-vernalized ka individuals, about 288 copies of pux4 were detected. a high number of transcripts indicates a high level of gene expression. as seen by comparing the expression levels across the first rows of figures 1.67 and 1.68, the expression levels of pux4 are higher in vernalized plants than non-vernalized plants. 65baduel p, et al. habitat-associated life history and stress-tolerance variation in arabidopsis arenosa .plant physiology 2016; 171: 437-451. 1.7. exploratory data analysis 69 gene name ka nv 1 ka nv 2 ka nv 3 ka v 1 ka v 2 ka v 3 1 pux4 288.20 322.55 305.35 1429.29 1408.25 1487.08 2 tzp 79.36 93.34 73.44 1203.40 1230.49 1214.03 3 gad2 590.59 492.69 458.02 2639.42 2645.05 2705.32 4 gaut6 86.88 99.25 57.98 586.24 590.03 579.71 5 fb 791.08 912.12 746.94 3430.03 3680.12 3467.06 figure 1.67: five rows and seven columns from the arenosa dataset, showing expression levels in ka plants. gene name tbg nv 1 tbg nv 2 tbg nv 3 tbg v 1 tbg v 2 tbg v 3 1 pux4 365.23 288.13 365.01 601.39 800.64 698.73 2 tzp 493.23 210.27 335.33 939.72 974.36 993.14 3 gad2 1429.14 1339.50 2215.27 1630.77 1500.36 1621.28 4 gaut6 129.63 76.40 135.02 320.57 298.91 399.27 5 fb 1472.35 1120.49 1313.14 3092.37 3230.72 3173.00 figure 1.68: five rows and seven columns from the arenosa dataset, showing expression levels in tbg plants. the three measured individuals in a particular group represent biological replicates, individuals of the same type grown under identical conditions; collecting data from multiple individuals of the same group captures the inherent biological variability between organisms. averaging expression levels across these replicates provides an estimate of the typical expression level in the larger population. figure 1.69 shows the mean expression levels for five genes. gene name ka nv ka v tbg nv tbg v 1 pux4 305.36 1441.54 339.46 700.25 2 tzp 82.05 1215.97 346.28 969.07 3 gad2 513.77 2663.26 1661.30 1584.14 4 gaut6 81.37 585.33 113.68 339.58 5 fb 816.71 3525.74 1301.99 3165.36 figure 1.69: mean gene expression levels of five cold-responsive genes, for nonvernalized and vernalized ka and tbg. figure 1.70(a) plots the mean gene expression levels of all 1,088 genes for each group. the expression levels are heavily right-skewed, with many genes present at unusually high levels relative to other genes. this is an example of a situation in which a transformation can be useful for clarifying the features of a distribution. in figure 1.70(b), it is easier to see that expression levels of vernalized plants are shifted upward relative to non-vernalized plants. additionally, while median expression is slightly higher in non-vernalized tbg than non-vernalized ka, median expression in vernalized ka is higher than in vernalized tbg. vernalization appears to trigger a stronger change in expression of cold-responsive genes in ka plants than in tbg plants. figure 1.70 is only a starting point for exploring how expression of cold-responsive genes differs between ka and tbg plants. consider a gene-level approach, in which the responsiveness of a gene to vernalization is quantified as the ratio of expression in a vernalized sample to expression in a non-vernalized sample. 70 chapter 1. introduction to data expression (number of transcripts) 02000400060008000 ka nv ka v tbg nv tbg v (a) expression (log number of transcripts) −20246810 ka nv ka v tbg nv tbg v (b) figure 1.70: (a) mean gene expression levels for non-vernalized ka, vernalized ka, non-vernalized tbg, and vernalized tbg plants. (b) log-transformed mean gene expression levels. figure 1.71(a) shows responsiveness for five genes, calculated separately between v and nv tbg and v and nv ka, using the means in figure 1.69. the ratios provide a measure of how much expression di ffers between vernalized and non-vernalized individuals. for example, the gene tzp is expressed almost 15 times as much in vernalized ka than it is in non-vernalized ka. in contrast, the genegad 2 is expressed slightly less in vernalized tbg than in non-vernalized tbg. as with the mean gene expression levels, it is useful to apply a log transformation (figure 1.71(b)). on the log scale, values close to 0 are indicative of low responsiveness, while large values in either direction correspond to high responsiveness. figure 1.72 shows the log2-transformed expression ratios as a side-by-side boxplot.66 gene name tbg ka 1 pux4 2.06 4.72 2 tzp 2.80 14.82 3 gad2 0.95 5.18 4 gaut6 2.99 7.19 5 fb 2.43 4.32 (a)gene name tbg ka 1 pux4 1.04 2.24 2 tzp 1.48 3.89 3 gad2 -0.07 2.37 4 gaut6 1.58 2.85 5 fb 1.28 2.11 (b) figure 1.71: (a) ratio of mean expression in vernalized individuals to mean expression in non-vernalized individuals. (b) log2-transformation of expression ratios in figure 1.71(a). 66one gene is omitted because the expression ratio in ka is 0, and the logarithm of 0 is undefined. 1.7. exploratory data analysis 71 log2 expression ratio −10−50510 tbg ka figure 1.72: responsiveness for 1,087 genes in arenosa , calculated as the log2 ratio of vernalized over non-vernalized expression levels. figure 1.72 directly illustrates how the magnitude of response to vernalization in tbg is smaller than in ka. the spread of responsiveness in ka is larger than for tbg, as indicated by the larger iqr and range of values; this indicates that more genes in ka are di fferentially expressed between vernalized and non-vernalized samples. additionally, the median responsiveness in ka is higher than in tbg. there are several outliers for both ka and tbg, with large outliers representing genes that were much more highly expressed in vernalized plants than non-vernalized plants, and vice versa for low outliers. these highly cold-responsive genes likely play a role in how plants cope with colder temperatures; they could be involved in regulating freezing tolerance, or controlling how plants detect cold temperatures. with the help of computing software, it is a simple matter to identify the outliers and address questions such as whether particular genes are highly vernalizationresponsive in both ka and tbg. 72 chapter 1. introduction to data advanced data visualization there are many ways to numerically and graphically summarize data that are not explicitly introduced in this chapter. presentation-style graphics in published manuscripts can be especially complex, and may feature techniques specific to a certain field as well as novel approaches designed to highlight particular features of a dataset. this section discusses the figures generated by the baduel, et al. research team to visualize the di fferences in vernalization response between ka and tbg a. arenosa plants. figure 1.73: figure 4 from the original manuscript. plot a compares mean expression levels between non-vernalized ka and tbg; plot b compares mean expression levels between vernalized ka and tbg. each dot in figure 1.73 represents a gene; each gene is plotted by its mean expression level in ka against its mean expression level in tbg. the overall trend can be summarized by a line fit to the points.67for the slope of the line to equal 1, each gene would have to be equally expressed in ka and tbg. in the upper plot, the slope of the line is less than 1, which indicates that for non-vernalized plants, cold-responsive genes have a higher expression in tbg than in ka. in the lower plot, the slope is greater than 1, indicating that the trend is reversed in vernalized plants: cold-responsive genes are more highly expressed in ka. this trend is also discernible from the sideby-side boxplot in figure 1.70. using a scatterplot, however, makes it possible to directly compare expression in ka versus tbg on a gene-by-gene basis, and also locate particular genes of interest that are known from previous research (e.g., the labeled genes in figure 1.73.)68the colors in the plot signify plot density, with warmer colors representing a higher concentration of points. 67lines of best fit are discussed in chapter 6. 68only a subset of the 1,088 genes are plotted in figure 1.73. 1.7. exploratory data analysis 73 figure 1.74: figure 3 from the original manuscript. each gene is plotted based on the values of the log2 expression ratio in ka versus tbg. figure 1.74, like figure 1.72, compares the cold-responsiveness in ka versus tbg, calculating responsiveness as the log2 ratio of vernalized over non-vernalized expression levels. as in figure 1.73, each dot represents a single gene. the slope of the best fitting line is greater than 1, indicating that the assayed genes typically show greater responsiveness in ka than in tbg.69 while presentation-style graphics may use relatively sophisticated approaches to displaying data that seem far removed from the simple plots discussed in this chapter, the end goal remains the same – to e ffectively highlight key features of data. 69these 608 genes are a subset of the ones plotted in figure 1.73; genes with expression ratio 0 are not included. 74 chapter 1. introduction to data 1.8 notes introductory treatments of statistics often emphasize the value of formal methods of probability and inference, topics which are covered in the remaining chapters of this text. however, numerical and graphical summaries are essential for understanding the features of a dataset and should be applied before the process of inference begins. it is inadvisable to begin conducting tests or constructing models without a careful understanding of the strengths and weaknesses of a dataset. for example, are some measurements out of range, or the result of errors in data recording? the tools of descriptive statistics form the basis of exploratory data analysis; having the intuition for exploring and interpreting data in the context of a research question is an essential statistical skill. with computing software, it is a relatively simple matter to produce numerical and graphical summaries, even with large datasets. the challenge lies instead in understanding how to wade through a dataset, disentangle complex relationships between variables, and piece together the underlying story. it is important to note that the graphical methods illustrated in the text are relatively simple, static graphs that, for instance, do not show changes dynamically over time. they will be surprisingly useful in the later chapters. but there has been considerable progress in the visual display of data in the last decade, and many wonderful displays exist that show complex, time dependent data. for examples of sophisticated graphical displays, we especially recommend the bubble charts available at the gapminder web site ( https://www.gapminder.org ) that show international trends in public health outcomes and the graphical displays of data in the upshot section of the new york times ( https://www.nytimes.com/section/upshot ). there are four labs associated with chapter 1. the first lab introduces basic commands for working with data in r, and shows how to produce the graphical and numerical summaries discussed in this chapter. the exercises in lab 1 rely heavily on the introduction to rand r studio in lab 00 (getting started). the lab notes corresponding to lab 1 provide a systematic introduction torfunctions useful for getting started with applied data analysis. the remaining three labs explore the data presented in the case studies in section 1.7, outlining analyses driven by questions similar to what one might encounter in practice. does the state of california discriminate in its distribution of funds for developmental disability support (lab 2)? are particular genes associated with a subtype of pediatric leukemia (lab 3)? is there a genetic basis to the cold weather response in the plant arabidopsis arenosa (lab 4)? labs 3 and 4 demonstrate how computing is essential for data analysis; even though the two datasets are relatively small by modern standards, they are already too large to feasibly analyze without statistical computing software. all three labs illustrate how important questions can be examined even with relatively simple statistical concepts. 1.9. exercises 75 1.9 exercises 1.9.1 case study 1.1 migraine and acupuncture, part i. a migraine is a particularly painful type of headache, which patients sometimes wish to treat with acupuncture. to determine whether acupuncture relieves migraine pain, researchers conducted a randomized controlled study where 89 females diagnosed with migraine headaches were randomly assigned to one of two groups: treatment or control. 43 patients in the treatment group received acupuncture that is specifically designed to treat migraines. 46 patients in the control group received placebo acupuncture (needle insertion at non-acupoint locations). 24 hours after patients received acupuncture, they were asked if they were pain free. results are summarized in the contingency table below.70 pain free yes no total treatment 10 33 43groupcontrol 2 44 46 total 12 77 89 identified on the antero-internal part of the antitragus, the anterior part of the lobe and the upper auricular concha, on the same side of pain. the majority of these points were effective very rapidly (within 1 min), while the remaining points produced a slower antalgic response, between 2 and 5 min. the insertion of a semi-permanent needle in these zones allowed stable control of the migraine pain, which occurred within 30 min and still persisted 24 h later. since the most active site in controlling migraine pain was the antero-internal part of the antitragus, the aim of this study was to verify the therapeutic value of this elective area (appropriate point) and to compare it with an area of the ear (representing the sciatic nerve) which is probably inappropriate in terms of giving a therapeutic effect on migraine attacks, since it has no somatotopic correlation with head pain. materials and methods the study enrolled 94 females, diagnosed as migraine without aura following the international classification of headache disorders [ 5], who were subsequently examined at the women’s headache centre, department of gynaecology and obstetrics of turin university. they were all included in the study during a migraine attack provided that it started no more than 4 h previously. according to a predetermined computer-made randomization list, the eligible patients were randomly and blindly assigned to the following two groups: group a ( n=46) (average age 35.93 years, range 15–60), group b ( n=48) (average age 33.2 years, range 16–58). before enrollment, each patient was asked to give an informed consent to participation in the study. migraine intensity was measured by means of a vas before applying nct (t0). in group a, a specific algometer exerting a maximum pressure of 250 g (sedatelec, france) was chosen to identify the tender points with pain–pressure test (ppt). every tender point located within the identified area by the pilot study (fig. 1, area m) was tested with nct for 10 s starting from the auricle, that was ipsilateral, to the side of prevalent cephalic pain. if the test was positive and the reduction was at least 25% in respect to basis, a semipermanent needle (asp sedatelec, france) was inserted after 1 min. on the contrary, if pain did not lessen after 1 min, a further tender point was challenged in the same area and so on. when patients became aware of an initial decrease in the pain in all the zones of the head affected, they were invited to use a specific diary card to score the intensity of the pain with a vas at the following intervals: after 10 min (t1), after 30 min (t2), after 60 min (t3), after 120 min (t4), and after 24 h (t5).in group b, the lower branch of the anthelix was repeatedly tested with the algometer for about 30 s to ensure it was not sensitive. on both the french and chinese auricular maps, this area corresponds to the representation of the sciatic nerve (fig. 1, area s) and is specifically used to treat sciatic pain. four needles were inserted in this area, two for each ear. in all patients, the ear acupuncture was always performed by an experienced acupuncturist. the analysis of the diaries collecting vas data was conducted by an impartial operator who did not know the group each patient was in. the average values of vas in group a and b were calculated at the different times of the study, and a statistical evaluation of the differences between the values obtained in t0, t1, t2, t3 and t4 in the two groups studied was performed using an analysis of variance (anova) for repeated measures followed by multiple ttest of bonferroni to identify the source of variance. moreover, to evaluate the difference between group b and group a, a ttest for unpaired data was always performed for each level of the variable ‘‘time’’. in the case of proportions, a chi square test was applied. all analyses were performed using the statistical package for the social sciences (spss) software program. all values given in the following text are reported as arithmetic mean ( ±sem). results only 89 patients out of the entire group of 94 (43 in group a, 46 in group b) completed the experiment. four patients withdrew from the study, because they experienced an unbearable exacerbation of pain in the period preceding the last control at 24 h (two from group a and two from group b) and were excluded from the statistical analysis since they requested the removal of the needles. one patient from group a did not give her consent to the implant of the semi-permanent needles. in group a, the mean number of fig. 1 the appropriate area (m) versus the inappropriate area ( s) used in the treatment of migraine attackss174 neurol sci (2011) 32 (suppl 1):s173–s175 123figure from the original paper displaying the appropriate area (m) versus the inappropriate area (s) used in the treatment of migraine attacks. (a) what percent of patients in the treatment group were pain free 24 hours after receiving acupuncture? (b) what percent were pain free in the control group? (c) in which group did a higher percent of patients become pain free 24 hours after receiving acupuncture? (d) your findings so far might suggest that acupuncture is an e ffective treatment for migraines for all people who su ffer from migraines. however this is not the only possible conclusion that can be drawn based on your findings so far. what is one other possible explanation for the observed di fference between the percentages of patients that are pain free 24 hours after receiving acupuncture in the two groups? 1.2 sinusitis and antibiotics, part i. researchers studying the e ffect of antibiotic treatment for acute sinusitis compared to symptomatic treatments randomly assigned 166 adults diagnosed with acute sinusitis to one of two groups: treatment or control. study participants received either a 10-day course of amoxicillin (an antibiotic) or a placebo similar in appearance and taste. the placebo consisted of symptomatic treatments such as acetaminophen, nasal decongestants, etc. at the end of the 10-day period, patients were asked if they experienced improvement in symptoms. the distribution of responses is summarized below.71 self-reported improvement in symptoms yes no total treatment 66 19 85groupcontrol 65 16 81 total 131 35 166 (a) what percent of patients in the treatment group experienced improvement in symptoms? (b) what percent experienced improvement in symptoms in the control group? (c) in which group did a higher percentage of patients experience improvement in symptoms? (d) your findings so far might suggest a real di fference in e ffectiveness of antibiotic and placebo treatments for improving symptoms of sinusitis. however, this is not the only possible conclusion that can be drawn based on your findings so far. what is one other possible explanation for the observed di fference between the percentages of patients in the antibiotic and placebo treatment groups that experience improvement in symptoms of sinusitis? 70g. allais et al. “ear acupuncture in the treatment of migraine attacks: a randomized trial on the e fficacy of appropriate versus inappropriate acupoints”. in: neurological sci. 32.1 (2011), pp. 173–175. 71j.m. garbutt et al. “amoxicillin for acute rhinosinusitis: a randomized controlled trial”. in: jama: the journal of the american medical association 307.7 (2012), pp. 685–692. 76 chapter 1. introduction to data 1.9.2 data basics 1.3 air pollution and birth outcomes, study components. researchers collected data to examine the relationship between air pollutants and preterm births in southern california. during the study air pollution levels were measured by air quality monitoring stations. specifically, levels of carbon monoxide were recorded in parts per million, nitrogen dioxide and ozone in parts per hundred million, and coarse particulate matter (pm 10) in\u0016g=m3. length of gestation data were collected on 143,196 births between the years 1989 and 1993, and air pollution exposure during gestation was calculated for each birth. the analysis suggested that increased ambient pm 10and, to a lesser degree, co concentrations may be associated with the occurrence of preterm births.72 (a) identify the main research question of the study. (b) who are the subjects in this study, and how many are included? (c) what are the variables in the study? identify each variable as numerical or categorical. if numerical, state whether the variable is discrete or continuous. if categorical, state whether the variable is ordinal. 1.4 buteyko method, study components. the buteyko method is a shallow breathing technique developed by konstantin buteyko, a russian doctor, in 1952. anecdotal evidence suggests that the buteyko method can reduce asthma symptoms and improve quality of life. in a scientific study to determine the e ffectiveness of this method, researchers recruited 600 asthma patients aged 18-69 who relied on medication for asthma treatment. these patients were randomly split into two research groups: one practiced the buteyko method and the other did not. patients were scored on quality of life, activity, asthma symptoms, and medication reduction on a scale from 0 to 10. on average, the participants in the buteyko group experienced a significant reduction in asthma symptoms and an improvement in quality of life.73 (a) identify the main research question of the study. (b) who are the subjects in this study, and how many are included? (c) what are the variables in the study? identify each variable as numerical or categorical. if numerical, state whether the variable is discrete or continuous. if categorical, state whether the variable is ordinal. 1.5 cheaters, study components. researchers studying the relationship between honesty, age and selfcontrol conducted an experiment on 160 children between the ages of 5 and 15. participants reported their age, sex, and whether they were an only child or not. the researchers asked each child to toss a fair coin in private and to record the outcome (white or black) on a paper sheet, and said they would only reward children who report white. the study’s findings can be summarized as follows: “half the students were explicitly told not to cheat and the others were not given any explicit instructions. in the no instruction group probability of cheating was found to be uniform across groups based on child’s characteristics. in the group that was explicitly told to not cheat, girls were less likely to cheat, and while rate of cheating didn’t vary by age for boys, it decreased with age for girls.”74 (a) identify the main research question of the study. (b) who are the subjects in this study, and how many are included? (c) how many variables were recorded for each subject in the study in order to conclude these findings? state the variables and their types. 72b. ritz et al. “e ffect of air pollution on preterm birth among children born in southern california between 1989 and 1993”. in: epidemiology 11.5 (2000), pp. 502–511. 73j. mcgowan. “health education: does the buteyko institute method make a di fference?” in: thorax 58 (2003). 74alessandro bucciol and marco piovesan. “luck or cheating? a field experiment on honesty with children”. in: journal of economic psychology 32.1 (2011), pp. 73–78. 1.9. exercises 77 1.6 hummingbird taste behavior, study components. researchers hypothesized that a particular taste receptor in hummingbirds, t1r1-t1r3, played a primary role in dictating taste behavior; specifically, in determining which compounds hummingbirds detect as sweet. in a series of field tests, hummingbirds were presented simultaneously with two filled containers, one containing test stimuli and a second containing sucrose. the test stimuli included aspartame, erythritol, water, and sucrose. aspartame is an artificial sweetener that tastes sweet to humans, but is not detected by hummingbird t1r1-t1r3 , while erythritol is an artificial sweetener known to activate t1r1-t1r3. data were collected on how long a hummingbird drank from a particular container for a given trial, measured in seconds. for example, in one field test comparing aspartame and sucrose, a hummingbird drank from the aspartame container for 0.54 seconds and from the sucrose container for 3.21 seconds. (a) which tests are controls? which tests are treatments? (b) identify the response variable(s) in the study. are they numerical or categorical? (c) describe the main research question. 1.7 egg coloration. the evolutionary significance of variation in egg coloration among birds is not fully understood. one hypothesis suggests that egg coloration may be an indication of female quality, with healthier females being capable of depositing blue-green pigment into eggshells instead of using it for themselves as an antioxidant. in a study conducted on 32 collared flycatchers, half of the females were given supplementary diets before and during egg laying. eggs were measured for darkness of blue color using spectrophotometry; for example, the mean amount of blue-green chroma was 0.594 absorbance units. egg mass was also recorded. (a) identify the control and treatment groups. (b) describe the main research question. (c) identify the primary response variable of interest, and whether it is numerical or categorical. 1.8 smoking habits of uk residents. a survey was conducted to study the smoking habits of uk residents. below is a data matrix displaying a portion of the data collected in this survey. note that “£\" stands for british pounds sterling, “cig\" stands for cigarettes, and “n/a” refers to a missing component of the data.75 sex age marital grossincome smoke amtweekends amtweekdays 1 female 42 single under £2,600 yes 12 cig/day 12 cig/day 2 male 44 single £10,400 to £15,600 no n/a n/a 3 male 53 married above £36,400 yes 6 cig/day 6 cig/day :::::::::::::::::::::::: 1691 male 40 single £2,600 to £5,200 yes 8 cig/day 8 cig/day (a) what does each row of the data matrix represent? (b) how many participants were included in the survey? (c) for each variable, indicate whether it is numerical or categorical. if numerical, identify the variable as continuous or discrete. if categorical, indicate if the variable is ordinal. 75national stem centre, large datasets from stats4schools. 78 chapter 1. introduction to data 1.9 the microbiome and colon cancer. a study was conducted to assess whether the abundance of particular bacterial species in the gastrointestinal system is associated with the development of colon cancer. the following data matrix shows a subset of the data collected in the study. cancer stage is coded 1-4, with larger values indicating cancer that is more di fficult to treat. the abundance levels are given for five bacterial species; abundance is calculated as the frequency of that species divided by the total number of bacteria from all species. age gender stage bug 1 bug 2 bug 3 bug 4 bug 5 1 71 female 2 0.03 0.09 0.52 0.00 0.00 2 53 female 4 0.16 0.08 0.08 0.00 0.00 3 55 female 2 0.00 0.01 0.31 0.00 0.00 4 44 male 2 0.11 0.14 0.00 0.07 0.05 ::::::::::::::::::::::::::: 73 48 female 3 0.21 0.05 0.00 0.00 0.04 (a) what does each row of the data matrix represent? (b) identify explanatory and response variables. (c) for each variable, indicate whether it is numerical or categorical. 1.9.3 data collection principles 1.10 cheaters, scope of inference. exercise 1.5 introduces a study where researchers studying the relationship between honesty, age, and self-control conducted an experiment on 160 children between the ages of 5 and 15. the researchers asked each child to toss a fair coin in private and to record the outcome (white or black) on a paper sheet, and said they would only reward children who report white. half the students were explicitly told not to cheat and the others were not given any explicit instructions. di fferences were observed in the cheating rates in the instruction and no instruction groups, as well as some di fferences across children’s characteristics within each group. (a) identify the population of interest and the sample in this study. (b) comment on whether or not the results of the study can be generalized to the population, and if the findings of the study can be used to establish causal relationships. 1.11 air pollution and birth outcomes, scope of inference. exercise 1.3 introduces a study where researchers collected data to examine the relationship between air pollutants and preterm births in southern california. during the study, air pollution levels were measured by air quality monitoring stations. length of gestation data were collected on 143,196 births between the years 1989 and 1993, and air pollution exposure during gestation was calculated for each birth. it can be assumed that the 143,196 births are e ffectively the entire population of births during this time period. (a) identify the population of interest and the sample in this study. (b) comment on whether or not the results of the study can be generalized to the population, and if the findings of the study can be used to establish causal relationships. 1.12 herbal remedies. echinacea has been widely used as an herbal remedy for the common cold, but previous studies evaluating its e fficacy as a remedy have produced conflicting results. in a new study, researchers randomly assigned 437 volunteers to receive either a placebo or echinacea treatment before being infected with rhinovirus. healthy young adult volunteers were recruited for the study from the university of virginia community. (a) identify the population of interest and the sample in this study. (b) comment on whether or not the results of the study can be generalized to a larger population. (c) can the findings of the study be used to establish causal relationships? justify your answer. 1.9. exercises 79 1.13 buteyko method, scope of inference. exercise 1.4 introduces a study on using the buteyko shallow breathing technique to reduce asthma symptoms and improve quality of life. as part of this study 600 asthma patients aged 18-69 who relied on medication for asthma treatment were recruited and randomly assigned to two groups: one practiced the buteyko method and the other did not. those in the buteyko group experienced, on average, a significant reduction in asthma symptoms and an improvement in quality of life. (a) identify the population of interest and the sample in this study. (b) comment on whether or not the results of the study can be generalized to the population, and if the findings of the study can be used to establish causal relationships. 1.14 vitamin supplements. in order to assess the e ffectiveness of taking large doses of vitamin c in reducing the duration of the common cold, researchers recruited 400 healthy volunteers from sta ffand students at a university. a quarter of the patients were randomly assigned a placebo, and the rest were randomly allocated between 1g vitamin c, 3g vitamin c, or 3g vitamin c plus additives to be taken at onset of a cold for the following two days. all tablets had identical appearance and packaging. no significant di fferences were observed in any measure of cold duration or severity between the four medication groups, and the placebo group had the shortest duration of symptoms.76 (a) was this an experiment or an observational study? why? (b) what are the explanatory and response variables in this study? (c) participants are ultimately able to choose whether or not to use the pills prescribed to them. we might expect that not all of them will adhere and take their pills. does this introduce a confounding variable to the study? explain your reasoning. 1.15 chicks and antioxidants. environmental factors early in life can have long-lasting e ffects on an organism. in one study, researchers examined whether dietary supplementation with vitamins c and e influences body mass and corticosterone level in yellow-legged gull chicks. chicks were randomly assigned to either the nonsupplemented group or the vitamin supplement experimental group. the initial study group consisted of 108 nests, with 3 eggs per nest. chicks were assessed at age 7 days. (a) what type of study is this? (b) what are the experimental and control treatments in this study? (c) explain why randomization is an important feature of this experiment. 1.16 exercise and mental health. a researcher is interested in the e ffects of exercise on mental health and he proposes the following study: use stratified random sampling to recruit 18-30, 31-40 and 41-55 year olds from the population. next, randomly assign half the subjects from each age group to exercise twice a week, and instruct the rest not to exercise. conduct a mental health exam at the beginning and at the end of the study, and compare the results. (a) what type of study is this? (b) what are the treatment and control groups in this study? (c) does this study make use of blocking? if so, what is the blocking variable? (d) comment on whether or not the results of the study can be used to establish a causal relationship between exercise and mental health, and indicate whether or not the conclusions can be generalized to the population at large. (e) suppose you are given the task of determining if this proposed study should get funding. would you have any reservations about the study proposal? 76c. audera et al. “mega-dose vitamin c in treatment of the common cold: a randomised controlled trial”. in: medical journal of australia 175.7 (2001), pp. 359–362. 80 chapter 1. introduction to data 1.17 internet use and life expectancy. the following scatterplot was created as part of a study evaluating the relationship between estimated life expectancy at birth (as of 2014) and percentage of internet users (as of 2009) in 208 countries for which such data were available.77 (a) describe the relationship between life expectancy and percentage of internet users. (b) what type of study is this? (c) state a possible confounding variable that might explain this relationship and describe its potential e ffect. 0204060801005060708090 % internet userslife expectancy at birth 1.18 stressed out. a study that surveyed a random sample of otherwise healthy high school students found that they are more likely to get muscle cramps when they are stressed. the study also noted that students drink more co ffee and sleep less when they are stressed. (a) what type of study is this? (b) can this study be used to conclude a causal relationship between increased stress and muscle cramps? (c) state possible confounding variables that might explain the observed relationship between increased stress and muscle cramps. 1.19 evaluate sampling methods. a university wants to assess how many hours of sleep students are getting per night. for each proposed method below, discuss whether the method is reasonable or not. (a) survey a simple random sample of 500 students. (b) stratify students by their field of study, then sample 10% of students from each stratum. (c) cluster students by their class year (e.g. freshmen in one cluster, sophomores in one cluster, etc.), then randomly sample three clusters and survey all students in those clusters. 1.20 city council survey. a city council has requested a household survey be conducted in a suburban area of their city. the area is broken into many distinct and unique neighborhoods, some including large homes, some with only apartments, and others a diverse mixture of housing structures. identify the sampling methods described below, and comment on whether or not you think they would be e ffective in this setting. (a) randomly sample 50 households from the city. (b) divide the city into neighborhoods, and sample 20 households from each neighborhood. (c) divide the city into neighborhoods, randomly sample 10 neighborhoods, and sample all households from those neighborhoods. (d) divide the city into neighborhoods, randomly sample 10 neighborhoods, and then randomly sample 20 households from those neighborhoods. (e) sample the 200 households closest to the city council o ffices. 77cia factbook, country comparisons, 2014. 1.9. exercises 81 1.21 flawed reasoning. identify the flaw(s) in reasoning in the following scenarios. explain what the individuals in the study should have done di fferently if they wanted to make such conclusions. (a) students at an elementary school are given a questionnaire that they are asked to return after their parents have completed it. one of the questions asked is, “do you find that your work schedule makes it di fficult for you to spend time with your kids after school?\" of the parents who replied, 85% said “no\". based on these results, the school o fficials conclude that a great majority of the parents have no di fficulty spending time with their kids after school. (b) a survey is conducted on a simple random sample of 1,000 women who recently gave birth, asking them about whether or not they smoked during pregnancy. a follow-up survey asking if the children have respiratory problems is conducted 3 years later, however, only 567 of these women are reached at the same address. the researcher reports that these 567 women are representative of all mothers. (c) an orthopedist administers a questionnaire to 30 of his patients who do not have any joint problems and finds that 20 of them regularly go running. he concludes that running decreases the risk of joint problems. 1.22 reading the paper. below are excerpts from two articles published in the ny times : (a) an article titled risks: smokers found more prone to dementia states the following:78 “researchers analyzed data from 23,123 health plan members who participated in a voluntary exam and health behavior survey from 1978 to 1985, when they were 50-60 years old. 23 years later, about 25% of the group had dementia, including 1,136 with alzheimer’s disease and 416 with vascular dementia. after adjusting for other factors, the researchers concluded that pack-a-day smokers were 37% more likely than nonsmokers to develop dementia, and the risks went up with increased smoking; 44% for one to two packs a day; and twice the risk for more than two packs.\" based on this study, can it be concluded that smoking causes dementia later in life? explain your reasoning. (b) another article titled the school bully is sleepy states the following:79 “the university of michigan study, collected survey data from parents on each child’s sleep habits and asked both parents and teachers to assess behavioral concerns. about a third of the students studied were identified by parents or teachers as having problems with disruptive behavior or bullying. the researchers found that children who had behavioral issues and those who were identified as bullies were twice as likely to have shown symptoms of sleep disorders.\" a friend of yours who read the article says, “the study shows that sleep disorders lead to bullying in school children.\" is this statement justified? if not, how best can you describe the conclusion that can be drawn from this study? 1.23 alcohol consumption and stis. an observational study published last year in the american journal of preventive medicine investigated the e ffects of an increased alcohol sales tax in maryland on the rates of gonorrhea and chlamydia.80after a tax increase from 6% to 9% in 2011, the statewide gonorrhea rate declined by 24%, the equivalent of 1,600 cases per year. in a statement to the new york times , the lead author of the paper was quoted saying, \"policy makers should consider raising liquor taxes if they’re looking for ways to prevent sexually transmitted infections. in the year and a half following the alcohol tax rise in maryland, this prevented 2,400 cases of gonorrhea and saved half a million dollars in health care costs.\" explain whether the lead author’s statement is accurate. 78r.c. rabin. “risks: smokers found more prone to dementia”. in: new york times (2010). 79t. parker-pope. “the school bully is sleepy”. in: new york times (2011). 80s. staras, et al., 2015. maryland alcohol sales tax and sexually transmitted infections. the american journal of preventive medicine 50: e73-e80. 82 chapter 1. introduction to data 1.24 income and education in us counties. the scatterplot below shows the relationship between per capita income (in thousands of dollars) and percent of population with a bachelor’s degree in 3,143 counties in the us in 2010. (a) what are the explanatory and response variables? (b) describe the relationship between the two variables. make sure to discuss unusual observations, if any. (c) can we conclude that having a bachelor’s degree increases one’s income? percent with bachelor's degreeper capita income (in $1,000) 10 30 50 70204060 1.25 eat better, feel better. in a public health study on the e ffects of consumption of fruits and vegetables on psychological well-being in young adults, participants were randomly assigned to three groups: (1) diet-asusual, (2) an ecological momentary intervention involving text message reminders to increase their fruits and vegetable consumption plus a voucher to purchase them, or (3) a fruit and vegetable intervention in which participants were given two additional daily servings of fresh fruits and vegetables to consume on top of their normal diet. participants were asked to take a nightly survey on their smartphones. participants were student volunteers at the university of otago, new zealand. at the end of the 14-day study, only participants in the third group showed improvements to their psychological well-being across the 14-days relative to the other groups.81 (a) what type of study is this? (b) identify the explanatory and response variables. (c) comment on whether the results of the study can be generalized to the population. (d) comment on whether the results of the study can be used to establish causal relationships. (e) a newspaper article reporting on the study states, “the results of this study provide proof that giving young adults fresh fruits and vegetables to eat can have psychological benefits, even over a brief period of time.” how would you suggest revising this statement so that it can be supported by the study? 1.9.4 numerical data 1.26 means and sds. for each part, compare distributions (1) and (2) based on their means and standard deviations. you do not need to calculate these statistics; simply state how the means and the standard deviations compare. make sure to explain your reasoning. hint: it may be useful to sketch dot plots of the distributions. (a) (1) 3, 5, 5, 5, 8, 11, 11, 11, 13 (2) 3, 5, 5, 5, 8, 11, 11, 11, 20 (b) (1) -20, 0, 0, 0, 15, 25, 30, 30 (2) -40, 0, 0, 0, 15, 25, 30, 30(c) (1) 0, 2, 4, 6, 8, 10 (2) 20, 22, 24, 26, 28, 30 (d) (1) 100, 200, 300, 400, 500 (2) 0, 50, 300, 550, 600 81tamlin s conner et al. “let them eat fruit! the e ffect of fruit and vegetable consumption on psychological well-being in young adults: a randomized controlled trial”. in: plos one 12.2 (2017), e0171206. 1.9. exercises 83 1.27 medians and iqrs. for each part, compare distributions (1) and (2) based on their medians and iqrs. you do not need to calculate these statistics; simply state how the medians and iqrs compare. make sure to explain your reasoning. (a) (1) 3, 5, 6, 7, 9 (2) 3, 5, 6, 7, 20 (b) (1) 3, 5, 6, 7, 9 (2) 3, 5, 8, 7, 9(c) (1) 1, 2, 3, 4, 5 (2) 6, 7, 8, 9, 10 (d) (1) 0, 10, 50, 60, 100 (2) 0, 100, 500, 600, 1000 1.28 mix-and-match. describe the distribution in the histograms below and match them to the box plots. (a)506070 (b)050100 (c)0246 (1)0246 (2)55606570 (3)020406080100 1.29 air quality. daily air quality is measured by the air quality index (aqi) reported by the environmental protection agency. this index reports the pollution level and what associated health e ffects might be a concern. the index is calculated for five major air pollutants regulated by the clean air act and takes values from 0 to 300, where a higher value indicates lower air quality. aqi was reported for a sample of 91 days in 2011 in durham, nc. the relative frequency histogram below shows the distribution of the aqi values on these days.82 (a) based on the histogram, describe the distribution of daily aqi. (b) estimate the median aqi value of this sample. (c) would you expect the mean aqi value of this sample to be higher or lower than the median? explain your reasoning. daily aqi10203040506000.050.10.150.2 82us environmental protection agency, airdata, 2011. 84 chapter 1. introduction to data 1.30 nursing home residents. since states with larger numbers of elderly residents would naturally have more nursing home residents, the number of nursing home residents in a state is often adjusted for the number of people 65 years of age or order (65+). that adjustment is usually given as the number of nursing home residents age 65+ per 1,000 members of the population age 65+. for example, a hypothetical state with 200 nursing home residents age 65+ and 50,000 people age 65+ would have the same adjusted number of residents as a state with 400 residents and a total age 65+ population of 100,000 residents: 4 residents per 1,000. use the two plots below to answer the following questions. both plots show the distribution of the number of nursing home residents per 1,000 members of the population 65+ (in each state). adjusted number of residents102030405060708002468101214 203040506070 (a) is the distribution of adjusted number of nursing home residents symmetric or skewed? are there any states that could be considered outliers? (b) which plot is more informative: the histogram or the boxplot? explain your answer. (c) what factors might influence the substantial amount of variability among di fferent states? this question cannot be answered from the data; speculate using what you know about the demographics of the united states. 1.31 income at the coffee shop. the first histogram below shows the distribution of the yearly incomes of 40 patrons at a college co ffee shop. suppose two new people walk into the co ffee shop: one making $225,000 and the other $250,000. the second histogram shows the new income distribution. summary statistics are also provided. (1)60000 62500 65000 67500 70000 (2)60000 110000 160000 210000 260000 (1) (2) n 40 42 min. 60,680 60,680 1st qu. 63,620 63,710 median 65,240 65,350 mean 65,090 73,300 3rd qu. 66,160 66,540 max. 69,890 250,000 sd 2,122 37,321 (a) would the mean or the median best represent what we might think of as a typical income for the 42 patrons at this co ffee shop? what does this say about the robustness of the two measures? (b) would the standard deviation or the iqr best represent the amount of variability in the incomes of the 42 patrons at this co ffee shop? what does this say about the robustness of the two measures? 1.32 midrange. the midrange of a distribution is defined as the average of the maximum and the minimum of that distribution. is this statistic robust to outliers and extreme skew? explain your reasoning. 1.9. exercises 85 1.9.5 categorical data 1.33 flossing habits. suppose that an anonymous questionnaire is given to patients at a dentist’s o ffice once they arrive for an appointment. one of the questions asks \"how often do you floss?\", and four answer options are provided: a) at least twice a day, b) at least once a day, c) a few times a week, and d) a few times a month. at the end of a week, the answers are tabulated: 31 individuals chose answer a), 55 chose b), 39 chose c), and 12 chose d). (a) describe how these data could be numerically and graphically summarized. (b) assess whether the results of this survey can be generalized to provide information about flossing habits in the general population. 1.34 views on immigration. 910 randomly sampled registered voters from tampa, fl were asked if they thought workers who have illegally entered the us should be (i) allowed to keep their jobs and apply for us citizenship, (ii) allowed to keep their jobs as temporary guest workers but not allowed to apply for us citizenship, or (iii) lose their jobs and have to leave the country. the results of the survey by political ideology are shown below.83 political ideology conservative moderate liberal total (i) apply for citizenship 57 120 101 278 (ii) guest worker 121 113 28 262response(iii) leave the country 179 126 45 350 (iv) not sure 15 4 1 20 total 372 363 175 910 (a) what percent of these tampa, fl voters identify themselves as conservatives? (b) what percent of these tampa, fl voters are in favor of the citizenship option? (c) what percent of these tampa, fl voters identify themselves as conservatives and are in favor of the citizenship option? (d) what percent of these tampa, fl voters who identify themselves as conservatives are also in favor of the citizenship option? what percent of moderates share this view? what percent of liberals share this view? 1.9.6 relationships between two variables 1.35 mammal life spans. data were collected on life spans (in years) and gestation lengths (in days) for 62 mammals. a scatterplot of life span versus length of gestation is shown below.84 (a) does there seem to be an association between length of gestation and life span? if so, what type of association? explain your reasoning. (b) what type of an association would you expect to see if the axes of the plot were reversed, i.e. if we plotted length of gestation versus life span? ● ●●● ● ●● ●● ●●● ●● ●●● ● ●●● ● ●●● ● ● ●●● ●●●●● ●● ●● ● ● ●●● ●●●● ● ●● ●● ●● gestation (days)life span (years) 0 200 400 6000255075100 83surveyusa, news poll #18927, data collected jan 27-29, 2012. 84t. allison and d.v. cicchetti. “sleep in mammals: ecological and constitutional correlates”. in: arch. hydrobiol 75 (1975), p. 442. 86 chapter 1. introduction to data 1.36 associations. indicate which of the plots show a (a) positive association (b) negative association (c) no association also determine if the positive and negative associations are linear or nonlinear. each part may refer to more than one plot. ●●● ●● ●●● ● ● ●●●●● ●●● ● ●●●● ●● ● ●●● ● ● ●●●● ●● ●● ●●●● ●●● ●● ● ●●● ●●● ● ●● ●● ● ●●● ●● ● ●● ●●● ● ●● ●● ●● ●● ●● ● ●● ● ●● ● ● ● ●● ●●●●● ●● (1)● ●●●● ●●●● ● ●● ●●● ●● ● ●●●● ●●●●●●● ●● ●● ●● ●●●● ● ● ●●● ●● ●● ● ●●● ●● ●● ●●●● ●●● ●● ●● ● ●●●●● ●● ●● ●● ● ●●● ● ●● ● ●●●● ●●●● ● ● ●●●● (2) ●● ●●● ●●●● ●● ●●●● ●●● ●●●●●●●●●● ●●●●●●●● ●●●● ●●●●●●●● ●●●●● ●●●● ●●●●●●●●●●●●● ●●●●● ●●●●●●●●●●●●● ●●●●●● ●●●●●●● (3)● ●● ●● ●● ●●●●●● ●● ●●● ● ●●●● ●● ●●●● ● ●●● ● ●● ●● ●● ●●●●●● ● ●●● ●●● ●● ● ●● ● ●●● ●●● ●● ● ●● ● ●● ● ● ●● ● ●●●● ●● ●●●● ● ●●●● ●● ●●●● ●● (4) 1.37 adolescent fertility. data are available on the number of children born to women aged 15-19 from 189 countries in the world for the years 1997, 2000, 2002, 2005, and 2006. the data are defined using a scaling similar to that used for the nursing home data in exercise 1.30. the values for the annual adolescent fertility rates represent the number of live births among women aged 15-19 per 1,000 female members of the population of that age. fertility rate (live births per 1000 women)050100150200250 19972000200220052006 (a) in 2006, the standard deviation of the distribution of adolescent fertility is 75.73. write a sentence explaining the 75thpercentile in the context of this data. (b) for the years 2000-2006, data are not available for iraq. why might those observations be missing? would the five-number summary have been a ffected very much if the values had been available? (c) from the side-by-side boxplots shown above, describe how the distribution of fertility rates changes over time. is there a trend? 1.9. exercises 87 1.38 smoking and stenosis. researchers collected data from an observational study to investigate the association between smoking status and the presence of aortic stenosis, a narrowing of the aorta that impedes blood flow to the body. smoking status non-smoker smoker total absent 67 43 110disease statuspresent 54 51 105 total 121 94 215 (a) what percentage of the 215 participants were both smokers and had aortic stenosis? this percentage is one component of the joint distribution of smoking and stenosis; what are the other three numbers of the joint distribution? (b) among the smokers, what proportion have aortic stenosis? this number is a component of the conditional distribution of stenosis for the two categories of smokers. what proportion of non-smokers have aortic stenosis? (c) in this context, relative risk is the ratio of the proportion of smokers with stenosis to the proportion of non-smokers with stenosis. relative risks greater than 1 indicate that smokers are at a higher risk for aortic stenosis than non-smokers; relative risks of 1.2 or higher are generally considered cause for alarm. calculate the relative risk for the 215 participants, comparing smokers to non-smokers. does there seem to be evidence that smoking is associated with an increased probability of stenosis? 1.39 anger and cardiovascular health. trait anger is defined as a relatively stable personality trait that is manifested in the frequency, intensity, and duration of feelings associated with anger. people with high trait anger have rage and fury more often, more intensely, and with long-laster episodes than people with low trait anger. it is thought that people with high trait anger might be particularly susceptible to coronary heart disease; 12,986 participants were recruited for a study examining this hypothesis. participants were followed for five years. the following table shows data for the participants identified as having normal blood pressure (normotensives). trait anger score low moderate high total yes 53 110 27 190chd eventno 3057 4704 606 8284 total 3110 4731 633 8474 (a) what percentage of participants have moderate anger scores? (b) what percentage of individuals who experienced a chd event have moderate anger scores? (c) what percentage of participants with high trait anger scores experienced a chd event (i.e., heart attack)? (d) what percentage of participants with low trait anger scores experienced a chd event? (e) are individuals with high trait anger more likely to experience a chd event than individuals with low trait anger? calculate the relative risk of a chd event for individuals with high trait anger compared to low trait anger. (f) researchers also collected data on various participant traits, such as level of blood cholesterol (measured in mg/dl). what graphical summary might be useful for examining how blood cholesterol level di ffers between anger groups? 1.9.7 exploratory data analysis since exploratory data analysis relies heavily on the use of computation, refer to the labs for exercises related to this section, which are free and may be found at openintro.org/book/biostat . 88 chapter 2 probability 2.1 defining probability 2.2 conditional probability 2.3 extended example 2.4 notes 2.5 exercises 89 what are the chances that a woman with an abnormal mammogram has breast cancer? what is the probability that a woman with an abnormal mammogram has breast cancer, given that she is in her 40’s? what is the likelihood that out of 100 women who undergo a mammogram and test positive for breast cancer, at least one of the women has received a false positive result? these questions use the language of probability to express statements about outcomes that may or may not occur. more specifically, probability is used to quantify the level of uncertainty about each outcome. like all mathematical tools, probability becomes easier to understand and work with once important concepts and terminology have been formalized. this chapter introduces that formalization, using two types of examples. one set of examples uses settings familiar to most people – rolling dice or picking cards from a deck. the other set of examples draws from medicine, biology, and public health, reflecting the contexts and language specific to those fields. the approaches to solving these two types of problems are surprisingly similar, and in both cases, seemingly di fficult problems can be solved in a series of reliable steps. for labs, slides, and other resources, please visit www.openintro.org/book/biostat 90 chapter 2. probability 2.1 defining probability 2.1.1 some examples the rules of probability can easily be modeled with classic scenarios, such as flipping coins or rolling dice. when a coin is flipped, there are only two possible outcomes, heads or tails. with a fair coin, each outcome is equally likely; thus, the chance of flipping heads is 1/2, and likewise for tails. the following examples deal with rolling a die or multiple dice; a die is a cube with six faces numbered 1,2,3,4,5, and 6. example 2.1 what is the chance of getting 1when rolling a die? if the die is fair, then there must be an equal chance of rolling a 1as any other possible number. since there are six outcomes, the chance must be 1-in-6 or, equivalently, 1 =6. example 2.2 what is the chance of not rolling a 2? not rolling a 2is the same as getting a 1,3,4,5, or6, which makes up five of the six equally likely outcomes and has probability 5 =6. example 2.3 consider rolling two fair dice. what is the chance of getting two 1s? if 1=6thof the time the first die is a 1and 1=6thofthose times the second die is also a 1, then the chance that both dice are 1is (1=6)(1=6) or 1=36. probability can also be used to model less artificial contexts, such as to predict the inheritance of genetic disease. cystic fibrosis (cf) is a life-threatening genetic disorder caused by mutations in the cftr gene located on chromosome 7. defective copies of cftr can result in the reduced quantity and function of the cftr protein, which leads to the buildup of thick mucus in the lungs and pancreas.1cf is an autosomal recessive disorder; an individual only develops cf if they have inherited two a ffected copies of cftr . individuals with one normal (wild-type) copy and one defective (mutated) copy are known as carriers; they do not develop cf, but may pass the diseasecausing mutation onto their o ffspring. 1the cftr protein is responsible for transporting sodium and chloride ions across cell membranes. 2.1. defining probability 91 example 2.4 suppose that both members of a couple are cf carriers. what is the probability that a child of this couple will be a ffected by cf? assume that a parent has an equal chance of passing either gene copy (i.e., allele) to a child. solution 1: enumerate all of the possible outcomes and exploit the fact that the outcomes are equally likely, as in example 2.1. figure 2.1 shows the four possible genotypes for a child of these parents. the paternal chromosome is in blue and the maternal chromosome in green, while chromosomes with the wild-type and mutated versions of cftr are marked with + and \u0000, respectively. the child is only a ffected if they have genotype ( \u0000/\u0000), with two mutated copies of cftr . each of the four outcomes occurs with equal likelihood, so the child will be a ffected with probability 1-in-4, or 1 =4. it is important to recognize that the child being an una ffected carrier (+/\u0000) consists of two distinct outcomes, not one. solution 2: calculate the proportion of outcomes that produce an a ffected child, as in example 2.3. during reproduction, one parent will pass along an a ffected copy half of the time. when the child receives an a ffected allele from one parent, half of the those times, they will also receive an a ffected allele from the other parent. thus, the proportion of times the child will have two a ffected copies is (1=2)\u0002(1=2) = 1=4. figure 2.1: pattern of cf inheritance for a child of two una ffected carriers 92 chapter 2. probability guided practice 2.5 suppose the father has cf and the mother is an una ffected carrier. what is the probability that their child will be a ffected by the disease?2 2.1.2 probability probability is used to assign a level of uncertainty to the outcomes of phenomena that either happen randomly (e.g. rolling dice, inheriting of disease alleles), or appear random because of a lack of understanding about exactly how the phenomenon occurs (e.g. a woman in her 40’s developing breast cancer). modeling these complex phenomena as random can be useful, and in either case, the interpretation of probability is the same: the chance that some event will occur. mathematicians and philosophers have struggled for centuries to arrive at a clear statement of how probability is defined, or what it means. the most common definition is used in this text. probability the probability of an outcome is the proportion of times the outcome would occur if the random phenomenon could be observed an infinite number of times. this definition of probability can be illustrated by simulation. suppose a die is rolled many times. let ˆpnbe the proportion of outcomes that are 1after the first nrolls. as the number of rolls increases, ˆpnwill converge to the probability of rolling a 1,p= 1=6. figure 2.2 shows this convergence for 100,000 die rolls. the tendency of ˆpnto stabilize around pis described by the law of large numbers . the behavior shown in figure 2.2 matches most people’s intuition about probability, but proving mathematically that the behavior is always true is surprisingly di fficult and beyond the level of this text. n (number of rolls)1 10 100 1,000 10,000 100,0000.00.10.20.3 p^ n figure 2.2: the fraction of die rolls that are 1at each stage in a simulation. the proportion tends to get closer to the probability 1 =6\u00190:167 as the number of rolls increases. occasionally the proportion veers o fffrom the probability and appear to defy the law of large numbers, as ˆpndoes many times in figure 2.2. however, the likelihood of these large deviations becomes smaller as the number of rolls increases. 2since the father has cf, he must have two a ffected copies; he will always pass along a defective copy of the gene. since the mother will pass along a defective copy half of the time, the child will be a ffected half of the time, or with probability (1)\u0002(1=2) = 1=2. 2.1. defining probability 93 law of large numbers as more observations are collected, the proportion ˆpnof occurrences with a particular outcome converges to the probability pof that outcome. probability is defined as a proportion, and it always takes values between 0 and 1 (inclusively). it may also be expressed as a percentage between 0% and 100%. the probability of rolling a 1,p, can also be written as p(rolling a 1). this notation can be further abbreviated. for instance, if it is clear that the process is “rolling a die”,p(rolling a 1) can be written as p(1). there also exists a notation for an event itself; the event aof rolling a 1 can be written as a=frolling a 1g, with associated probability p(a).p(a) probability of outcomea 2.1.3 disjoint or mutually exclusive outcomes two outcomes are disjoint ormutually exclusive if they cannot both happen at the same time. when rolling a die, the outcomes 1and 2are disjoint since they cannot both occur. however, the outcomes 1and “rolling an odd number” are not disjoint since both occur if the outcome of the roll is a 1.3 what is the probability of rolling a 1or a 2? when rolling a die, the outcomes 1and 2are disjoint. the probability that one of these outcomes will occur is computed by adding their separate probabilities: p(1or2) =p(1) +p(2) = 1=6 + 1=6 = 1=3: what about the probability of rolling a 1,2,3,4,5, or6? here again, all of the outcomes are disjoint, so add the individual probabilities: p(1or2or3or4or5or6) =p(1) +p(2) +p(3) +p(4) +p(5) +p(6) = 1=6 + 1=6 + 1=6 + 1=6 + 1=6 + 1=6 = 1: addition rule of disjoint outcomes ifa1anda2represent two disjoint outcomes, then the probability that either one of them occurs is given by p(a1ora2) =p(a1) +p(a2): if there are kdisjoint outcomes a1, ...,ak, then the probability that either one of these outcomes will occur is p(a1) +p(a2) +\u0001\u0001\u0001+p(ak): (2.6) 3the terms disjoint and mutually exclusive are equivalent and interchangeable. 94 chapter 2. probability guided practice 2.7 consider the cf example. is the event that two carriers of cf have a child that is also a carrier represented by mutually exclusive outcomes? calculate the probability of this event.4 probability problems often deal with setsorcollections of outcomes. let arepresent the event in which a die roll results in 1or2andbrepresent the event that the die roll is a 4or a 6. we write aas the set of outcomes f1,2gandb=f4,6g. these sets are commonly called events . becausea andbhave no elements in common, they are disjoint events. aandbare represented in figure 2.3. figure 2.3: three events, a,b, andd, consist of outcomes from rolling a die. a andbare disjoint since they do not have any outcomes in common. the addition rule applies to both disjoint outcomes and disjoint events. the probability that one of the disjoint events aorboccurs is the sum of the separate probabilities: p(aorb) =p(a) +p(b) = 1=3 + 1=3 = 2=3: guided practice 2.8 (a) verify the probability of event a,p(a), is 1=3 using the addition rule. (b) do the same for eventb.5 guided practice 2.9 (a) using figure 2.3 as a reference, which outcomes are represented by event d? (b) are events b andddisjoint? (c) are events aandddisjoint?6 4yes, there are two mutually exclusive outcomes for which a child of two carriers can also be a carrier - a child can either receive an a ffected copy of cftr from the mother and a normal copy from the father, or vice versa (since each parent can only contribute one allele). thus, the probability that a child will be a carrier is 1/4 + 1/4 = 1/2. 5(a)p(a) =p(1or2) =p(1) +p(2) =1 6+1 6=2 6=1 3. (b) similarly, p(b) = 1=3. 6(a) outcomes 2and 3. (b) yes, events banddare disjoint because they share no outcomes. (c) the events aandd share an outcome in common, 2, and so are not disjoint. 2.1. defining probability 95 guided practice 2.10 in guided practice 2.9, you confirmed banddfrom figure 2.3 are disjoint. compute the probability that event bor eventdoccurs.7 2.1.4 probabilities when events are not disjoint venn diagrams are useful when outcomes can be categorized as “in” or “out” for two or three variables, attributes, or random processes. the venn diagram in figure 2.5 uses one oval to represent diamonds and another to represent face cards (the cards labeled jacks, queens, and kings); if a card is both a diamond and a face card, it falls into the intersection of the ovals. 2|3|4|5|6|7|8|9|10|j|q|k|a| 2}3}4}5}6}7}8}9}10}j}q}k}a} 2~3~4~5~6~7~8~9~10~j~q~k~a~ 2345678910jqka figure 2.4: a regular deck of 52 cards is split into four suits: |(club),}(diamond), ~(heart),(spade). each suit has 13 labeled cards: 2,3, ...,10,j(jack), q(queen), k(king), and a(ace). thus, each card is a unique combination of a suit and a label, e.g.4~and j|. figure 2.5: a venn diagram for diamonds and face cards. guided practice 2.11 (a) what is the probability that a randomly selected card is a diamond? (b) what is the probability that a randomly selected card is a face card?8 7sincebanddare disjoint events, use the addition rule: p(bord) =p(b) +p(d) =1 3+1 3=2 3. 8(a) there are 52 cards and 13 diamonds. if the cards are thoroughly shu ffled, each card has an equal chance of being drawn, so the probability that a randomly selected card is a diamond is p(}) =13 52= 0:250. (b) likewise, there are 12 face cards, sop(face card) =12 52=3 13= 0:231. 96 chapter 2. probability letarepresent the event that a randomly selected card is a diamond and brepresent the event that it is a face card. events aandbare not disjoint – the cards j},q}, and k}fall into both categories. as a result, adding the probabilities of the two events together is not su fficient to calculate p(aorb): p(a) +p(b) =p(}) +p(face card) = 12 =52 + 13=52: instead, a small modification is necessary. the three cards that are in both events were counted twice. to correct the double counting, subtract the probability that both events occur: p(aorb) =p(face card or}) =p(face card) + p(})\u0000p(face card and}) (2.12) = 13=52 + 12=52\u00003=52 = 22=52 = 11=26: equation (2.12) is an example of the general addition rule . general addition rule ifaandbare any two events, disjoint or not, then the probability that at least one of them will occur is p(aorb) =p(a) +p(b)\u0000p(aandb); (2.13) wherep(aandb) is the probability that both events occur. note that in the language of statistics, \"or\" is inclusive such that aorboccurs means a,b, or bothaandboccur. guided practice 2.14 (a) ifaandbare disjoint, describe why this implies p(aandb) = 0. (b) using part (a), verify that the general addition rule simplifies to the addition rule for disjoint events if aandbare disjoint.9 guided practice 2.15 human immunodeficiency virus (hiv) and tuberculosis (tb) a ffect substantial proportions of the population in certain areas of the developing world. individuals sometimes are co-infected (i.e., have both diseases). children of hiv-infected mothers may have hiv and tb can spread from one family member to another. in a mother-child pair, let a=fthe mother has hiv g, b=fthe mother has tb g,c=fthe child has hiv g,d=fthe child has tbg. write out the definitions of the events aorb,aandb,aandc,aord.10 9(a) ifaandbare disjoint, aandbcan never occur simultaneously. (b) if aandbare disjoint, then the last term of equation (2.13) is 0 (see part (a)) and we are left with the addition rule for disjoint events. 10eventsaorb: the mother has hiv, the mother has tb, or the mother has both hiv and tb. events aandb: the mother has both hiv and tb. events aandc: the mother has hiv and the child has hiv. aord: the mother has hiv, the child has tb, or the mother has hiv and the child has tb. 2.1. defining probability 97 2.1.5 probability distributions aprobability distribution consists of all disjoint outcomes and their associated probabilities. figure 2.6 shows the probability distribution for the sum of two dice. dice sum 2 3 4 5 6 7 8 9 10 11 12 probability1 362 363 364 365 366 365 364 363 362 361 36 figure 2.6: probability distribution for the sum of two dice. rules for a probability distribution a probability distribution is a list of all possible outcomes and their associated probabilities that satisfies three rules: 1. the outcomes listed must be disjoint. 2. each probability must be between 0 and 1. 3. the probabilities must total to 1. dice sum234567891011120.000.050.100.15probability figure 2.7: the probability distribution of the sum of two dice. probability distributions can be summarized in a bar plot. the probability distribution for the sum of two dice is shown in figure 2.7, with the bar heights representing the probabilities of outcomes. figure 2.8 shows a bar plot of the birth weight data for 3,999,386 live births in the united states in 2010, for which total counts have been converted to proportions. since birth weight trends do not change much between years, it is valid to consider the plot as a representation of the probability distribution of birth weights for upcoming years, such as 2017. the data are available as part of the us cdc national vital statistics system.11 the graph shows that while most babies born weighed between 2000 and 5000 grams (2 to 5 kg), there were both small (less than 1000 grams) and large (greater than 5000 grams) babies. pediatricians consider birth weights between 2.5 and 5 kg as normal.12a probability distribution gives a sense of which outcomes can be considered unusual (i.e., outcomes with low probability). 11http://205.207.175.93/vitalstats/reportfolders/reportfolders.aspx 12https://www.nlm.nih.gov/medlineplus/birthweight.html 98 chapter 2. probability 499 or less 500 − 999 1000 − 1499 1500 − 1999 2000 − 2499 2500 − 2999 3000 − 3499 3500 − 3999 4000 − 4499 4500 − 4999 5000 − 8165 not stated birth weight (grams)probability 0.00.10.20.30.4 figure 2.8: distribution of birth weights (in grams) of babies born in the us in 2010. continuous probability distributions probability distributions for events that take on a finite number of possible outcomes, such as the sum of two dice rolls, are referred to as discrete probability distributions . consider how the probability distribution for adult heights in the us might best be represented. unlike the sum of two dice rolls, height can occupy any value over a continuous range. thus, height has a continuous probability distribution , which is specified by a probability density function rather than a table; figure 2.9 shows a histogram of the height for 3 million us adults from the mid-1990’s, with an overlaid density curve.13 just as in the discrete case, the probabilities of all possible outcomes must still sum to 1; the total area under a probability density function equals 1. height (cm)140 160 180 200 figure 2.9: the continuous probability distribution of heights for us adults. 13this sample can be considered a simple random sample from the us population. it relies on the usda food commodity intake database. 2.1. defining probability 99 example 2.16 estimate the probability that a randomly selected adult from the us population has height between 180 and 185 centimeters. in figure 2.10(a), the two bins between 180 and 185 centimeters have counts of 195,307 and 156,239 people. find the proportion of the histogram’s area that falls in the range 180cm and 185: add the heights of the bins in the range and divide by the sample size: 195;307 + 156;239 3,000,000= 0:1172: the probability can be calculated precisely with the use of computing software, by finding the area of the shaded region under the curve between 180and 185: p(height between 180and 185) = area between 180and 185= 0:1157: height (cm)140 160 180 200 (a) height (cm)140 160 180 200 (b) figure 2.10: (a) a histogram with bin sizes of 2.5 cm, with bars between 180and 185cm shaded. (b) density for heights in the us adult population with the area between 180and 185cm shaded. example 2.17 what is the probability that a randomly selected person is exactly 180cm? assume that height can be measured perfectly. this probability is zero. a person might be close to 180cm, but not exactly 180cm tall. this also coheres with the definition of probability as an area under the density curve; there is no area captured between 180cm and 180cm. guided practice 2.18 suppose a person’s height is rounded to the nearest centimeter. is there a chance that a random person’s measured height will be 180cm?14 14this has positive probability. anyone between 179.5 cm and 180.5 cm will have a measured height of 180cm. this a more realistic scenario to encounter in practice versus example 2.17. 100 chapter 2. probability 2.1.6 complement of an event rolling a die produces a value in the set f1,2,3,4,5,6g. this set of all possible outcomes is called the sample space (s) for rolling a die.s sample space letd=f2,3grepresent the event that the outcome of a die roll is 2or3. the complement ofac complement of outcome adrepresents all outcomes in the sample space that are not in d, which is denoted by dc=f1,4,5, 6g. that is,dcis the set of all possible outcomes not already included in d. figure 2.11 shows the relationship between d,dc, and the sample space s. figure 2.11: event d=f2,3gand its complement, dc=f1,4,5,6g.srepresents the sample space, which is the set of all possible events. guided practice 2.19 (a) compute p(dc) =p(rolling a 1,4,5, or6). (b) what is p(d) +p(dc)?15 guided practice 2.20 eventsa=f1,2gandb=f4,6gare shown in figure 2.3 on . (a) write out what acandbc represent. (b) compute p(ac) andp(bc). (c) compute p(a) +p(ac) andp(b) +p(bc).16 a complement of an event ais constructed to have two very important properties: every possible outcome not in ais inac, andaandacare disjoint. if every possible outcome not in ais inac, this implies that p(aorac) = 1: (2.21) then, by addition rule for disjoint events, p(aorac) =p(a) +p(ac): (2.22) combining equations (2.21) and (2.22) yields a useful relationship between the probability of an event and its complement. 15(a) the outcomes are disjoint and each has probability 1 =6, so the total probability is 4 =6 = 2=3. (b) we can also see that p(d) =1 6+1 6= 1=3. sincedanddcare disjoint, p(d) +p(dc) = 1. 16brief solutions: (a) ac=f3,4,5,6gandbc=f1,2,3,5g. (b) noting that each outcome is disjoint, add the individual outcome probabilities to get p(ac) = 2=3 andp(bc) = 2=3. (c)aandacare disjoint, and the same is true of bandbc. therefore,p(a) +p(ac) = 1 andp(b) +p(bc) = 1. 2.1. defining probability 101 complement the complement of event ais denotedac, andacrepresents all outcomes not in a.aandac are mathematically related: p(a) +p(ac) = 1;i.e.p(a) = 1\u0000p(ac): (2.23) in simple examples, computing either aoracis feasible in a few steps. however, as problems grow in complexity, using the relationship between an event and its complement can be a useful strategy. guided practice 2.24 letarepresent the event of selecting an adult from the us population with height between 180 and 185 cm, as calculated in example 2.16. what is p(ac)?17 guided practice 2.25 letarepresent the event in which two dice are rolled and their total is less than 12. (a) what does the eventacrepresent? (b) determine p(ac) from figure 2.6 on . (c) determine p(a).18 guided practice 2.26 consider again the probabilities from figure 2.6 and rolling two dice. find the following probabilities: (a) the sum of the dice is not6. (b) the sum is at least 4. that is, determine the probability of the eventb=f4,5, ...,12g. (c) the sum is no more than 10. that is, determine the probability of the eventd=f2,3, ...,10g.19 2.1.7 independence just as variables and observations can be independent, random phenomena can also be independent. two processes are independent if knowing the outcome of one provides no information about the outcome of the other. for instance, flipping a coin and rolling a die are two independent processes – knowing that the coin lands heads up does not help determine the outcome of the die roll. on the other hand, stock prices usually move up or down together, so they are not independent. 17p(ac) = 1\u0000p(a) = 1\u00000:1157 = 0:8843. 18(a) the complement of a: when the total is equal to 12. (b)p(ac) = 1=36. (c) use the probability of the complement from part (b), p(ac) = 1=36, and equation (2.23): p(less than 12) = 1\u0000p(12) = 1\u00001=36 = 35=36. 19(a) first find p(6) = 5=36, then use the complement: p(not 6) = 1\u0000p(6) = 31=36. (b) first find the complement, which requires much less e ffort:p(2or3) = 1=36 + 2=36 = 1=12. then calculate p(b) = 1\u0000p(bc) = 1\u00001=12 = 11=12. (c) as before, finding the complement is the more direct way to determine p(d). first find p(dc) =p(11or12) = 2=36 + 1=36 = 1=12. then calculate p(d) = 1\u0000p(dc) = 11=12. 102 chapter 2. probability example 2.3 provides a basic example of two independent processes: rolling two dice. what is the probability that both will be 1? suppose one of the dice is blue and the other green. if the outcome of the blue die is a 1, it provides no information about the outcome of the green die. this question was first encountered in example 2.3: 1 =6thof the time the blue die is a 1, and 1=6thof those times the green die will also be 1. this is illustrated in figure 2.12. because the rolls are independent, the probabilities of the corresponding outcomes can be multiplied to obtain the final answer: (1=6)(1=6) = 1=36. this can be generalized to many independent processes. figure 2.12: 1 =6thof the time, the first roll is a 1. then 1=6thofthose times, the second roll will also be a 1. complicated probability problems, such as those that arise in biology or medicine, are often solved with the simple ideas used in the dice example. for instance, independence was used implicitly in the second solution to example 2.4, when calculating the probability that two carriers will have an a ffected child with cystic fibrosis. genes are typically passed along from the mother and father independently. this allows for the assumption that, on average, half of the o ffspring who receive a mutated gene copy from the mother will also receive a mutated copy from the father. guided practice 2.27 what if there were also a red die independent of the other two? what is the probability of rolling the three dice and getting all 1s?20 guided practice 2.28 three us adults are randomly selected. the probability the height of a single adult is between 180 and 185 cm is 0.1157.21 (a) what is the probability that all three are between 180 and 185 cm tall? (b) what is the probability that none are between 180 and 185 cm tall? 20the same logic applies from example 2.3. if 1 =36thof the time the blue and green dice are both 1, then 1=6thofthose times the red die will also be 1, so multiply: p(blue =1andgreen =1andred=1) =p(blue =1)p(green =1)p(red=1) = (1=6)(1=6)(1=6) = 1=216: 21brief answers: (a) 0 :1157\u00020:1157\u00020:1157 = 0:0015. (b) (1\u00000:1157)3= 0:692. 2.1. defining probability 103 multiplication rule for independent processes ifaandbrepresent events from two di fferent and independent processes, then the probability that bothaandboccur is given by: p(aandb) =p(a)p(b): (2.29) similarly, if there are keventsa1, ...,akfromkindependent processes, then the probability they all occur is p(a1)p(a2)\u0001\u0001\u0001p(ak): example 2.30 mandatory drug testing. mandatory drug testing in the workplace is common practice for certain professions, such as air tra ffic controllers and transportation workers. a false positive in a drug screening test occurs when the test incorrectly indicates that a screened person is an illegal drug user. suppose a mandatory drug test has a false positive rate of 1.2% (i.e., has probability 0.012 of indicating that an employee is using illegal drugs when that is not the case). given 150 employees who are in reality drug free, what is the probability that at least one will (falsely) test positive? assume that the outcome of one drug test has no e ffect on the others. first, note that the complement of at least 1 person testing positive is that no one tests positive (i.e., all employees test negative). the multiplication rule can then be used to calculate the probability of 150 negative tests. p(at least 1 \"+\") = p(1 or 2 or 3 . . . or 150 are \"+\") = 1\u0000p(none are \"+\") = 1\u0000p(150 are \"-\") = 1\u0000p(\"-\")150 = 1\u0000(0:988)150= 1\u00000:16 = 0:84: even when using a test with a small probability of a false positive, the company is more than 80% likely to incorrectly claim at least one employee is an illegal drug user! guided practice 2.31 because of the high likelihood of at least one false positive in company wide drug screening programs, an individual with a positive test is almost always re-tested with a di fferent screening test: one that is more expensive than the first, but has a lower false positive probability. suppose the second test has a false positive rate of 0.8%. what is the probability that an employee who is not using illegal drugs will test positive on both tests?22 22the outcomes of the two tests are independent of one another; p(aandb) =p(a)\u0002p(b), where events aandbare the results of the two tests. the probability of a false positive with the first test is 0.012 and 0.008 with the second. thus, the probability of an employee who is not using illegal drugs testing positive on both tests is 0 :012\u00020:008 = 9:6\u000210\u00005 104 chapter 2. probability figure 2.13: inheritance of abo blood groups. example 2.32 abo blood groups. there are four di fferent common blood types (a, b, ab, and o), which are determined by the presence of certain antigens located on cell surfaces. antigens are substances used by the immune system to recognize self versus non-self; if the immune system encounters antigens not normally found on the body’s own cells, it will attack the foreign cells. when patients receive blood transfusions, it is critical that the antigens of transfused cells match those of the patient’s, or else an immune system response will be triggered. the abo blood group system consists of four di fferent blood groups, which describe whether an individual’s red blood cells carry the a antigen, b antigen, both, or neither. the abo gene has three alleles:ia,ib, and i. the iallele is recessive to both iaandib, and does not produce antigens; thus, an individual with genotype iaiis blood group a and an individual with genotype ibiis blood group b. the iaandiballeles are codominant, such that individuals of iaibgenotype are ab. individuals homozygous for the iallele are known as blood group o, with neither a nor b antigens. suppose that both members of a couple have group ab blood. a) what is the probability that a child of this couple will have group a blood? b) what is the probability that they have two children with group a blood? a) an individual with group ab blood is genotype iaib. twoiaibparents can produce children with genotypes iaib,iaia, oribib. of these possibilities, only children with genotype iaia have group a blood. each parent has 0.5 probability of passing down their iaallele. thus, the probability that a child of this couple will have group a blood is p(parent 1 passes down ia allele)\u0002p(parent 2 passes down iaallele) = 0:5\u00020:5 = 0:25. b) inheritance of alleles is independent between children. thus, the probability of two children having group a blood equals p(child 1 has group a blood) \u0002p(child 2 has group a blood). the probability of a child of this couple having group a blood was previously calculated as 0.25. the answer is given by 0 :25\u00020:25 = 0:0625. 2.1. defining probability 105 the previous examples in this section have used independence to solve probability problems. the definition of independence can also be used to check whether two events are independent – two eventsaandbare independent if they satisfy equation (2.29). example 2.33 is the event of drawing a heart from a deck of cards independent of drawing an ace? the probability the card is a heart is 1 =4 (13=52 = 1=4) and the probability that it is an ace is 1=13 (4=52 = 1=13). the probability that the card is the ace of hearts ( a~) is 1=52. check whether equation 2.29 is satisfied: p(~)p(a) =\u00121 4\u0013\u00121 13\u0013 =1 52=p(~and a): since the equation holds, the event that the card is a heart and the event that the card is an ace are independent events. example 2.34 in the general population, about 15% of adults between 25 and 40 years of age are hypertensive. suppose that among males of this age, hypertension occurs about 18% of the time. is hypertension independent of sex? assume that the population is 50% male, 50% female; it is given in the problem that hypertension occurs about 15% of the time in adults between ages 25 and 40. p(hypertension)\u0002p(male) = (0:15)(0:50) = 0:075,0:18: equation 2.29 is not satisfied, therefore hypertension is not independent of sex. in other words, knowing whether an individual is male or female is informative as to whether they are hypertensive. if hypertension and sex were independent, then we would expect hypertension to occur at an equal rate in males as in females. 106 chapter 2. probability 2.2 conditional probability while it is di fficult to obtain precise estimates, the us cdc estimated that in 2012, approximately 29.1 million americans had type 2 diabetes – about 9.3% of the population.23a health care practitioner seeing a new patient would expect a 9.3% chance that the patient might have diabetes. however, this is only the case if nothing is known about the patient. the prevalence of type 2 diabetes varies with age. between the ages of 20 and 44, only about 4% of the population have diabetes, but almost 27% of people age 65 and older have the disease. knowing the age of a patient provides information about the chance of diabetes; age and diabetes status are not independent. while the probability of diabetes in a randomly chosen member of the population is 0.093, the conditional probability of diabetes in a person known to be 65 or older is 0.27. conditional probability is used to characterize how the probability of an outcome varies with the knowledge of another factor or condition, and is closely related to the concepts of marginal and joint probabilities. 2.2.1 marginal and joint probabilities figures 2.14 and 2.15 provide additional information about the relationship between diabetes prevalence and age.24figure 2.14 is a contingency table for the entire us population in 2012; the values in the table are in thousands (to make the table more readable). diabetes no diabetes sum less than 20 years 200 86,664 86,864 20 to 44 years 4,300 98,724 103,024 45 to 64 years 13,400 68,526 81,926 greater than 64 years 11,200 30,306 41,506 sum 29,100 284,220 313,320 figure 2.14: contingency table showing type 2 diabetes status and age group, in thousands. in the first row, for instance, figure 2.14 shows that in the entire population of approximately 313,320,000 people, approximately 200,000 individuals were in the less than 20 years age group and diagnosed with diabetes – about 0.1%. the table also indicates that among the approximately 86,864,000 individuals less than 20 years of age, only 200,000 su ffered from type 2 diabetes, approximately 0.2%. the distinction between these two statements is small but important. the first provides information about the size of the group with type 2 diabetes population that is less than 20 years of age, relative to the entire population. in contrast, the second statement is about the size of the diabetes population within the less than 20 years of age group, relative to the size of that age group. 2321 million of these cases are diagnosed, while the cdc predicts that 8.1 million cases are undiagnosed; that is, approximately 8.1 million people are living with diabetes, but they (and their physicians) are unaware that they have the condition. 24because the cdc provides only approximate numbers for diabetes prevalence, the numbers in the table are approximations of actual population counts. 2.2. conditional probability 107 guided practice 2.35 what fraction of the us population are 45 to 64 years of age and have diabetes? what fraction of the population age 45 to 64 have diabetes?25 the entries in figure 2.15 show the proportions of the population in each of the eight categories defined by diabetes status and age, obtained by dividing each value in the cells of figure 2.14 by the total population size. diabetes no diabetes sum less than 20 years 0.001 0.277 0.277 20 to 44 years 0.014 0.315 0.329 45 to 64 years 0.043 0.219 0.261 greater than 64 years 0.036 0.097 0.132 sum 0.093 0.907 1.000 figure 2.15: probability table summarizing diabetes status and age group. if these proportions are interpreted as probabilities for randomly chosen individuals from the population, the value 0.014 in the first column of the second row implies that the probability of selecting someone at random who has diabetes and whose age is between 20 and 44 is 0.014, or 1.4%. the entries in the eight main table cells (i.e., excluding the values in the margins) are joint probabilities , which specify the probability of two events happening at the same time – in this case, diabetes and a particular age group. in probability notation, this joint probability can be expressed as 0 :014 =p(diabetes and age 20 to 44).26 the values in the last row and column of the table are the sums of the corresponding rows or columns. the sum of the of the probabilities of the disjoint events (diabetes, age 20 to 44) and (no diabetes, age 20 to 44), 0.329, is the probability of being in the age group 20 to 44. the row and column sums are marginal probabilities ; they are probabilities about only one type of event, such as age. for example, the sum of the first column (0.093) is the marginal probability of a member of the population having diabetes. marginal and joint probabilities amarginal probability is a probability only related to a single event or process, such as p(a). ajoint probability is the probability that two or more events or processes occur jointly, such as p(aandb). guided practice 2.36 what is the interpretation of the value 0.907 in the last row of the table? and of the value 0.097 directly above it?27 25the first value is given by the intersection of \"45 - 64 years of age\" and \"diabetes\", divided by the total population number: 13;400;000=313;320;000 = 0:043. the second value is given by dividing 13,400,000 by 81,926,000, the number of individuals in that age group: 13 ;400;000=81;926;000 = 0:164. 26alternatively, this is commonly written as as p(diabetes, age 20 to 44), with a comma replacing “and”. 27the value 0.907 in the last row indicates the total proportion of individuals in the population who do not have diabetes. the value 0.097 indicates the joint probability of not having diabetes and being in the greater than 64 years age group. 108 chapter 2. probability 2.2.2 defining conditional probability the probability that a randomly selected individual from the us has diabetes is 0.093, the sum of the first column in figure 2.15. how does that probability change if it is known that the individual’s age is 64 or greater? the conditional probability can be calculated from figure 2.14, which shows that 11,200,000 of the 41,506,000 people in that age group have diabetes, so the likelihood that someone from that age group has diabetes is: 11;200;000 41;506;000= 0:27; or 27%. the additional information about a patient’s age allows for a more accurate estimate of the probability of diabetes. similarly, the conditional probability can be calculated from the joint and marginal proportions in figure 2.15. consider the main di fference between the conditional probability versus the joint and marginal probabilities. both the joint probability and marginal probabilities are probabilities relative to the entire population. however, the conditional probability is the probability of having diabetes, relative only to the segment of the population greater than the age of 64. intuitively, the denominator in the calculation of a conditional probability must account for the fact that only a segment of the population is being considered, rather than the entire population. the conditional probability of diabetes given age 64 or older is simply the joint probability of having diabetes and being greater than 64 years of age divided by the marginal probability of being in that age group: prop. of population with diabetes, age 64 or greater prop. of population greater than age 64=11;200;000=313;320;000 41;506;000=313;320;000 =0:036 0:132 = 0:270: this leads to the mathematical definition of conditional probability. conditional probability the conditional probability of an event agiven an event or condition bis: p(ajb) =p(aandb) p(b): (2.37) guided practice 2.38 calculate the probability that a randomly selected person has diabetes, given that their age is between 45 and 64.28 28letabe the event a person has diabetes, and bthe event that their age is between 45 and 64. use the information in figure 2.15 to calculate p(ajb).p(ajb) =p(aandb) p(b)=0:043 0:261= 0:165: 2.2. conditional probability 109 guided practice 2.39 calculate the probability that a randomly selected person is between 45 and 64 years old, given that the person has diabetes.29 conditional probabilities have similar properties to regular (unconditional) probabilities. sum of conditional probabilities leta1, ...,akrepresent all the disjoint outcomes for a variable or process. then if bis an event, possibly for another variable or process, we have: p(a1jb) +\u0001\u0001\u0001+p(akjb) = 1: the rule for complements also holds when an event and its complement are conditioned on the same information: p(ajb) = 1\u0000p(acjb): guided practice 2.40 calculate the probability a randomly selected person is older than 20 years of age, given that the person has diabetes.30 29again, letabe the event a person has diabetes, and bthe event that their age is between 45 and 64. find p(bja). p(bja) =p(aandb) p(a)=0:043 0:093= 0:462: 30letabe the event that a person has diabetes, and bbe the event that their age is less than 20 years. the desired probability is p(bcja) = 1\u0000p(bja) = 1\u00000:001 0:093= 0:989. 110 chapter 2. probability 2.2.3 general multiplication rule section 2.1.7 introduced the multiplication rule for independent processes. here, the general multiplication rule is introduced for events that might not be independent. general multiplication rule ifaandbrepresent two outcomes or events, then p(aandb) =p(ajb)p(b): it is useful to think of aas the outcome of interest and bas the condition. this general multiplication rule is simply a rearrangement of the definition for conditional probability in equation (2.37) on . example 2.41 suppose that among male adults between 25 and 40 years of age, hypertension occurs about 18% of the time. assume that the population is 50% male, 50% female. what is the probability of randomly selecting a male with hypertension from the population of individuals 25-40 years of age? letabe the event that a person has hypertension, and bthe event that they are a male adult between 25 and 40 years of age. p(ajb), the probability of hypertension given male sex, is 0.18. thus,p(aandb) = (0:18)(0:50) = 0:09. 2.2.4 independence and conditional probability if two events are independent, knowing the outcome of one should provide no information about the other. example 2.42 letxandyrepresent the outcomes of rolling two dice. use the formula for conditional probability to compute p(y=1jx=1). what isp(y= 1)? is this di fferent fromp(y=1jx=1)? p(y=1andx=1) p(x=1)=1=36 1=6= 1=6: the probability p(y= 1) = 1=6 is the same as the conditional probability. the probability that y= 1 was unchanged by knowledge about x, since the events xandyare independent. 2.2. conditional probability 111 using the multiplication rule for independent events allows for a mathematical illustration of why the condition information has no influence in example 2.42: p(y=1jx=1) =p(y=1andx=1) p(x=1) =p(y=1)p(x=1) p(x=1) =p(y=1): this is a specific instance of the more general result that if two events aandbare independent, p(ajb) =p(a) as long asp(b)>0: p(ajb) =p(aandb) p(b) =p(a)p(b) p(b) =p(a): guided practice 2.43 in the us population, about 45% of people are blood group o. suppose that 40% of asian people living in the us are blood group o, and that the asian population in the united states is approximately 4%. do these data suggest that blood group is independent of ethnicity?31 2.2.5 bayes’ theorem this chapter began with a straightforward question – what are the chances that a woman with an abnormal (i.e., positive) mammogram has breast cancer? for a clinician, this question can be rephrased as the conditional probability that a woman has breast cancer, given that her mammogram is abnormal. this conditional probability is called the positive predictive value (ppv) of a mammogram. more concisely, if a= {a woman has breast cancer}, and b= {a mammogram is positive}, the ppv of a mammogram is p(ajb). the characteristics of a mammogram (and other diagnostic tests) are given with the reverse conditional probabilities—the probability that the mammogram correctly returns a positive result if a woman has breast cancer, as well as the probability that the mammogram correctly returns a negative result if a woman does not have breast cancer. these are the probabilities p(bja) and p(bcjac), respectively. given the probabilities p(bja) andp(bcjac), as well as the marginal probability of disease p(a), how can the positive predictive value p(ajb) be calculated? there are several possible strategies for approaching this type of problem—1) constructing tree diagrams, 2) using a purely algebraic approach using bayes’ theorem, and 3) creating contingency tables based on calculating conditional probabilities from a large, hypothetical population. 31letarepresent blood group o, and brepresent asian ethnicity. since p(ajb) = 0:40 does not equal p(a) = 0:45, the two events are not independent. blood group does not seem to be independent of ethnicity. 112 chapter 2. probability example 2.44 in canada, about 0.35% of women over 40 will develop breast cancer in any given year. a common screening test for cancer is the mammogram, but it is not perfect. in about 11% of patients with breast cancer, the test gives a false negative : it indicates a woman does not have breast cancer when she does have breast cancer. similarly, the test gives a false positive in 7% of patients who do not have breast cancer: it indicates these patients have breast cancer when they actually do not.32if a randomly selected woman over 40 is tested for breast cancer using a mammogram and the test is positive – that is, the test suggests the woman has cancer – what is the probability she has breast cancer? read on in the text for three solutions to this example. example 2.44 solution 1. tree diagram. cancer status mammogram cancer, 0.0035positive, 0.890.0035*0.89 = 0.00312 negative, 0.110.0035*0.11 = 0.00038 no cancer, 0.9965positive, 0.070.9965*0.07 = 0.06976 negative, 0.930.9965*0.93 = 0.92675 figure 2.16: a tree diagram for breast cancer screening. atree diagram is a tool to organize outcomes and probabilities around the structure of data, and is especially useful when two or more processes occur in a sequence, with each process conditioned on its predecessors. in figure 2.16, the primary branches split the population by cancer status, and show the marginal probabilities 0.0035 and 0.9965 of having cancer or not, respectively. the secondary branches are conditioned on the primary branch and show conditional probabilities; for example, the top branch is the probability that a mammogram is positive given that an individual has cancer. the problem provides enough information to compute the probability of testing positive if breast cancer is present, since this probability is the complement of the probability of a false negative: 1\u00000:11 = 0:89. joint probabilities can be constructed at the end of each branch by multiplying the numbers from right to left, such as the probability that a woman tests positive given that she has breast cancer (abbreviated as bc), times the probability she has breast cancer: p(bc and mammogram+) =p(mammogram+jbc)\u0002p(bc) = (0:89)(0:0035) = 0:00312: 32the probabilities reported here were obtained using studies reported at www.breastcancer.org and www.ncbi.nlm.nih.gov/pmc/articles/pmc1173421. 2.2. conditional probability 113 using the tree diagram allows for the information in the problem to be mapped out in a way that makes it easier to calculate the desired conditional probability. in this case, the diagram makes it clear that there are two scenarios in which someone can test positive: either testing positive when having breast cancer or by testing positive in the absence of breast cancer. to find the probability that a woman has breast cancer given that she tests positive, apply the conditional probability formula: divide the probability of testing positive when having breast cancer by the probability of testing positive. the probability of a positive test result is the sum of the two corresponding scenarios: p(mammogram+) =p(mammogram+and has bc) + p(mammogram+and no bc) =[p(mammogram+jhas bc)\u0002p(has bc)] + [ p(mammogram+jno bc)\u0002p(no bc)] =(0:0035)(0:89) + (0:9965)(0:07) = 0:07288: thus, if the mammogram screening is positive for a patient, the probability that the patient has breast cancer is given by: p(has bcjmammogram+) =p(has bc and mammogram+) p(mammogram+) =0:00312 0:07288\u00190:0428: even with a positive mammogram, there is still only a 4% chance of breast cancer! it may seem surprising that even when the false negative and false positive probabilities of the test are small (0.11 and 0.07, respectively), the conditional probability of disease given a positive test could also be so small. in this population, the probability that a woman does not have breast cancer is high (1 - 0.0035 = 0.9965), which results in a relatively high number of false positives in comparison to true positives. calculating probabilities for diagnostic tests is done so often in medicine that the topic has some specialized terminology. the sensitivity of a test is the probability of a positive test result when disease is present, such as a positive mammogram when a patient has breast cancer. the specificity of a test is the probability of a negative test result when disease is absent.33the probability of disease in a population is referred to as the prevalence . with specificity and sensitivity information for a particular test, along with disease prevalence, the positive predictive value (ppv) can be calculated: the probability that disease is present when a test result is positive. similarly, the negative predictive value is the probability that disease is absent when test results are negative. these terms are used for nearly all diagnostic tests used to screen for diseases. guided practice 2.45 identify the prevalence, sensitivity, specificity, and ppv from the scenario in example 2.44.34 33the sensitivity and specificity are, respectively, the probability of a true positive test result and the probability of a true negative test result. 34the prevalence of breast cancer is 0.0035. the sensitivity is the probability of a positive test result when disease is present, which is the complement of a false negative: 1 \u00000:11 = 0:89. the specificity is the probability of a negative test result when disease is absent, which is the complement of a false positive: 1 \u00000:07 = 0:93. the ppv is 0.04, the probability of breast cancer given a positive mammogram. 114 chapter 2. probability example 2.44 solution 2. bayes’ rule. the process used to solve the problem via the tree diagram can be condensed into a single algebraic expression by substituting the original probability expressions into the numerator and denominator: p(has bcjmammogram+) =p(has bc and mammogram+) p(mammogram+) =p(mammogram+jhas bc)\u0002p(has bc) [p(mammogram+jhas bc)\u0002p(has bc)] + [ p(mammogram+jno bc)\u0002p(no bc)]: the expression can also be written in terms of diagnostic testing language, where d= {has disease}, dc= {does not have disease}, t+= {positive test result}, and t\u0000= {negative test result}. p(djt+) =p(dandt+) p(t+) =p(t+jd)\u0002p(d) [p(t+jd)\u0002p(d)] + [p(t+jdc)\u0002p(dc)] ppv =sensitivity\u0002prevalence [sensitivity\u0002prevalence] + [(1 - specificity) \u0002(1 - prevalence)]: the generalization of this formula is known as bayes’ theorem or bayes’ rule. bayes’ theorem consider the following conditional probability for variable 1 and variable 2: p(outcomea1of variable 1joutcomebof variable 2) : bayes’ theorem states that this conditional probability can be identified as the following fraction: p(bja1)p(a1) p(bja1)p(a1) +p(bja2)p(a2) +\u0001\u0001\u0001+p(bjak)p(ak); (2.46) wherea2,a3, ..., andakrepresent all other possible outcomes of the first variable. the numerator identifies the probability of getting both a1andb. the denominator is the marginal probability of getting b. this bottom component of the fraction describes the adding of probabilities from the di fferent ways to get b. to apply bayes’ theorem correctly, there are two preparatory steps: (1) first identify the marginal probabilities of each possible outcome of the first variable: p(a1), p(a2), ...,p(ak). (2) then identify the probability of the outcome b, conditioned on each possible scenario for the first variable: p(bja1),p(bja2), ...,p(bjak). once these probabilities are identified, they can be applied directly within the formula. 2.2. conditional probability 115 example 2.44 solution 3. contingency table. the positive predictive value (ppv) of a diagnostic test can be calculated by constructing a two-way contingency table for a large, hypothetical population and calculating conditional probabilities by conditioning on rows or columns. using a large enough hypothetical population results in an empirical estimate of ppv that is very close to the exact value obtained via using the previously discussed approaches. begin by constructing an empty 2 \u00022 table, with the possible outcomes of the diagnostic test as the rows, and the possible disease statuses as the columns (figure 2.17). include cells for the row and column sums. choose a large number n, for the hypothetical population size. typically, nof 100,000 is sufficient for an accurate estimate. breast cancer present breast cancer absent sum mammogram positive – – – mammogram negative – – – sum – – 100,000 figure 2.17: a 2\u00022 table for the mammogram example, with hypothetical population sizenof 100,000. continue populating the table, using the provided information about the prevalence of breast cancer in this population (0.35%), the chance of a false negative mammogram (11%), and the chance of a false positive (7%): 1. calculate the two column totals (the number of women with and without breast cancer) from p(bc), the disease prevalence: n\u0002p(bc) = 100;000\u0002:0035 = 350 women with bc n\u0002[1\u0000p(bc)] = 100;000\u0002[1\u0000:0035] = 99;650 women without bc alternatively, the number of women without breast cancer can be calculated by subtracting the number of women with breast cancer from n. 2. calculate the two numbers in the first column: the number of women who have breast cancer and tested either negative (false negative) or positive (true positive). women with bc\u0002p(false \"-\") = 350\u0002:11 = 38:5 false \"-\" results women with bc\u0002[1\u0000p(false \"-\")] = 350\u0002[1\u0000:11] = 311:5 true \"+\" results 3. calculate the two numbers in the second column: the number of women who do not have breast cancer and tested either positive (false positive) or negative (true negative). women without bc \u0002p(false \"+\") = 99 ;650\u0002:07 = 6;975:5 false \"+\" results women without bc \u0002[1\u0000p(false \"+\")] = 99 ;650\u0002[1\u0000:07] = 92;674:5 true \"-\" results 4. complete the table by calculating the two row totals: the number of positive and negative mammograms out of 100,000. (true \"+\" results) + (false \"+\" results) = 311 :5 + 6;975:5 = 7;287 \"+\" mammograms (true \"-\" results) + (false \"-\" results) = 38 :5 + 92;674:5 = 92;713 \"-\" mammograms 116 chapter 2. probability 5. finally, calculate the ppv of the mammogram by using the ratio of the number of true positives to the total number of positive mammograms. this estimate is more than accurate enough, with the calculated value di ffering only in the third decimal place from the exact calculation, true \"+\" results \"+\" mammograms=311:5 7;287= 0:0427: breast cancer present breast cancer absent sum mammogram positive 311.5 6,975.5 7,287 mammogram negative 38.5 92,674.5 92,713 sum 350 99,650 100,000 figure 2.18: completed table for the mammogram example. the table shows again why the ppv of the mammogram is low: almost 7,300 women will have a positive mammogram result in this hypothetical population, but only ~312 of those women actually have breast cancer. guided practice 2.47 some congenital disorders are caused by errors that occur during cell division, resulting in the presence of additional chromosome copies. trisomy 21 occurs in approximately 1 out of 800 births. cell-free fetal dna (cfdna) testing is one commonly used way to screen fetuses for trisomy 21. the test sensitivity is 0.98 and the specificity is 0.995. calculate the ppv and npv of the test.35 35ppv =p(t+jd)\u0002p(d) [p(t+jd)\u0002p(d)] + [p(t+jdc)\u0002p(dc)]=(0:98)(1=800) (0:98)(1=800) + (1\u00000:995)(799=800)= 0:197. npv =p(t\u0000jdc)\u0002p(dc) [p(t\u0000jd)\u0002p(d)] + [p(t\u0000jdc)\u0002p(dc)]=(0:995)(799=800) (1\u00000:98)(1=800) + (0:995)(799=800)= 0:999975. 2.3. extended example 117 2.3 extended example: cat genetics so far, the principles of probability have only been illustrated with short examples. in a more complex setting, it can be surprisingly di fficult to accurately translate a problem scenario into the language of probability. this section demonstrates how the rules of probability can be applied to work through a relatively sophisticated conditioning problem. problem statement the gene that controls white coat color in cats, kit , is known to be responsible for multiple phenotypes such as deafness and blue eye color. a dominant allele wat one location in the gene has complete penetrance for white coat color; all cats with the wallele have white coats. there is incomplete penetrance for blue eyes and deafness; not all white cats will have blue eyes and not all white cats will be deaf. however, deafness and blue eye color are strongly linked, such that white cats with blue eyes are much more likely to be deaf. the variation in penetrance for eye color and deafness may be due to other genes as well as environmental factors. suppose that 30% of white cats have one blue eye, while 10% of white cats have two blue eyes. about 73% of white cats with two blue eyes are deaf and 40% of white cats with one blue eye are deaf. only 19% of white cats with other eye colors are deaf. a) calculate the prevalence of deafness among white cats. b) given that a white cat is deaf, what is the probability that it has two blue eyes? c) suppose that deaf, white cats have an increased chance of being blind, but that the prevalence of blindness di ffers according to eye color. while deaf, white cats with two blue eyes or two non-blue eyes have probability 0.20 of developing blindness, deaf and white cats with one blue eye have probability 0.40 of developing blindness. white cats that are not deaf have probability 0.10 of developing blindness, regardless of their eye color. i. what is the prevalence of blindness among deaf, white cats? ii. what is the prevalence of blindness among white cats? iii. given that a cat is white and blind, what is the probability that it has two blue eyes? defining notation before beginning any calculations, it is essential to clearly define any notation that will be used. for this problem, there are several events of interest: deafness, number of blue eyes (either 0, 1, or 2), and blindness. – letdrepresent the event that a white cat is deaf. – letb0= {zero blue eyes}, b1= {one blue eye}, and b2= {two blue eyes}. – letlrepresent the event that a white cat is blind. note that since all cats mentioned in the problem are white, it is not necessary to define whiteness as an event; white cats represent the sample space. 118 chapter 2. probability part a) deafness the prevalence of deafness among white cats is the proportion of white cats that are deaf; i.e., the probability of deafness among white cats. in the notation of probability, this question asks for the value ofp(d). example 2.48 the following information has been given in the problem. re-write the information using the notation defined earlier. suppose that 30% of white cats have one blue eye, while 10% of white cats have two blue eyes. about 73% of white cats with two blue eyes are deaf and 40% of white cats with one blue eye are deaf. only 19% of white cats with other eye colors are deaf. the first sentence provides information about the prevalence of white cats with one blue eye and white cats with two blue eyes: p(b1) = 0:30 andp(b2) = 0:10. the only other possible eye color combination is zero blue eyes (i.e., two non-blue eyes); i.e., since p(b0) +p(b1) +p(b2) = 1,p(b0) = 1\u0000p(b1)\u0000p(b2) = 0:60. 60% of white cats have two non-blue eyes. while it is not di fficult to recognize that the second and third sentences provide information about deafness in relation to eye color, it can be easy to miss that these probabilities are conditional probabilities. a close reading should focus on the language—\"about 73% ofwhite cats with two blue eyes are deaf...\": i.e., out of the white cats that have two blue eyes, 73% are deaf. thus, these are probabilities of deafness conditioned on eye color. from these sentences, p(djb2) = 0:73, p(djb1) = 0:40, andp(djb0) = 0:19. consider that there are three possible ways to partition the event d, that a white cat is deaf: a cat could be deaf and have two blue eyes, be deaf and have one blue eye (and one non-blue eye), or be deaf and have two non-blue eyes. thus, by the addition rule of disjoint outcomes: p(d) =p(dandb2) +p(dandb1) +p(dandb0): although the joint probabilities of being deaf and having particular eye colors are not given in the problem, these can be solved for based on the given information. the definition of conditional probability p(ajb) relates the joint probability p(aandb) with the marginal probability p(b).36 p(ajb) =p(aandb) p(b)p(aandb) =p(ajb)p(b): thus, the probability p(d) is given by: p(d) =p(dandb2) +p(dandb1) +p(dandb0) =p(djb2)p(b2) +p(djb1)p(b1) +p(djb0)p(b0) =(0:73)(0:10) + (0:40)(0:30) + (0:19)(0:60) =0:307: the prevalence of deafness among white cats is 0.307. 36this rearrangement of the definition of conditional probability, p(aandb) =p(ajb)p(b), is also known as the general multiplication rule. 2.3. extended example 119 part b) deafness and eye color the probability that a white cat has two blue eyes, given that it is deaf, can be expressed as p(b2jd). example 2.49 using the definition of conditional probability, solve for p(b2jd). p(b2jd) =p(dandb2) p(d)=p(djb2)p(b2) p(d)=(0:73)(0:10) 0:307= 0:238: the probability that a white cat has two blue eyes, given that it is deaf, is 0.238. it is also possible to think of this as a bayes’ rule problem, where there are three possible partitions of the event of deafness, d. in this problem, it is possible to directly solve from the definition of conditional probability since p(d) was solved for in part a); note that the expanded denominator below matches the earlier work to calculate p(d). p(b2jd) =p(dandb2) p(d)=p(djb2)p(b2) p(djb2)p(b2) +p(djb1)p(b1) +p(djb0)p(b0): part c) blindness, deafness, and eye color example 2.50 the following information has been given in the problem. re-write the information using the notation defined earlier. suppose that deaf, white cats have an increased chance of being blind, but that the prevalence of blindness di ffers according to eye color. while deaf, white cats with two blue eyes or two non-blue eyes have probability 0.20 of developing blindness, deaf and white cats with one blue eye have probability 0.40 of developing blindness. white cats that are not deaf have probability 0.10 of developing blindness, regardless of their eye color. the second sentence gives probabilities of blindness, conditional on eye color and being deaf: p(ljb2;d) =p(ljb0;d) = 0:20, andp(ljb1;d) = 0:40. the third sentence gives the probability that a white cat is blind, given that it is not deaf: p(ljdc) = 0:10. part i. asks for the prevalence of blindness among deaf, white cats: p(ljd). as in part a), the event of blindness given deafness can be partitioned by eye color: p(ljd) =p(landb0jd) +p(landb1jd) +p(landjd): 120 chapter 2. probability example 2.51 expand the previous expression using the general multiplication rule, p(aandb) =p(ajb)p(b). the general multiplication rule may seem di fficult to apply when conditioning is present, but the principle remains the same. think of the conditioning as a way to restrict the sample space; in this context, conditioning on deafness implies that for this part of the problem, all the cats being considered are deaf (and white). for instance, consider the first term, p(landb0jd), the probability of being blind and having two non-blue eyes, given deafness. how could this be rewritten if the probability were simply p(landb0)? p(landb0) =p(ljb0)p(b0) now, recall that for this part of the problem, the sample space is restricted to deaf (and white) cats. thus, all of the terms in the expansion should include conditioning on deafness: p(landb0jd) =p(ljd;b 0)p(b0jd): thus, p(ljd) =p(ljd;b 0)p(b0jd) +p(ljd;b 1)p(b1jd) +p(ljd;b 2)p(b2jd): althoughp(ljd;b 0),p(ljd;b 1), andp(ljd;b 2) are given from the problem statement, p(b0jd, p(b1jd), andp(b2jd) are not. however, note that the probability that a white cat has two blue eyes given that it is deaf, p(b2jd), was calculated in part b). guided practice 2.52 calculatep(b0jd) andp(b1jd).37 there is now su fficient information to calculate p(ljd): p(ljd) =p(landb0jd) +p(landb1jd) +p(landb2jd) =p(ljd;b 0)p(b0jd) +p(ljd;b 1)p(b1jd) +p(ljd;b 2)p(b2jd) =(0:20)(0:371) + (0:40)(0:391) + (0:20)(0:238) =0:278: the prevalence of blindness among deaf, white cats is 0.278. part ii. asks for the prevalence of blindness among white cats, p(l). again, partitioning is an effective strategy. instead of partitioning by eye color, however, partition by deafness. 37 p(b0jd) =p(dandb0) p(d)=p(djb0)p(b0) p(d)=(0:19)(0:60) 0:307= 0:371: p(b1jd) =p(dandb1) p(d)=p(djb1)p(b1) p(d)=(0:40)(0:30) 0:307= 0:391: 2.3. extended example 121 example 2.53 calculate the prevalence of blindness among white cats, p(l). p(l) =p(landd) +p(landdc) =p(ljd)p(d) +p(ljdc)p(dc) =(0:278)(0:307) + (0:10)(1\u00000:307) =0:155: p(d) was calculated in part a), while p(ljd) was calculated in part c, i. the conditioning probability of blindness given a white cat is not deaf is 0.10, as given in the question statement. by the definition of the complement, p(dc) = 1\u0000p(d). the prevalence of blindness among white cats is 0.155. part iii. asks for the probability that a cat has two blue eyes, given that it is white and blind. this probability can be expressed as p(b2jl). recall that since all cats being discussed in the problem are white, it is not necessary to condition on coat color. start out with the definition of conditional probability: p(b2jl) =p(b2andl) p(l): the key to calculating p(b2jl) relies on recognizing that the event a cat is blind and has two blue eyes can be partitioned by whether or not the cat is also deaf: p(b2jl) =p(b2andlandd) +p(b2andlanddc) p(l): (2.54) example 2.55 draw a tree diagram to organize the events involved in this problem. identify the branches that represent the possible paths for a white cat to both have two blue eyes and be blind. when drawing a tree diagram, remember that each branch is conditioned on the previous branches. while there are various possible trees, the goal is to construct a tree for which as many of the branches as possible have known probabilities. the tree for this problem will have three branch points, corresponding to either deafness, blindness, or eye color. the first set of branches contain unconditional probabilities, the second set contains conditional probabilities given one event, and the third set contains conditional probabilities given two events. recall that the probabilities p(ljd;b 0),p(ljd;b 1), andp(ljd;b 2) were provided in the problem statement. these are the only probabilities conditioned on two events that have previously appeared in the problem, so blindness is the most convenient choice of third branch point. it is not immediately obvious whether it will be more e fficient to start with deafness or eye color, since unconditional and conditional probabilities related to both have appeared in the problem. figure 2.19 shows two trees, one starting with deafness and the other starting with eye color. the two possible paths for a white cat to both have two blue eyes and be blind are shown in green. 122 chapter 2. probability (a) (b) figure 2.19: in (a), the first branch is based on deafness, while in (b), the first branch is based on eye color. example 2.56 expand equation 2.54 according to the tree shown in figure 2.19(a), and solve for p(b2jl). p(b2jl) =p(b2andlandd) +p(b2andlanddc) p(l) =p(ljb2;d)p(b2jd)p(d) +p(ljb2;dc)p(b2jdc)p(dc) p(l) =(0:20)(0:238)(0:307) + (0:10)p(b2jdc)p(dc) 0:155: two of the probabilities have not been calculated previously: p(b2jdc) andp(dc). from the definition of the complement, p(dc) = 1\u0000p(d) = 0:693;p(d) was calculated in part a). to calculate p(b2jdc), apply the definition of conditional probability as in part b), where p(b2jd) was calculated: p(b2jdc) =p(dcandb2) p(dc)=p(dcjb2)(p(b2) p(dc)=(1\u00000:73)(0:10) 0:693= 0:0390: p(b2jl) =(0:20)(0:238)(0:307) + (0:10)(0:0390)(0:693) 0:155 = 0:112 the probability that a white cat has two blue eyes, given that it is blind, is 0.112. 2.3. extended example 123 guided practice 2.57 expand equation 2.54 according to the tree shown in figure 2.19(b), and solve for p(b2jl).38 a tree diagram is useful for visualizing the di fferent possible ways that a certain set of outcomes can occur. although conditional probabilities can certainly be calculated without the help of tree diagrams, it is often easy to make errors with a strictly algebraic approach. once a tree is constructed, it can be used to solve for several probabilities of interest. the following example shows how one of the previous trees can be applied to answer a di fferent question than the one posed in part c), iii. 38 p(b2jl) =p(ljb2;d)p(djb2)p(b2) +p(ljb2;dc)p(dcjb2)p(b2) p(l)=(0:20)(0:73)(0:10) + (0:10)(1\u00000:73)(0:10) 0:155= 0:112: 124 chapter 2. probability example 2.58 what is the probability that a white cat has one blue eye and one non-blue eye, given that it is not blind? calculatep(b1jlc). start with the definition of conditional probability, then expand. p(b1jlc) =p(b1andlc) p(lc)=p(b1andlcandd) +p(b1andlcanddc) p(lc): figure 2.20 is a reproduction of the earlier tree diagram (figure 2.19(b)), with yellow arrows showing the two paths of interest. as before, expand the numerator and fill in the known values. p(b1jlc) =p(b1andlcandd) +p(b1andlcanddc) p(lc) =p(lcjd;b 1)p(djb1)p(b1) +p(lcjdc;b1)p(dcjb1)p(b1) p(lc) =p(lcjd;b1)(0:40)(0:30) + p(lcjdc;b1)p(dcjb1)(0:30) p(lc): the probabilities in bold are not known. apply the definition of the complement; recall that the rule for complements holds when an event and its complement are conditioned on the same information: p(ajb) = 1\u0000p(acjb). –p(lc) = 1\u0000p(l) = 1\u00000:155 = 0:845 –p(dcjb1) = 1\u0000p(djb1) = 1\u00000:40 = 0:60 –p(lcjd;b 1) = 1\u0000p(ljd;b 1) = 1\u00000:40 = 0:60 the definition of the complement can also be applied to calculate p(lcjdc;b1). the problem statement originally specified that white cats that are not deaf have probability 0.10 of developing blindness regardless of eye color: p(ljdc) = 0:10. thus,p(lcjdc;b1) =p(lcjdc). by the definition of the complement, p(lcjdc) = 1\u0000p(ljdc) = 1\u00000:10 = 0:90. p(b1jlc) =p(b1andlcandd) +p(b1andlcanddc) p(lc) =p(lcjd;b 1)p(djb1)p(b1) +p(lcjdc;b1)p(dcjb1)p(b1) p(lc) =(0:60)(0:40)(0:30) + (0:90)(0:60)(0:30) 0:845 =0:277: the probability that a white cat has one blue eye and one non-blue eye, given that it is not blind, is 0.277. 2.3. extended example 125 figure 2.20: the two possible paths for a white cat to both have one blue eye (and one non-blue eye) and to not be blind are shown in yellow. 126 chapter 2. probability 2.4 notes probability is a powerful framework for quantifying uncertainty and randomness. in particular, conditional probability represents a way to update the uncertainty associated with an event given that specific information has been observed. for example, the probability that a person has a particular disease can be adjusted based on observed information, such as age, sex, or the results of a diagnostic test. as discussed in the text, there are several possible approaches to solving conditional probability problems, including the use of tree diagrams or contingency tables. it can also be intuitive to use a simulation approach in computing software; refer to the labs for details about this method. regardless of the specific approach that will be used for calculation, it is always advisable to start any problem by understanding the problem context (i.e., the sample space, given information, probabilities of interest) and reading the problem carefully, in order to avoid mistakes when translating between words and probability notation. a common mistake is to confuse joint and conditional probabilities. probability distributions were briefly introduced in section 2.1.5. this topic will be discussed in greater detail in the next chapter. probability forms the foundation for data analysis and statistical inference, since nearly every conclusion to a study should be accompanied by a measure of uncertainty. for example, the publication reporting the results of the leap study discussed in chapter 1 included the probability that the observed results could have been due to chance variation. this aspect of probability will be discussed in later chapters. the four labs for chapter 2 cover basic principles of probability, conditional probability, positive predictive value of a diagnostic test (via bayes’ theorem), and the calculation of probabilities conditional on several events in the context of genetic inheritance. probabilities can be calculated algebraically, using formulas given in this and other texts, but can also be calculated with simple simulations, since a probability represents a proportion of times an event happens when an experiment is repeated many times. computers are particularly good at keeping track of events during many replications of an experiment. the labs for this chapter use both algebraic and simulation methods, and are particularly useful for building programming skills with the rlanguage. in medicine, the positive predictive value of a diagnostic test may be one of the most important applications of probability theory. it is certainly the most common. the positive predictive value of a test is the conditional probability of the presence of a disease or condition, given a positive test for the condition, and is often used when counseling patients about their risk for being diagnosed with a disease in the future. the lab on positive predictive value examines the conditional probability of a trisomy 21 genetic mutation (down syndrome) given that a test based on cell-free dna suggests its presence. 2.5. exercises 127 2.5 exercises 2.5.1 defining probability 2.1 true or false. determine if the statements below are true or false, and explain your reasoning. (a) assume that a couple has an equal chance of having a boy or a girl. if a couple’s previous three children have all been boys, then the chance that their next child is a boy is somewhat less than 50%. (b) drawing a face card (jack, queen, or king) and drawing a red card from a full deck of playing cards are mutually exclusive events. (c) drawing a face card and drawing an ace from a full deck of playing cards are mutually exclusive events. 2.2 dice rolls. if you roll a pair of fair dice, what is the probability of (a) getting a sum of 1? (b) getting a sum of 5? (c) getting a sum of 12? 2.3 colorblindness. red-green colorblindness is a commonly inherited form of colorblindness; the gene involved is transmitted on the x chromosome in a recessive manner. if a male inherits an a ffected x chromosome, he is necessarily colorblind (genotype x\u0000y). however, a female can only be colorblind if she inherits two defective copies (genotype x\u0000x\u0000); heterozygous females are not colorblind. suppose that a couple consists of a genotype x+ymale and a genotype x+x\u0000female. (a) what is the probability of the couple producing a colorblind male? (b) true or false: among the couple’s o ffspring, colorblindness and female sex are mutually exclusive events. 2.4 diabetes and hypertension. diabetes and hypertension are two of the most common diseases in western, industrialized nations. in the united states, approximately 9% of the population have diabetes, while about 30% of adults have high blood pressure. the two diseases frequently occur together: an estimated 6% of the population have both diabetes and hypertension. (a) are having diabetes and having hypertension disjoint? (b) draw a venn diagram summarizing the variables and their associated probabilities. (c) letarepresent the event of having diabetes, and bthe event of having hypertension. calculate p(aorb). (d) what percent of americans have neither hypertension nor diabetes? (e) is the event of someone being hypertensive independent of the event that someone has diabetes? 128 chapter 2. probability 2.5 educational attainment by gender. the table below shows the distribution of education level attained by us residents by gender based on data collected during the 2010 american community survey.39 gender male female less than 9th grade 0.07 0.13 9th to 12th grade, no diploma 0.10 0.09 highest hs graduate (or equivalent) 0.30 0.20 education some college, no degree 0.22 0.24 attained associate’s degree 0.06 0.08 bachelor’s degree 0.16 0.17 graduate or professional degree 0.09 0.09 total 1.00 1.00 (a) what is the probability that a randomly chosen individual is a high school graduate? assume that there is an equal proportion of males and females in the population. (b) define event aas having a graduate or professional degree. calculate the probability of the complement, ac. (c) what is the probability that a randomly chosen man has at least a bachelor’s degree? (d) what is the probability that a randomly chosen woman has at least a bachelor’s degree? (e) what is the probability that a man and a woman getting married both have at least a bachelor’s degree? note any assumptions made – are they reasonable? 2.6 poverty and language. the american community survey is an ongoing survey that provides data every year to give communities the current information they need to plan investments and services. the 2010 american community survey estimates that 14.6% of americans live below the poverty line, 20.7% speak a language other than english (foreign language) at home, and 4.2% fall into both categories.40 (a) are living below the poverty line and speaking a foreign language at home disjoint? (b) draw a venn diagram summarizing the variables and their associated probabilities. (c) what percent of americans live below the poverty line and only speak english at home? (d) what percent of americans live below the poverty line or speak a foreign language at home? (e) what percent of americans live above the poverty line and only speak english at home? (f) is the event that someone lives below the poverty line independent of the event that the person speaks a foreign language at home? 2.7 urgent care visits. urgent care centers are open beyond typical o ffice hours and provide a broader range of services than that of many primary care o ffices. a study conducted to collect information about urgent care centers in the united states reported that in one week, 15.8% of centers saw 0-149 patients, 33.7% saw 150299 patients, 28.8% saw 300-449 patients, and 21.7% saw 450 or more patients. assume that the data can be treated as a probability distribution of patient visits for any given week. (a) what is the probability that three random urgent care centers in a county all see between 300-449 patients in a week? note any assumptions made. are the assumptions reasonable? (b) what is the probability that ten random urgent care centers throughout a state all see 450 or more patients in a week? note any assumptions made. are the assumptions reasonable? (c) with the information provided, is it possible to compute the probability that one urgent care center sees between 150-299 patients in one week and 300-449 patients in the next week? explain why or why not. 39u.s. census bureau, 2010 american community survey 1-year estimates, educational attainment. 40u.s. census bureau, 2010 american community survey 1-year estimates, characteristics of people by language spoken at home. 2.5. exercises 129 2.8 school absences. data collected at elementary schools in dekalb county, ga suggest that each year roughly 25% of students miss exactly one day of school, 15% miss 2 days, and 28% miss 3 or more days due to sickness.41 (a) what is the probability that a student chosen at random doesn’t miss any days of school due to sickness this year? (b) what is the probability that a student chosen at random misses no more than one day? (c) what is the probability that a student chosen at random misses at least one day? (d) if a parent has two kids at a dekalb county elementary school, what is the probability that neither kid will miss any school? note any assumptions made and evaluate how reasonable they are. (e) if a parent has two kids at a dekalb county elementary school, what is the probability that both kids will miss some school, i.e. at least one day? note any assumptions made and evaluate how reasonable they are. 2.9 disjoint vs. independent. in parts (a) and (b), identify whether the events are disjoint, independent, or neither (events cannot be both disjoint and independent). (a) you and a randomly selected student from your class both earn a’s in this course. (b) you and your class study partner both earn a’s in this course. (c) if two events can occur at the same time, must they be dependent? 2.10 health coverage, frequencies. the behavioral risk factor surveillance system (brfss) is an annual telephone survey designed to identify risk factors in the adult population and report emerging health trends. the following table summarizes two variables for the respondents: health status and health coverage, which describes whether each respondent had health insurance.42 health status excellent very good good fair poor total health no 459 727 854 385 99 2,524 coverage yes 4,198 6,245 4,821 1,634 578 17,476 total 4,657 6,972 5,675 2,019 677 20,000 (a) if one individual is drawn at random, what is the probability that the respondent has excellent health and doesn’t have health coverage? (b) if one individual is drawn at random, what is the probability that the respondent has excellent health or doesn’t have health coverage? 41s.s. mizan et al. “absence, extended absence, and repeat tardiness related to asthma status among elementary school children”. in: journal of asthma 48.3 (2011), pp. 228–234. 42office of surveillance, epidemiology, and laboratory services behavioral risk factor surveillance system, brfss 2010 survey data. 130 chapter 2. probability 2.5.2 conditional probability 2.11 global warming. a pew research poll asked 1,306 americans “from what you’ve read and heard, is there solid evidence that the average temperature on earth has been getting warmer over the past few decades, or not?\". the table below shows the distribution of responses by party and ideology, where the counts have been replaced with relative frequencies.43 response earth is not don’t know warming warming refuse total conservative republican 0.11 0.20 0.02 0.33 party and mod/lib republican 0.06 0.06 0.01 0.13 ideology mod/cons democrat 0.25 0.07 0.02 0.34 liberal democrat 0.18 0.01 0.01 0.20 total 0.60 0.34 0.06 1.00 (a) are believing that the earth is warming and being a liberal democrat mutually exclusive? (b) what is the probability that a randomly chosen respondent believes the earth is warming or is a liberal democrat? (c) what is the probability that a randomly chosen respondent believes the earth is warming given that he is a liberal democrat? (d) what is the probability that a randomly chosen respondent believes the earth is warming given that he is a conservative republican? (e) does it appear that whether or not a respondent believes the earth is warming is independent of their party and ideology? explain your reasoning. (f) what is the probability that a randomly chosen respondent is a moderate/liberal republican given that he does not believe that the earth is warming? 2.12 abo blood groups. the abo blood group system consists of four di fferent blood groups, which describe whether an individual’s red blood cells carry the a antigen, b antigen, both, or neither. the abo gene has three alleles: ia,ib, and i. the iallele is recessive to both iaandib, while the iaandiballels are codominant. individuals homozygous for the iallele are known as blood group o, with neither a nor b antigens. alleles inherited blood type iaandiaa iaandibab iaandi a ibandibb ibandi b iandi o blood group follows the rules of mendelian single-gene inheritance – alleles are inherited independently from either parent, with probability 0.5. (a) suppose that both members of a couple have group ab blood. what is the probability that a child of this couple will have group a blood? (b) suppose that one member of a couple is genotype ibiand the other is iai. what is the probability that their first child has type o blood and the next two do not? (c) suppose that one member of a couple is genotype ibiand the other is iai. given that one child has type o blood and two do not, what is the probability of the first child having type o blood? 43pew research center, majority of republicans no longer see evidence of global warming, data collected on october 27, 2010. 2.5. exercises 131 2.13 seat belts. seat belt use is the most e ffective way to save lives and reduce injuries in motor vehicle crashes. in a 2014 survey, respondents were asked, \"how often do you use seat belts when you drive or ride in a car?\". the following table shows the distribution of seat belt usage by sex. seat belt usage always nearly always sometimes seldom never total sexmale 146,018 19,492 7,614 3,145 4,719 180,988 female 229,246 16,695 5,549 1,815 2,675 255,980 total 375,264 36,187 13,163 4,960 7,394 436,968 (a) calculate the marginal probability that a randomly chosen individual always wears seatbelts. (b) what is the probability that a randomly chosen female always wears seatbelts? (c) what is the conditional probability of a randomly chosen individual always wearing seatbelts, given that they are female? (d) what is the conditional probability of a randomly chosen individual always wearing seatbelts, given that they are male? (e) calculate the probability that an individual who never wears seatbelts is male. (f) does gender seem independent of seat belt usage? 2.14 health coverage, relative frequencies. the behavioral risk factor surveillance system (brfss) is an annual telephone survey designed to identify risk factors in the adult population and report emerging health trends. the following table displays the distribution of health status of respondents to this survey (excellent, very good, good, fair, poor) conditional on whether or not they have health insurance. health status excellent very good good fair poor total health no 0.0230 0.0364 0.0427 0.0192 0.0050 0.1262 coverage yes 0.2099 0.3123 0.2410 0.0817 0.0289 0.8738 total 0.2329 0.3486 0.2838 0.1009 0.0338 1.0000 (a) are being in excellent health and having health coverage mutually exclusive? (b) what is the probability that a randomly chosen individual has excellent health? (c) what is the probability that a randomly chosen individual has excellent health given that he has health coverage? (d) what is the probability that a randomly chosen individual has excellent health given that he doesn’t have health coverage? (e) do having excellent health and having health coverage appear to be independent? 2.15 hiv in swaziland. swaziland has the highest hiv prevalence in the world: 25.9% of this country’s population is infected with hiv.44the elisa test is one of the first and most accurate tests for hiv. for those who carry hiv, the elisa test is 99.7% accurate. for those who do not carry hiv, the test is 92.6% accurate. calculate the ppv and npv of the test. 44source: cia factbook, country comparison: hiv/aids - adult prevalence rate. 132 chapter 2. probability 2.16 assortative mating. assortative mating is a nonrandom mating pattern where individuals with similar genotypes and/or phenotypes mate with one another more frequently than what would be expected under a random mating pattern. researchers studying this topic collected data on eye colors of 204 scandinavian men and their female partners. the table below summarizes the results. for simplicity, we only include heterosexual relationships in this exercise.45 partner (female) blue brown green total blue 78 23 13 114 self (male)brown 19 23 12 54 green 11 9 16 36 total 108 55 41 204 (a) what is the probability that a randomly chosen male respondent or his partner has blue eyes? (b) what is the probability that a randomly chosen male respondent with blue eyes has a partner with blue eyes? (c) what is the probability that a randomly chosen male respondent with brown eyes has a partner with blue eyes? what about the probability of a randomly chosen male respondent with green eyes having a partner with blue eyes? (d) does it appear that the eye colors of male respondents and their partners are independent? explain your reasoning. 2.17 it’s never lupus. lupus is a medical phenomenon where antibodies that are supposed to attack foreign cells to prevent infections instead see plasma proteins as foreign bodies, leading to a high risk of blood clotting. it is believed that 2% of the population su ffer from this disease. the test is 98% accurate if a person actually has the disease. the test is 74% accurate if a person does not have the disease. there is a line from the fox television show house that is often used after a patient tests positive for lupus: “it’s never lupus.\" do you think there is truth to this statement? use appropriate probabilities to support your answer. 2.18 predisposition for thrombosis. a genetic test is used to determine if people have a predisposition for thrombosis , which is the formation of a blood clot inside a blood vessel that obstructs the flow of blood through the circulatory system. it is believed that 3% of people actually have this predisposition. the genetic test is 99% accurate if a person actually has the predisposition, meaning that the probability of a positive test result when a person actually has the predisposition is 0.99. the test is 98% accurate if a person does not have the predisposition. (a) what is the probability that a randomly selected person who tests positive for the predisposition by the test actually has the predisposition? (b) what is the probability that a randomly selected person who tests negative for the predisposition by the test actually does not have the predisposition? 45b. laeng et al. “why do blue-eyed men prefer women with the same eye color?” in: behavioral ecology and sociobiology 61.3 (2007), pp. 371–384. 2.5. exercises 133 2.19 views on evolution. a 2013 analysis conducted by the pew research center found that 60% of survey respondents agree with the statement \"humans and other living things have evolved over time\" while 33% say that \"humans and other living things have existed in their present form since the beginning of time\" (7% responded \"don’t know\"). they also found that there are di fferences among partisan groups in beliefs about evolution. while roughly two-thirds of democrats (67%) and independents (65%) say that humans and other living things have evolved over time, 48% of republicans reject the idea of evolution. suppose that 45% of respondents identified as democrats, 40% identified as republicans, and 15% identified as political independents. the survey was conducted among a national sample of 1,983 adults. (a) suppose that a person is randomly selected from the population and found to identify as a democrat. what is the probability that this person does not agree with the idea of evolution? (b) suppose that a political independent is randomly selected from the population. what is the probability that this person does not agree with the idea of evolution? (c) suppose that a person is randomly selected from the population and found to identify as a republican. what is the probability that this person agrees with the idea of evolution? (d) suppose that a person is randomly selected from the population and found to support the idea of evolution. what is the probability that this person identifies as a republican? 2.20 cystic fibrosis testing. the prevalence of cystic fibrosis in the united states is approximately 1 in 3,500 births. various screening strategies for cf exist. one strategy uses dried blood samples to check the levels of immunoreactive trypsogen (irt); irt levels are commonly elevated in newborns with cf. the sensitivity of the irt screen is 87% and the specificity is 99%. (a) in a hypothetical population of 100,000, how many individuals would be expected to test positive? of those who test positive, how many would be true positives? calculate the ppv of irt. (b) in order to account for lab error or physiological fluctuations in irt levels, infants who tested positive on the initial irt screen are asked to return for another irt screen at a later time, usually two weeks after the first test. this is referred to as an irt/irt screening strategy. calculate the ppv of irt/irt. 2.21 mumps. mumps is a highly contagious viral infection that most often occurs in children, but can affect adults, particularly if they are living in shared living spaces such as college dormitories. it is most recognizable by the swelling of salivary glands at the side of the face under the ears, but earlier symptoms include headaches, fever, and joint pain. suppose a college student at a university presents to a physician with symptoms of headaches, fever, and joint pain. let a= {headaches, fever, and joint pain}, and suppose that the possible disease state of the patient can be partitioned into: b1= normal,b2= common cold, b3= mumps. from clinical experience, the physician estimates p(ajbi):p(ajb1) = 0:001,p(ajb2) = 0:70,p(ajb3) = 0:95. the physician, aware that some students have contracted the mumps, then estimates that for students at this university,p(b1) = 0:95,p(b2) = 0:025, andp(b3) = 0:025. given the previous symptoms, which of the disease states is most likely? 2.22 twins. about 30% of human twins are identical, and the rest are fraternal. identical twins are necessarily the same sex – half are males and the other half are females. one-quarter of fraternal twins are both male, one-quarter both female, and one-half are mixes: one male, one female. you have just become a parent of twins and are told they are both girls. given this information, what is the probability that they are identical? 134 chapter 2. probability 2.23 iq testing. a psychologist conducts a study on intelligence in which participants are asked to take an iq test consisting of nquestions, each with mchoices. (a) one thing the psychologist must be careful about when analyzing the results is accounting for lucky guesses. suppose that for a given question a particular participant either knows the answer or guesses. the participant knows the correct answer with probability p, and does not know the answer (and therefore will have to guess) with probability 1 \u0000p. the participant guesses completely randomly. what is the conditional probability that the participant knew the answer to a question, given that they answered it correctly? (b) about 1 in 1,100 people have iqs over 150. if a subject receives a score of greater than some specified amount, they are considered by the psychologist to have an iq over 150. but the psychologist’s test is not perfect. although all individuals with iq over 150 will definitely receive such a score, individuals with iqs less than 150 can also receive such scores about 0.1% of the time due to lucky guessing. given that a subject in the study is labeled as having an iq over 150, what is the probability that they actually have an iq below 150? 2.24 breast cancer and age. the strongest risk factor for breast cancer is age; as a woman gets older, her risk of developing breast cancer increases. the following table shows the average percentage of american women in each age group who develop breast cancer, according to statistics from the national cancer institute. for example, approximately 3.56% of women in their 60’s get breast cancer. age group prevalence 30 - 40 0.0044 40 - 50 0.0147 50 - 60 0.0238 60 - 70 0.0356 70 - 80 0.0382 a mammogram typically identifies a breast cancer about 85% of the time, and is correct 95% of the time when a woman does not have breast cancer. (a) calculate the ppv for each age group. describe any trend(s) you see in the ppv values as prevalence changes. explain the reason for the trend(s) in language that someone who has not taken a statistics course would understand. (b) suppose that two new mammogram imaging technologies have been developed which can improve the ppv associated with mammograms; one improves sensitivity to 99% (but specificity remains at 95%), while the other improves specificity to 99% (while sensitivity remains at 85%). which technology o ffers a higher increase in ppv? explain why. 2.5. exercises 135 2.25 prostate-specific antigen. prostate-specific antigen (psa) is a protein produced by the cells of the prostate gland. blood psa level is often elevated in men with prostate cancer, but a number of benign (not cancerous) conditions can also cause a man’s psa level to rise. the psa test for prostate cancer is a laboratory test that measures psa levels from a blood sample. the test measures the amount of psa in ng/ml (nanograms per milliliter of blood). the sensitivity and specificity of the psa test depend on the cuto ffvalue used to label a psa level as abnormally high. in the last decade, 4.0 ng/ml has been considered the upper limit of normal, and values 4.1 and higher were used to classify a psa test as positive. using this value, the sensitivity of the psa test is 20% and the specificity is 94%. the likelihood that a man has undetected prostate cancer depends on his age. this likelihood is also called the prevalence of undetected cancer in the male population. the following table shows the prevalence of undetected prostate cancer by age group. age group prevalence ppv npv <50 years 0.001 50 - 60 years 0.020 61 - 70 years 0.060 71 - 80 years 0.100 (a) calculate the missing ppv and npv values. (b) describe any trends you see in the ppv and npv values. (c) explain the reason for the trends in part b), in language that someone who has not taken a statistics course would understand. (d) the cuto fffor a positive test is somewhat controversial. explain, in your own words, how lowering the cutofffor a positive test from 4.1 ng/ml to 2.5 ng/ml would a ffect sensitivity and specificity. 136 chapter 2. probability 2.5.3 extended example 2.26 eye color. one of the earliest models for the genetics of eye color was developed in 1907, and proposed a single-gene inheritance model, for which brown eye color is always dominant over blue eye color. suppose that in the population, 25% of individuals are homozygous dominant ( bb), 50% are heterozygous ( bb), and 25% are homozygous recessive ( bb). (a) suppose that two parents have brown eyes. what is the probability that their first child has blue eyes? (b) does the probability change if it is now known that the paternal grandfather had blue eyes? justify your answer. (c) given that their first child has brown eyes, what is the probability that their second child has blue eyes? ignore the condition given in part (b). 2.27 colorblindness. the most common form of colorblindness is a recessive, sex-linked hereditary condition caused by a defect on the x chromosome. females are xx, while males are xy. individuals inherit one chromosome from each parent, with equal probability; for example, an individual has a 50% chance of inheriting their father’s x chromosome, and a 50% chance of inheriting their father’s y chromosome. if a male has an x chromosome with the defect, he is colorblind. however, a female with only one defective x chromosome will not be colorblind. thus, colorblindness is more common in males than females; 7% of males are colorblind but only 0.5% of females are colorblind. (a) assume that the x chromosome with the wild-type allele is x+and the one with the disease allele is x\u0000. what is the expected frequency of each possible female genotype: x+x+,x+x\u0000, andx\u0000x\u0000? what is the expected frequency of each possible male genotype: x+yandx\u0000y? (b) suppose that two parents are not colorblind. what is the probability that they have a colorblind child? 2.28 rapid feathering. sex linkage refers to the inheritance pattern that results from a mutation occurring on a gene located on a sex chromosome. a classic example of a sex-linked trait in humans is red-green color blindness; females can only be red-green colorblind if they have two copies of the mutation (one on each x chromosome), while a single copy of the mutation is su fficient to confer colorblindness in males (since males only have one x chromosome). in birds, females are the heterogametic sex (with sex chromosomes zw) and males are the homogametic sex (with sex chromosomes zz). a commonly known sex-linked trait in domestic chickens is the rapid feathering trait, which is carried on the z chromosome. chickens with the rapid feathering trait grow feathers at a faster rate; this di fference is especially pronounced within the first few days from hatching. the wild-type allelek\u0000is dominant to the mutant alelle k+; presence of the k\u0000allele produces slow feathering. females can be either genotype zk+worzk\u0000w. males can be either heterozygous ( zk+zk\u0000), homozygous for slow feathering (zk\u0000zk\u0000), or homozygous for rapid feathering ( zk+zk+). in a population of chickens, 9% of males are rapid feathering and 16% of females are rapid feathering. suppose that slow feathering chickens are mated. what is the probability that out of their 12 o ffspring, at least two are rapid feathering? 2.5. exercises 137 2.29 genetics of australian cattle dogs. australian cattle dogs are known to have a high prevalence of congenital deafness. deafness in both ears is referred to as bilateral deafness, while deafness in one ear is referred to as unilateral deafness. deafness in dogs is associated with the white spotting gene sthat controls the expression of coat and eye pigmentation. the dominant allele sproduces solid color, while the three recessive alleles contribute to increasing amounts of white in coat pigmentation: irish spotting ( si), piebald (sp), and extreme white piebald (sw). thespandswalleles are responsible for the distinctive australian cattle dog coat pattern of white hair evenly speckled throughout either a predominantly red or black coat. the dogs are born with white coats, and the speckled pattern develops as they age. while all australian cattle dogs have some combination of the spandswalleles, the gene displays incomplete penetrance such that individuals show some variation in phenotype despite having the same genotype. individuals with low penetrance of the alleles tend to have additional patterns on their coat, such as a dark \"mask\" around one or both eyes (in other words, a unilateral mask or a bilateral mask). high penetrance of the piebald alleles is associated with deafness. suppose that 40% of australian cattle dogs have black coats; these individuals are commonly referred to as \"blue heelers\" as opposed to \"red heelers\". among blue heelers, 35% of individuals have bilateral masks and 25% have unilateral masks. about 50% of red heelers exhibit no eye masking and 10% have bilateral masks. letmrepresent the event that an australian cattle dog has a facial mask, where m2represents a bilateral mask,m1represents a unilateral mask, and m0indicates lack of a mask. (a) calculate the probability an australian cattle dog has a facial mask and a black coat. (b) calculate the prevalence of bilateral masks in australian cattle dogs. (c) among australian cattle dogs with bilateral facial masks, what is the probability of being a red heeler? (d) unilateral deafness occurs in red heelers with probability 0.15, in both dogs that either lack facial masking or exhibit a unilateral mask; for both unmasked and unilaterally masked red heelers, 60% of dogs are not deaf. the overall prevalence of bilaterally masked australian cattle dogs with bilateral deafness and red coats is 1.2% and the overall prevalence of bilaterally masked australian cattle dogs with unilateral deafness and red coats is 4.5%; these prevalences are the same for australian cattle dogs with black coats. among blue heelers with either no facial masking or a unilateral mask, the probability of unilateral deafness is 0.05 and the probability of bilateral deafness is 0.01. letdrepresent the event that an australian cattle dog is deaf (i.e., deaf in at least one ear), where d2 represents bilateral deafness and d1represents unilateral deafness. i. what is the probability that an australian cattle dog has a bilateral mask, no hearing deficits, and a red coat? ii. calculate the proportion of bilaterally masked blue heelers without hearing deficits. iii. compare the prevalence of deafness between red heelers and blue heelers. iv. if a dog is known to have no hearing deficits, what is the probability it is a blue heeler? 138 chapter 3 distributions of random variables 3.1 random variables 3.2 binomial distribution 3.3 normal distribution 3.4 poisson distribution 3.5 distributions related to bernoulli trials 3.6 distributions for pairs of random variables 3.7 notes 3.8 exercises 139 when planning clinical research studies, investigators try to anticipate the results they might see under certain hypotheses. the treatments for some forms of cancer, such as advanced lung cancer, are only e ffective in a small percentage of patients: typically 20% or less. suppose that a study testing a new treatment will be conducted on 20 participants, where the working assumption is that 20% of the patients will respond to the treatment. how might the possible outcomes of the study be represented, along with their probabilities? it is possible to express various outcomes using the probability notation in the previous chapter, e.g. if awere the event that one patient responds to treatment, but this would quickly become unwieldy. instead, the anticipated outcome in the study can be represented as a random variable , which numerically summarizes the possible outcomes of a random experiment. for example, let xrepresent the number of patients who respond to treatment; a numerical value xcan be assigned to each possible outcome, and the probabilities of 1 ;2;:::;x patients having a good response can be expressed as p(x= 1);p(x= 2);:::;p (x=x). the distribution of a random variable specifies the probability of each possible outcome associated with the random variable. this chapter will begin by outlining general properties of random variables and their distributions. the rest of the chapter discusses specific named distributions that are commonly used throughout probability and statistics. for labs, slides, and other resources, please visit www.openintro.org/book/biostat 140 chapter 3. distributions of random v ariables 3.1 random variables 3.1.1 distributions of random variables formally, a random variable assigns numerical values to the outcome of a random phenomenon, and is usually written with a capital letter such as x,y, orz. if a coin is tossed three times, the outcome is the sequence of observed heads and tails. one such outcome might be tth: tails on the first two tosses, heads on the third. if the random variable xis the number of heads for the three tosses, x= 1; ifyis the number of tails, then y= 2. for the sequence tht, only the order has changed, but the values of xandyremain the same. for the sequence hhh, however, x= 3 andy= 0. even in this simple setting, is possible to define other random variables; for example, if zis the toss when the first h occurs, then z= 3 for the first set of tosses (tth) and 1 for the third set (hhh). figure 3.1: possible outcomes for number of heads in three tosses of a coin. if probabilities can be assigned to the outcomes in a random phenomenon or study, then those can be used to assign probabilities to values of a random variable. using independence, p(hhh) = (1=2)3= 1=8. sincexin the above example can only be three if the three tosses are all heads,p(x= 3) = 1=8. the distribution of a random variable is the collection of probabilities for all of the variable’s unique values. figure 3.1 shows the eight possible outcomes when a coin is cossed three times: ttt, htt, tht, tth, hht, hth, thh, hhh. for the first set of tosses, x= 0; for the next three,x= 1, thenx= 2 for the following three tosses and x= 3 for the last set (hhh). using independence again, each of the 8 outcomes have probability 1/8, so p(x= 0) =p(x= 3) = 1=8 andp(x= 1) =p(x= 2) = 3=8. figure 3.2 shows the probability distribution for x. probability distributions for random variables follow the rules for probability; for instance, the sum of the probabilities must be 1.00. the possible outcomes of xare labeled with a corresponding lower case letter xand subscripts. the values of x are x1= 0,x2= 1,x3= 2, andx4= 3; these occur with probabilities 1 =8, 3=8, 3=8 and 1=8. i 1 2 3 4 total xi 0 1 2 3 – p(x=xi) 1/8 3/8 3/8 1/8 8/8 = 1.00 figure 3.2: tabular form for the distribution of the number of heads in three coin tosses. 3.1. random v ariables 141 0 1 2 3probabilities 0.00.10.20.30.4 figure 3.3: bar plot of the distribution of the number of heads in three coin tosses. bar graphs can be used to show the distribution of a random variable. figure 3.3 is a bar graph of the distribution of xin the coin tossing example. when bar graphs are used to show the distribution of a dataset, the heights of the bars show the frequency of observations; in contrast, bar heights for a probability distribution show the probabilities of possible values of a random variable. xis an example of a discrete random variable since it takes on a finite number of values.1a continuous random variable can take on any real value in an interval. in the hypothetical clinical study described at the beginning of this section, how unlikely would it be for 12 or more patients to respond to the treatment, given that only 20% of patients are expected to respond? suppose xis a random variable that will denote the possible number of responding patients, out of a total of 20. xwill have the same probability distribution as the number of heads in a 20 tosses of a weighted coin, where the probability of landing heads is 0.20. the graph of the probability distribution for xin figure 3.4 can be used to approximate this probability. the event of 12 or more consists of nine values (12, 13, . . . , 20); the graph shows that the probabilities for each value is extremely small, so the chance of 12 or more responses must be less than 0.01.2 0 5 10 15 200.000.050.100.150.20 xprobability figure 3.4: bar plot of the distribution of the number of responses in a study with 20 participants and response probability 0.20 1some discrete random variables have an infinite number of possible values, such as all the non-negative integers. 2formulas in section 3.2 can be used to show that the exact probability is slightly larger than 0.0001. 142 chapter 3. distributions of random v ariables 3.1.2 expectation just like distributions of data, distributions of random variables also have means, variances, standard deviations, medians, etc.; these characteristics are computed a bit di fferently for random variables. the mean of a random variable is called its expected value and written e(x). to calculate the mean of a random variable, multiply each possible value by its corresponding probability and add these products. expected value of a discrete random variable ifxtakes on outcomes x1, ...,xkwith probabilities p(x=x1), ...,p(x=xk), the expected value ofxis the sum of each outcome multiplied by its corresponding probability: e(x) =x1p(x=x1) +\u0001\u0001\u0001+xkp(x=xk) =kx i=1xip(x=xi): (3.1) the greek letter \u0016may be used in place of the notation e(x). example 3.2 calculate the expected value of x, wherexrepresents the number of heads in three tosses of a fair coin. xcan take on values 0, 1, 2, and 3. the probability of each xkis given in figure 3.2. e(x) =x1p(x=x1) +\u0001\u0001\u0001+xkp(x=xk) = (0)(p(x= 0)) + (1)(p(x= 1)) + (2)(p(x= 2)) + (3)(p(x= 3)) = (0)(1=8) + (1)(3=8) + (2)(3=8) + (3)(1=8) = 12=8 = 1:5: the expected value of xis 1.5. the expected value for a random variable represents the average outcome. for example, e(x) = 1:5 represents the average number of heads in three tosses of a coin, if the three tosses were repeated many times.3it often happens with discrete random variables that the expected value is not precisely one of the possible outcomes of the variable.e(x) expected value ofx guided practice 3.3 calculate the expected value of y, whereyrepresents the number of heads in three tosses of an unfair coin, where the probability of heads is 0.70.4 3the expected value e(x) can also be expressed as \u0016, e.g.\u0016= 1:5 4first, calculate the probability distribution. p(y= 0) = (1\u00000:70)3= 0:027 andp(y= 3) = (0:70)3= 0:343:note that there are three ways to obtain 1 head (htt, tht, tth), thus, p(y= 1) = (3)(0 :70)(1\u00000:70)2= 0:189. by the same logic, p(y= 2) = (3)(0:70)2(1\u00000:70) = 0:441. thus,e(y) = (0)(0:027)+(1)(0:189)+(2)(0:441)+(3)(0:343) = 2:1. the expected value ofyis 2.1. 3.1. random v ariables 143 3.1.3 variability of random variables the variability of a random variable can be described with variance and standard deviation. for data, the variance is computed by squaring deviations from the mean ( xi\u0000\u0016) and then averaging over the number of values in the dataset (section 1.4.2). in the case of a random variable, the squared deviations from the mean of the random variable are used instead, and their sum is weighted by the corresponding probabilities. this weighted sum of squared deviations equals the variance; the standard deviation is the square root of the variance. variance of a discrete random variable ifxtakes on outcomes x1, ...,xkwith probabilities p(x=x1), . . . ,p(x=xk) and expected value \u0016=e(x), then the variance of x, denoted by var( x) or\u001b2, is var(x) = (x1\u0000\u0016)2p(x=x1) +\u0001\u0001\u0001+ (xk\u0000\u0016)2p(x=xk) =kx i=1(xi\u0000\u0016)2p(x=xi): (3.4) the standard deviation of x, labeledsd(x) or\u001b, is the square root of the variance.var(x) variance ofx the variance of a random variable can be interpreted as the expectation of the terms ( xi\u0000\u0016)2; i.e.,\u001b2=e(x\u0000\u0016)2. while this compact form is not useful for direct computation, it can be helpful for understanding the concept of variability in the context of a random variable; variance is simply the average of the deviations from the mean. example 3.5 compute the variance and standard deviation of x, the number of heads in three tosses of a fair coin. in the formula for the variance, k= 4 and\u0016x=e(x) = 1:5. \u001b2 x= (x1\u0000\u0016x)2p(x=x1) +\u0001\u0001\u0001+ (x4\u0000\u0016)2p(x=x4) = (0\u00001:5)2(1=8) + (1\u00001:5)2(3=8) + (2\u00001:5)2(3=8) + (3\u00001:5)2(1=8) = 3=4: the variance is 3 =4 = 0:75 and the standard deviation isp 3=4 = 0:866. the coin tossing scenario provides a simple illustration of the mean and variance of a random variable. for the rest of this section, a more realistic example will be discussed—calculating expected health care costs. in most typical health insurance plans in the united states, members of the plan pay annually in three categories: a monthly premium, a deductible amount that members pay each year before the insurance covers service, and “out-of-pocket” costs which include co-payments for each physician visit or prescription.5picking a new health plan involves estimating costs for the next year based on a person’s best guess at the type and number of services that will be needed. 5the deductible also includes care and supplies that are not covered by insurance. 144 chapter 3. distributions of random v ariables in 2015, harvard university o ffered several alternative plans to its employees. in the health maintenance organization (hmo) plan for employees earning less than $70,000 per year, the monthly premium was $79, and the co-payment for each o ffice visit or physical therapy session was $20. after a new employee examined her health records for the last 10 years, she noticed that in three of the 10 years, she visited the o ffice of her primary care physician only once, for one annual physical. in four of the 10 years, she visited her physician three times: once for a physical, and twice for cases of the flu. in two of the years, she had four visits. in one of the 10 years, she experienced a knee injury that required 3 o ffice visits and 5 physical therapy sessions. example 3.6 ignoring the cost of prescription drugs, over-the-counter medications, and the annual deductible amount, calculate the expectation and the standard deviation of the expected annual health care cost for this employee. let the random variable xdenote annual health care costs, where xirepresents the costs in a year forinumber of visits. if the last ten years are an accurate picture of annual costs for this employee, xwill have four possible values. the total cost of the monthly premiums in a single year is 12 \u0002$79 = $948. the cost of each visit is $20, so the total visit cost for a year is $20 times the number of visits. for example, the first column in the table contains information about the years in which the employee had one o ffice visit. adding the $948 for the annual premium and $20 for one visit results inx1= $968;p(x=xi) = 3=10 = 0:30. i 1 2 3 4 sum number of visits 1 3 4 8 xi 968 1008 1028 1108 p(x=xi) 0.30 0.40 0.20 0.10 1.00 xip(x=xi) 290.40 403.20 205.60 110.80 1010.00 the expected cost of health care for a year,p ixip(x=xi), is\u0016= $1010:00. i 1 2 3 4 sum number of visits 1 3 4 8 xi 968 1008 1028 1108 p(x=xi) 0.30 0.40 0.20 0.10 1.00 (xi)p(x=xi) 290.40 403.20 205.60 110.80 1010.00 xi\u0000\u0016 -42.00 -2.00 18.00 98.00 (xi\u0000\u0016)21764.00 4.00 324.00 9604 (xi\u0000\u0016)2p(x=xi) 529.20 1.60 64.80 960.40 1556.00 the variance of x,p i(xi\u0000\u0016)2p(x=xi), is\u001b2= 1556:00, and the standard deviation is \u001b= $39:45.6 6note that the standard deviation always has the same units as the original measurements. 3.1. random v ariables 145 3.1.4 linear combinations of random variables sums of random variables arise naturally in many problems. in the health insurance example, the amount spent by the employee during her next five years of employment can be represented as x1+x2+x3+x4+x5, wherex1is the cost of the first year, x2the second year, etc. if the employee’s domestic partner has health insurance with another employer, the total annual cost to the couple would be the sum of the costs for the employee ( x) and for her partner ( y), orx+y. in each of these examples, it is intuitively clear that the average cost would be the sum of the average of each term. sums of random variables represent a special case of linear combinations of variables. linear combinations of random variables and their expected values ifxandyare random variables, then a linear combination of the random variables is given by ax+by; whereaandbare constants. the mean of a linear combination of random variables is e(ax+by) =ae(x) +be(y) =a\u0016x+b\u0016y: the formula easily generalizes to a sum of any number of random variables. for example, the average health care cost for 5 years, given that the cost for services remains the same, is e(x1+x2+x3+x4+x5) =e(5x1) = 5e(x1) = (5)(1010) = $5 ;050: the formula implies that for a random variable z,e(a+z) =a+e(z). this could have been used when calculating the average health costs for the employee by defining aas the fixed cost of the premium ( a= $948) and zas the cost of the physician visits. thus, the total annual cost for a year could be calculated as: e(a+z) =a+e(z) = $948 +e(z) = $948 +:30(1\u0002$20) +:40(3\u0002$20) + :20(4\u0002$20) + 0:10(8\u0002$20) = $1;010:00. guided practice 3.7 suppose the employee will begin a domestic partnership in the next year. although she and her companion will begin living together and sharing expenses, they will each keep their existing health insurance plans; both, in fact, have the same plan from the same employer. in the last five years, her partner visited a physician only once in four of the ten years, and twice in the other six years. calculate the expected total cost of health insurance to the couple in the next year.7 calculating the variance and standard deviation of a linear combination of random variables requires more care. the formula given here requires that the random variables in the linear combination be independent, such that an observation on one of the variables provides no information about the value of the other variable. 7letxrepresent the costs for the employee and yrepresent the costs for her partner. e(x) = $1;010:00, as previously calculated.e(y) = 948+0:4(1\u0002$20)+0:6(2\u0002$20) = $980 :00. thus,e(x+y) =e(x)+e(y) = $1;010:00+$980:00 = $1;990:00. 146 chapter 3. distributions of random v ariables variability of linear combinations of random variables var(ax+by) =a2var(x) +b2var(y): this equation is valid only if the random variables are independent of each other. for the transformation a+bz, the variance is b2var(z), since a constant ahas variance 0. whenb= 1, variance of a+zis var(z)—adding a constant to a random variable has no e ffect on the variability of the random variable. example 3.8 calculate the variance and standard deviation for the combined cost of next year’s health care for the two partners, assuming that the costs for each person are independent. letxrepresent the sum of costs for the employee and ythe sum of costs for her partner. first, calculate the variance of health care costs for the partner. the partner’s costs are the sum of the annual fixed cost and the variable annual costs, so the variance will simply be the variance of the variable costs. if zrepresents the component of the variable costs, e(z) = 0:4(1\u0002$20)+0:6(2\u0002$20) = $8 + $24 = $32. thus, the variance of zequals var(z) = 0:4(20\u000032)2+ 0:6(40\u000032)2= 96: under the assumption of independence, var( x+y) = var(x) + var(y) = 1556 + 96 = 1652, and the standard deviation isp 1652 = $40:64. the example of health insurance costs has been simplified to make the calculations clearer. it ignores the fact that many plans have a deductible amount, and that plan members pay for services at different rates before and after the deductible has been reached. often, insured individuals no longer need to pay for services at all once a maximum amount has been reached in a year. the example also assumes that the proportions of number of physician visits per year, estimated from the last 10 years, can be treated as probabilities measured without error. had a di fferent timespan been chosen, the proportions might well have been di fferent. it also relies on the assumption that health care costs for the two partners are independent. two individuals living together may pass on infectious diseases like the flu, or may participate together in activities that lead to similar injuries, such as skiing or long distance running. section 3.6 shows how to adjust a variance calculation when independence is unrealistic. 3.2. binomial distribution 147 3.2 binomial distribution the hypothetical clinical study and coin tossing example discussed earlier in this chapter are both examples of experiments that can be modeled with a binomial distribution. the binomial distribution is a more general case of another named distribution, the bernoulli distribution. 3.2.1 bernoulli distribution psychologist stanley milgram began a series of experiments in 1963 to study the e ffect of authority on obedience. in a typical experiment, a participant would be ordered by an authority figure to give a series of increasingly severe shocks to a stranger. milgram found that only about 35% of people would resist the authority and stop giving shocks before the maximum voltage was reached. over the years, additional research suggested this number is approximately consistent across communities and time.8 each person in milgram’s experiment can be thought of as a trial . suppose that a trial is labeled a success if the person refuses to administer the worst shock. if the person does administer the worst shock, the trial is a failure . the probability of a success can be written as p= 0:35. the probability of a failure is sometimes denoted with q= 1\u0000p. when an individual trial only has two possible outcomes, it is called a bernoulli random variable . it is arbitrary as to which outcome is labeled success. bernoulli random variables are often denoted as 1for a success and 0for a failure. suppose that ten trials are observed, of which 6 are successes and 4 are failures: 0 1 1 1 1 0 1 1 0 0 . the sample proportion ,ˆp, is the sample mean of these observations: ˆp=# of successes # of trials=0 + 1 + 1 + 1 + 1 + 0 + 1 + 1 + 0 + 0 10= 0:6: since 0and 1are numerical outcomes, the mean and standard deviation of a bernoulli random variable can be defined. if pis the true probability of a success, then the mean of a bernoulli random variable xis given by \u0016=e[x] =p(x= 0)\u00020 +p(x= 1)\u00021 = (1\u0000p)\u00020 +p\u00021 = 0 +p=p: similarly, the variance of xcan be computed: \u001b2=p(x= 0)(0\u0000p)2+p(x= 1)(1\u0000p)2 = (1\u0000p)p2+p(1\u0000p)2=p(1\u0000p): the standard deviation is \u001b=p p(1\u0000p). 8find further information on milgram’s experiment at www.cnr.berkeley.edu/ucce50/ag-labor/7article/article35.htm. 148 chapter 3. distributions of random v ariables bernoulli random variable ifxis a random variable that takes value 1 with probability of success pand 0 with probability 1\u0000p, thenxis a bernoulli random variable with mean pand standard deviationp p(1\u0000p). supposexrepresents the outcome of a single toss of a fair coin, where heads is labeled success. xis a bernoulli random variable with probability of success p= 0:50; this can be expressed as x\u0018bern(p), or specifically, x\u0018bern(0:50). it is essential to specify the probability of success when characterizing a bernoulli random variable. for example, although the outcome of a single toss of an unfair coin can also be represented by a bernoulli, it will have a di fferent probability distribution since pdoes not equal 0.50 for an unfair coin. the success probability pis the parameter of the distribution, and identifies a specific bernoulli distribution out of the entire family of bernoulli distributions where pcan be any value between 0 and 1 (inclusive).bern(p) bernoulli dist. withpprob. of success example 3.9 suppose that four individuals are randomly selected to participate in milgram’s experiment. what is the chance that there will be exactly one successful trial, assuming independence between trials? suppose that the probability of success remains 0.35. consider a scenario in which there is one success (i.e., one person refuses to give the strongest shock). label the individuals as a,b,c, andd: p(a=refuse; b=shock; c=shock; d=shock ) =p(a=refuse )p(b=shock )p(c=shock )p(d=shock ) = (0:35)(0:65)(0:65)(0:65) = (0:35)1(0:65)3= 0:096: however, there are three other possible scenarios: either b,c, ordcould have been the one to refuse. in each of these cases, the probability is also (0 :35)1(0:65)3. these four scenarios exhaust all the possible ways that exactly one of these four people could refuse to administer the most severe shock, so the total probability of one success is (4)(0 :35)1(0:65)3= 0:38. 3.2. binomial distribution 149 3.2.2 the binomial distribution the bernoulli distribution is unrealistic in all but the simplest of settings. however, it is a useful building block for other distributions. the binomial distribution describes the probability of having exactly ksuccesses in nindependent bernoulli trials with probability of a success p. in example 3.9, the goal was to calculate the probability of 1 success out of 4 trials, with probability of success 0.35 ( n= 4,k= 1,p= 0:35). like the bernoulli distribution, the binomial is a discrete distribution, and can take on only a finite number of values. a binomial variable has values 0, 1, 2, . . . , n. a general formula for the binomial distribution can be developed from re-examining example 3.9. there were four individuals who could have been the one to refuse, and each of these four scenarios had the same probability. thus, the final probability can be written as: [# of scenarios]\u0002p(single scenario :) (3.10) the first component of this equation is the number of ways to arrange the k= 1 successes among then= 4 trials. the second component is the probability of any of the four (equally probable) scenarios. considerp(single scenario) under the general case of ksuccesses and n\u0000kfailures in the n trials. in any such scenario, the multiplication rule for independent events can be applied: pk(1\u0000p)n\u0000k: secondly, there is a general formula for the number of ways to choose ksuccesses in ntrials, i.e. arrange ksuccesses and n\u0000kfailures: n k! =n! k!(n\u0000k)!: the quantity\u0000n k\u0001is read n choose k .9the exclamation point notation (e.g. k!) denotes a factorial expression.10 using the formula, the number of ways to choose k= 1 successes in n= 4 trials can be computed as: 4 1! =4! 1!(4\u00001)!=4! 1!3!=4\u00023\u00022\u00021 (1)(3\u00022\u00021)= 4: substituting nchoosekfor the number of scenarios and pk(1\u0000p)n\u0000kfor the single scenario probability in equation (3.10) yields the general binomial formula. 9other notation for nchoosekincludesnck,ckn, andc(n;k). 100! = 1 , 1! = 1, 2! = 2 \u00021 = 2,:::,n! =n\u0002(n\u00001)\u0002:::2\u00021. 150 chapter 3. distributions of random v ariables binomial distribution suppose the probability of a single trial being a success is p. the probability of observing exactlyksuccesses in nindependent trials is given by p(x=k) = n k! pk(1\u0000p)n\u0000k=n! k!(n\u0000k)!pk(1\u0000p)n\u0000k: (3.11) additionally, the mean, variance, and standard deviation of the number of observed successes are, respectively \u0016=np \u001b2=np(1\u0000p) \u001b=p np(1\u0000p): (3.12) a binomial random variable xcan be expressed as x\u0018bin(n;p).bin(n;p) binomial dist. withntrials &pprob. of success is it binomial? four conditions to check. (1) the trials are independent. (2) the number of trials, n, is fixed. (3) each trial outcome can be classified as a success orfailure . (4) the probability of a success, p, is the same for each trial. example 3.13 what is the probability that 3 of 8 randomly selected participants will refuse to administer the worst shock? first, check the conditions for applying the binomial model. the number of trials is fixed ( n= 8) and each trial outcome can be classified as either success or failure. the sample is random, so the trials are independent, and the probability of success is the same for each trial. for the outcome of interest, k= 3 successes occur in n= 8 trials, and the probability of a success is p= 0:35. thus, the probability that 3 of 8 will refuse is given by p(x= 3) = 8 3! (0:35)3(1\u00000:35)8\u00003=8! 3!(8\u00003)!(0:35)3(1\u00000:35)8\u00003 = (56)(0:35)3(0:65)5 = 0:28: 3.2. binomial distribution 151 example 3.14 what is the probability that at most 3 of 8 randomly selected participants will refuse to administer the worst shock? the event of at most 3 out of 8 successes can be thought of as the combined probability of 0, 1, 2, and 3 successes. thus, the probability that at most 3 of 8 will refuse is given by: p(x\u00143) =p(x= 0) +p(x= 1) +p(x= 2) +p(x= 3) = 8 0! (0:35)0(1\u00000:35)8\u00000+ 8 1! (0:35)1(1\u00000:35)8\u00001 + 8 2! (0:35)2(1\u00000:35)8\u00002+ 8 3! (0:35)3(1\u00000:35)8\u00003 = (1)(0:35)0(1\u00000:35)8+ (8)(0:35)1(1\u00000:35)7 + (28)(0:35)2(1\u00000:35)6+ (56)(0:35)3(1\u00000:35)5 = 0:706: example 3.15 if 40 individuals were randomly selected to participate in the experiment, how many individuals would be expected to refuse to administer the worst shock? what is the standard deviation of the number of people expected to refuse? both quantities can directly be computed from the formulas in equation (3.12). the expected value (mean) is given by: \u0016=np= 40\u00020:35 = 14. the standard deviation is: \u001b=p np(1\u0000p) =p 40\u00020:35\u00020:65 = 3:02. guided practice 3.16 the probability that a smoker will develop a severe lung condition in their lifetime is about 0.30. suppose that 5 smokers are randomly selected from the population. what is the probability that (a) one will develop a severe lung condition? (b) that no more than one will develop a severe lung condition? (c) that at least one will develop a severe lung condition?11 11letp= 0:30;x\u0018bin(5;0:30). (a)p(x= 1) =\u00005 1\u0001(0:30)1(1\u00000:30)5\u00001= 0:36 (b)p(x\u00141) =p(x= 0) +p(x= 1) =\u00005 0\u0001(0:30)0(1\u00000:30)5\u00000+ 0:36 = 0:53 (c)p(x\u00151) = 1\u0000p(x= 0) = 1\u00000:36 = 0:83 152 chapter 3. distributions of random v ariables 3.3 normal distribution among the many distributions seen in practice, one is by far the most common: the normal distribution , which has the shape of a symmetric, unimodal bell curve. many variables are nearly normal, which makes the normal distribution useful for a variety of problems. for example, characteristics such as human height closely follow the normal distribution. 3.3.1 normal distribution model the normal distribution model always describes a symmetric, unimodal, bell-shaped curve. however, the curves can di ffer in center and spread; the model can be adjusted using mean and standard deviation. changing the mean shifts the bell curve to the left or the right, while changing the standard deviation stretches or constricts the curve. figure 3.5 shows the normal distribution with mean 0 and standard deviation 1 in the left panel and the normal distribution with mean 19 and standard deviation 4 in the right panel. figure 3.6 shows these distributions on the same axis. −3−2−10123y 7111519232731 figure 3.5: both curves represent the normal distribution; however, they di ffer in their center and spread. the normal distribution with mean 0 and standard deviation 1 is called the standard normal distribution . 0 10 20 30 figure 3.6: the normal models shown in figure 3.5 but plotted together and on the same scale. for any given normal distribution with mean \u0016and standard deviation \u001b, the distribution can be written as n(\u0016;\u001b);\u0016and\u001bare the parameters of the normal distribution. for example, n(0;1)n(\u0016;\u001b) normal dist. with mean\u0016 & st. dev.\u001brefers to the standard normal distribution, as shown in figure 3.5. unlike the bernoulli and binomial distributions, the normal distribution is a continuous distribution. 3.3. normal distribution 153 3.3.2 standardizing with z-scores the z-score of an observation quantifies how far the observation is from the mean, in units of z z-score, the standardized observationstandard deviation(s). if xis an observation from a distribution n(\u0016;\u001b), the z-score is mathematically defined as: z=x\u0000\u0016 \u001b: an observation equal to the mean has a z-score of 0. observations above the mean have positive z-scores, while observations below the mean have negative z-scores. for example, if an observation is one standard deviation above the mean, it has a z-score of 1; if it is 1.5 standard deviations below the mean, its z-score is -1.5. z-scores can be used to identify which observations are more extreme than others, and are especially useful when comparing observations from di fferent normal distributions. one observationx1is said to be more unusual than another observation x2if the absolute value of its z-score is larger than the absolute value of the other observation’s z-score: jz1j>jz2j. in other words, the further an observation is from the mean in either direction, the more extreme it is. example 3.17 the sat and the act are two standardized tests commonly used for college admissions in the united states. the distribution of test scores are both nearly normal. for the sat, n(1500;300); for the act,n(21;5). while some colleges request that students submit scores from both tests, others allow students the choice of either the act or the sat. suppose that one student scores an 1800 on the sat (student a) and another scores a 24 on the act (student b). a college admissions o fficer would like to compare the scores of the two students to determine which student performed better. calculate a z-score for each student; i.e., convert xto z. using\u0016sat= 1500,\u001bsat= 300, andxa= 1800, find student a’s z-score: za=xa\u0000\u0016sat \u001bsat=1800\u00001500 300= 1: for student b: zb=xb\u0000\u0016act \u001bact=24\u000021 5= 0:6: student a’s score is 1 standard deviation above average on the sat, while student b’s score is 0.6 standard deviations above the mean on the act. as illustrated in figure 3.7, student a’s score is more extreme, indicating that student a has scored higher with respect to other scores than student b. the z-score the z-score of an observation quantifies how far the observation is from the mean, in units of standard deviation(s). the z-score for an observation xthat follows a distribution with mean \u0016and standard deviation \u001bcan be calculated using z=x\u0000\u0016 \u001b: 154 chapter 3. distributions of random v ariables x9001200150018002100student a 1116212631student b figure 3.7: scores of students a and b plotted on the distributions of sat and act scores. example 3.18 how high would a student need to score on the act to have a score equivalent to student a’s score of 1800 on the sat? as shown in example 3.7, a score of 1800 on the sat is 1 standard deviation above the mean. act scores are normally distributed with mean 21 and standard deviation 5. to convert a value from the standard normal curve (z) to one on a normal distribution n(\u0016;\u001b): x=\u0016+z\u001b: thus, a student would need a score of 21 + 1(5) = 26 on the act to have a score equivalent to 1800 on the sat. guided practice 3.19 systolic blood pressure (sbp) for adults in the united states aged 18-39 follow an approximate normal distribution, n(115;17:5). as age increases, systolic blood pressure also tends to increase. mean systolic blood pressure for adults 60 years of age and older is 136 mm hg, with standard deviation 40 mm hg. systolic blood pressure of 140 mm hg or higher is indicative of hypertension (high blood pressure). (a) how many standard deviations away from the mean is a 30-year-old with systolic blood pressure of 125 mm hg? (b) compare how unusual a systolic blood pressure of 140 mm hg is for a 65-year-old, versus a 30-year-old.12 12(a) calculate the z-score:x\u0000\u0016 \u001b=125\u0000115 17:5= 0:571. a 30-year-old with systolic blood pressure of 125 mm hg is about 0.6 standard deviations above the mean. (b) for x1= 140 mm hg: z1=x1\u0000\u0016 \u001b=140\u0000115 17:5= 1:43. forx2= 140 mm hg: z2=x2\u0000\u0016 \u001b=140\u0000136 40= 0:1. while an sbp of 140 mm hg is almost 1.5 standard deviations above the mean for a 30-year-old, it is only 0.1 standard deviations above the mean for a 65-year-old. 3.3. normal distribution 155 3.3.3 the empirical rule the empirical rule (also known as the 68-95-99.7 rule) states that for a normal distribution, almost all observations will fall within three standard deviations of the mean. specifically, 68% of observations are within one standard deviation of the mean, 95% are within two sd’s, and 99.7% are within three sd’s. μ−3σμ−2σμ−σμμ+σμ+2σμ+3σ99.7%95%68% figure 3.8: probabilities for falling within 1, 2, and 3 standard deviations of the mean in a normal distribution. while it is possible for a normal random variable to take on values 4, 5, or even more standard deviations from the mean, these occurrences are extremely rare if the data are nearly normal. for example, the probability of being further than 4 standard deviations from the mean is about 1-in30,000. 3.3.4 calculating normal probabilities the normal distribution is a continuous probability distribution. recall from section 2.1.5 that the total area under the density curve is always equal to 1, and the probability that a variable has a value within a specified interval is the area under the curve over that interval. by using either statistical software or normal probability tables, the normal model can be used to identify a probability or percentile based on the corresponding z-score (and vice versa). negative z positive z figure 3.9: the area to the left of zrepresents the percentile of the observation. 156 chapter 3. distributions of random v ariables second decimal place of z z 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.0 0.5000 0.5040 0.5080 0.5120 0.5160 0.5199 0.5239 0.5279 0.5319 0.5359 0.1 0.5398 0.5438 0.5478 0.5517 0.5557 0.5596 0.5636 0.5675 0.5714 0.5753 0.2 0.5793 0.5832 0.5871 0.5910 0.5948 0.5987 0.6026 0.6064 0.6103 0.6141 0.3 0.6179 0.6217 0.6255 0.6293 0.6331 0.6368 0.6406 0.6443 0.6480 0.6517 0.4 0.6554 0.6591 0.6628 0.6664 0.6700 0.6736 0.6772 0.6808 0.6844 0.6879 0.5 0.6915 0.6950 0.6985 0.7019 0.7054 0.7088 0.7123 0.7157 0.7190 0.7224 0.6 0.7257 0.7291 0.7324 0.7357 0.7389 0.7422 0.7454 0.7486 0.7517 0.7549 0.7 0.7580 0.7611 0.7642 0.7673 0.7704 0.7734 0.7764 0.7794 0.7823 0.7852 0.8 0.7881 0.7910 0.7939 0.7967 0.7995 0.8023 0.8051 0.8078 0.8106 0.8133 0.9 0.8159 0.8186 0.8212 0.8238 0.8264 0.8289 0.8315 0.8340 0.8365 0.8389 1.0 0.8413 0.8438 0.8461 0.8485 0.8508 0.8531 0.8554 0.8577 0.8599 0.8621 1.1 0.8643 0.8665 0.8686 0.8708 0.8729 0.8749 0.8770 0.8790 0.8810 0.8830 ::::::::::::::::::::::::::::::::: figure 3.10: a section of the normal probability table. the percentile for a normal random variable with z= 0:43 has been highlighted , and the percentile closest to 0.8000 has also been highlighted . anormal probability table is given in appendix b.1 on and abbreviated in figure 3.10. this table can be used to identify the percentile corresponding to any particular zscore; for instance, the percentile of z= 0:43 is shown in row 0 :4 and column 0 :03 in figure 3.10: 0.6664, or the 66 :64thpercentile. first, find the proper row in the normal probability table up through the first decimal, and then determine the column representing the second decimal value. the intersection of this row and column is the percentile of the observation. this value also represents the probability that the standard normal variable z takes on a value of 0.43 or less; i.e. p(z\u00140:43) = 0:6664. the table can also be used to find the z-score associated with a percentile. for example, to identify z for the 80thpercentile, look for the value closest to 0.8000 in the middle portion of the table: 0.7995. the z-score for the 80thpercentile is given by combining the row and column z values: 0.84. example 3.20 student a from example 3.17 earned a score of 1800 on the sat, which corresponds to z= 1. what percentile is this score associated with? in this context, the percentile is the percentage of people who earned a lower sat score than student a. from the normal table, zof 1.00 is 0.8413. thus, the student is in the 84thpercentile of test takers. this area is shaded in figure 3.11. guided practice 3.21 determine the proportion of sat test takers who scored better than student a on the sat.13 13if 84% had lower scores than student a, the number of people who had better scores must be 16%. 3.3. normal distribution 157 60090012001500180021002400 figure 3.11: the normal model for sat scores, with shaded area representing scores below 1800. 3.3.5 normal probability examples there are two main types of problems that involve the normal distribution: calculating probabilities from a given value (whether xorz), or identifying the observation that corresponds to a particular probability. example 3.22 cumulative sat scores are well-approximated by a normal model, n(1500;300). what is the probability that a randomly selected test taker scores at least 1630 on the sat? for any normal probability problem, it can be helpful to start out by drawing the normal curve and shading the area of interest. 1630 to find the shaded area under the curve, convert 1630 to a z-score: z=x\u0000\u0016 \u001b=1630\u00001500 300=130 300= 0:43: look up the percentile of z= 0:43 in the normal probability table shown in figure 3.10 or in appendix b.1 on : 0.6664. however, note that the percentile describes those who had a z-score lower than 0.43, or in other words, the area below 0.43. to find the area abovez= 0:43, subtract the area of the lower tail from the total area under the curve, 1: 1.0000 0.6664 0.3336 = the probability that a student scores at least 1630 on the sat is 0.3336. 158 chapter 3. distributions of random v ariables discrete versus continuous probabilities recall that the probability of a continuous random variable equaling some exact value is always 0. as a result, for a continuous random variable x,p(x\u0014x) =p(x < x ) and p(x\u0015x) =p(x>x ). it is valid to state that p(x\u0015x) = 1\u0000p(x\u0014x) = 1\u0000p(x<x ). this is notthe case for discrete random variables. for example, for a discrete random variable y,p(y\u00152) = 1\u0000p(y <2) = 1\u0000p(y\u00141). it would be incorrect to claim that p(y\u00152) = 1\u0000p(y\u0014 2). guided practice 3.23 what is the probability of a student scoring at most 1630 on the sat?14 guided practice 3.24 systolic blood pressure for adults 60 years of age and older in the united states is approximately normally distributed: n(136;40). what is the probability of an adult in this age group having systolic blood pressure of 140 mm hg or greater?15 14this probability was calculated as part of example 3.22: 0.6664. a picture for this exercise is represented by the shaded area below “0.6664” in example 3.22. 15the z-score for this observation was calculated in exercise 3.19 as 0.1. from the table, the p(z\u00150:1) = 1\u00000:54 = 0:46. 3.3. normal distribution 159 example 3.25 the height of adult males in the united states between the ages of 20 and 62 is nearly normal, with mean 70 inches and standard deviation 3.3 inches.16what is the probability that a random adult male is between 5’9” and 6’2”? these heights correspond to 69 inches and 74 inches. first, draw the figure. the area of interest is an interval, rather than a tail area. 6974 to find the middle area, find the area to the left of 74; from that area, subtract the area to the left of 69. first, convert to z-scores: z74=x\u0000\u0016 \u001b=74\u000070 3:3= 1:21; z 62=x\u0000\u0016 \u001b=69\u000070 3:3=\u00000:30: from the normal probability table, the areas are respectively, 0 :8868 and 0:3821. the middle area is 0:8868\u00000:3821 = 0:5048. the probability of being between heights 5’9” and 6’2” is 0.5048. 0.8868 0.3821 0.5048 = guided practice 3.26 what percentage of adults in the united states ages 60 and older have blood pressure between 145 and 130 mm hg?17 16as based on a sample of 100 men, from the usda food commodity intake database. 17first calculate z-scores, then find the percent below 145 mm hg and below 130 mm hg: z145= 0:23!0:5890, z130=\u00000:15!0:4404 (area above). final answer: 0 :5890\u00000:4404 = 0:1486. 160 chapter 3. distributions of random v ariables example 3.27 how tall is a man with height in the 40thpercentile? first, draw a picture. the lower tail probability is 0.40, so the shaded area must start before the mean. 70 40% (0.40) determine the z-score associated with the 40thpercentile. because the percentile is below 50%, z will be negative. look for the probability inside the negative part of table that is closest to 0.40: 0.40 falls in row \u00000:2 and between columns 0 :05 and 0:06. since it falls closer to 0 :05, choose z=\u00000:25. convert the z-score to x, wherex\u0018n(70;3:3). x=\u0016+\u001bz= 70 + (\u00000:25)(3:3) = 69:18: a man with height in the 40thpercentile is 69.18 inches tall, or about 5’ 9”. guided practice 3.28 (a) what is the 95thpercentile for sat scores? (b) what is the 97 :5thpercentile of the male heights?18 18(a) look for 0.95 in the probability portion (middle part) of the normal probability table: row 1.6 and (about) column 0.05, i.e.z95= 1:65. knowing z95= 1:65,\u0016= 1500, and \u001b= 300, convert z to x: 1500 + (1:65)(300) = 1995. (b) similarly, findz97:5= 1:96, and convert to x:x97:5= 76:5 inches. 3.3. normal distribution 161 3.3.6 normal approximation to the binomial distribution the normal distribution can be used to approximate other distributions, such as the binomial distribution. the binomial formula is cumbersome when sample size is large, particularly when calculating probabilities for a large number of observations. under certain conditions, the normal distribution can be used to approximate binomial probabilities. this method was widely used when calculating binomial probabilities by hand was the only option. nowadays, modern statistical software is capable of calculating exact binomial probabilities even for very large n. the normal approximation to the binomial is discussed here since it is an important result that will be revisited in later chapters. consider the binomial model when probability of success is p= 0:10. figure 3.12 shows four hollow histograms for simulated samples from the binomial distribution using four di fferent sample sizes:n= 10;30;100;300. as the sample size increases from n= 10 ton= 300, the distribution is transformed from a blocky and skewed distribution into one resembling the normal curve. n = 100 2 4 6 n = 300246810 n = 10005101520 n = 3001020304050 figure 3.12: hollow histograms of samples from the binomial model when p= 0:10. the sample sizes for the four plots are n= 10, 30, 100, and 300, respectively. normal approximation of the binomial distribution the binomial distribution with probability of success pis nearly normal when the sample size nis sufficiently large such that npandn(1\u0000p) are both at least 10. the approximate normal distribution has parameters corresponding to the mean and standard deviation of the binomial distribution: \u0016=np \u001b =p np(1\u0000p) 162 chapter 3. distributions of random v ariables example 3.29 approximately 20% of the us population smokes cigarettes. a local government commissioned a survey of 400 randomly selected individuals to investigate whether their community might have a lower smoker rate than 20%. the survey found that 59 of the 400 participants smoke cigarettes. if the true proportion of smokers in the community is 20%, what is the probability of observing 59 or fewer smokers in a sample of 400 people? the desired probability is equivalent to the sum of the individual probabilities of observing k= 0, 1, ..., 58, or 59 smokers in a sample of n= 400:p(x\u001459). confirm that the normal approximation is valid:np= 400\u00020:20 = 80,n(1\u0000p) = 400\u00020:8 = 320. to use the normal approximation, calculate the mean and standard deviation from the binomial model: \u0016=np= 80 \u001b=p np(1\u0000p) = 8: convert 59 to a z-score: z=59\u000080 8=\u00002:63. use the normal probability table to identify the left tail area, which is 0.0043. this estimate is very close to the answer derived from the exact binomial calculation: p(k= 0 ork= 1 or\u0001\u0001\u0001ork= 59) =p(k= 0) +p(k= 1) +\u0001\u0001\u0001+p(k= 59) = 0:0041: however, even when the conditions for using the approximation are met, the normal approximation to the binomial tends to perform poorly when estimating the probability of a small range of counts. suppose the normal approximation is used to compute the probability of observing 69, 70, or 71 smokers in 400 when p= 0:20. in this setting, the exact binomial and normal approximation result in notably di fferent answers: the approximation gives 0.0476, while the binomial returns 0.0703. the cause of this discrepancy is illustrated in figure 3.13, which shows the areas representing the binomial probability (outlined) and normal approximation (shaded). notice that the width of the area under the normal distribution is 0.5 units too slim on both sides of the interval. 60 70 80 90 100 figure 3.13: a normal curve with the area between 69 and 71 shaded. the outlined area represents the exact binomial probability. the normal approximation can be improved if the cuto ffvalues for the range of observations is modified slightly: the lower value should be reduced by 0.5 and the upper value increased by 0.5. the normal approximation with continuity correction gives 0.0687 for the probability of observing 69, 70, or 71 smokers in 400 when p= 0:20, which is closer to the exact binomial result of 0.0703. this adjustment method is known as a continuity correction, which allows for increased accuracy when a continuous distribution is used to approximate a discrete one. the modification is typically not necessary when computing a tail area, since the total interval in that case tends to be quite wide. 3.3. normal distribution 163 3.3.7 evaluating the normal approximation the normal model can also be used to approximate data distributions. while using a normal model can be convenient, it is important to remember that normality is always an approximation. testing the appropriateness of the normal assumption is a key step in many data analyses. male heights (inches)6065707580● ●●● ● ●●● ●● ●●● ● ● ●● ● ●●● ●● ● ●● ● ●● ●● ●● ●● ●●● ●● ●●● ●●● ● ●● ●● ●●●● ●● ● ● ●● ●●● ●● ●●● ● ●● ● ● ● ●● ●●●● ●● ● ●●●●● ●● ●● ● ●●● ●●● theoretical quantilesmale heights (inches) −2−1012657075 figure 3.14: a sample of 100 male heights. since the observations are rounded to the nearest whole inch, the points in the normal probability plot appear to jump in increments. example 3.27 suggests the distribution of heights of us males is well approximated by the normal model. there are two visual methods used to assess the assumption of normality. the first is a simple histogram with the best fitting normal curve overlaid on the plot, as shown in the left panel of figure 3.14. the sample mean ̄xand standard deviation sare used as the parameters of the best fitting normal curve. the closer this curve fits the histogram, the more reasonable the normal model assumption. more commonly, a normal probability plot is used, such as the one shown in the right panel of figure 3.14.19if the points fall on or near the line, the data closely follow the normal model. 19also called a quantile-quantile plot , or q-q plot. 164 chapter 3. distributions of random v ariables example 3.30 three datasets were simulated from a normal distribution, with sample sizes n= 40,n= 100, and n= 400; the histograms and normal probability plots of the datasets are shown in figure 3.15. what happens as sample size increases? as sample size increases, the data more closely follows the normal distribution; the histograms become more smooth, and the points on the q-q plots show fewer deviations from the line. it is important to remember that when evaluating normality in a small dataset, apparent deviations from normality may simply be due to small sample size. remember that all three of these simulated datasets are drawn from a normal distribution. when assessing the normal approximation in real data, it will be rare to observe a q-q plot as clean as the one shown for n= 400. typically, the normal approximation is reasonable even if there are some small observed departures from normality in the tails, such as in the plot for n= 100. −3−2−10123−3−2−10123−3−2−10123 ●●● ● ● ●● ●●●● ● ●● ● ●●● ●● ● ●● ● ●●● ● ●● ● ●● ●● ●● ● ●● theoretical quantilesobserved −2−1012−2−101 ●●●● ●● ●● ●● ●● ●● ● ●● ● ● ● ●● ●●● ● ● ●● ●● ●●●● ● ● ●● ● ●● ●● ●● ●●● ● ●●●● ● ●●● ●● ●● ●●● ●● ●● ●● ●● ●●● ● ● ●●●● ● ●● ●●●● ● ●●●● ●●● ●●● theoretical quantilesobserved −2−1012−2−1012 ● ●●● ●●●● ●● ●●● ●●● ● ● ●●● ●● ●● ●● ●●● ● ●● ●●●●● ●●● ●● ●● ● ●● ● ●●●● ●●● ● ●● ●●● ● ●●●● ●●●●● ● ● ● ●● ●●● ●● ●● ●● ●● ● ●●●● ● ●● ●● ●● ● ●●● ●● ● ●● ● ●●● ● ●●●● ●● ● ●●● ●●●● ●● ● ● ●● ● ●● ●● ●●●●● ● ●● ●● ●●● ●●● ●● ●● ●●●● ●●●● ●● ●●● ●●●●●●● ●●● ●● ●●●●● ● ●●●● ●●●● ●● ● ● ●● ● ●●●● ●●●● ●● ●● ●● ●● ● ●● ● ●●● ●● ●● ●● ●●● ● ●● ●● ●●● ● ●● ●●●● ● ●●●● ● ●●●● ● ●●● ●● ● ●●●● ●● ● ●●●●●● ●● ● ●●●●● ●● ● ●● ●●●● ●● ● ●●●●● ●●●● ●● ●●●●● ●● ●● ●●●●● ●● ● ●●● ● ●●●●● ●●● ● ●● ●●● ● ●●● ● ●● ●● ●● ●● ● ● ● ● ●●● ●● ●●● ●● ●●● ●● ● ●● ●● ●● ●● ●● ●●●●●● ●●● theoretical quantilesobserved −3−10123−3−2−10123 figure 3.15: histograms and normal probability plots for three simulated normal data sets;n= 40 (left),n= 100 (middle), n= 400 (right). 3.3. normal distribution 165 example 3.31 would it be reasonable to use the normal distribution to accurately calculate percentiles of heights of nba players? consider all 435 nba players from the 2008-9 season presented in figure 3.16.20 the histogram in the left panel is slightly left skewed, and the points in the normal probability plot do not closely follow a straight line, particularly in the upper quantiles. the normal model is not an accurate approximation of nba player heights. height (inches)7075808590● ●●●● ● ●● ● ●● ●●●● ●● ●●● ●● ●●●● ●● ●● ●●●●● ●● ●● ● ●● ●● ●●●● ●● ●●● ●● ●●●● ● ●● ● ●●● ●● ●●●● ●●● ● ● ● ●● ● ●● ● ●●● ●● ●● ●●●●●●● ●● ●●● ●● ● ● ●●● ●●● ●● ●● ●● ●●● ● ●● ●● ●●● ● ● ●● ●● ●●● ● ● ●●● ● ●● ●●●●● ●●● ● ●●● ●● ●●● ●●● ●●● ●● ●●● ●●●● ● ●●● ●● ● ●● ● ●● ●● ●● ● ●● ● ●● ●● ● ●● ●●● ●● ●● ●● ● ●●●●● ● ●●● ● ●●● ●● ● ●●●● ●●● ● ●●● ●●● ●●● ●● ● ●●●● ●● ●●● ●●●● ●● ●● ●● ●● ●● ● ● ●● ●● ●● ●●● ●● ●●●● ●● ●● ●●● ●●● ● ● ●●● ● ●● ● ●●●● ●● ●● ●●●● ●● ● ●●● ● ●● ●● ●●● ●●● ●●●● ●● ●● ●● ●● ●●● ● ●● ● ● ● ●●● ● ●●● ●●● ● ● ● ●●● ●●● ●● ● ●● ●●● ● ●●● ●● ● ●●● ●● ●● ●● ● ●●● ●● ●● ●●● ● ●●● ●●● ●● ●● ●● ● ●●● ● ●● theoretical quantilesobserved −3−2−101237075808590 figure 3.16: histogram and normal probability plot for the nba heights from the 2008-9 season. example 3.32 consider the poker winnings of an individual over 50 days. a histogram and normal probability plot of these data are shown in figure 3.17 evaluate whether a normal approximation is appropriate. the data are very strongly right skewed in the histogram, which corresponds to the very strong deviations on the upper right component of the normal probability plot. these data show very strong deviations from the normal model; the normal approximation should not be applied to these data. poker earnings (us$)−2000−1000 01000200030004000●●●● ●●● ●●●● ● ●●● ●●●●●● ● ● ●●● ●● ●●●●● ●● ●●● ●● ●● ● ● ●● ●●● ● theoretical quantiles−2−1 012−10000100020003000 figure 3.17: a histogram of poker data with the best fitting normal plot and a normal probability plot. 20these data were collected from www.nba.com. 166 chapter 3. distributions of random v ariables guided practice 3.33 determine which data sets represented in figure 3.18 plausibly come from a nearly normal distribution.21 ● ● ●●●●● ●●● ● ●●●● ● ●● ● ●●● ●●● ● ●● ● ●● ● ●● ● ● ●●● ●●● ● ●● ● ●● ●● ● ●● ●●● ●●● ●● ●● ●● ●● ● ●● ●●● ●● ●● ●● ●●● ●●● ● ●●●●●● ●● ●● ●●●●observed theoretical quantiles−2−101204080120 ● ●● ● ●● ●●● ●● ●● ●● ● ●● ●● ● ● ●●● ● ●●●●● ●● ●●●● ● ●● ●● ●●● ●● ●●●observed theoretical quantiles−2−101268101214 ●● ●● ●● ●● ●●●●● ● ●●● ●●● ●● ●● ●● ● ●●● ●●● ● ●● ●●● ● ●●● ●●●●● ● ●● ●●● ●●● ●●● ●● ●● ●●●● ● ●●● ● ●●● ● ● ●●● ● ●● ● ●●● ● ●● ● ●●● ●● ●● ●●●● ● ●● ●●●●●● ●●● ● ●●● ●●● ●●● ● ●● ●● ●● ●● ●● ●● ●● ●●● ●● ● ●● ●● ●● ●●● ● ●● ● ● ●● ●● ● ●● ●●●●● ●●● ●●●● ●● ● ●●● ● ●● ● ●● ●●● ●●● ●●● ● ● ● ●● ●●● ● ●●● ●●●● ●● ●●● ●● ●● ●● ● ●●● ●●● ●● ●● ●● ●● ●●● ●● ●● ● ●●● ●● ●●● ●● ●● ●● ●●● ● ●●●● ●●● ●● ●● ●● ●●●●● ●● ●● ● ●● ●● ●●●● ●●● ●●●● ●●● ●●● ●●● ●●● ●● ●● ●●● ●● ●●● ●● ●●●● ● ●●● ●● ● ● ●● ●●●● ●● ● ●● ●●● ●●● ●● ● ●●●● ● ●● ●●● ●●● ● ●● ●●● ●●●● ●● ● ●●● ● ●● ●● ●● ●● ● ● ●●● ●● ●● ●●●●● ●●● ● ● ●● ●●● ●●●●● ●●● ●● ●●● ● ●●● ●● ●● ●● ●● ●●● ●●●● ●● ●●●● ●● ●●●●● ●●● ●● ●● ● ●●● ●● ●●● ●● ● ●●●● ●●●●● ●observed theoretical quantiles−3−2−10123−3−2−1 ● ●● ● ●●●● ● ●●● ●● ●observed theoretical quantiles−1010204060 figure 3.18: four normal probability plots for guided practice 3.33. 21answers may vary. the top-left plot shows some deviations in the smallest values in the dataset; specifically, the left tail shows some large outliers. the top-right and bottom-left plots do not show any obvious or extreme deviations from the lines for their respective sample sizes, so a normal model would be reasonable. the bottom-right plot has a consistent curvature that suggests it is not from the normal distribution. from examining the vertical coordinates of the observations, most of the data are between -20 and 0, then there are about five observations scattered between 0 and 70; this distribution has strong right skew. 3.3. normal distribution 167 when observations spike downwards on the left side of a normal probability plot, this indicates that the data have more outliers in the left tail expected under a normal distribution. when observations spike upwards on the right side, the data have more outliers in the right tail than expected under the normal distribution. guided practice 3.34 figure 3.19 shows normal probability plots for two distributions that are skewed. one distribution is skewed to the low end (left skewed) and the other to the high end (right skewed). which is which?22 ●● ● ●● ●●● ●●●● ● ●●●●● ● ●●● ●●●observed theoretical quantiles−2−1012012● ●● ● ●●●●● ● ●●● ●● ●●● ●● ●● ●●●● ● ●●● ● ● ●● ●●●●● ● ●● ● ●● ●●●● ●observed theoretical quantiles−2−101251015 figure 3.19: normal probability plots for guided practice 3.34. 22examine where the points fall along the vertical axis. in the first plot, most points are near the low end with fewer observations scattered along the high end; this describes a distribution that is skewed to the high end. the second plot shows the opposite features, and this distribution is skewed to the low end. 168 chapter 3. distributions of random v ariables 3.4 poisson distribution the poisson distribution is a discrete distribution used to calculate probabilities for the number of occurrences of a rare event. in technical terms, it is used as a model for count data. for example, historical records of hospitalizations in new york city indicate that among a population of approximately 8 million people, 4.4 people are hospitalized each day for an acute myocardial infarction (ami), on average. a histogram of showing the distribution of the number of amis per day on 365 days for nyc is shown in figure 3.20.23 frequency 0 5 10020406080 figure 3.20: a histogram of the number of people hospitalized for an ami on 365 days for nyc, as simulated from a poisson distribution with mean 4.4. poisson distribution the poisson distribution is a probability model for the number of events that occur in a population. the probability that exactly kevents occur is given by p(x=k) =e\u0000\u0015(\u0015)k k!; wherekmay take a value 0, 1, 2, . . . the mean and standard deviation of this distribution are \u0015andp \u0015, respectively. a poisson random variable xcan be expressed as x\u0018pois(\u0015).pois(\u0015) poisson dist. with rate\u0015 when events accumulate over time in such a way that the probability an event occurs in an interval is proportional to the length of an interval and that the number of events in non-overlapping intervals are independent, the parameter \u0015(the greek letter lambda ) represents the average num-\u0015 rate for the poisson dist. ber of events per unit time; i.e., the rate per unit time. in this setting, the number of events in tunits of time has probability p(x=k) =e\u0000\u0015t(\u0015t)k k!; wherektakes on values 0, 1, 2, . . . . when used this way, the mean and standard deviation are \u0015t andp \u0015t, respectively. the rate parameter \u0015represents the expected number of events per unit time, while the quantity \u0015trepresents the expected number events over a time period of tunits. the histogram in figure 3.20 approximates a poisson distribution with rate equal to 4.4 events per day, for a population of 8 million. 23these data are simulated. in practice, it would be important to check for an association between successive days. 3.4. poisson distribution 169 example 3.35 in new york city, what is the probability that 2 individuals are hospitalized for ami in seven days, if the rate is known to be 4.4 deaths per day? from the given information, \u0015= 4:4,k= 2, andt= 7. p(x=k) =e\u0000\u0015t(\u0015t)k k! p(x= 2) =e\u00004:4\u00027(4:4\u00027)2 2!= 1:99\u000210\u000011: guided practice 3.36 in new york city, what is the probability that (a) at most 2 individuals are hospitalized for ami in seven days, (b) at least 3 individuals are hospitalized for ami in seven days?24 a rigorous set of conditions for the poisson distribution is not discussed here. generally, the poisson distribution is used to calculate probabilities for rare events that accumulate over time, such as the occurrence of a disease in a population. example 3.37 for children ages 0 - 14, the incidence rate of acute lymphocytic leukemia (all) was approximately 30 diagnosed cases per million children per year in 2010. approximately 20% of the us population of 319,055,000 are in this age range. what is the expected number of cases of all in the us over five years? the incidence rate for one year can be expressed as 30 =1;000;000 = 0:00003; for five years, the rate is (5)(0 :00003) = 0 :00015. the number of children age 0-14 in the population is (0:20)(319;055;000)\u001963;811;000. \u0015= (relevant population size)(rate per child) = 63;811;000\u00020:00015 = 9;571:5 the expected number of cases over five years is 9,571.5 cases. 24(a)p(x\u00142) =p(x= 0) +p(x= 1) +p(x= 2) =e\u00004:4\u00027(4:4\u00027)0 0!+e\u00004:4\u00027(4:4\u00027)1 1!+e\u00004:4\u00027(4:4\u00027)2 2!= 2:12\u000210\u000011(b) p(x\u00153) = 1\u0000p(x<3) = 1\u0000p(x\u00142) = 1\u00002:12\u000210\u000011\u00191 170 chapter 3. distributions of random v ariables 3.5 distributions related to bernoulli trials the binomial distribution is not the only distribution that can be built from a series of repeated bernoulli trials. this section discusses the geometric, negative binomial, and hypergeometric distributions. 3.5.1 geometric distribution the geometric distribution describes the waiting time until one success for a series of independent bernoulli random variables, in which the probability of success premains constant. example 3.38 recall that in the milgram shock experiments, the probability of a person refusing to give the most severe shock is p= 0:35. suppose that participants are tested one at a time until one person refuses; i.e., until the first occurrence of a successful trial. what are the chances that the first occurrence happens with the first trial? the second trial? the third? the probability that the first trial is successful is simply p= 0:35. if the second trial is the first successful one, then the first one must have been unsuccessful. thus, the probability is given by (0 :65)(0:35) = 0:228. similarly, the probability that the first success is the third trial: (0 :65)(0:65)(0:35) = 0:148. this can be stated generally. if the first success is on the nthtrial, then there are n\u00001 failures and finally 1 success, which corresponds to the probability (0 :65)n\u00001(0:35). the geometric distribution from example 3.38 is shown in figure 3.21. in general, the probabilities for a geometric distribution decrease exponentially . probability number of trials24681012140.00.10.20.3 ... figure 3.21: the geometric distribution when the probability of success is p= 0:35. 3.5. distributions related to bernoulli trials 171 geometric distribution if the probability of a success in one trial is pand the probability of a failure is 1 \u0000p, then the probability of finding the first success in the kthtrial is given by p(x=k) = (1\u0000p)k\u00001p: the mean (i.e. expected value), variance, and standard deviation of this wait time are given by \u0016=1 p\u001b2=1\u0000p p2\u001b=s 1\u0000p p2 a geometric random variable xcan be expressed as x\u0018geom(p).geom(p) geometric dist. withpprob. of success guided practice 3.39 if individuals were examined until one did not administer the most severe shock, how many might need to be tested before the first success?25 example 3.40 what is the probability of the first success occurring within the first 4 people? this is the probability it is the first ( k= 1), second ( k= 2), third ( k= 3), or fourth ( k= 4) trial that is the first success, which represent four disjoint outcomes. compute the probability of each case and add the separate results: p(x= 1;2;3;or 4) =p(x= 1) +p(x= 2) +p(x= 3) +p(x= 4) = (0:65)1\u00001(0:35) + (0:65)2\u00001(0:35) + (0:65)3\u00001(0:35) + (0:65)4\u00001(0:35) = 0:82: alternatively, find the complement of p(x = 0), since the described event is the complement of no success in 4 trials: 1 \u0000(0:65)4(0:35)0= 0:82. there is a 0.82 probability that the first success occurs within 4 trials. note that there are di ffering conventions for defining the geometric distribution; while this text uses the definition that the distribution describes the total number of trials including the success, others define the distribution as the number of trials required before the success is obtained. inr, the latter definition is used. 25about 1=p= 1=0:35 = 2:86 individuals. 172 chapter 3. distributions of random v ariables 3.5.2 negative binomial distribution the geometric distribution describes the probability of observing the first success on the kth trial. the negative binomial distribution is more general: it describes the probability of observing therthsuccess on the kthtrial. suppose a research assistant needs to successfully extract rna from four plant samples before leaving the lab for the day. yesterday, it took 6 attempts to attain the fourth successful extraction. the last extraction must have been a success; that leaves three successful extractions and two unsuccessful ones that make up the first five attempts. there are ten possible sequences, which are shown in 3.22. extraction attempt 1 2 3 4 5 6 1f f1 s2 s3 s4 s 2f1 sf2 s3 s4 s 3f1 s2 sf3 s4 s 4f1 s2 s3 sf4 s 51 sf f2 s3 s4 s 61 sf2 sf3 s4 s 71 sf2 s3 sf4 s 81 s2 sf f3 s4 s 91 s2 sf3 sf4 s 101 s2 s3 sf f4 s figure 3.22: the ten possible sequences when the fourth successful extraction is on the sixth attempt. guided practice 3.41 each sequence in figure 3.22 has exactly two failures and four successes with the last attempt always being a success. if the probability of a success is p= 0:8, find the probability of the first sequence.26 if the probability of a successful extraction is p= 0:8, what is the probability that it takes exactly six attempts to reach the fourth successful extraction? as expressed by 3.41, there are 10 different ways that this event can occur. the probability of the first sequence was identified in guided practice 3.41 as 0.0164, and each of the other sequences have the same probability. thus, the total probability is (10)(0 :0164) = 0:164. 26the first sequence: 0 :2\u00020:2\u00020:8\u00020:8\u00020:8\u00020:8 = 0:0164. 3.5. distributions related to bernoulli trials 173 a general formula for computing a negative binomial probability can be generated using similar logic as for binomial probability. the probability is comprised of two pieces: the probability of a single sequence of events, and then the number of possible sequences. the probability of observingrsuccesses out of kattempts can be expressed as (1 \u0000p)k\u0000rpr. next, identify the number of possible sequences. in the above example, 10 sequences were identified by fixing the last observation as a success and looking for ways to arrange the other observations. in other words, the goal is to arranger\u00001 successes in k\u00001 trials. this can be expressed as: k\u00001 r\u00001! =(k\u00001)! (r\u00001)!((k\u00001)\u0000(r\u00001))!=(k\u00001)! (r\u00001)!(k\u0000r)!: negative binomial distribution the negative binomial distribution describes the probability of observing the rthsuccess on thekthtrial, for independent trials: p(x=k) = k\u00001 r\u00001! pr(1\u0000p)k\u0000r; (3.42) wherepis the probability an individual trial is a success. the mean and variance are given by \u0016=r p\u001b2=r(1\u0000p) p2 a negative binomial random variable xcan be expressed as x\u0018nb(r;p).nb(r;p) neg. bin. dist. withk successes &pprob. of success is it negative binomial? four conditions to check. (1) the trials are independent. (2) each trial outcome can be classified as a success or failure. (3) the probability of a success ( p) is the same for each trial. (4) the last trial must be a success. example 3.43 calculate the probability of a fourth successful extraction on the fifth attempt. the probability of a single success is p= 0:8, the number of successes is r= 4, and the number of necessary attempts under this scenario is k= 5. k\u00001 r\u00001! pr(1\u0000p)k\u0000r=4! 3!1!(0:8)4(0:2) = 4\u00020:08192 = 0:328: 174 chapter 3. distributions of random v ariables guided practice 3.44 assume that each extraction attempt is independent. what is the probability that the fourth success occurs within 5 attempts?27 binomial versus negative binomial the binomial distribution is used when considering the number of successes for a fixed number of trials. for negative binomial problems, there is a fixed number of successes and the goal is to identify the number of trials necessary for a certain number of successes (note that the last observation must be a success). guided practice 3.45 on 70% of days, a hospital admits at least one heart attack patient. on 30% of the days, no heart attack patients are admitted. identify each case below as a binomial or negative binomial case, and compute the probability. (a) what is the probability the hospital will admit a heart attack patient on exactly three days this week? (b) what is the probability the second day with a heart attack patient will be the fourth day of the week? (c) what is the probability the fifth day of next month will be the first day with a heart attack patient?28 inr, the negative binomial distribution is defined as the number of failures that occur before a target number of successes is reached; i.e., k\u0000r. in this text, the distribution is defined in terms of the total number of trials required to observe rsuccesses, where the last trial is necessarily a success. 27if the fourth success ( r= 4) is within five attempts, it either took four or five tries ( k= 4 ork= 5): p(k= 4 ork= 5) =p(k= 4) +p(k= 5) = 4\u00001 4\u00001! 0:84+ 5\u00001 4\u00001! (0:8)4(1\u00000:8) = 1\u00020:41 + 4\u00020:082 = 0:41 + 0:33 = 0:74: 28in each part, p= 0:7. (a) the number of days is fixed, so this is binomial. the parameters are k= 3 andn= 7: 0.097. (b) the last \"success\" (admitting a patient) is fixed to the last day, so apply the negative binomial distribution. the parameters arer= 2,k= 4: 0.132. (c) this problem is negative binomial with r= 1 andk= 5: 0.006. note that the negative binomial case whenr= 1 is the same as using the geometric distribution. 3.5. distributions related to bernoulli trials 175 3.5.3 hypergeometric distribution suppose that a large number of deer live in a forest. researchers are interested in using the capture-recapture method to estimate total population size. a number of deer are captured in an initial sample and marked, then released; at a later time, another sample of deer are captured, and the number of marked and unmarked deer are recorded.29an estimate of the total population can be calculated based on the assumption that the proportion of marked deer in the second sample should equal the proportion of marked deer in the entire population. for example, if 50 deer were initially captured and marked, and then 5 out of 40 deer (12.5%) in a second sample are found to be marked, then the population estimate is 400 deer, since 50 out of 400 is 12.5%. the capture-recapture method sets up an interesting scenario that requires a new probability distribution. let nrepresent the total number of deer in the forest, mthe number of marked deer captured in the original sample, and nthe number of deer in the second sample. what are the probabilities of obtaining 0 ;1;:::;m marked deer in the second sample, if nandmare known? it is helpful to think in terms of a series of bernoulli trials, where each capture in the second sample represents a trial; consider the trial a success if a marked deer is captured, and a failure if an unmarked deer is captured. if the deer were sampled with replacement , such that one deer was sampled, checked if it were marked versus unmarked, then released before another deer was sampled, then the probability of obtaining some number of marked deer in the second sample would be binomially distributed with probability of success m=n (out ofntrials). the trials are independent, and the probability of success remains constant across trials. however, in capture-recapture, the goal is to collect a representative sample such that the proportion of marked deer in the sample can be used to estimate the total population—the sampling is done without replacement . once a trial occurs and a deer is sampled, it is not returned to the population before the next trial. the probability of success is not constant from trial to trial; i.e., these trials are dependent. for example, if a marked deer has just been sampled, then the probability of sampling a marked deer in the next trial decreases, since there is one fewer marked deer available. suppose that out of 9 deer, 4 are marked. what is the probability of observing 1 marked deer in a sample of size 3, if the deer are sampled without replacement? first, consider the total number of ways to draw 3 deer from the population; as shown in figure 3.23, samples may consist of 3, 2, 1, or 0 marked deer. there are\u00004 3\u0001ways to obtain a sample consisting of 3 marked deer out of the 4 total marked deer. by independence, there are\u00004 2\u0001\u00005 1\u0001ways to obtain a sample consisting of exactly 2 marked deer and 1 unmarked deer. in total, there are 84 possible combinations; this quantity is equivalent to\u00009 3\u0001. only\u00004 1\u0001\u00005 2\u0001= 40 of those combinations represent the desired event of exactly 1 marked deer. thus, the probability of observing 1 marked deer in a sample of size 3, under sampling without replacement, equals 40 =84 = 0:476. figure 3.23: possible samples of marked and unmarked deer in a sample n= 3, wherem= 4 andn\u0000m= 5. striped circles represent marked deer, and empty circles represent unmarked deer. 29it is assumed that enough time has passed so that the marked deer redistribute themselves in the population, and that marked and unmarked deer have equal probability of being captured in the second sample. 176 chapter 3. distributions of random v ariables guided practice 3.46 suppose that out of 9 deer, 4 are marked. what is the probability of observing 1 marked deer in a sample of size 3, if the deer are sampled with replacement?30 hypergeometric distribution the hypergeometric distribution describes the probability of observing ksuccesses in a sample of sizen, from a population of size n, where there are msuccesses, and individuals are sampled without replacement: p(x=k) =\u0000m k\u0001\u0000n\u0000m n\u0000k\u0001 \u0000n n\u0001: letp=m=n , the probability of success. the mean and variance are given by \u0016=np \u001b2=np(1\u0000p)n\u0000n n\u00001 a hypergeometric random variable xcan be written as x\u0018hgeom(m;n\u0000m;n). is it hypergeometric? three conditions to check. (1) the trials are dependent. (2) each trial outcome can be classified as a success or failure. (3) the probability of a success is di fferent for each trial. guided practice 3.47 a small clinic would like to draw a random sample of 10 individuals from their patient list of 120, of which 30 patients are smokers. (a) what is the probability of 6 individuals in the sample being smokers? (b) what is the probability that at least 2 individuals in the sample smoke?31 30letxrepresent the number of marked deer in the sample of size 3. if the deer are sampled with replacement, x\u0018 bin(3;4=9), andp(x= 1) =\u00003 1\u0001(4=9)1(5=9)2= 0:412. 31(a) letxrepresent the number of smokers in the sample. p(x= 6) =\u000030 6\u0001\u000090 4\u0001 \u0000120 10\u0001= 0:013. (b)p(x\u00152) = 1\u0000p(x\u00141) = 1\u0000p(x= 0)\u0000p(x= 1) = 1\u0000\u000030 0\u0001\u000090 10\u0001 \u0000120 10\u0001\u0000\u000030 1\u0001\u000090 9\u0001 \u0000120 10\u0001= 0:768. 3.6. distributions for pairs of random v ariables 177 3.6 distributions for pairs of random variables example 3.8 calculated the variability in health care costs for an employee and her partner relying on the assumption that the number of health episodes between the two are not related. it could be reasonable to assume that the health status of one person gives no information about the other’s health, given that the two are not physically related and were not previously living together. however, associations between random variables can be subtle. for example, couples are often attracted to each other because of common interests or lifestyles, which suggests that health status may actually be related. the relationship between a pair of discrete random variables is a feature of the joint distribution of the pair. in this example the joint distribution of annual costs is a table of all possible combinations of costs for the employee and her partner, using the probabilities and costs from the last 10 years (these costs were previously calculated in example 3.6 and guided practice 3.7). entries in the table are probabilities of pairs of annual costs. for example, the entry 0.25 in the second row and second column of figure 3.24 indicates that in approximately 25% of the last 10 years, the employee paid $1,008 in costs while her partner paid $988. partner costs, y employee costs, x $968 $988 $968 0.18 0.12 $1,008 0.15 0.25 $1,028 0.04 0.16 $1,108 0.03 0.07 figure 3.24: joint distribution of health care costs. more generally, the definition of a joint distribution for a pair of random variables xandy uses the notion of joint probabilities discussed in section 2.2.1. joint distribution the joint distribution px;y(x;y) for a pair of random variables ( x;y) is the collection of probabilities p(xi;yj) =p(x=xiandy=yj) for all pairs of values ( xi;yj) that the random variables xandytake on. 178 chapter 3. distributions of random v ariables joint distributions are often displayed in tabular form as in figure 3.24. if xandyhavek1 andk2possible values respectively, there will be ( k1)(k2) possible (x;y) pairs. this is unlike pairs of values (x;y) observed in a dataset, where each observed value of xis usually paired with only one value of y. a joint distribution is often best displayed as a table of probabilities, with ( k1)(k2) entries. figure 3.25 shows the general form of the table for the joint distribution of two discrete distributions. values ofy values ofx y 1y2\u0001\u0001\u0001yk2 x1p(x1;y1)p(x1;y2)\u0001\u0001\u0001p(x1;yk2) x2p(x2;y1)p(x2;y2)\u0001\u0001\u0001p(x2;yk2) :::\u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 \u0001\u0001\u0001 xk1p(xk1;y1)p(xk1;y2)\u0001\u0001\u0001p(xk1;yk2) figure 3.25: table for a joint distribution. entries are probabilities for pairs (xi;yj). these probabilities can be written as p(xi;yj) or more specifically, px;y(xi;yj). when two variables xandyhave a joint distribution, the marginal distribution ofxis the collection of probabilities for xwhenyis ignored.32ifxrepresents employee costs and y represents partner costs, the event ( x= $968) consists of the two disjoint events ( x= $968;y= $968) and (x= $968;y= $988), sop(x= $968) = 0:18 + 0:12 = 0:30, the sum of the first row of the table. the row sums are the values of the marginal distribution of x, while the column sums are the values of the marginal distributions of y. the marginal distributions of xandyare shown in figure 3.26, along with the joint distribution of xandy. the term marginal distribution is apt in this setting—the marginal probabilities appear in the table margins. partner costs, y employee costs, x $968 $988 marg. dist., x $968 0.18 0.12 0.30 $1,008 0.15 0.25 0.40 $1,028 0.04 0.16 0.20 $1,108 0.03 0.07 0.10 marg. dist., y 0.40 0.60 1.00 figure 3.26: joint and marginal distributions of health care costs for a pair of random variables xandy, the conditional distribution ofygiven a value xof the variable xis the probability distribution of ywhen its values are restricted to the value xfor x. just as marginal and joint probabilities are used to calculate conditional probabilities, joint and marginal distributions can be used to obtain conditional distributions. if information is observed about the value of one of the correlated random variables, such as x, then this information can be used to obtain an updated distribution for y; unlike the marginal distribution of y, the conditional distribution of ygivenxaccounts for information from x. 32the marginal distribution of xcan be written as px(x), and a specific value in the marginal distribution written as px(xi). 3.6. distributions for pairs of random v ariables 179 conditional distribution the conditional distribution pyjx(yjx) for a pair of random variables ( x;y) is the collection of probabilities p(y=yjjx=xi) =p(y=yjandx=xi) p(x=xj) for all pairs of values ( xi;yj) that the random variables xandytake on. example 3.48 if it is known that the employee’s annual health care cost is $968, what is the conditional distribution of the partner’s annual health care cost? note that there is a di fferent conditional distribution of yfor every possible value of x; this problem specifically asks for the conditional distribution of ygiven thatx= $968. pyjx($968j$968) =p(y= $968jx= $968) =p(y= $968 and x= $968) p(x= $968)=0:18 0:30= 0:60 pyjx($988j$968) =p(y= $988jx= $968) =p(y= $988 and x= $968) p(x= $968)=0:12 0:30= 0:40 with the knowledge that the employee’s annual health care cost is $968, there is a probability of 0.60 that the partner’s cost is $968 and 0.40 that the partner’s cost is $988. guided practice 3.49 consider two random variables, xandy, with the joint distribution shown in figure 3.27. (a) compute the marginal distributions of xandy. (b) identify the joint probability px;y(1;2). (c) what is the value of px;y(2;1)? (d) compute the conditional distribution of xgiven thaty= 2.33 y= 1y= 2 x= 1 0.20 0.40 x= 4 0.30 0.10 figure 3.27: joint distribution of xandy the variance calculation in example 3.8 relied on the assumption that the patterns of health care expenses for the two partners were unrelated. in example 3.48, 0.40 is the conditional probability that the partner’s health care costs will be $988, given that the employee’s cost is $968. the marginal probability that the partner’s health care cost is $988 is 0.60, which is di fferent from 0.40. the patterns of health care costs are related in that knowing the value of the employee’s costs changes the probabilities associated with partner’s costs. the marginal and conditional distributions of the partner’s costs are not the same. the notion of independence of two events discussed in chapter 2 can be applied to the setting of random variables. recall that two events aandbare independent if the conditional proba33(a) the marginal distribution of x:px(1) = 0:60,px(4) = 0:40. the marginal distribution of y:py(1) = 0:50,py(2) = 0:50 (b)px;y(1;2) =p(x= 1;y= 2) = 0:40 (c) since xcannot take on value 2, px;y(2;1) = 0. (d) the conditional distribution ofxgiven thaty= 2:pxjy(1j2) =px;y(1;2) py(2)=0:40 0:50= 0:80,pxjy(4j2) =px;y(4;2) py(2)=0:10 0:50= 0:20. 180 chapter 3. distributions of random v ariables bilityp(ajb) equals the marginal probability p(a) or equivalently, if the product of the marginal probabilities p(a) andp(b) equals the joint probability p(aandb). a pair (x;y) of random variables are called independent random variables if the conditional distribution for y, given any value of x, is the same as the marginal distribution of y. additionally, if all joint probabilities p(x=xi;y=yj) that comprise the joint distribution of xandy can be computed from the product of the marginal probabilities, p(x=xi)p(y=yj),xandyare independent. independent random variables two random variables xandyare independent if the probabilities p(y=yjjx=xi) =p(y=yj) for all pairs of values ( xi;yj). equivalently, xandyare independent if the probabilities p(y=yjandx=xi) =p(y=yj)p(x=xi) for all pairs of values ( xi;yj). example 3.50 demonstrate that the employee’s health care costs and the partner’s health care costs are not independent random variables. as shown in example 3.48, the conditional distribution of the partner’s annual health care cost given that the employee’s annual cost is $968 is p(y= $968jx= $968) = 0 :60,p(y= $988jx= $968) = 0:40. however, the marginal distribution of the partner’s annual health care cost is p(y= $968) = 0:40,p(y= $968) = 0:60. thus,xandyare not independent. this can also be demonstrated from examining the joint distribution, as shown in figure 3.26. the probability that the employee’s cost and partner’s cost are both $968 is 0.18. the marginal probabilities p(x= $968) and p(y= $968), respectively, are 0.30 and 0.40. since (0 :40)(0:30), 0:18,xandyare dependent random variables. note that demonstrating p(y=yjjx=xi) =p(y=yj) orp(y=yjandx=xi) =p(y=yj)p(x=xi) does not hold for any one ( xi;yj) pair is su fficient to prove that xandyare not independent, since independence requires these conditions to hold over allpairs of values ( xi;yj). guided practice 3.51 based on figure 3.27, check whether xandyare independent.34 two random variables that are not independent are called correlated random variables . the correlation between two random variables is a measure of the strength of the relationship between them, just as it was for pairs of data points explored in section 1.6.1. there are many examples of correlated random variables, such as height and weight in a population of individuals, or the gestational age and birth weight of newborns. when two random variables are positively correlated, they tend to increase or decrease together. if one of the variables increases while the other decreases (or vice versa) they are negatively 34xandyare not independent. one way to demonstrate this is to compare px(1) withpxjy(1j2). ifxwere independent ofy, then conditioning on y= 2 should not provide any information about x, andpx(1) should equal pxjy(1j2). however, px(1) = 0:60 andpxjy(1j2) = 0:80. thus,xandyare not independent. 3.6. distributions for pairs of random v ariables 181 correlated. correlation is easy to identify in a scatterplot, but is more di fficult to identify in a table of a joint distribution. fortunately, there is a formula to calculate correlation for a joint distribution specified in a table. correlation between random variables is similar to correlation between pairs of observations in a dataset, with some important di fferences. calculating a correlation rin a dataset was introduced in section 1.6.1 and uses the formula: r=1 n\u00001nx i=1 xi\u0000x sx! yi\u0000y sy! : (3.52) the correlation coe fficientris an average of products, with each term in the product measuring the distance between xand its mean xand the distance between yand its mean y, after the distances have been scaled by respective standard deviations. the compact formula for the correlation between two random variables xandyuses the same idea: \u001ax;y=e x\u0000\u0016x \u001bx! y\u0000\u0016y \u001by! ; (3.53) where\u001ax;yis the correlation between the two variables, and \u0016x;\u0016y,\u001bx;\u001byare the respective means and standard deviations for xandy. just as with the mean of a random variable, the expectation in the formula for correlation is a weighted sum of products, with each term weighted by the probability of values for the pair ( x;y). equation 3.53 is useful for understanding the analogy between correlation of random variables and correlation of observations in a dataset, but it cannot be used to calculate \u001ax;ywithout the probability weights. the weights come from the joint distribution of the pair of variables ( x;y). equation 3.54 is an expansion of equation 3.53. the double summation adds up terms over all combinations of the indices iandj. \u001ax;y=x ix jp(i;j)(xi\u0000\u0016x) sd(x)(yj\u0000\u0016y) sd(y): (3.54) example 3.55 compute the correlation between annual health care costs for the employee and her partner. as calculated previously, e(x) = $1010, var( x) = 1556,e(y) = $980, and var( y) = 96. thus, sd(x) = $39:45 andsd(y) = $9:80. \u001ax;y=p(x1;y1)(x1\u0000\u0016x) sd(x)(y1\u0000\u0016y) sd(y)+p(x1;y2)(x1\u0000\u0016x) sd(x)(y2\u0000\u0016y) sd(y) +\u0001\u0001\u0001+p(x4;y1)(x4\u0000\u0016x) sd(x)(y1\u0000\u0016y) sd(y)+p(x4;y2)(x4\u0000\u0016x) sd(x)(y2\u0000\u0016y) sd(y) = (0:18)(968\u00001010) 39:45(968\u0000980) 9:8+ (0:12)(968\u00001010) 39:45(988\u0000980) 9:8 +\u0001\u0001\u0001+ (0:03)(1108\u00001010) 39:45(968\u0000980) 9:8+ (0:07)(1108\u00001010) 39:45(988\u0000980) 9:8 = 0:22: the correlation between annual health care costs for these two individuals is positive. it is reasonable to expect that there might be a positive correlation in health care costs for two individuals in a relationship; for example, if one person contracts the flu, then it is likely the other person will also contract the flu, and both may need to see a doctor. 182 chapter 3. distributions of random v ariables guided practice 3.56 based on figure 3.27, compute the correlation between xandy. for your convenience, the following values are provided: e(x) = 2:2, var(x) = 2:16,e(y) = 1:5, var(y) = 0:25.35 when two random variables xandyare correlated: variance(x+y) = variance( x) + variance( y) + 2\u001bx\u001bycorrelation( x;y) (3.57) variance(x\u0000y) = variance( x) + variance( y)\u00002\u001bx\u001bycorrelation( x;y): (3.58) when random variables are positively correlated the variance of the sum or the di fference of two variables will be larger than the sum of the two variances. when they are negatively correlated the variance of the sum or di fference will be smaller than the sum of the two variances. the standard deviation for the sum or di fference will always be the square root of the variance. example 3.59 calculate the standard deviation of the sum of the health care costs for the couple. this calculation uses equation 3.57 to calculate the variance of the sum. the standard deviation will be the square root of the variance. var(x+y) = var(x) + var(y) + 2\u001bx\u001by\u001ax;y = (1556 + 96) + (2)(39 :45)(9:80)(0:22) = 1822:10: the standard deviation isp 1822:10 = $42:69. because the health care costs are correlated, the standard deviation of the total cost is larger than the value calculated in example 3.8 under the assumption that the annual costs were independent. guided practice 3.60 compute the standard deviation of x\u0000yfor the pair of random variables shown in figure 3.27.36 35the correlation between xandyis\u001ax;y= (0:20)(1\u00002:2)p 2:16(1\u00001:5)p 0:25+\u0001\u0001\u0001+ (0:10)(4\u00002:2)p 2:16(2\u00001:5)p 0:25=\u00000:0208: 36var(x\u0000y) = var(x) + var(y)\u00002\u001bx\u001by\u001ax;y= 2:16 + 0:25\u00002(p 2:16)(p 0:25)(\u00000:0208) = 2:44. thus,sd(x\u0000y) =p 2:44 = 1:56: 3.6. distributions for pairs of random v ariables 183 example 3.61 the association of american medical colleges (aamc) introduced a new version of the medical college admission test (mcat) in the spring of 2015. data from the scores were recently released by aamc.37the test consists of 4 components: chemical and physical foundations of biological systems; critical analysis and reasoning skills; biological and biochemical foundations of living systems; psychological, social and biological foundations of behavior. the overall score is the sum of the individual component scores. the grading for each of the four components is scaled so that the mean score is 125. the means and standard deviations for the four components and the total scores for the population taking the exam in may 2015 exam are shown in figure 3.28. show that the standard deviation in the table for the total score does not agree with that obtained under the assumption of independence. the variance of each component of the score is the square of each standard deviation. under the assumption of independence, the variance of the total score would be var(total score) = 3 :02+ 3:02+ 3:02+ 3:12 = 36:61; so the standard deviation is 6.05, which is less than 10.6. since the observed standard deviation is larger than that calculated under independence, this suggests the component scores are positively correlated. it would not be reasonable to expect that the component scores are independent. think about a student taking the mcat exam: someone who scores well on one component of the exam is likely to score well on the other parts. component mean standard deviation chem. phys. found. 125 3.0 crit. analysis 125 3.0 living systems 125 3.0 found. behavior 125 3.1 total score 500 10.6 figure 3.28: means and standard deviations for mcat scores 37https://www.aamc.org/students/download/434504/data/percentilenewmcat.pdf 184 chapter 3. distributions of random v ariables 3.7 notes thinking in terms of random variables and distributions of probabilities makes it easier to describe all possible outcomes of an experiment or process of interest, versus only considering probabilities on the scale of individual outcomes or sets of outcomes. several of the fundamental concepts of probability can naturally be extended to probability distributions. for example, the process of obtaining a conditional distribution is analogous to the one for calculating a conditional probability. many processes can be modeled using a specific named distribution. the statistical techniques discussed in later chapters, such as hypothesis testing and regression, are often based on particular distributional assumptions. in particular, many methods rely on the assumption that data are normally distributed. the discussion of random variables and their distribution provided in this chapter only represents an introduction to the topic. in this text, properties of random variables such as expected value or correlation are presented in the context of discrete random variables; these concepts are also applicable to continuous random variables. a course in probability theory will cover additional named distributions as well as more advanced methods for working with distributions. lab 1 introduces the general notion of a random variable and its distribution using a simulation, then discusses the binomial distribution. lab 2 discusses the normal distribution and working with normal probabilities, as well as the poisson distribution. lab 3 covers the geometric, negative binomial, and hypergeometric distributions. all three labs include practice problems that illustrate the use of rfunctions for probability distributions and introduce additional features of the rprogramming language. lab 4 discusses distributions for pairs of random variables and some r functions useful for matrix calculations. 3.8. exercises 185 3.8 exercises 3.8.1 random variables 3.1 college smokers. at a university, 13% of students smoke. (a) calculate the expected number of smokers in a random sample of 100 students from this university. (b) the university gym opens at 9 am on saturday mornings. one saturday morning at 8:55 am there are 27 students outside the gym waiting for it to open. should you use the same approach from part (a) to calculate the expected number of smokers among these 27 students? 3.2 ace of clubs wins. consider the following card game with a well-shu ffled deck of cards. if you draw a red card, you win nothing. if you get a spade, you win $5. for any club, you win $10 plus an extra $20 for the ace of clubs. (a) create a probability model for the amount you win at this game. also, find the expected winnings for a single game and the standard deviation of the winnings. (b) what is the maximum amount you would be willing to pay to play this game? explain your reasoning. 3.3 hearts win. in a new card game, you start with a well-shu ffled full deck and draw 3 cards without replacement. if you draw 3 hearts, you win $50. if you draw 3 black cards, you win $25. for any other draws, you win nothing. (a) create a probability model for the amount you win at this game, and find the expected winnings. also compute the standard deviation of this distribution. (b) if the game costs $5 to play, what would be the expected value and standard deviation of the net profit (or loss)? (c) if the game costs $5 to play, should you play this game? explain. 3.4 baggage fees. an airline charges the following baggage fees: $25 for the first bag and $35 for the second. suppose 54% of passengers have no checked luggage, 34% have one piece of checked luggage and 12% have two pieces. we suppose a negligible portion of people check more than two bags. (a) build a probability model, compute the average revenue per passenger, and compute the corresponding standard deviation. (b) about how much revenue should the airline expect for a flight of 120 passengers? with what standard deviation? note any assumptions you make and if you think they are justified. 3.5 gull clutch size. large black-tailed gulls usually lay one to three eggs, and rarely have a fourth egg clutch. it is thought that clutch sizes are e ffectively limited by how e ffectively parents can incubate their eggs. suppose that on average, gulls have a 25% of laying 1 egg, 40% of laying 2 eggs, 30% chance of laying 3 eggs, and 5% chance of laying 4 eggs. (a) calculate the expected number of eggs laid by a random sample of 100 gulls. (b) calculate the standard deviation of the number of eggs laid by a random sample of 100 gulls. 186 chapter 3. distributions of random v ariables 3.6 scooping ice cream. ice cream usually comes in 1.5 quart boxes (48 fluid ounces), and ice cream scoops hold about 2 ounces. however, there is some variability in the amount of ice cream in a box as well as the amount of ice cream scooped out. we represent the amount of ice cream in the box as xand the amount scooped out as y. suppose these random variables have the following means, standard deviations, and variances: mean sd variance x 48 1 1 y 2 0.25 0.0625 (a) an entire box of ice cream, plus 3 scoops from a second box is served at a party. how much ice cream do you expect to have been served at this party? what is the standard deviation of the amount of ice cream served? (b) how much ice cream would you expect to be left in the box after scooping out one scoop of ice cream? that is, find the expected value of x\u0000y. what is the standard deviation of the amount left in the box? (c) using the context of this exercise, explain why we add variances when we subtract one random variable from another. 3.8.2 binomial distribution 3.7 underage drinking, part i. data collected by the substance abuse and mental health services administration (samsha) suggests that 69.7% of 18-20 year olds consumed alcoholic beverages in any given year.38 (a) suppose a random sample of ten 18-20 year olds is taken. is the use of the binomial distribution appropriate for calculating the probability that exactly six consumed alcoholic beverages? explain. (b) calculate the probability that exactly 6 out of 10 randomly sampled 18- 20 year olds consumed an alcoholic drink. (c) what is the probability that exactly four out of ten 18-20 year olds have notconsumed an alcoholic beverage? (d) what is the probability that at most 2 out of 5 randomly sampled 18-20 year olds have consumed alcoholic beverages? (e) what is the probability that at least 1 out of 5 randomly sampled 18-20 year olds have consumed alcoholic beverages? 3.8 chickenpox, part i. the us cdc estimates that 90% of americans have had chickenpox by the time they reach adulthood. (a) suppose we take a random sample of 100 american adults. is the use of the binomial distribution appropriate for calculating the probability that exactly 97 out of 100 randomly sampled american adults had chickenpox during childhood? explain. (b) calculate the probability that exactly 97 out of 100 randomly sampled american adults had chickenpox during childhood. (c) what is the probability that exactly 3 out of a new sample of 100 american adults have nothad chickenpox in their childhood? (d) what is the probability that at least 1 out of 10 randomly sampled american adults have had chickenpox? (e) what is the probability that at most 3 out of 10 randomly sampled american adults have nothad chickenpox? 38samhsa, o ffice of applied studies, national survey on drug use and health, 2007 and 2008. 3.8. exercises 187 3.9 underage drinking, part ii. we learned in exercise 3.7 that about 70% of 18-20 year olds consumed alcoholic beverages in any given year. we now consider a random sample of fifty 18-20 year olds. (a) how many people would you expect to have consumed alcoholic beverages? and with what standard deviation? (b) would you be surprised if there were 45 or more people who have consumed alcoholic beverages? (c) what is the probability that 45 or more people in this sample have consumed alcoholic beverages? how does this probability relate to your answer to part (b)? 3.10 chickenpox, part ii. we learned in exercise 3.8 that about 90% of american adults had chickenpox before adulthood. we now consider a random sample of 120 american adults. (a) how many people in this sample would you expect to have had chickenpox in their childhood? and with what standard deviation? (b) would you be surprised if there were 105 people who have had chickenpox in their childhood? (c) what is the probability that 105 or fewer people in this sample have had chickenpox in their childhood? how does this probability relate to your answer to part (b)? 3.11 donating blood. when patients receive blood transfusions, it is critical that the blood type of the donor is compatible with the patients, or else an immune system response will be triggered. for example, a patient with type o- blood can only receive type o- blood, but a patient with type o+ blood can receive either type o+ or type o-. furthermore, if a blood donor and recipient are of the same ethnic background, the chance of an adverse reaction may be reduced. according to a 10-year donor database, 0.37 of white, non-hispanic donors are o+ and 0.08 are o-. (a) consider a random sample of 15 white, non-hispanic donors. calculate the expected value of individuals who could be a donor to a patient with type o+ blood. with what standard deviation? (b) what is the probability that 3 or more of the people in this sample could donate blood to a patient with type o- blood? 3.12 sickle cell anemia. sickle cell anemia is a genetic blood disorder where red blood cells lose their flexibility and assume an abnormal, rigid, “sickle\" shape, which results in a risk of various complications. if both parents are carriers of the disease, then a child has a 25% chance of having the disease, 50% chance of being a carrier, and 25% chance of neither having the disease nor being a carrier. if two parents who are carriers of the disease have 3 children, what is the probability that (a) two will have the disease? (b) none will have the disease? (c) at least one will neither have the disease nor be a carrier? (d) the first child with the disease will the be 3rdchild? 3.13 hepatitis c. hepatitis c is spread primarily through contact with the blood of an infected person, and is nearly always transmitted through needle sharing among intravenous drug users. suppose that in a month’s time, an iv drug user has a 30% chance of contracting hepatitis c through needle sharing. what is the probability that 3 out of 5 iv drug users contract hepatitis c in a month? assume that the drug users live in different parts of the country. 3.14 arachnophobia. a gallup poll found that 7% of teenagers (ages 13 to 17) su ffer from arachnophobia and are extremely afraid of spiders. at a summer camp there are 10 teenagers sleeping in each tent. assume that these 10 teenagers are independent of each other.39 (a) calculate the probability that at least one of them su ffers from arachnophobia. (b) calculate the probability that exactly 2 of them su ffer from arachnophobia. (c) calculate the probability that at most 1 of them su ffers from arachnophobia. (d) if the camp counselor wants to make sure no more than 1 teenager in each tent is afraid of spiders, does it seem reasonable for him to randomly assign teenagers to tents? 39gallup poll, what frightens america’s youth?, march 29, 2005. 188 chapter 3. distributions of random v ariables 3.15 wolbachia infection. approximately 12,500 stocks of drosophila melanogaster flies are kept at the bloomington drosophila stock center for research purposes. a 2006 study examined how many stocks were infected with wolbachia, an intracellular microbe that can manipulate host reproduction for its own benefit. about 30% of stocks were identified as infected. researchers working with infected stocks should be cautious of the potential confounding e ffects that wolbachia infection may have on experiments. consider a random sample of 250 stocks. (a) calculate the probability that exactly 60 stocks are infected. (b) calculate the probability that at most 60 stocks are infected. (c) calculate the probability that at least 80 stocks are infected. (d) if a researcher wants to make sure that no more than 40% of the stocks used for an experiment are infected, does it seem reasonable to take a random sample of 250? 3.16 male children. while it is often assumed that the probabilities of having a boy or a girl are the same, the actual probability of having a boy is slightly higher at 0.51. suppose a couple plans to have 3 kids. (a) use the binomial model to calculate the probability that two of them will be boys. (b) write out all possible orderings of 3 children, 2 of whom are boys. use these scenarios to calculate the same probability from part (a) but using the addition rule for disjoint outcomes. confirm that your answers from parts (a) and (b) match. (c) if we wanted to calculate the probability that a couple who plans to have 8 kids will have 3 boys, briefly describe why the approach from part (b) would be more tedious than the approach from part (a). 3.17 hyponatremia. hyponatremia (low sodium levels) occurs in a certain proportion of marathon runners during a race. suppose that historically, the proportion of runners who develop hyponatremia is 0.12. in a certain marathon, there are 200 runners participating. (a) how many cases of hyponatremia are expected during the marathon? (b) what is the probability of more than 30 cases of hyponatremia occurring? 3.18 sleep deprivation. consider a senior statistics concentrator with a packed extracurricular schedule, taking five classes, and writing a thesis. each time she takes an exam, she either scores very well (at least two standard deviations above the mean) or does not. her performance on any given exam depends on whether she is operating on a reasonable amount of sleep the night before (more than 7 hours), relatively little sleep (between 4 - 7 hours, inclusive), or practically no sleep (less than 4 hours). when she has had practically no sleep, she scores very well about 30% of the time. when she has had relatively little sleep, she scores very well 40% of the time. when she has had a reasonable amount of sleep, she scores very well 42% of the time. over the course of a semester, she has a reasonable amount of sleep 50% of nights, and practically no sleep 30% of nights. (a) what is her overall probability of scoring very well on an exam? (b) what is the probability she had practically no sleep the night before an exam where she scored very well? (c) suppose that one day she has three exams scheduled. what is the probability that she scores very well on exactly two of the exams, under the assumption that her performance on each exam is independent of her performance on another exam? (d) what is the probability that she had practically no sleep the night prior to a day when she scored very well on exactly two out of three exams? 3.8. exercises 189 3.8.3 normal distribution 3.19 area under the curve, part i. what percent of a standard normal distribution n(\u0016= 0;\u001b= 1) is found in each region? be sure to draw a graph. (a)z<\u00001:35 (b) z>1:48 (c) \u00000:4<z< 1:5 (d)jzj>2 3.20 area under the curve, part ii. what percent of a standard normal distribution n(\u0016= 0;\u001b= 1) is found in each region? be sure to draw a graph. (a)z>\u00001:13 (b) z<0:18 (c) z>8 (d) jzj<0:5 3.21 the standard normal distribution. consider the standard normal distribution with mean \u0016= 0 and standard deviation \u001b= 1. (a) what is the probability that an outcome zis greater than 2.60? (b) what is the probability that zis less than 1.35? (c) what is the probability that zis between -1.70 and 3.10? (d) what value of zcuts offthe upper 15% of the distribution? (e) what value of zmarks o ffthe lower 20% of the distribution? 3.22 triathlon times. in triathlons, it is common for racers to be placed into age and gender groups. the finishing times of men ages 30-34 has mean of 4,313 seconds with a standard deviation of 583 seconds. the finishing times of the women ages 25-29 has a mean of 5,261 seconds with a standard deviation of 807 seconds. the distribution of finishing times for both groups is approximately normal. note that a better performance corresponds to a faster finish. (a) if a man of the 30-34 age group finishes the race in 4,948 seconds, what percent of the triathletes in the group did he finish faster than? (b) if a woman of the 25-29 age group finishes the race in 5,513 seconds, what percent of the triathletes in the group did she finish faster than? (c) calculate the cuto fftime for the fastest 5% of athletes in the men’s group. (d) calculate the cuto fftime for the slowest 10% of athletes in the women’s group. 3.23 gre scores. the graduate record examination (gre) is a standardized test commonly taken by graduate school applicants in the united states. the total score is comprised of three components: quantitative reasoning, verbal reasoning, and analytical writing. the first two components are scored from 130 - 170. the mean score for verbal reasoning section for all test takers was 151 with a standard deviation of 7, and the mean score for the quantitative reasoning was 153 with a standard deviation of 7.67. suppose that both distributions are nearly normal. (a) a student scores 160 on the verbal reasoning section and 157 on the quantitative reasoning section. relative to the scores of other students, which section did the student perform better on? (b) calculate the student’s percentile scores for the two sections. what percent of test takers performed better on the verbal reasoning section? (c) compute the score of a student who scored in the 80thpercentile on the quantitative reasoning section. (d) compute the score of a student who scored worse than 70% of the test takers on the verbal reasoning section. 3.24 osteoporosis. the world health organization defines osteoporosis in young adults as a measured bone mineral density 2.5 or more standard deviations below the mean for young adults. assume that bone mineral density follows a normal distribution in young adults. what percentage of young adults su ffer from osteoporosis according to this criterion? 190 chapter 3. distributions of random v ariables 3.25 la weather. the average daily high temperature in june in la is 77\u000ef with a standard deviation of 5\u000ef. suppose that the temperatures in june closely follow a normal distribution. (a) what is the probability of observing an 83\u000ef temperature or higher in la during a randomly chosen day in june? (b) how cold are the coldest 10% of the days during june in la? 3.26 clutch volume. a study investigating maternal investment in a frog species found on the tibetan plateau reported data on the volume of egg clutches measured across 11 study sites. the distribution is roughly normal, with approximate distribution n(882:5;380)mm3. (a) what is the probability of observing an egg clutch between volume 700-800 mm3? (b) how large are the largest 5% of egg clutches? 3.27 glucose levels. fasting blood glucose levels for normal non-diabetic individuals are normally distributed in the population, with mean \u0016= 85 mg/dl and standard deviation \u001b= 7:5 mg/dl. (a) what is the probability that a randomly chosen member of the population has a fasting glucose level higher than 100 mg/dl? (b) what value of fasting glucose level defines the lower 5thpercentile of the distribution? 3.28 arsenic poisoning. arsenic blood concentration is normally distributed with mean \u0016= 3:2\u0016g/dl and standard deviation \u001b= 1:5\u0016g/dl. what range of arsenic blood concentration defines the middle 95% of this distribution? 3.29 age at childbirth. in the last decade, the average age of a mother at childbirth is 26.4 years, with standard deviation 5.8 years. the distribution of age at childbirth is approximately normal. (a) what proportion of women who give birth are 21 years of age or older? (b) giving birth at what age puts a woman in the upper 2.5% of the age distribution? 3.30 find the sd. find the standard deviation of the distribution in the following situations. (a) mensa is an organization whose members have iqs in the top 2% of the population. iqs are normally distributed with mean 100, and the minimum iq score required for admission to mensa is 132. (b) cholesterol levels for women aged 20 to 34 follow an approximately normal distribution with mean 185 milligrams per deciliter (mg/dl). women with cholesterol levels above 220 mg/dl are considered to have high cholesterol and about 18.5% of women fall into this category. 3.31 underage drinking, part iii. as first referenced in exercise 3.7, about 70% of 18-20 year olds consumed alcoholic beverages in 2008. consider a random sample of fifty 18-20 year olds. (a) of these fifty people, how many would be expected to have consumed alcoholic beverages? with what standard deviation? (b) evaluate the conditions for using the normal approximation to the binomial. what is the probability that 45 or more people in this sample have consumed alcoholic beverages? 3.32 chickenpox, part iii. as first referenced in exercise 3.8, about 90% of american adults had chickenpox before adulthood. consider a random sample of 120 american adults. (a) how many people in this sample would be expected to have had chickenpox in their childhood? with what standard deviation? (b) evaluate the conditions for using the normal approximation to the binomial. what is the probability that 105 or fewer people in this sample have had chickenpox in their childhood? 3.8. exercises 191 3.33 university admissions. suppose a university announced that it admitted 2,500 students for the following year’s freshman class. however, the university has dorm room spots for only 1,786 freshman students. if there is a 70% chance that an admitted student will decide to accept the o ffer and attend this university, what is the approximate probability that the university will not have enough dormitory room spots for the freshman class? 3.34 sat scores. sat scores (out of 2400) are distributed normally with a mean of 1500 and a standard deviation of 300. suppose a school council awards a certificate of excellence to all students who score at least 1900 on the sat, and suppose we pick one of the recognized students at random. what is the probability this student’s score will be at least 2100? (the material covered in section 2.2 would be useful for this question.) 3.35 scores on stats final. the final exam scores of 20 introductory statistics students are plotted below. do these data appear to follow a normal distribution? explain your reasoning. scores60 70 80 90 ●● ●●● ● ●●● ● ●● ●● ●●● ●●● theoretical quantilessample quantiles −2 −1 0 1 260708090 3.36 heights of female college students. the heights of 25 female college students are plotted below. do these data appear to follow a normal distribution? explain your reasoning. heights505560657075 ●●●●●●●●●●●●●●●●●●●●●●●●● theoretical quantilessample quantiles −2 −1 0 1 255606570 3.8.4 poisson distribution 3.37 computing poisson probabilities. this is a simple exercise in computing probabilities for a poisson random variable. suppose that xis a poisson random variable with rate parameter \u0015= 2. calculate p(x= 2), p(x\u00142), andp(x\u00153). 3.38 stenographer’s typos. a very skilled court stenographer makes one typographical error (typo) per hour on average. (a) what are the mean and the standard deviation of the number of typos this stenographer makes in an hour? (b) calculate the probability that this stenographer makes at most 3 typos in a given hour. (c) calculate the probability that this stenographer makes at least 5 typos over 3 hours. 192 chapter 3. distributions of random v ariables 3.39 customers at a coffee shop. a coffee shop serves an average of 75 customers per hour during the morning rush. (a) what are the mean and the standard deviation of the number of customers this co ffee shop serves in one hour during this time of day? (b) would it be considered unusually low if only 60 customers showed up to this co ffee shop in one hour during this time of day? (c) calculate the probability that this co ffee shop serves 70 customers in one hour during this time of day. 3.40 osteosarcoma in nyc. osteosarcoma is a relatively rare type of bone cancer. it occurs most often in young adults, age 10 - 19; it is diagnosed in approximately 8 per 1,000,000 individuals per year in that age group. in new york city (including all five boroughs), the number of young adults in this age range is approximately 1,400,000. (a) what is the expected number of cases of osteosarcoma in nyc in a given year? (b) what is the probability that 15 or more cases will be diagnosed in a given year? (c) the largest concentration of young adults in nyc is in the borough of brooklyn, where the population in that age range is approximately 450,000. what is the probability of 10 or more cases in brooklyn in a given year? (d) suppose that in a given year, 10 cases of osteosarcoma were observed in nyc, with all 10 cases occurring among young adults living in brooklyn. an o fficial from the nyc public health department claims that the probability of this event (that is, the probability of 10 or more cases being observed, and all of them occurring in brooklyn) is what was calculated in part c). is the o fficial correct? explain your answer. you may assume that your answer to part c) is correct. this question can be answered without doing any calculations. (e) suppose that over five years, there was one year in which 10 or more cases of osteosarcoma were observed in brooklyn. is the probability of this event equal to the probability calculated in part c)? explain your answer. 3.41 how many cars show up? for monday through thursday when there isn’t a holiday, the average number of vehicles that visit a particular retailer between 2pm and 3pm each afternoon is 6.5, and the number of cars that show up on any given day follows a poisson distribution. (a) what is the probability that exactly 5 cars will show up next monday? (b) what is the probability that 0, 1, or 2 cars will show up next monday between 2pm and 3pm? (c) there is an average of 11.7 people who visit during those same hours from vehicles. is it likely that the number of people visiting by car during this hour is also poisson? explain. 3.42 lost baggage. occasionally an airline will lose a bag. suppose a small airline has found it can reasonably model the number of bags lost each weekday using a poisson model with a mean of 2.2 bags. (a) what is the probability that the airline will lose no bags next monday? (b) what is the probability that the airline will lose 0, 1, or 2 bags on next monday? (c) suppose the airline expands over the course of the next 3 years, doubling the number of flights it makes, and the ceo asks you if it’s reasonable for them to continue using the poisson model with a mean of 2.2. what is an appropriate recommendation? explain. 3.8. exercises 193 3.43 hemophilia. hemophilia is a sex-linked bleeding disorder that slows the blood clotting process. in severe cases of hemophilia, continued bleeding occurs after minor trauma or even in the absence of injury. hemophilia a ffects 1 in 5,000 male births. in the united states, about 400 males are born with hemophilia each year; there are approximately 4,000,000 births per year. note: this problem is best done using statistical software. (a) what is the probability that at most 380 newborns in a year are born with hemophilia? (b) what is the probability that 450 or more newborns in a year are born with hemophilia? (c) consider a hypothetical country in which there are approximately 1.5 million births per year. if the incidence rate of hemophilia is equal to that in the us, how many newborns are expected to have hemophilia in a year, with what standard deviation? 3.44 opioid overdose. the us centers for disease control (cdc) has been monitoring the rate of deaths from opioid overdoses for at least the last 15 years. in 2013, the rate of opioid-related deaths has risen to 6.8 deaths per year per 100,000 non-hispanic white members. in 2014-2015, the population of essex county, ma, was approximately 769,000, of whom 73% are non-hispanic white. assume that incidence rate of opioid deaths in essex county is the same as the 2013 national rate. note: this problem is best done using statistical software. (a) in 2014, essex county reported 146 overdose fatalities from opioids. assume that all of these deaths occurred in the non-hispanic white members of the population. what is the probability of 146 or more such events a year? (b) what was the observed rate of opioid-related deaths in essex county in 2014, stated in terms of deaths per 100,000 non-hispanic white members of the population? (c) in 2015, essex county reported 165 opioid-related deaths in its non-hispanic white population. using the rate from part (b), calculate the probability of 165 or more such events. 3.8.5 distributions related to bernoulli trials 3.45 married women. the 2010 american community survey estimates that 47.1% of women ages 15 years and over are married. suppose that a random sample of women in this age group are selected for a research study.40 (a) on average, how many women would need to be sampled in order to select a married woman? what is the standard deviation? (b) if the proportion of married women were actually 30%, what would be the new mean and standard deviation? (c) based on the answers to parts (a) and (b), how does decreasing the probability of an event a ffect the mean and standard deviation of the wait time until success? 40u.s. census bureau, 2010 american community survey, marital status. 194 chapter 3. distributions of random v ariables 3.46 donating blood, part ii. recall from problem 3.11 that a patient with type o+ blood can receive either type o+ or type o- blood, while a patient with type o- blood can only receive type o- blood. according to data collected from blood donors, 0.37 of white, non-hispanic donors are type o+ and 0.08 are type o-. for the following questions, assume that only white, non-hispanic donors are being tested. (a) on average, how many donors would need to be randomly sampled for a type o+ donor to be identified? with what standard deviation? (b) what is the probability that 4 donors must be sampled to identify a type o+ donor? (c) what is the probability that more than 4 donors must be sampled to identify a type o+ donor? (d) what is the probability of the first type o- donor being found within the first 4 people? (e) on average, how many donors would need to be randomly sampled for a type o- donor to be identified? with what standard deviation? (f) what is the probability that fewer than 4 donors must be tested before a type o- donor is found? 3.47 wolbachia infection, part ii. recall from problem 3.15 that 30% of the drosophila stocks at the bdsc are infected with wolbachia. suppose a research assistant randomly samples a stock one at a time until identifying an infected stock. (a) calculate the probability that an infected stock is found within the first 5 stocks sampled. (b) what is the probability that no more than 5 stocks must be tested before an infected one is found? (c) calculate the probability that at least 3 stocks must be tested for an infected one to be found. 3.48 with and without replacement. in the following situations assume that half of the specified population is male and the other half is female. (a) suppose you’re sampling from a room with 10 people. what is the probability of sampling two females in a row when sampling with replacement? what is the probability when sampling without replacement? (b) now suppose you’re sampling from a stadium with 10,000 people. what is the probability of sampling two females in a row when sampling with replacement? what is the probability when sampling without replacement? (c) we often treat individuals who are sampled from a large population as independent. using your findings from parts (a) and (b), explain whether or not this assumption is reasonable. 3.49 eye color. a husband and wife both have brown eyes but carry genes that make it possible for their children to have brown eyes (probability 0.75), blue eyes (0.125), or green eyes (0.125). (a) what is the probability the first blue-eyed child they have is their third child? assume that the eye colors of the children are independent of each other. (b) on average, how many children would such a pair of parents have before having a blue-eyed child? what is the standard deviation of the number of children they would expect to have until the first blue-eyed child? 3.8. exercises 195 3.50 defective rate. a machine that produces a special type of transistor (a component of computers) has a 2% defective rate. the production is considered a random process where each transistor is independent of the others. (a) what is the probability that the 10thtransistor produced is the first with a defect? (b) what is the probability that the machine produces no defective transistors in a batch of 100? (c) on average, how many transistors would you expect to be produced before the first with a defect? what is the standard deviation? (d) another machine that also produces transistors has a 5% defective rate where each transistor is produced independent of the others. on average how many transistors would you expect to be produced with this machine before the first with a defect? what is the standard deviation? (e) based on your answers to parts (c) and (d), how does increasing the probability of an event a ffect the mean and standard deviation of the wait time until success? 3.51 rolling a die. calculate the following probabilities and indicate which probability distribution model is appropriate in each case. you roll a fair die 5 times. what is the probability of rolling (a) the first 6 on the fifth roll? (b) exactly three 6s? (c) the third 6 on the fifth roll? 3.52 playing darts. calculate the following probabilities and indicate which probability distribution model is appropriate in each case. a very good darts player can hit the direct center of the board 65% of the time. what is the probability that a player: (a) hits the bullseye for the 10thtime on the 15thtry? (b) hits the bullseye 10 times in 15 tries? (c) hits the first bullseye on the third try? 3.53 cilantro preference. cilantro leaves are widely used in many world cuisines. while some people enjoy it, others claim that it has a soapy, pungent aroma. a recent study conducted on participants of european ancestry identified a genetic variant that is associated with soapy-taste detection. in the initial questionnaire, 1,994 respondents out of 14,604 reported that they thought cilantro tasted like soap. suppose that participants are randomly selected one by one. (a) what is the probability that the first soapy-taste detector is the third person selected? (b) what is the probability that in a sample of ten people, no more than two are soapy-taste detectors? (c) what is the probability that three soapy-taste detectors are identified from sampling ten people? (d) what is the mean and standard deviation of the number of people that must be sampled if the goal is to identify four soapy-taste detectors? 3.54 serving in volleyball. a not-so-skilled volleyball player has a 15% chance of making the serve, which involves hitting the ball so it passes over the net on a trajectory such that it will land in the opposing team’s court. suppose that serves are independent of each other. (a) what is the probability that on the 10thtry, the player makes their 3rdsuccessful serve? (b) suppose that the player has made two successful serves in nine attempts. what is the probability that their 10thserve will be successful? (c) even though parts (a) and (b) discuss the same scenario, explain the reason for the discrepancy in probabilities. 196 chapter 3. distributions of random v ariables 3.55 cilantro preference, part ii. recall from problem 3.53 that in a questionnaire, 1,994 respondents out of 14,604 reported that they thought cilantro tasted like soap. suppose that a random sample of 15 individuals are selected for further study. (a) what is the mean and variance of the number of people sampled that are soapy-taste detectors? (b) what is the probability that 4 of the people sampled are soapy-taste detectors? (c) what is the probability that at most 2 of the people sampled are soapy-taste detectors? (d) suppose that the 15 individuals were sampled with replacement. what is the probability of selecting 4 soapy-taste detectors? (e) compare the answers from parts (b) and (d). explain why the answers are essentially the same. 3.56 dental caries. a study to examine oral health of schoolchildren in belgium found that of the 4,351 children examined, 44% were caries free (i.e., free of decay, restorations, and missing teeth). suppose that children are sampled one by one. (a) what is the probability that at least three caries free children are identified from sampling seven children? (b) what is the probability that the first caries free child is the second one selected? (c) suppose that in a single school of 350 children, the incidence rate of caries equals the national rate. if 10 schoolchildren are selected at random, what is the probability that at most 2 have caries? (d) what is the probability that in a sample of 50 children, no more than 15 are caries free? 3.8.6 distributions for pairs of random variables 3.57 joint distributions, part i. supposexandyhave the following joint distribution. y = -1 y = 1 x = 0 0.20 0.40 x = 1 0.30 0.10 (a) calculate the marginal distributions of xandy. (b) calculate the mean, variance, and standard deviation of x. (c) what are the standardized values of x? (d) the mean and standard deviation of yare 0 and 1, respectively, and the two standardized values for y are -1 and 1. calculate \u001ax;y, the correlation coe fficient ofxandy. (e) arexandyindependent? explain your answer. 3.58 joint distributions, part ii. supposexandyhave the following joint distribution. y = -1 y = 1 x = 0 0.25 0.25 x = 1 0.25 0.25 (a) arexandyindependent? (b) calculate the correlation between xandy. (c) are your answers to parts (a) and (b) consistent? explain your answer. 3.8. exercises 197 3.59 joint distributions, part iii. consider the following joint probability distribution: y x -1 0 1 -1 0.10 0 0.35 0 0 0.10 0.10 1 0.15 0.10 0.10 (a) calculate the marginal distributions. (b) compute \u0016x. (c) compute \u001b2 y. (d) calculate the conditional distribution of x, giveny= 0. 3.60 dice rolls and coin tosses. letxrepresent the outcome from a roll of a fair six-sided die. then, toss a fair coinxtimes and let ydenote the number of tails observed. (a) consider the joint probability table of xandy. how many entries are in the table for the joint distribution ofxandy? how many entries equal 0? (b) compute the joint probability p(x= 1;y= 0). (c) compute the joint probability p(x= 1;y= 2). (d) compute the joint probability p(x= 6;y= 3). 3.61 health insurance claims. in the health insurance example introduced in example 3.6, the largest annual expense for the annual employee ($1,108) was caused by 8 visits to a provider for a knee injury requiring physical therapy. the couple has confidence that this or a similar injury will not happen again in the coming year, and wonders about the e ffect of reduced visits on expected total health care costs and its variability. a new joint distribution of health care cost for the couple is shown in the following table: partner costs, y employee costs, x $968 $988 $968 0.18 0.12 $1,008 0.15 0.25 $1,028 0.07 0.23 (a) for the partner, will there be a change to the expected cost and its standard deviation? (b) calculated the expected value and standard deviation for the employee’s costs. (c) calculate the expected total cost for the couple. (d) calculate the new correlation for employee and partner costs. (e) calculate the standard deviation of the total cost. 198 chapter 4 foundations for inference 4.1 variability in estimates 4.2 confidence intervals 4.3 hypothesis testing 4.4 notes 4.5 exercises 199 not surprisingly, many studies are now demonstrating the adverse e ffect of obesity on health outcomes. a 2017 study conducted by the consortium studying the global burden of disease estimates that high body mass index (a measure of body fat that adjusts for height and weight) may account for as many as 4.0 million deaths globally.1in addition to the physiologic e ffects of being overweight, other studies have shown that perceived weight status (feeling that one is overweight or underweight) may have a significant e ffect on self-esteem.2,3 as stated in its mission statement, the united states centers for disease control and prevention (us cdc) \"serves as the national focus for developing and applying disease prevention and control, environmental health, and health promotion and health education activities designed to improve the health of the people of the united states\".4since it is not feasible to measure the health status and outcome of every single us resident, the cdc estimates features of health from samples taken from the population, via large surveys that are repeated periodically. these surveys include the national health interview survey (nhis), the national health and nutrition examination survey (nhanes), the youth risk behavior surveillance system (yrbss) and the behavior risk factor surveillance system (brfss). in the language of statistics, the average weight of all us adults is apopulation parameter ; the mean weight in a sample or survey is an estimate of population average weight. the principles of statistical inference provide not only estimates of population parameters, but also measures of uncertainty that account for the fact that di fferent random samples will produce di fferent estimates because of the variability of random sampling; i.e., two di fferent random samples will not include exactly the same people. this chapter introduces the important ideas in drawing estimates from samples by discussing methods of inference for a population mean, \u0016, including three widely used tools: point estimates for a population mean, interval estimates that include both a point estimate and a margin of error, and a method for testing scientific hypotheses about \u0016. the concepts used in this chapter will appear throughout the rest of the book, which discusses inference for other settings. while particular equations or formulas may change to reflect the details of a problem at hand, the fundamental ideas will not. 1doi: 10.1056/nejmoa1614362 2j ment health policy econ. 2010 jun;13(2):53-63 3doi: 10.1186/1471-2458-7-80 4https://www.cdc.gov/maso/pdf/cdcmiss.pdf 200 the brfss was established in 1984 in 15 states to collect data using telephone interviews about health-related risk behaviors, chronic health conditions, and the use of preventive services. it now collects data in all 50 states and the district of columbia from more than 400,000 interviews conducted each year. the data setcdccontains a small number of variables from a random sample of 20,000 responses from the 264,684 interviews from the brfss conducted in the year 2000. part of this dataset is shown in figure 4.1, with the variables described in figure 4.2.5 case age gender weight wtdesire height genhlth 1 1 77 m 175 175 70 good 2 2 33 f 125 115 64 good 3 3 49 f 105 105 60 good 20000 20000 83 m 170 165 69 good figure 4.1: four cases from the cdcdataset. variable variable definition. case case number in the dataset, ranging from 1 to 20,000. age age in years. gender a factor variable, with levels mfor male, ffor female. weight weight in pounds. wtdesire weight that the respondent wishes to be, in pounds. height height in inches. genhlth a factor variable describing general health status, with levels excellent ,very good ,good ,fair ,poor . figure 4.2: some variables and their descriptions for the cdcdataset. few studies are as large as the original brfss dataset (more than 250,000 cases); in fact, few are as large as the 20,000 cases in the dataset cdc. the dataset cdcis large enough that estimates calculated from cdccan be thought of as essentially equivalent to the population characteristics of the entire us adult population. this chapter uses a random sample of 60 cases from cdc, stored as cdc.samp , to illustrate the e ffect of sampling variability and the ideas behind inference. in other words, suppose that cdcrepresents the population, and that cdc.samp is a sample from the population; the goal is to estimate characteristics of the population of 20,000 using only the data from the 60 individuals in the sample. for labs, slides, and other resources, please visit www.openintro.org/book/biostat 5with small modifications (character strings re-coded as factors), the data appears in this text as it does in an openintro lab. https://www.openintro.org/go?id=statlab_r_core_intro_to_data 4.1. v ariability in estimates 201 4.1 variability in estimates a natural way to estimate features of the population, such as the population mean weight, is to use the corresponding summary statistic calculated from the sample.6the mean weight in the sample of 60 adults in cdc.samp isxweight = 173:3 lbs; this sample mean is a point estimate of the population mean, \u0016weight . if a di fferent random sample of 60 individuals were taken from cdc, the new sample mean would likely be di fferent as a result of sampling variation . while estimates generally vary from one sample to another, the population mean is a fixed value. guided practice 4.1 how would one estimate the di fference in average weight between men and women? given that xmen= 185:1 lbs andxwomen = 162:3 lbs, what is a good point estimate for the population di fference?7 point estimates become more accurate with increasing sample size. figure 4.3 shows the sample mean weight calculated for random samples drawn from cdc, where sample size increases by 1 for each draw until sample size equals 500. the red dashed horizontal line in the figure is drawn at the average weight of all adults in cdc, 169.7 lbs, which represents the population mean weight.8 mean weight 0100 200 300 400 500130140150160170180190200 sample size figure 4.3: the mean weight computed for a random sample from cdc, increasing sample size one at a time until n= 500. the sample mean approaches the population mean (i.e., mean weight in cdc) as sample size increases. note how a sample size around 50 may produce a sample mean that is as much as 10 lbs higher or lower than the population mean. as sample size increases, the fluctuations around the population mean decrease; in other words, as sample size increases, the sample mean becomes less variable and provides a more reliable estimate of the population mean. 6other population parameters, such as population median or population standard deviation, can also be estimated using sample versions. 7given thatxmen = 185:1 lbs andxwomen = 162:3 lbs, the di fference of the two sample means, 185 :1\u0000162:3 = 22:8lbs, is a point estimate of the di fference. the data in the random sample suggests that adult males are, on average, about 23 lbs heavier than adult females. 8it is not exactly the mean weight of all us adults, but will be very close since cdcis so large. 202 chapter 4. foundations for inference 4.1.1 the sampling distribution for the mean the sample mean weight calculated from cdc.samp is 173.3 lbs. another random sample of 60 participants might produce a di fferent value of x, such as 169.5 lbs; repeated random sampling could result in additional di fferent values, perhaps 172.1 lbs, 168.5 lbs, and so on. each sample meanxcan be thought of as a single observation from a random variable x. the distribution of xis called the sampling distribution of the sample mean , and has its own mean and standard deviation like the random variables discussed in chapter 3. the concept of a sampling distribution can be illustrated by taking repeated random samples from cdc. figure 4.4 shows a histogram of sample means from 1,000 random samples of size 60 from cdc. the histogram provides an approximation of the theoretical sampling distribution of xfor samples of size 60. sample meanfrequency 150155160165170175180185190 150155160165170175180185190020406080100120140 figure 4.4: a histogram of 1000 sample means for weight among us adults, where the samples are of size n= 60. sampling distribution the sampling distribution is the distribution of the point estimates based on samples of a fixed size from a certain population. it is useful to think of a particular point estimate as being drawn from a sampling distribution. since the complete sampling distribution consists of means for all possible samples of size 60, drawing a much larger number of samples provides a more accurate view of the distribution; the left panel of figure 4.5 shows the distribution calculated from 100,000 sample means. a normal probability plot of these sample means is shown in the right panel of figure 4.5. all of the points closely fall around a straight line, implying that the distribution of sample means is nearly normal (see section 3.3). this result follows from the central limit theorem. central limit theorem, informal description if a sample consists of at least 30 independent observations and the data are not strongly skewed, then the distribution of the sample mean is well approximated by a normal model. 4.1. v ariability in estimates 203 sample meanfrequency 150155160165170175180185190020004000 ●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●sample means theoretical quantiles−4−2024150160170180190 figure 4.5: the left panel shows a histogram of the sample means for 100,000 random samples. the right panel shows a normal probability plot of those sample means. the sampling distribution for the mean is unimodal and symmetric around the mean of the random variable x. statistical theory can be used to show that the mean of the sampling distribution forxis exactly equal to the population mean \u0016. however, in almost any study, conclusions about a population parameter must be drawn from the data collected from a single sample. the sampling distribution of xis a theoretical concept, since obtaining repeated samples by conducting a study many times is not possible. in other words, it is not feasible to calculate the population mean \u0016by finding the mean of the sampling distribution forx. 4.1.2 standard error of the mean the standard error (se) of the sample mean measures the sample-to-sample variability of x,se standard errorthe extent to which values of the repeated sample means oscillate around the population mean. the theoretical standard error of the sample mean is calculated by dividing the population standard deviation (\u001bx) by the square root of the sample size n. since the population standard deviation \u001b is typically unknown, the sample standard deviation sis often used in the definition of a standard error;sis a reasonably good estimate of \u001b. ifxrepresents the sample mean weight, its standard error (denoted by se) is sex=sxpn=49:04p 60= 6:33: this estimate tends to be su fficiently good when the sample size is at least 30 and the population distribution is not strongly skewed. in the case of skewed distributions, a larger sample size is necessary. 204 chapter 4. foundations for inference the probability tools of section 3.1 can be used to derive the formula \u001bx=\u001bx=pn, but the derivation is not shown here. larger sample sizes produce sampling distributions that have lower variability. increasing the sample size causes the distribution of xto be clustered more tightly around the population mean \u0016, allowing for more accurate estimates of \u0016from a single sample, as shown in figure 4.6. when sample size is large, it is more likely that any particular sample will have a mean close to the population mean. sample meanfrequency 150155160165170175180185190 150155160165170175180185190020406080100120140 (a) sample meanfrequency 150155160165170175180185190 150155160165170175180185190020406080100120140 (b) figure 4.6: (a) reproduced from figure 4.4, an approximation of the sampling distribution of xwithn= 60. (b) an approximation of the sampling distribution ofxwithn= 200. the standard error (se) of the sample mean givennindependent observations from a population with standard deviation \u001b, the standard error of the sample mean is equal to sex=sxpn: this is an accurate estimate of the theoretical standard deviation of xwhen the sample size is at least 30 and the population distribution is not strongly skewed. summary: point estimate terminology – the population mean and standard deviation are denoted by \u0016and\u001b. – the sample mean and standard deviation are denoted by xands. – the distribution of the random variable xrefers to the collection of sample means if multiple samples of the same size were repeatedly drawn from a population. – the mean of the random variable xequals the population mean \u0016. in the notation of chapter 3,\u0016x=e(x) =\u0016. – the standard deviation of x(\u001bx) is called the standard error (se) of the sample mean. – the theoretical standard error of the sample mean, as calculated from a single sample of sizen, is equal to\u001bpn. the standard error is abbreviated by se and is usually estimated by usings, the sample standard deviation, such that se=spn. 4.2. confidence interv als 205 4.2 confidence intervals 4.2.1 interval estimates for a population parameter while a point estimate consists of a single value, an interval estimate provides a plausible range of values for a parameter. when estimating a population mean \u0016, aconfidence interval for \u0016has the general form (x\u0000m;x+m) =x\u0006m; wheremis the margin of error . intervals that have this form are called two-sided confidence intervals because they provide both lower and upper bounds, x\u0000mandx+m, respectively. onesided sided intervals are discussed in section 4.2.3. the standard error of the sample mean is the standard deviation of its distribution; additionally, the distribution of sample means is nearly normal and centered at \u0016. under the normal model, the sample mean xwill be within 1.96 standard errors (i.e., standard deviations) of the population mean\u0016approximately 95% of the time.9thus, if an interval is constructed that spans 1.96 standard errors from the point estimate in either direction, a data analyst can be 95% confident that the interval x\u00061:96\u0002se (4.2) contains the population mean. the value 95% is an approximation, accurate when the sampling distribution for the sample mean is close to a normal distribution. this assumption holds when the sample size is su fficiently large (guidelines for ‘su fficiently large’ are given in section 4.4). μ = 169.7●●●●●●●●●●●●●●●●●●●●●●●●●● figure 4.7: twenty-five samples of size n= 60 were taken from cdc. for each sample, a 95% confidence interval was calculated for the population average adult weight. only 1 of these 25 intervals did not contain the population mean, \u0016= 169:7 lbs. the phrase \"95% confident\" has a subtle interpretation: if many samples were drawn from a population, and a confidence interval is calculated from each one using equation 4.2, about 95% of those intervals would contain the population mean \u0016. figure 4.7 illustrates this process with 25 samples taken from cdc. of the 25 samples, 24 contain the mean weight in cdcof 169.7 lbs, while one does not. 9in other words, the z-score of 1.96 is associated with 2.5% area to the right (and z= -1.96 has 2.5% area to the left); this can be found on normal probability tables or from using statistical software. 206 chapter 4. foundations for inference just as with the sampling distribution of the sample mean, the interpretation of a confidence interval relies on the abstract construct of repeated sampling. a data analyst, who can only observe one sample, does not know whether the population mean lies within the single interval calculated. the uncertainty is due to random sampling—by chance, it is possible to select a sample from the population that has unusually high (or low) values, resulting in a sample mean xthat is relatively far from\u0016, and by extension, a confidence interval that does not contain \u0016. example 4.3 the sample mean adult weight from the 60 observations in cdc.samp isxweight = 173:3 lbs, and the standard deviation is sweight = 49:04 lbs. use equation 4.2 to calculate an approximate 95% confidence interval for the average adult weight in the us population. the standard error for the sample mean is se x=49:04p 60= 6:33 lbs. the 95% confidence interval is xweight\u00061:96sex= 173:3\u0006(1:96)(6:33) = (160:89;185:71) lbs. the data support the conclusion that, with 95% confidence, the average weight of us adults is between approximately 161 and 186 lbs. figure 4.5 visually shows that the sampling distribution is nearly normal. to assess normality of the sampling distribution without repeated sampling, it is necessary to check whether the data are skewed. although figure 4.8 shows some skewing, the sample size is large enough that the confidence interval should be reasonably accurate. weight (lbs)frequency 50 100 150 200 250 300051015202530 figure 4.8: histogram of weight incdc.samp 4.2. confidence interv als 207 guided practice 4.4 there are 31 females in the sample of 60 us adults, and the average and standard deviation of weight for these individuals are 162.3 lbs and 57.74 lbs, respectively. a histogram of weight for the 31 females is shown in figure 4.9. calculate an approximate 95% confidence interval for the average weight of us females. is the interval likely to be accurate?10 weight of femalesfrequency 100 150 200 250 300 350 40005101520 figure 4.9: histogram of weight for the 31 females in cdc.samp . 4.2.2 changing the confidence level ninety-five percent confidence intervals are the most commonly used interval estimates, but intervals with confidence levels other than 95% can also be constructed. the general formula for a confidence interval (for the population mean \u0016) is given by x\u0006z?\u0002se; (4.5) wherez?is chosen according to the confidence level. when calculating a 95% confidence level, z? is 1.96, since the area within 1.96 standard deviations of the mean captures 95% of the distribution. to construct a 99% confidence interval, z?must be chosen such that 99% of the normal curve is captured between - z?andz?. example 4.6 letybe a normally distributed random variable. ninety-nine percent of the time, ywill be within how many standard deviations of the mean? this is equivalent to the z-score with 0.005 area to the right of zand 0.005 to the left of \u0000z. in the normal probability table, this is the z-value that with 0.005 area to its right and 0.995 area to its left. the closest two values are 2.57 and 2.58; for convenience, round up to 2.58. the unobserved random variable ywill be within 2.58 standard deviations of \u001699% of the time, as shown in figure 4.10. 10applying equation 4.2: 162 :3\u0006(1:96)(57:73=p 31)!(149:85;174:67). the usual interpretation would be that a data analyst can be about 95% confident the average weight of us females is between approximately 150 and 175 lbs. however, the histogram of female weights shows substantial right skewing, and several females with recorded weights larger than 200 lbs. the confidence interval is probably not accurate; a larger sample should be collected in order for the sampling distribution of the mean to be approximately normal. chapter 5 will introduce the t-distribution, which is more reliable with small sample sizes than the z-distribution. 208 chapter 4. foundations for inference standard deviations from the mean−3 −2 −1 0 1 2 395%, extends −1.96 to 1.9699%, extends −2.58 to 2.58 figure 4.10: the area between - z?andz?increases asjz?jbecomes larger. if the confidence level is 99%, z?is chosen such that 99% of the normal curve is between -z?andz?, which corresponds to 0.5% in the lower tail and 0.5% in the upper tail: z?= 2:58. a 99% confidence interval will have the form x\u00062:58\u0002se; (4.7) and will consequently be wider than a 95% interval for \u0016calculated from the same data, since the margin of error mis larger. example 4.8 create a 99% confidence interval for the average adult weight in the us population using the data incdc.samp . the point estimate is xweight = 173:3 and the standard error is sex= 6:33. apply the 99% confidence interval formula: xweight\u00062:58\u0002sex!(156:97;189:63). a data analyst can be 99% confident that the average adult weight is between 156.97 and 189.63 lbs. the 95% confidence interval for the average adult weight is (160.89, 185.71) lbs. increasing the confidence level to 99% results in the interval (156.97, 189.63) lbs; this wider interval is more likely to contain the population mean \u0016. however, increasing the confidence level comes at a cost: a wider interval is less informative in providing a precise estimate of the population mean. consider the extreme: to be \"100% confident\" that an interval contains \u0016, the interval must span all possible values of \u0016. for example, with 100% confidence the average weight is between 0 and 1000 lbs; while this interval necessarily contains \u0016, it has no interpretive value and is completely uninformative.11 decreasing the confidence level produces a narrower interval; the estimate is more precise, but also more prone to inaccuracy. for example, consider a 50% confidence interval for average adult weight using cdc.samp : thez?value is 0.67, and the confidence interval is (169.06, 177.54) lbs. this interval provides a more precise estimate of the population average weight \u0016than the 99% or 95% confidence intervals, but the increased precision comes with less confidence about whether the 11strictly speaking, to be 100% confident requires an interval spanning all positive numbers; 1000 lbs has been arbitrarily chosen as an upper limit for human weight. 4.2. confidence interv als 209 interval contains \u0016. in a theoretical setting of repeated sampling, if 100 50% confidence intervals were computed, only half could be expected to contain \u0016. the choice of confidence level is a trade-o ffbetween obtaining a precise estimate and calculating an interval that can be reasonably expected to contain the population parameter. in published literature, the most used confidence intervals are the 90%, 95%, and 99%. 4.2.3 one-sided confidence intervals one-sided confidence intervals for a population mean provide either a lower bound or an upper bound, but not both. one-sided confidence intervals have the form (x\u0000m;1) or (\u00001;x+m): while the margin of error mfor a one-sided interval is still calculated from the standard error ofxand az?value, the choice of z?is a different than for a two-sided interval. for example, the intent of a 95% one-sided upper confidence interval is to provide an upper bound msuch that a data analyst can be 95% confident that a population mean \u0016is less than x+m. thez?value must correspond to the point on the normal distribution that has 0.05 area in the right tail, z?= 1:645.12 a one-sided upper 95% confidence interval will have the form (\u00001;x+ 1:645\u0002se): example 4.9 calculate a lower 95% confidence interval for the population average adult weight in the united states. in the sample of 60 adults in cdc.samp , the mean and standard error are x= 173:3 and se= 6:33 days. the lower bound is 173 :3\u0000(1:645\u00026:33) = 163:89. the lower 95% interval (163 :89;1) suggests that one can be 95% confident that the population average adult weight is at least 163.9 lbs. guided practice 4.10 calculate an upper 99% confidence interval for the population average adult weight in the united states. the mean and standard error for weight in cdc.samp arex= 173:3 andse= 6:33 days.13 4.2.4 interpreting confidence intervals the correct interpretation of an xx% confidence interval is, \"we are xx% confident that the population parameter is between . . . \" while it may be tempting to say that a confidence interval captures the population parameter with a certain probability, this is a common error. the confidence level only quantifies how plausible it is that the parameter is within the interval; there is no probability associated with whether a parameter is contained in a specific confidence interval. the confidence coe fficient reflects the nature of a procedure that is correct xx% of the time, given that the assumptions behind the calculations are true. 12previously, with a two-sided interval, 1.96 was chosen in order to have a total area of 0.05 from both the right and left tails. 13for a one-sided 99% confidence interval, the z?value corresponds to the point with 0.01 area in the right tail, z?= 2:326. thus, the upper bound for the interval is 173 :3 + (2:326\u00026:33) = 188:024:the upper 99% interval ( \u00001;188:024) suggests that one can be 99% confident that the population average adult weight is at most 188.0 lbs. 210 chapter 4. foundations for inference the conditions regarding the validity of the normal approximation can be checked using the numerical and graphical summaries discussed in chapter 1. however, the condition that data should be from a random sample is sometimes overlooked. if the data are not from a random sample, then the confidence interval no longer has interpretive value, since there is no population mean to which the confidence interval applies. for example, while only simple arithmetic is needed to calculate a confidence interval for bmi from the famuss dataset in chapter 1, the participants in the study are almost certainly not a random sample from some population; thus, a confidence interval should not be calculated in this setting. example 4.11 body mass index (bmi) is one measure of body weight that adjusts for height. the national health and nutrition examination survey (nhanes) consists of a set of surveys and measurements conducted by the us cdc to assess the health and nutritional status of adults and children in the united states. the dataset nhanes.samp contains 76 variables and is a random sample of 200 individuals from the measurements collected in the years 2009-2010 and 2012-2013.14use nhanes.samp to calculate a 95% confidence interval for adult bmi in the us population, and assess whether the data suggest americans tend to be overweight. in the random sample of 200 participants, bmi is available for all 135 of the participants that are 21 years of age or older. as shown in the histogram (figure 4.11), the data are right-skewed, with one large outlier. the outlier corresponds to an implausibly extreme bmi value of 69.0; since it seems likely that the value represents an error from when the data was recorded, this data point is excluded from the following analysis. the mean and standard deviation in this sample of 134 are 28.8 and 6.7 kg =meter2, respectively. the sample size is large enough to justify using the normal approximation when computing the confidence interval. the standard error of the mean is se = 6 :7=p 134 = 0:58, so the 95% confidence interval is given by xbmi\u0006(1:96)(se) = 28 :8\u0006(1:96)(0:58) = (27:7;29:9): based on this sample, a data analyst can be 95% confident that the average bmi of us adults is between 27.7 and 29.9 kg =m2. the world health organization (who) and other agencies use bmi to set normative guidelines for body weight. the current guidelines are shown in figure 4.12. the confidence interval (27.7, 29.9) kg =m2certainly suggests that the average bmi in the us population is higher than 21.7, the middle of the range for normal bmis, and even higher than 24.99, the upper limit of the normal weight category. these data indicate that americans tend to be overweight. 14the sample was drawn from a larger sample of 20,293 participants in the nhanes package, available from the comprehensive r archive network (cran). the cdc uses a complex sampling design that samples some demographic subgroups with larger probabilities, but nhanes.samp has been adjusted so that it can be viewed as a random sample of the us population. 4.2. confidence interv als 211 bmifrequency 20 30 40 50 60 7001020304050 figure 4.11: the distribution of bmifor the 135 adults in nhanes.samp . category bmi range underweight <18:50 normal (healthy weight) 18.5-24.99 overweight \u001525 obese \u001530 figure 4.12: who body weight categories based on bmi. 212 chapter 4. foundations for inference 4.3 hypothesis testing important decisions in science, such as whether a new treatment for a disease should be approved for the market, are primarily data-driven. for example, does a clinical study of a new cholesterol-lowering drug provide robust evidence of a beneficial e ffect in patients at risk for heart disease? a confidence interval can be calculated from the study data to provide a plausible range of values for a population parameter, such as the population average decrease in cholesterol levels. a drug is considered to have a beneficial e ffect on a population of patients if the population average e ffect is large enough to be clinically important. it is also necessary to evaluate the strength of the evidence that a drug is e ffective; in other words, is the observed e ffect larger than would be expected from chance variation alone? hypothesis testing is a method for calculating the probability of making a specific observation under a working hypothesis, called the null hypothesis. by assuming that the data come from a distribution specified by the null hypothesis, it is possible to calculate the likelihood of observing a value as extreme as the one represented by the sample. if the chances of such an extreme observation are small, there is enough evidence to reject the null hypothesis in favor of an alternative hypothesis. null and alternative hypotheses the null hypothesis ( h0)often represents either a skeptical perspective or a claim to be tested. the alternative hypothesis ( ha)is an alternative claim and is often represented by a range of possible parameter values. generally, an investigator suspects that the null hypothesis is not true and performs a hypothesis test in order to evaluate the strength of the evidence against the null hypothesis. the logic behind rejecting or failing to reject the null hypothesis is similar to the principle of presumption of innocence in many legal systems. in the united states, a defendant is assumed innocent until proven guilty; a verdict of guilty is only returned if it has been established beyond a reasonable doubt that the defendant is not innocent. in the formal approach to hypothesis testing, the null hypothesis ( h0) is not rejected unless the evidence contradicting it is so strong that the only reasonable conclusion is to reject h0in favor ofha. the next section presents the steps in formal hypothesis testing, which is applied when data are analyzed to support a decision or make a scientific claim. 4.3.1 the formal approach to hypothesis testing in this section, hypothesis testing will be used to address the question of whether americans generally wish to be heavier or lighter than their current weight. in the cdcdata, the two variables weight and wtdesire are, respectively, the recorded actual and desired weights for each respondent, measured in pounds. suppose that \u0016is the population average of the di fference weight\u0000wtdesire . using the observations from cdc.samp , assess the strength of the claim that, on average, there is no systematic preference to be heavier or lighter. 4.3. hypothesis testing 213 step 1: formulating null and alternative hypotheses the claim to be tested is that the population average of the di fference between actual and desired weight for us adults is equal to 0. h0:\u0016= 0: in the absence of prior evidence that people typically wish to be lighter (or heavier), it is reasonable to begin with an alternative hypothesis that allows for di fferences in either direction. ha:\u0016,0: the alternative hypothesis ha:\u0016,0 is called a two-sided alternative . a one-sided alternative could be used if, for example, an investigator felt there was prior evidence that people typically wish to weigh less than they currently do: ha:\u0016>0. more generally, when testing a hypothesis about a population mean \u0016, the null and alternative hypotheses are written as follows – for a two-sided alternative: h0:\u0016=\u00160; ha:\u0016,\u00160: – for a one-sided alternative: h0:\u0016=\u00160; ha:\u0016<\u0016 0 orh0:\u0016=\u00160; ha:\u0016>\u0016 0: the symbol \u0016denotes a population mean, while \u00160refers to the numeric value specified by the null hypothesis; in this example, \u00160= 0. note that null and alternative hypotheses are statements about the underlying population, not the observed values from a sample. step 2: specifying a significance level, it is important to specify how rare or unlikely an event must be in order to represent su fficient evidence against the null hypothesis. this should be done during the design phase of a study, to prevent any bias that could result from defining ’rare’ only after analyzing the results. when testing a statistical hypothesis, an investigator specifies a significance level , , that defines a ’rare’ event. typically, is chosen to be 0 :05, though it may be larger or smaller, depending on context; this is discussed in more detail in section 4.3.4. an level of 0:05 implies that an event occurring with probability lower than 5% will be considered su fficient evidence against h0. step 3: calculating the test statistic calculating the test statistic tis analogous to standardizing observations with z-scores as discussed in chapter 3. the test statistic quantifies the number of standard deviations between the sample meanxand the population mean \u0016: t=x\u0000\u00160 s=pn; wheresis the sample standard deviation and nis the number of observations in the sample. if x= weight\u0000wtdesire , then for the 60 recorded di fferences in cdc.samp ,x= 18:2 ands= 33:46. in this sample, respondents weigh on average about 18 lbs more than they wish. the test statistic is t=18:2\u00000 33:46=p 60= 4:22: the observed sample mean is 4.22 standard deviations to the right of \u00160= 0. 214 chapter 4. foundations for inference step 4: calculating the ppp-value theppp-value is the probability of observing a sample mean as or more extreme than the observed value, under the assumption that the null hypothesis is true. in samples of size 40 or more, the t-statistic will have a standard normal distribution unless the data are strongly skewed or extreme outliers are present. recall that a standard normal distribution has mean 0 and standard deviation 1. for two-sided tests, with ha:\u0016,\u00160, thep-value is the sum of the area of the two tails defined by thet-statistic: 2p(z\u0015jtj) =p(z\u0014\u0000jtj) +p(z\u0015jtj) (figure 4.13). t−statistic μ = 0 figure 4.13: a two-sided p-value forha:\u0016,\u00160on a standard normal distribution. the shaded regions represent observations as or more extreme than xin either direction. for one-sided tests with ha:\u0016>\u0016 0, thep-value is given by p(z\u0015t), as shown in figure 4.14. ifha:\u0016<\u0016 0, thep-value is the area to the left of the t-statistic,p(z\u0014t). μ = 0 t−statistic figure 4.14: a one-sided p-value forha:\u0016>\u0016 0on a standard normal distribution is represented by the shaded area to the right of the t-statistic. this area equals the probability of making an observation as or more extreme than x, if the null hypothesis is true. thep-value can either be calculated from software or from the normal probability tables. for the weight-di fference example, the p-value is vanishingly small: p=p(z\u0014\u00004:22) +p(z>4:22)< 0:001. 4.3. hypothesis testing 215 step 5: drawing a conclusion to reach a conclusion about the null hypothesis, directly compare pand . note that for a conclusion to be informative, it must be presented in the context of the original question; it is not useful to only state whether or not h0is rejected. ifp> , the observed sample mean is not extreme enough to warrant rejecting h0; more formally stated, there is insu fficient evidence to reject h0. a highp-value suggests that the di fference between the observed sample mean and \u00160can reasonably be attributed to random chance. ifp\u0014 , there is su fficient evidence to reject h0and accept ha. in the cdc.samp weightdifference data, the p-value is very small, with the t-statistic lying to the right of the population mean. the chance of drawing a sample with mean as large or larger than 18.2 if the distribution were centered at 0 is less than 0.001. thus, the data support the conclusion that on average, the difference between actual and desired weight is not 0 and is positive; people generally seem to feel they are overweight. guided practice 4.12 suppose that the mean weight di fference in the sampled group of 60 adults had been 7 pounds instead of 18.2 pounds, but with the same standard deviation of 33.46 pounds. would there still be enough evidence at the = 0:05 level to reject h0:\u0016= 0 in favor of ha:\u0016,0?15 15re-calculate the t-statistic: (7\u00000)=(33:46=p 60) = 1:62. thep-valuep(z\u0014\u00001:62) +p(z\u00151:62) = 0:105. sincep> , there is insu fficient evidence to reject h0. in this case, a sample average di fference of 7 is not large enough to discount the possibility that the observed di fference is due to sampling variation, and that the observations are from a distribution centered at 0. 216 chapter 4. foundations for inference 4.3.2 two examples example 4.13 while fish and other types of seafood are important for a healthy diet, nearly all fish and shellfish contain traces of mercury. dietary exposure to mercury can be particularly dangerous for young children and unborn babies. regulatory organizations such as the us food and drug administration (fda) provide guidelines as to which types of fish have particularly high levels of mercury and should be completely avoided by pregnant women and young children; additionally, certain species known to have low mercury levels are recommended for consumption. while there is no international standard that defines excessive mercury levels in saltwater fish species, general consensus is that fish with levels above 0.50 parts per million (ppm) should not be consumed. a study conducted to assess mercury levels for saltwater fish caught o ffthe coast of new jersey found that a sample of 23 bluefin tuna had mean mercury level of 0.52 ppm, with standard deviation 0.16 ppm.16based on these data, should the fda add bluefin tuna from new jersey to the list of species recommended for consumption, or should a warning be issued about their mercury levels? let\u0016be the population average mercury content for bluefin tuna caught o ffthe coast of new jersey. conduct a two-sided test of the hypothesis \u0016= 0:50 ppm in order to assess the evidence for either definitive safety or potential danger. formulate the null and alternative hypotheses .h0:\u0016= 0:50 ppm vs. ha:\u0016,0:50 ppm specify the significance level, . a significance level of = 0:05 seems reasonable. calculate the test statistic . thet-statistic has value t=x\u0000\u00160 s=pn=0:52\u00000:50 0:16=p 23= 0:599: calculate the p-value . for this two-sided alternative ha:\u0016,0:50, thep-value is p(z\u0014\u0000jtj) +p(z\u0015jtj) = 2\u0002p(z\u00150:599) = 0:549: draw a conclusion . thep-value is larger than the specified significance level , as shown in figure 4.15.17the data do not show that the mercury content of bluefin tuna caught o ffthe coast of new jersey di ffers significantly from 0.50 ppm. since p> , there is insu fficient evidence to reject the null hypothesis that the mean mercury level for the new jersey coastal population of bluefin tuna is 0.50 ppm. note that \"failure to reject\" is not equivalent to \"accepting\" the null hypothesis. recall the earlier analogy related to the principle of \"innocent until proven guilty\". if there is not enough evidence to prove that the defendant is guilty, the o fficial decision must be \"not guilty\", since the defendant may not necessarily be innocent. similarly, while there is not enough evidence to suggest that \u0016is not equal to 0.5 ppm, it would be incorrect to claim that the evidence states that \u0016is0.5 ppm. from these data, there is not statistically significant evidence to either recommend these fish as clearly safe for consumption or to warn consumers against eating them. based on these data, the food and drug administration might decide to monitor this species more closely and conduct further studies. 16j. burger, m. gochfeld, science of the total environment 409 (2011) 1418–1429 17the grey shaded regions are bounded by -1.96 and 1.96, since the area within 1.96 standard deviations of the mean captures 95% of the distribution. 4.3. hypothesis testing 217 t = 0.599μ = 0 figure 4.15: the large blue shaded regions represent the p-value, the area to the right oft= 0:599 and to the left of \u0000t=\u00000:599. the smaller grey shaded regions represents the rejection region as defined by ; in this case, an area of 0.025 in each tail. the t-statistic calculated from xwould have to lie within either of the extreme tail areas to constitute su fficient evidence against the null hypothesis. example 4.14 in 2015, the national sleep foundation published new guidelines for the amount of sleep recommended for adults: 7-9 hours of sleep per night.18the nhanes survey includes a question asking respondents about how many hours per night they sleep; the responses are available in nhanes.samp . in the sample of 134 adults used in the bmi example, the average reported hours of sleep is 6.90, with standard deviation 1.39. is there evidence that american adults sleep less than 7 hours per night? let\u0016be the population average of hours of sleep per night for us adults. conduct a one-sided test, since the question asks whether the average amount of sleep per night might be less than 7 hours. formulate the null and alternative hypotheses .h0:\u0016= 7 hours vs. ha:\u0016<7 hours. specify the significance level, . let = 0:05, since the question does not reference a di fferent value. calculate the test statistic . thet-statistic has value t=x\u0000\u00160 s=pn=6:90\u00007:00 1:33=p 134=\u00000:864: calculate the p-value . for this one-sided alternative ha:\u0016<7, thep-value is p(z\u0014t) =p(z<\u00000:864) = 0:19: since the alternative states that \u00160is less than 7, the p-value is represented by the area to the left oft=\u00000:864, as shown in figure 4.16. draw a conclusion . thep-value is larger than the specified significance level . the null hypothesis is not rejected since the data do not represent su fficient evidence to support the claim that american adults sleep less than 7 hours per night. 18sleep health: journal of the national sleep foundation, vol. 1, issue 1, pp. 40 - 43 218 chapter 4. foundations for inference t = −0.864 μ = 0 figure 4.16: the large blue shaded region represents the p-value, the area to the left oft=\u00000:864. the smaller grey shaded region represents the rejection region of area 0.05 in the left tail. guided practice 4.15 from these data, is there su fficient evidence at the = 0:10 significance level to support the claim that american adults sleep more than 7 hours per night?19 4.3.3 hypothesis testing and confidence intervals the relationship between a hypothesis test and the corresponding confidence interval is defined by the significance level ; the two approaches are based on the same inferential logic, and differ only in perspective. the hypothesis testing approach asks whether xis far enough away from\u00160to be considered extreme, while the confidence interval approach asks whether \u00160is close enough toxto be plausible. in both cases, \"far enough\" and \"close enough\" are defined by , which determines the z?used to calculate the margin of error m=z?(s=pn). hypothesis test . for a two-sided test, xneeds to be at least munits away from \u00160in either direction to be considered extreme. the t-points marking o ffthe rejection region are equal to thez?value used in the confidence interval, with the positive and negative t-points accounting for the\u0006structure in the confidence interval. confidence interval . the plausible range of values for \u00160aroundxis defined as ( x\u0000m;x+m). if\u00160is plausible, it can at most be munits away in either direction from x. if the interval does not contain \u00160, then\u00160is implausible according to and there is su fficient evidence to rejecth0. 19thet-statistic does not change from 1.65. re-calculate the p-value since the alternative hypothesis is now ha:\u0016>7: p(z\u0015\u00000:864) = 0:81. sincep> , there is insu fficient evidence to reject h0at = 0:10. a common error when conducting one-sided tests is to assume that the p-value will always be the area in the smaller of the two tails to the right or left of the observed value. it is important to remember that the area corresponding to the p-value is in the direction specified by the alternative hypothesis. t = −0.864 4.3. hypothesis testing 219 suppose that a two-sided test is conducted at significance level ; the confidence level of the matching interval is (1 \u0000 )%. for example, a two-sided hypothesis test with = 0:05 can be compared to a 95% confidence interval. a hypothesis test will reject at = 0:05 if the 95% confidence interval does not contain the null hypothesis value of the population mean ( \u00160). the relationship between two-sided hypothesis tests and confidence intervals when testing the null hypothesis h0:\u0016=\u00160against the two-sided alternative ha:\u0016,\u00160,h0 will be rejected at significance level when the 100(1\u0000 )% confidence interval for \u0016does not contain\u00160. example 4.16 calculate the confidence interval for the average mercury level for bluefin tuna caught o ffthe coast of new jersey. the summary statistics for the sample of 21 fish are x= 0:53 ppm and s= 0:16 ppm. does the interval agree with the results of example 4.13? the 95% confidence interval is: x\u00061:96spn= 0:53\u00061:960:16p 21= (0:462;0:598) ppm: the confidence interval is relatively wide, containing values below 0.50 ppm that might be regarded as safe, in addition to values that might be regarded as potentially dangerous. this interval supports the conclusion reached from hypothesis testing; the sample data does not suggest that the mercury level di ffers significantly from 0.50 ppm in either direction. the same relationship applies for one-sided hypothesis tests. for example, a one-sided hypothesis test with = 0:05 andha:\u0016>\u0016 0corresponds to a one-sided 95% confidence interval that has a lower bound, but no upper bound (i.e., ( x\u0000m;1)). the relationship between one-sided hypothesis tests and confidence intervals – when testing the null hypothesis h0:\u0016=\u00160against the one-sided alternative ha:\u0016> \u00160,h0will be rejected at significance level when\u00160is smaller than the lower bound of the 100(1\u0000 )% confidence interval for \u0016. this is equivalent to \u00160having a value outside the lower one-sided confidence interval ( x\u0000m;1). – when testing the null hypothesis h0:\u0016=\u00160against the one-sided alternative ha:\u0016< \u00160,h0will be rejected at significance level whenever\u00160is larger than the upper bound of the 100(1\u0000 )% confidence interval for \u0016. this is equivalent to \u00160having a value outside the upper one-sided confidence interval ( \u00001;x+m). 220 chapter 4. foundations for inference example 4.17 previously, a hypothesis test was conducted at = 0:05 to test the null hypothesis h0:\u0016= 7 hours against the alternative ha:\u0016<7 hours, for the average sleep per night us adults. calculate the corresponding one-sided confidence interval and compare the information obtained from a confidence interval versus a hypothesis test. the summary statistics for the sample of 134 adults arex= 6:9 ands= 1:39. in theory, a one-sided upper confidence interval extends to 1on the left side, but since it is impossible to get negative sleep, it is more sensible to bound this confidence interval by 0. the upper one-sided 95% confidence interval is (0;x+ 1:645spn) = (0;6:9 + 1:6451:39p 134) = (0;7:1) hours. from these data, we can be 95% confident that the average sleep per night among us adults is at most 7.1 hours per night. the \u00160value of 7 hours is inside the one-sided interval; thus, there is not su fficient evidence to reject the null hypothesis h0:\u0016= 7 against the one-sided alternative h0:\u0016<7 hours at = 0:05. the interval provides a range of plausible values for a parameter based on the observed sample; in this case, the data suggest that the population average sleep per night for us adults is no larger than 7.1 hours. the p-value from a hypothesis test represents a measure of the strength of the evidence against the null hypothesis, indicating how unusual the observed sample would be under h0; the hypothesis test indicated that the data do not seem extreme enough ( p= 0:19) to contradict the hypothesis that the population average sleep hours per night is 7. in practice, both a p-value and a confidence interval are computed when using a sample to make inferences about a population parameter. 4.3.4 decision errors hypothesis tests can potentially result in incorrect decisions, such as rejecting the null hypothesis when the null is actually true. figure 4.17 shows the four possible ways that the conclusion of a test can be right or wrong. test conclusion fail to reject h0rejecth0in favor ofha h0true correct decision type 1 error realityhatrue type 2 error correct decision figure 4.17: four di fferent scenarios for hypothesis tests. rejecting the null hypothesis when the null is true represents a type i error , while a type ii error refers to failing to reject the null hypothesis when the alternative is true. 4.3. hypothesis testing 221 example 4.18 in a trial, the defendant is either innocent ( h0) or guilty (ha). after hearing evidence from both the prosecution and the defense, the court must reach a verdict. what does a type i error represent in this context? what does a type ii error represent? if the court makes a type i error, this means the defendant is innocent, but wrongly convicted (rejectingh0whenh0is true). a type ii error means the court failed to convict a defendant that was guilty (failing to reject h0whenh0is false). the probability of making a type i error is the same as the significance level , since determines the cuto ffpoint for rejecting the null hypothesis. for example, if is chosen to be 0.05, then there is a 5% chance of incorrectly rejecting h0. the rate of type i error can be reduced by lowering (e.g., to 0.01 instead of 0.05); doing so requires an observation to be more extreme to qualify as su fficient evidence against the null hypothesis. however, this inevitably raises the rate of type ii errors, since the test will now have a higher chance of failing to reject the null hypothesis when the alternative is true. example 4.19 in a courtroom setting, how might the rate of type i errors be reduced? what e ffect would this have on the rate of type ii errors? lowering the rate of type i error is equivalent to raising the standards for conviction such that fewer people are wrongly convicted. this increases type ii error, since higher standards for conviction leads to fewer convictions for people who are actually guilty. guided practice 4.20 in a courtroom setting, how might the rate of type ii errors be reduced? what e ffect would this have on the rate of type i errors?20 choosing a significance level reducing the error probability of one type of error increases the chance of making the other type. as a result, the significance level is often adjusted based on the consequences of any decisions that might follow from the result of a significance test. by convention, most scientific studies use a significance level of = 0:05; small enough such that the chance of a type i error is relatively rare (occurring on average 5 out of 100 times), but also large enough to prevent the null hypothesis from almost never being rejected. if a type i error is especially dangerous or costly, a smaller value of is chosen (e.g., 0.01). under this scenario, it is better to be cautious about rejecting the null hypothesis, so very strong evidence against h0 is required in order to reject the null and accept the alternative. conversely, if a type ii error is relatively dangerous, then a larger value of is chosen (e.g., 0.10). hypothesis tests with larger values of will rejecth0more often. for example, in the early stages of assessing a drug therapy, it may be important to continue further testing even if there is not very strong initial evidence for a beneficial e ffect. if the scientists conducting the research know that any initial positive results will eventually be more rigorously tested in a larger study, they might choose to use = 0:10 to reduce the chances of making a type ii error: prematurely ending research on what might turn out to be a promising drug. 20to lower the rate of type ii error, the court could lower the standards for conviction, or in other words, lower the bar for what constitutes su fficient evidence of guilt (increase , e.g. to 0.10 instead of 0.05). this will result in more guilty people being convicted, but also increase the rate of wrongful convictions, increasing the type i error. 222 chapter 4. foundations for inference a government agency responsible for approving drugs to be marketed to the general population, however, would likely be biased towards minimizing the chances of making a type i error—approving a drug that turns out to be unsafe or ine ffective. as a result, they might conduct tests at significance level 0.01 in order to reduce the chances of concluding that a drug works when it is in fact ine ffective. the us fda and the european medical agency (ema) customarily require that two independent studies show the e fficacy of a new drug or regimen using = 0:05, though other values are sometimes used. 4.3.5 choosing between one-sided and two-sided tests in some cases, the choice of a one-sided or two-sided test can influence whether the null hypothesis is rejected. for example, consider a sample for which the t-statistic is 1.80. if a twosided test is conducted at = 0:05, thep-value is p(z\u0014\u0000jtj) +p(z\u0015jtj) = 2p(z\u00151:80) = 0:072: there is insu fficient evidence to reject h0, sincep> . however, what if a one-sided test is conducted at = 0:05, withha:\u0016>\u0016 0? in this case, the p-value is p(z\u0015t) =p(z\u00151:80) = 0:036: the conclusion of the test is di fferent: since p< , there is su fficient evidence to reject h0in favor of the alternative hypothesis. figure 4.18 illustrates the di fferent outcomes from the tests. μ = 0α2= 0.025t = 1.80 figure 4.18: under a one-sided test at significance level = 0.05, at-statistic of 1.80 is within the rejection region (shaded light blue). however, it would not be within the rejection region under a two-sided test with = 0.05 (darker blue). two-sided tests are more \"conservative\" than one-sided tests; it is more di fficult to reject the null hypothesis with a two-sided test. the p-value for a one-sided test is exactly half the p-value for a two-sided test conducted at the same significance level; as a result, it is easier for the p-value from a one-sided test to be smaller than . additionally, since the rejection region for a two-sided test is divided between two tails, a test statistic needs to be more extreme in order to fall within a rejection region. while the t-statistic of 1.80 is not within the two-sided rejection region, it is within the one-sided rejection region.21 21the two-sided rejection regions are bounded by -1.96 and 1.96, while the one-sided rejection region begins at 1.65. 4.3. hypothesis testing 223 for a fixed sample size, a one-tailed test will have a smaller probability of type ii error in comparison to a two-tailed test conducted at the same level. in other words, with a one-sided test, it is easier to reject the null hypothesis if the alternative is actually true. the choice of test should be driven by context, although it is not always clear which test is appropriate. since it is easier to reject h0with the one-tailed test, it might be tempting to always use a one-tailed test when a significant result in a particular direction would be interesting or desirable. however, it is important to consider the potential consequences of missing a significant difference in the untested direction. generally, a two-sided test is the safest option, since it does not incorporate any existing biases about the direction of the results and can detect a di fference at either the upper or lower tail. in the 1980s, researchers were interested in assessing a new set of drugs expected to be more e ffective at reducing heart arrhythmias than previously available therapies. they designed a one-sided clinical trial, convinced that the newer therapy would reduce mortality. the trial was quickly terminated due to an unanticipated e ffect of the drug; an independent review board found that the newer therapy was almost 4 times as likely to kill patients as a placebo! in a clinical research setting, it can be dangerous and even unethical to conduct a onesided test under the belief that there is no possibility of patient harm from the drug intervention being tested. one-sided tests are appropriate if the consequences of missing an e ffect in the untested direction are negligible, or if a large observed di fference in the untested direction and a conclusion of \"no di fference\" lead to the same decision. for example, suppose that a company has developed a drug to reduce blood pressure that is cheaper to produce than current options available on the market. if the drug is shown to be equally e ffective or more e ffective than an existing drug, the company will continue investing in it. thus, they are only interested in testing the alternative hypothesis that the new drug is less e ffective than the existing drug, in which case, they will stop the project. it is acceptable to conduct a one-sided test in this situation since missing an e ffect in the other direction causes no harm. the decision as to whether to use a one-sided or two-sided test must be made before data analysis begins, in order to avoid biasing conclusions based on the results of a hypothesis test. in particular, changing to a one-sided test after discovering that the results are \"almost\" significant for the two-sided test is unacceptable. manipulating analyses in order to achieve low p-values leads to invalid results that are often not replicable. unfortunately, this kind of \"significance-chasing\" has become widespread in published science, leading to concern that most current published research findings are false. 224 chapter 4. foundations for inference 4.3.6 the informal use of ppp-values formal hypothesis tests are designed for settings where a decision or a claim about a hypothesis follows a test, such as in scientific publications where an investigator wishes to claim that an intervention changes an outcome. however, progress in science is usually based on a collection of studies or experiments, and it is often the case that the results of one study are used as a guide for the next study or experiment. sir ronald fisher was the first to propose using p-values as one of the statistical tools for evaluating an experiment. in his view, an outcome from an experiment that would only happen 1 in 20 times (p= 0.05) was worth investigating further. the use of p-values for formal decision making came later. while valuable, formal hypothesis testing can often be overused; not all significant results should lead to a definitive claim, but instead prompt further analysis. the formal use of p-values is emphasized here because of its prominence in the scientific literature, and because the steps outlined are fundamental to the scientific method for empirical research: specify hypotheses, state in advance how strong the evidence should be to constitute sufficient evidence against the null, specify the method of analysis and compute the test statistic, draw a conclusion. these steps are designed to avoid the pitfall of choosing a hypothesis or method of analysis that is biased by the data and hence reaches a conclusion that may not be reproducible. 4.4. notes 225 4.4 notes confidence intervals and hypothesis testing are two of the central concepts in inference for a population based on a sample. the confidence interval shows a range of population parameter values consistent with the observed sample, and is often used to design additional studies. hypothesis testing is a useful tool for evaluating the strength of the evidence against a working hypothesis according to a pre-specified standard for accepting or rejecting hypotheses. the calculation of p-values and confidence intervals is relatively straightforward; given the necessary summary statistics, , and confidence coe fficients, finding any p-value or confidence interval simply involves a set of formulaic steps. however, the more di fficult parts of any inference problem are the steps that do not involve any calculations. specifying appropriate null and alternative hypotheses for a test relies on an understanding of the problem context and the scientific setting of the investigation. similarly, a choice about a confidence coe fficient for an interval relies on judgment as to balancing precision against the chance of possible error. it is also not necessarily obvious when a significance level other than = 0:05 should be applied. these choices represent the largest distinction between a true statistics problem as compared to a purely mathematical exercise. furthermore, in order to rely on the conclusions drawn from making inferences, it is necessary to consider factors such as study design, measurement quality, and the validity of any assumptions made. for example, is it valid to use the normal approximation to calculate p-values? in small to moderate sample sizes (30 \u0014n\u001450), it may not be clear that the normal model is accurate. it is even necessary to be cautious about the use and interpretation of the p-value. for example, an article published in nature about the mis-use of p-values references a published study that showed people who meet their spouses online are more likely to have marital satisfaction, with p-value less than 0.001. however, statistical significance does not measure the importance or practical relevance of a result; in this case, the change in happiness moved from 5.48 to 5.64 on a 7-point scale. a p-value reported without context or other evidence is uninformative and potentially deceptive. these nuanced issues cannot be adequately covered in any introduction to statistics. it is unrealistic to encourage students to use their own judgment with aspects of inference that even experienced investigators find challenging. at the same time, it would also be misleading to suggest that the choices are always clear-cut in practice. it seems best to o ffer some practical guidance for getting started: – the default choice of is 0.05; similarly, the default confidence coe fficient for a confidence interval is 95%. – unless it is clear from the context of a problem that change in only one direction from the null hypothesis is of interest, the alternative hypothesis should be two-sided. – the use of a standard normal distribution to calculate p-values is reasonable for sample sizes of 30 or more if the distribution of data are not strongly skewed and there are no large outliers. if there is skew or a few large outliers, sample sizes of 50 or more are usually su fficient. – pay attention to the context of a problem, particularly when formulating hypotheses and drawing conclusions. the next chapters will discuss methods of inference in specific settings, such as comparing two groups. these settings expand on the concepts discussed in this chapter and o ffer additional opportunities to practice calculating tests and intervals, reading problems for context, and checking underlying assumptions behind methods of inference. 226 chapter 4. foundations for inference the labs for the chapter reinforce conceptual understanding of confidence intervals and hypothesis tests, and their link to sampling variability using the data from the yrbss and nhanes. both datasets are large enough to be viewed in an instructional setting as populations from which repeated samples can be drawn. they are useful platforms for illustrating the conceptual role of hypothetical repeated sampling in the properties of tests and intervals, a topic which many students find di fficult. students may find the last lab for this chapter (lab 4) particularly helpful for understanding conceptual details of inference, such as the distinction between the significance level and thep-value, and the definition of as the type i error rate. 4.5. exercises 227 4.5 exercises 4.5.1 variability in estimates 4.1 egg coloration. the evolutionary role of variation in bird egg coloration remains mysterious to biologists. one hypothesis suggests that egg color may play a role in sexual selection. for example, perhaps healthier females are able to deposit more blue-green pigment into eggshells instead of using it themselves as an antioxidant. researchers measured the blue-green chroma (bgc) of 70 di fferent collared flycatcher nests in an area of the czech republic. blue−green chroma0.56 0.58 0.60 0.62 0.6405101520 min 0.5675 q1 0.5977 median 0.6046 mean 0.6052 sd 0.0131 q3 0.6126 max 0.6355 (a) what is the point estimate for the average bgc of nests? (b) what is the point estimate for the standard deviation of the bgc of eggs across nests? (c) would a nest with average bgc of 0.63 be considered unusually high? explain your reasoning. (d) compute the standard error of the sample mean using the summary statistics. 228 chapter 4. foundations for inference 4.2 heights of adults. researchers studying anthropometry collected body girth measurements and skeletal diameter measurements, as well as age, weight, height and gender, for 507 physically active individuals. the histogram below shows the sample distribution of heights in centimeters.22 height150 160 170 180 190 200020406080100 min 147.2 q1 163.8 median 170.3 mean 171.1 sd 9.4 q3 177.8 max 198.1 (a) what is the point estimate for the average height of active individuals? (b) what is the point estimate for the standard deviation of the heights of active individuals? what about the iqr? (c) is a person who is 1m 80cm (180 cm) tall considered unusually tall? and is a person who is 1m 55cm (155cm) considered unusually short? explain your reasoning. (d) the researchers take another random sample of physically active individuals. would you expect the mean and the standard deviation of this new sample to be the ones given above? explain your reasoning. (e) the sample means obtained are point estimates for the mean height of all active individuals, if the sample of individuals is equivalent to a simple random sample. what measure is used to quantify the variability of such an estimate? compute this quantity using the data from the original sample under the condition that the data are a simple random sample. 4.3 hen eggs. the distribution of the number of eggs laid by a certain species of hen during their breeding period is on average, 35 eggs, with a standard deviation of 18.2. suppose a group of researchers randomly samples 45 hens of this species, counts the number of eggs laid during their breeding period, and records the sample mean. they repeat this 1,000 times, and build a distribution of sample means. (a) what is this distribution called? (b) would you expect the shape of this distribution to be symmetric, right skewed, or left skewed? explain your reasoning. (c) calculate the variability of this distribution and state the appropriate term used to refer to this value. (d) suppose the researchers’ budget is reduced and they are only able to collect random samples of 10 hens. the sample mean of the number of eggs is recorded, and we repeat this 1,000 times, and build a new distribution of sample means. how will the variability of this new distribution compare to the variability of the original distribution? 22g. heinz et al. “exploring relationships in body dimensions”. in: journal of statistics education 11.2 (2003). 4.5. exercises 229 4.5.2 confidence intervals 4.4 mental health, part i. the 2010 general social survey asked the question: “for how many days during the past 30 days was your mental health, which includes stress, depression, and problems with emotions, not good?\" based on responses from 1,151 us residents, the survey reported a 95% confidence interval of 3.40 to 4.24 days in 2010. (a) interpret this interval in context of the data. (b) what does “95% confident\" mean? explain in the context of the application. (c) if a new survey were to be done with 500 americans, would the standard error of the estimate be larger, smaller, or about the same? assume the standard deviation has remained constant since 2010. 4.5 relaxing after work, part i. the 2010 general social survey asked the question: “after an average work day, about how many hours do you have to relax or pursue activities that you enjoy?\" to a random sample of 1,155 americans.23a 95% confidence interval for the mean number of hours spent relaxing or pursuing activities they enjoy is (1.38, 1.92). (a) interpret this interval in context of the data. (b) suppose another set of researchers reported a confidence interval with a larger margin of error based on the same sample of 1,155 americans. how does their confidence level compare to the confidence level of the interval stated above? (c) suppose next year a new survey asking the same question is conducted, and this time the sample size is 2,500. assuming that the population characteristics, with respect to how much time people spend relaxing after work, have not changed much within a year. how will the margin of error of the new 95% confidence interval compare to the margin of error of the interval stated above? (d) suppose the researchers think that 90% confidence interval would be more appropriate. will this new interval be smaller or larger than the original 95% confidence interval? justify your answer. (assume that the standard deviation remains constant). 23national opinion research center, general social survey, 2010. 230 chapter 4. foundations for inference 4.6 thanksgiving spending, part i. the 2009 holiday retail season, which kicked o ffon november 27, 2009 (the day after thanksgiving), had been marked by somewhat lower self-reported consumer spending than was seen during the comparable period in 2008. to get an estimate of consumer spending, 436 randomly sampled american adults were surveyed. daily consumer spending for the six-day period after thanksgiving, spanning the black friday weekend and cyber monday, averaged $84.71. a 95% confidence interval based on this sample is ($80.31, $89.11). determine whether the following statements are true or false, and explain your reasoning. spending0 50 100 150 200 250 300020406080 (a) we are 95% confident that the average spending of these 436 american adults is between $80.31 and $89.11. (b) this confidence interval is not valid since the distribution of spending in the sample is right skewed. (c) 95% of random samples have a sample mean between $80.31 and $89.11. (d) we are 95% confident that the average spending of all american adults is between $80.31 and $89.11. (e) a 90% confidence interval would be narrower than the 95% confidence interval. (f) the margin of error is 4.4. 4.7 waiting at an er, part i. a hospital administrator hoping to improve wait times decides to estimate the average emergency room waiting time at her hospital. she collects a simple random sample of 64 patients and determines the time (in minutes) between when they checked in to the er until they were first seen by a doctor. a 95% confidence interval based on this sample is (128 minutes, 147 minutes), which is based on the normal model for the mean. determine whether the following statements are true or false, and explain your reasoning. (a) this confidence interval is not valid since we do not know if the population distribution of the er wait times is nearly normal. (b) we are 95% confident that the average waiting time of these 64 emergency room patients is between 128 and 147 minutes. (c) we are 95% confident that the average waiting time of all patients at this hospital’s emergency room is between 128 and 147 minutes. (d) 95% of random samples have a sample mean between 128 and 147 minutes. (e) a 99% confidence interval would be narrower than the 95% confidence interval since we need to be more sure of our estimate. (f) the margin of error is 9.5 and the sample mean is 137.5. (g) halving the margin of error of a 95% confidence interval requires doubling the sample size. 4.5. exercises 231 4.8 age at first marriage, part i. the national survey of family growth conducted by the centers for disease control gathers information on family life, marriage and divorce, pregnancy, infertility, use of contraception, and men’s and women’s health. one of the variables collected on this survey is the age at first marriage. the histogram below shows the distribution of ages at first marriage of 5,534 randomly sampled women between 2006 and 2010. the average age at first marriage among these women is 23.44 with a standard deviation of 4.72.24 age at first marriage10 15 20 25 30 35 40 4502004006008001000 estimate the average age at first marriage of women using a 95% confidence interval, and interpret this interval in context. discuss any relevant assumptions. 4.9 mental health, part ii. the general social survey (gss) is a sociological survey used to collect data on demographic characteristics and attitudes of residents of the united states. the 2010 general social survey asked the question, \"for how many days during the past 30 days was your mental health not good?\" based on responses from 1,151 us adults, the survey reported a 95% confidence interval of (3.40, 4.24) days. assume that the sampled us adults are representative of all us adults. (a) identify each of the following statements as true or false. justify your answers. i. the confidence interval of (3.40, 4.24) contains the mean days out of the past 30 days that u.s. adults experienced poor mental health. ii. there is a 95% chance that the mean days out of the past 30 days that u.s. adults experienced poor mental health is within the confidence interval (3.40, 4.24). iii. if we repeated this survey 1,000 times and constructed a 95% confidence interval each time, then approximately 950 of those intervals would contain the true mean days out of the past 30 days that u.s. adults experienced poor mental health. iv. the survey provides statistically significant evidence at the = 0:05 significance level that the mean days out of the past 30 days that u.s. adults experienced poor mental health is not 4.5 days. v. we can be 95% confident that the mean days out of the past 30 days that u.s. adults experienced poor mental health is 3.82 days. vi. we can be 95% confident that the interval (3.40, 4.24) days contains the mean days out of the past 30 days that the sampled adults experienced poor mental health. (b) would you expect the 90% confidence interval to be larger or smaller than the 95% confidence interval? explain your reasoning. (c) calculate the 90% confidence interval. 24centers for disease control and prevention, national survey of family growth, 2010. 232 chapter 4. foundations for inference 4.10 leisure time, part iii. in 2010, the general social survey collected responses from 1,154 us residents. the survey is conducted face-to-face with an in-person interview of a randomly selected sample of adults. one of the questions on the survey is \"after an average workday, about how many hours do you have to relax or pursue activities that you enjoy?\" a 95% confidence interval from the 2010 gss survey for the collected answers is 3.53 to 3.83 hours. identify each of the following statements as true or false. explain your answers. (a) if the researchers wanted to report a confidence interval with a smaller margin of error based on the same sample of 1,154 americans, the confidence interval would be larger. (b) we can be 95% confident that the interval (3.53, 3.83) hours contains the mean hours that the sampled adults have for leisure time after an average workday. (c) the confidence interval of (3.53, 3.83) hours contains the mean hours that u.s. adults have for leisure time after an average workday. (d) the survey provides statistically significant evidence at the = 0:05 significance level that the mean hours u.s. adults have for leisure time after the average workday is 3.6 hours. (e) there is a 5% chance that the interval (3.53, 3.83) hours does not contain the mean hours that u.s. adults have for leisure time after an average workday. (f) the interval (3.53, 3.83) hours provides evidence at the = 0:05 significance level that u.s. adults, on average, have fewer than 3.9 hours of leisure time after a typical workday. 4.5.3 hypothesis testing 4.11 identify hypotheses, part i. write the null and alternative hypotheses in words and then symbols for each of the following situations. (a) new york is known as “the city that never sleeps\". a random sample of 25 new yorkers were asked how much sleep they get per night. do these data provide convincing evidence that new yorkers on average sleep less than 8 hours a night? (b) employers at a firm are worried about the e ffect of march madness, a basketball championship held each spring in the us, on employee productivity. they estimate that on a regular business day employees spend on average 15 minutes of company time checking personal email, making personal phone calls, etc. they also collect data on how much company time employees spend on such non- business activities during march madness. they want to determine if these data provide convincing evidence that employee productivity decreases during march madness. 4.12 identify hypotheses, part ii. write the null and alternative hypotheses in words and using symbols for each of the following situations. (a) since 2008, chain restaurants in california have been required to display calorie counts of each menu item. prior to menus displaying calorie counts, the average calorie intake of diners at a restaurant was 1100 calories. after calorie counts started to be displayed on menus, a nutritionist collected data on the number of calories consumed at this restaurant from a random sample of diners. do these data provide convincing evidence of a di fference in the average calorie intake of a diners at this restaurant? (b) based on the performance of those who took the gre exam between july 1, 2004 and june 30, 2007, the average verbal reasoning score was calculated to be 462. in 2011 the average verbal score was slightly higher. do these data provide convincing evidence that the average gre verbal reasoning score has changed since 2004? 4.5. exercises 233 4.13 online communication. a study suggests that the average college student spends 10 hours per week communicating with others online. you believe that this is an underestimate and decide to collect your own sample for a hypothesis test. you randomly sample 60 students from your dorm and find that on average they spent 13.5 hours a week communicating with others online. a friend of yours, who o ffers to help you with the hypothesis test, comes up with the following set of hypotheses. indicate any errors you see. h0: ̄x<10hours ha: ̄x>13:5hours 4.14 age at first marriage, part ii. exercise 4.8 presents the results of a 2006 - 2010 survey showing that the average age of women at first marriage is 23.44. suppose a social scientist believes that this value has increased in 2012, but she would also be interested if she found a decrease. below is how she set up her hypotheses. indicate any errors you see. h0: ̄x= 23:44years ha: ̄x>23:44years 4.15 waiting at an er, part ii. exercise 4.7 provides a 95% confidence interval for the mean waiting time at an emergency room (er) of (128 minutes, 147 minutes). answer the following questions based on this interval. (a) a local newspaper claims that the average waiting time at this er exceeds 3 hours. is this claim supported by the confidence interval? explain your reasoning. (b) the dean of medicine at this hospital claims the average wait time is 2.2 hours. is this claim supported by the confidence interval? explain your reasoning. (c) without actually calculating the interval, determine if the claim of the dean from part (b) would be supported based on a 99% confidence interval? 4.16 gifted children, part i. researchers investigating characteristics of gifted children collected data from schools in a large city on a random sample of thirty-six children who were identified as gifted children soon after they reached the age of four. the following histogram shows the distribution of the ages (in months) at which these children first counted to 10 successfully. also provided are some sample statistics.25 age child first counted to 10 (in months)20 25 30 35 40036 n 36 min 21 mean 30.69 sd 4.31 max 39 (a) are conditions for inference satisfied? (b) suppose an online survey reports that children first count to 10 successfully when they are 32 months old, on average. perform a hypothesis test to evaluate if these data provide convincing evidence that the average age at which gifted children first count to 10 successfully is less than the general average of 32 months. use a significance level of 0.10. (c) interpret the p-value in context of the hypothesis test and the data. (d) calculate a 90% confidence interval for the average age at which gifted children first count to 10 successfully. (e) do your results from the hypothesis test and the confidence interval agree? explain. 25f.a. graybill and h.k. iyer. regression analysis: concepts and applications . duxbury press, 1994, pp. 511–516. 234 chapter 4. foundations for inference 4.17 nutrition labels. the nutrition label on a bag of potato chips says that a one ounce (28 gram) serving of potato chips has 130 calories and contains ten grams of fat, with three grams of saturated fat. a random sample of 35 bags yielded a sample mean of 134 calories with a standard deviation of 17 calories. is there evidence that the nutrition label does not provide an accurate measure of calories in the bags of potato chips? we have verified the independence, sample size, and skew conditions are satisfied. 4.18 waiting at an er, part iii. the hospital administrator mentioned in exercise 4.7 randomly selected 64 patients and measured the time (in minutes) between when they checked in to the er and the time they were first seen by a doctor. the average time is 137.5 minutes and the standard deviation is 39 minutes. she is getting grief from her supervisor on the basis that the wait times in the er has increased greatly from last year’s average of 127 minutes. however, she claims that the increase is probably just due to chance. (a) calculate a 95% confidence interval. is the change in wait times statistically significant at the = 0:05 level? (b) would the conclusion in part (a) change if the significance level were changed to = 0:01? (c) is the supervisor justified in criticizing the hospital administrator regarding the change in er wait times? how might you present an argument in favor of the administrator? 4.19 birth weights. suppose an investigator takes a random sample of n= 50 birth weights from several teaching hospitals located in an inner-city neighborhood. in her random sample, the sample mean xis 3,150 grams and the standard deviation is 250 grams. (a) calculate a 95% confidence interval for the population mean birth weight in these hospitals. (b) the typical weight of a baby at birth for the us population is 3,250 grams. the investigator suspects that the birth weights of babies in these teaching hospitals is di fferent than 3,250 grams, but she is not sure if it is smaller (from malnutrition) or larger (because of obesity prevalence in mothers giving birth at these hospitals). carry out the hypothesis test that she would conduct. 4.20 gifted children, part ii. exercise 4.16 describes a study on gifted children. in this study, along with variables on the children, the researchers also collected data on the mother’s and father’s iq of the 36 randomly sampled gifted children. the histogram below shows the distribution of mother’s iq. also provided are some sample statistics. mother's iq100 105 110 115 120 125 130 13504812 n 36 min 101 mean 118.2 sd 6.5 max 131 (a) perform a hypothesis test to evaluate if these data provide convincing evidence that the average iq of mothers of gifted children is di fferent than the average iq for the population at large, which is 100. use a significance level of 0.10. (b) calculate a 90% confidence interval for the average iq of mothers of gifted children. (c) do your results from the hypothesis test and the confidence interval agree? explain. 4.5. exercises 235 4.21 testing for fibromyalgia. a patient named diana was diagnosed with fibromyalgia, a long-term syndrome of body pain, and was prescribed anti-depressants. being the skeptic that she is, diana didn’t initially believe that anti-depressants would help her symptoms. however after a couple months of being on the medication she decides that the anti-depressants are working, because she feels like her symptoms are in fact getting better. (a) write the hypotheses in words for diana’s skeptical position when she started taking the anti-depressants. (b) what is a type 1 error in this context? (c) what is a type 2 error in this context? 4.22 testing for food safety. a food safety inspector is called upon to investigate a restaurant with a few customer reports of poor sanitation practices. the food safety inspector uses a hypothesis testing framework to evaluate whether regulations are not being met. if he decides the restaurant is in gross violation, its license to serve food will be revoked. (a) write the hypotheses in words. (b) what is a type 1 error in this context? (c) what is a type 2 error in this context? (d) which error is more problematic for the restaurant owner? why? (e) which error is more problematic for the diners? why? (f) as a diner, would you prefer that the food safety inspector requires strong evidence or very strong evidence of health concerns before revoking a restaurant’s license? explain your reasoning. 4.23 which is higher? in each part below, there is a value of interest and two scenarios (i and ii). for each part, report if the value of interest is larger under scenario i, scenario ii, or whether the value is equal under the scenarios. (a) the standard error of ̄xwhens= 120 and (i) n = 25 or (ii) n = 125. (b) the margin of error of a confidence interval when the confidence level is (i) 90% or (ii) 80%. (c) the p-value for a z-statistic of 2.5 when (i) n = 500 or (ii) n = 1000. (d) the probability of making a type 2 error when the alternative hypothesis is true and the significance level is (i) 0.05 or (ii) 0.10. 4.24 true or false. determine if the following statements are true or false, and explain your reasoning. if false, state how it could be corrected. (a) if a given value (for example, the null hypothesized value of a parameter) is within a 95% confidence interval, it will also be within a 99% confidence interval. (b) decreasing the significance level ( ) will increase the probability of making a type 1 error. (c) suppose the null hypothesis is \u0016= 5 and we fail to reject h0. under this scenario, the true population mean is 5. (d) if the alternative hypothesis is true, then the probability of making a type 2 error and the power of a test add up to 1. (e) with large sample sizes, even small di fferences between the null value and the true value of the parameter, a difference often called the e ffect size , will be identified as statistically significant. 236 chapter 5 inference for numerical data 5.1 single-sample inference with the t-distribution 5.2 two-sample test for paired data 5.3 two-sample test for independent data 5.4 power calculations for a difference of means 5.5 comparing means with anova 5.6 notes 5.7 exercises 237 chapter 4 introduced some primary tools of statistical inference—point estimates, interval estimates, and hypothesis tests. this chapter discusses settings where these tools are often used, including the analysis of paired observations and the comparison of two or more independent groups. the chapter also covers the important topic of estimating an appropriate sample size when a study is being designed. the chapter starts with introducing a new distribution, the t-distribution, which can be used for small sample sizes. for labs, slides, and other resources, please visit www.openintro.org/book/biostat 238 chapter 5. inference for numerical data 5.1 single-sample inference with the ttt-distribution the tools studied in chapter 4 all made use of the t-statistic from a sample mean, t=x\u0000\u0016 s=pn; where the parameter \u0016is a population mean, xandsare the sample mean and standard deviation, andnis the sample size. tests and confidence intervals were restricted to samples of at least 30 independent observations from a population where there was no evidence of strong skewness. this allowed for the central limit theorem to be applied, justifying use of the normal distribution to calculate probabilities associated with the t-statistic. in sample sizes smaller than 30, if the data are approximately symmetric and there are no large outliers, the t-statistic has what is called a t-distribution. when the normal distribution is used as the sampling distribution of the t-statistic,sis essentially being treated as a good replacement for the unknown population standard deviation \u001b. however, the sample standard deviation s, as an estimate of \u001b, has its own inherent variability like x. thetdensity function adjusts for the variability in sby having more probability in the left and right tails than the normal distribution. 5.1.1 the ttt-distribution figure 5.1 shows a t-distribution and normal distribution. like the standard normal distribution, thet-distribution is unimodal and symmetric about zero. however, the tails of a t-distribution are thicker than for the normal, so observations are more likely to fall beyond two standard deviations from the mean than under the normal distribution.1while the estimate of the standard error will be less accurate with smaller sample sizes, the thick tails of the t-distribution correct for the variability in s. −4 −2 0 2 4 figure 5.1: comparison of a t-distribution (solid line) and a normal distribution (dotted line). thet-distribution can be described as a family of symmetric distributions with a single parameter: degrees of freedom, which equals n\u00001. severalt-distributions are shown in figure 5.2. when there are more degrees of freedom, the t-distribution looks very much like the standard normal distribution. with degrees of freedom of 30 or more, the t-distribution is nearly indistinguishable from the normal distribution. since the t-statistics in chapter 4 were associated with sample sizes of at least 30, the degrees of freedom for the corresponding t-distributions were large enough to justify use of the normal distribution to calculate probabilities. 1the standard deviation of the t-distribution is actually a little more than 1. however, it is useful to think of the t-distribution as having a standard deviation of 1 in the context of using it to conduct inference. 5.1. single-sample inference with the t-distribution 239 −2 0 2 4 6 8normal t, df = 8 t, df = 4 t, df = 2 t, df = 1 figure 5.2: the larger the degrees of freedom, the more closely the t-distribution resembles the standard normal model. degrees of freedom (df) the degrees of freedom characterize the shape of the t-distribution. the larger the degrees of freedom, the more closely the distribution approximates the normal model. probabilities for the t-distribution can be calculated either by using distribution tables or using statistical software. the use of software has become the preferred method because it is more accurate, allows for complete flexibility in the choice of t-values on the horizontal axis, and is not limited to a small range of degrees of freedom. the remainder of this section illustrates the use of at-table , partially shown in figure 5.3, in place of the normal probability table. a larger t-table is in appendix b.2 on . the rlabs illustrate the use of software to calculate probabilities for thet-distribution. readers intending to use software can skip to the next section. one tail 0.100 0.050 0.025 0.010 0.005 two tails 0.200 0.100 0.050 0.020 0.010 df 1 3.08 6.31 12.71 31.82 63.66 2 1.89 2.92 4.30 6.96 9.92 3 1.64 2.35 3.18 4.54 5.84 ::::::::::::::: 17 1.33 1.74 2.11 2.57 2.90 18 1.33 1.73 2.10 2.55 2.88 19 1.33 1.73 2.09 2.54 2.86 20 1.33 1.72 2.09 2.53 2.85 ::::::::::::::: 400 1.28 1.65 1.97 2.34 2.59 500 1.28 1.65 1.96 2.33 2.59 1 1.28 1.64 1.96 2.33 2.58 figure 5.3: an abbreviated look at the t-table. each row represents a di fferent t-distribution. the columns describe the cuto ffs for specific tail areas. the row withdf= 18 has been highlighted . each row in the t-table represents a t-distribution with di fferent degrees of freedom. the columns correspond to tail probabilities. for instance, for a t-distribution with df= 18, row 18 is used (highlighted in figure 5.3). the value in this row that identifies the cuto fffor an upper tail of 5% is found in the column where one tail is 0.050. this cuto ffis 1.73. the cuto fffor the lower 5% is -1.73; just like the normal distribution, all t-distributions are symmetric. if the area in each tail is 5%, then the area in two tails is 10%; thus, this column can also be described as the column where two tails is 0.100. 240 chapter 5. inference for numerical data example 5.1 what proportion of the t-distribution with 18 degrees of freedom falls below -2.10? just like for a normal probability problem, it is advisable to start by drawing the distribution and shading the area below -2.10, as shown in figure 5.4. from the table, identify the column containing the absolute value of -2.10; it is the third column. since this is just the probability in one tail, examine the top line of the table; a one tail area for a value in the third column corresponds to 0.025. about 2.5% of the distribution falls below -2.10. −4 −2 0 2 4 figure 5.4: the t-distribution with 18 degrees of freedom. the area below -2.10 has been shaded. example 5.2 at-distribution with 20 degrees of freedom is shown in the left panel of figure 5.5. estimate the proportion of the distribution falling above 1.65 and below -1.65. identify the row in the t-table using the degrees of freedom: df\u000020. then, look for 1.65; the value is not listed, and falls between the first and second columns. since these values bound 1.65, their tail areas will bound the tail area corresponding to 1.65. the two tail area of the first and second columns is between 0.100 and 0.200. thus, between 10% and 20% of the distribution is more than 1.65 standard deviations from the mean. the precise area can be calculated using statistical software: 0.1146. −4 −2 0 2 4 figure 5.5: the t-distribution with 20 degrees of freedom, with the area further than 1.65 away from 0 shaded. 5.1. single-sample inference with the t-distribution 241 5.1.2 using the ttt-distribution for tests and confidence intervals for a population mean chapter 4 provided formulas for tests and confidence intervals for population means in random samples large enough for the t-statistic to have a nearly normal distribution. in samples smaller than 30 from approximately symmetric distributions without large outliers, the t-statistic has at-distribution with degrees of freedom equal to n\u00001. just like inference in larger samples, inference using the t-distribution also requires that the observations in the sample be independent. random samples from very large populations always produce independent observations; in smaller populations, observations will be approximately independent as long as the size of the sample is no larger than 10% of the population. formulas for tests and intervals using the t\u0000distribution are very similar to those using the normal distribution. for a sample of size nwith sample mean xand standard deviation s, two-sided confidence intervals with confidence coe fficient 100(1\u0000 )% have the form x\u0006t? df\u0002se; where se is the standard error of the sample mean ( s=pn) andt? dfis the point on a t-distribution withn\u00001 degrees of freedom and area (1 \u0000 =2) to its left. a one-sided interval with the same confidence coe fficient will have the form x+t? df\u0002se (one-sided upper confidence interval) ;or x\u0000t? df\u0002se (one-sided lower confidence interval) ; except that in this case t? dfis the point on a t-distribution with n\u00001 degrees of freedom and area (1\u0000 ) to its left. with the ability to conveniently calculate t?for any sample size or associated via computing software, the t-distribution can be used by default over the normal distribution. the rule of thumb thatn>30 qualifies as a large enough sample size to use the normal distribution dates back to when it was necessary to rely on distribution tables. 242 chapter 5. inference for numerical data example 5.3 dolphins are at the top of the oceanic food chain; as a consequence, dangerous substances such as mercury tend to be present in their organs and muscles at high concentrations. in areas where dolphins are regularly consumed, it is important to monitor dolphin mercury levels. this example uses data from a random sample of 19 risso’s dolphins from the taiji area in japan.2calculate the 95% confidence interval for average mercury content in risso’s dolphins from the taiji area using the data in figure 5.6. the observations are a simple random sample consisting of less than 10% of the population, so independence of the observations is reasonable. the summary statistics in figure 5.6 do not suggest any skew or outliers; all observations are within 2.5 standard deviations of the mean. based on this evidence, the approximate normality assumption seems reasonable. use thet-distribution to calculate the confidence interval: x\u0006t? df\u0002se =x\u0006t? 18\u0002s=p n = 4:4\u00062:10\u00022:3=p 19 = (3:29;5:51)\u0016g/wet g: thet?point can be read from the t-table on , in the column with area totaling 0.05 in the two tails (third column) and the row with 18 degrees of freedom. based on these data, one can be 95% confident the average mercury content of muscles in risso’s dolphins is between 3.29 and 5.51 \u0016g/wet gram. alternatively, the t?point can be calculated in rwith the function qt, which returns a value of 2.1009. nx s minimum maximum 19 4.4 2.3 1.7 9.2 figure 5.6: summary of mercury content in the muscle of 19 risso’s dolphins from the taiji area. measurements are in \u0016g/wet g (micrograms of mercury per wet gram of muscle). 2taiji is a significant source of dolphin and whale meat in japan. thousands of dolphins pass through the taiji area annually; assume that these 19 dolphins represent a simple random sample. data reference: endo t and haraguchi k. 2009. high mercury levels in hair samples from residents of taiji, a japanese whaling town. marine pollution bulletin 60(5):743-747. 5.1. single-sample inference with the t-distribution 243 guided practice 5.4 the fda’s webpage provides some data on mercury content of various fish species.3from a sample of 15 white croaker (pacific), a sample mean and standard deviation were computed as 0.287 and 0.069 ppm (parts per million), respectively. the 15 observations ranged from 0.18 to 0.41 ppm. assume that these observations are independent. based on summary statistics, does the normality assumption seem reasonable? if so, calculate a 90% confidence interval for the average mercury content of white croaker (pacific).4 example 5.5 according to the epa, regulatory action should be taken if fish species are found to have a mercury level of 0.5 ppm or higher. conduct a formal significance test to evaluate whether the average mercury content of croaker white fish (pacific) is di fferent from 0.50 ppm. use = 0:05. the fda regulatory guideline is a ‘one-sided’ statement; fish should not be eaten if the mercury level is larger than a certain value. however, without prior information on whether the mercury in this species tends to be high or low, it is best to do a two-sided test. state the hypotheses: h0:\u0016= 0:5 vsha:\u0016,0:5. let = 0:05. calculate the t-statistic: t=x\u0000\u00160 se=0:287\u00000:50 0:069=p 15=\u000011:96 the probability that the absolute value of a t-statistic with 14 df is smaller than -11.96 is smaller than 0.01. thus, p<0:01. there is evidence to suggest at the = 0:05 significance level that the average mercury content of this fish species is lower than 0.50 ppm, since xis less than 0.50. 3www.fda.gov/food/foodborneillnesscontaminants/metals/ucm115644.htm 4there are no obvious outliers; all observations are within 2 standard deviations of the mean. if there is skew, it is not evident. there are no red flags for the normal model based on this (limited) information. x\u0006t? 14\u0002se!0:287\u00061:76\u0002 0:0178!(0:256;0:318). we are 90% confident that the average mercury content of croaker white fish (pacific) is between 0.256 and 0.318 ppm. 244 chapter 5. inference for numerical data 5.2 two-sample test for paired data in the 2000 olympics, was the use of a new wetsuit design responsible for an observed increase in swim velocities? in a study designed to investigate this question, twelve competitive swimmers swam 1500 meters at maximal speed, once wearing a wetsuit and once wearing a regular swimsuit.5the order of wetsuit versus swimsuit was randomized for each of the 12 swimmers. figure 5.7 shows the average velocity recorded for each swimmer, measured in meters per second (m/s).6 swimmer.number wet.suit.velocity swim.suit.velocity velocity.di ff 1 1 1.57 1.49 0.08 2 2 1.47 1.37 0.10 3 3 1.42 1.35 0.07 4 4 1.35 1.27 0.08 5 5 1.22 1.12 0.10 6 6 1.75 1.64 0.11 7 7 1.64 1.59 0.05 8 8 1.57 1.52 0.05 9 9 1.56 1.50 0.06 10 10 1.53 1.45 0.08 11 11 1.49 1.44 0.05 12 12 1.51 1.41 0.10 figure 5.7: paired swim suit data the swimsuit velocity data are an example of paired data , in which two sets of observations are uniquely paired so that an observation in one set matches an observation in the other; in this case, each swimmer has two measured velocities, one with a wetsuit and one with a swimsuit. a natural measure of the e ffect of the wetsuit on swim velocity is the di fference between the measured maximum velocities ( velocity.diff = wet.suit.velocity - swim.suit.velocity ). even though there are two measurements per swimmer, using the di fference in velocities as the variable of interest allows for the problem to be approached like those in section 5.1. although it was not explicitly noted, the data used in section 4.3.1 were paired; each respondent had both an actual and desired weight. suppose the parameter \u000eis the population average of the di fference in maximum velocities during a 1500m swim if all competitive swimmers recorded swim velocities with each suit type. a hypothesis test can then be conducted with the null hypothesis that the mean population di fference in swim velocities between suit types equals 0 (i.e., there is no di fference in population average swim velocities), h0:\u000e= 0, against the alternative that the di fference is non-zero, ha:\u000e,0. stating hypotheses for paired data when testing a hypothesis about paired data, compare the groups by testing whether the population mean of the di fferences between the groups equals 0. – for a two-sided test, h0:\u000e= 0;ha:\u000e,0. – for a one-sided test, either h0:\u000e= 0;ha:\u000e>0 orh0:\u000e= 0;ha:\u000e<0. 5de lucas et. al, the e ffects of wetsuits on physiological and biomechanical indices during swimming. journal of science and medicine in sport, 2000; 3(1): 1-8 6the data are available as swim in the oibiostat rpackage. the data are also used in lock et. al statistics, unlocking the power of data , wiley, 2013. 5.2. two-sample test for paired data 245 some important assumptions are being made. first, it is assumed that the data are a random sample from the population. while the observations are likely independent, it is more di fficult to justify that this sample of 12 swimmers is randomly drawn from the entire population of competitive swimmers. nevertheless, it is often assumed in problems such as these that the participants are reasonably representative of competitive swimmers. second, it is assumed that the population of differences is normally distributed. this is a small sample, one in which normality would be difficult to confirm. the dot plot for the di fference in velocities in figure 5.8 shows approximate symmetry. difference in swim velocities (m/s)0.05 0.06 0.07 0.08 0.09 0.10 0.11 figure 5.8: a dot plot of di fferences in swim velocities. letxdiffdenote the sample average of the di fferences in maximum velocity, sdiffthe sample standard deviation of the di fferences, and nthe number of pairs in the dataset. the t-statistic used to testh0vs.hais: xdiff\u0000\u000e0 sdiff=pn; where in this case \u000e0= 0.7 example 5.6 using the data in figure 5.7, conduct a two-sided hypothesis test at = 0:05 to assess whether there is evidence to suggest that wetsuits have an e ffect on swim velocities during a 1500m swim. the hypotheses are h0:\u000e= 0 andha:\u000e,0. let = 0:05. calculate the t-statistic: t=xdiff\u0000\u000e0 sdiff=pn=0:078\u00000 0:022=p 12= 12:32 the two-sided p-value is p=p(t <\u000012.32) +p(t >12.32); wherethas at-distribution with n\u00001 = 11 degrees of freedom. the t-table shows that p<0:01. software can be used to show that p= 8:9\u000210\u00008, a very small value indeed. the data support the claim that the wetsuits changed swim velocity in a 1500m swim. the observed average increase of 0.078 m/s is significantly di fferent than the null hypothesis of no change, and suggests that swim velocities are higher when swimmers wear wetsuits as opposed to swimsuits. calculating confidence intervals for paired data is also based on the di fferences between the values in each pair; the same approach as for single-sample data can be applied on the di fferences. for example, a two-sided 95% confidence interval for paired data has the form: xdiff\u0000t? df\u0002sdiffpn;xdiff+t? df\u0002sdiffpn! ; wheret?is the point on a t-distribution with df=n\u00001 fornpairs, with area 0.025 to its right. 7this value is specified by the null hypothesis of no di fference. 246 chapter 5. inference for numerical data guided practice 5.7 using the data in figure 5.7, calculate a 95% confidence interval for the average di fference in swim velocities during a 1500m swim. is the interval consistent with the results of the hypothesis test?8 the general approach when analyzing paired data is to first calculate the di fferences between the values in each pair, then use those di fferences in methods for confidence intervals and tests for a single sample. any conclusion from an analysis should be stated in terms of the original paired measurements. 8use the values of xdiffandsdiffas calculated previously: 0.078 and 0.022. the t?value of 2.20 has df= 11 and 0.025 area to the right. the confidence interval is (0 :078\u00060:022p 12)!(0.064, 0.091) m/s. with 95% confidence, \u000elies between 0.064 m/s and 0.09 m/s. the interval does not include 0 (no change), which is consistent with the result of the hypothesis test. 5.3. two-sample test for independent data 247 5.3 two-sample test for independent data does treatment using embryonic stem cells (escs) help improve heart function following a heart attack? new and potentially risky treatments are sometimes tested in animals before studies in humans are conducted. in a 2005 paper in lancet , menard, et al. describe an experiment in which 18 sheep with induced heart attacks were randomly assigned to receive cell transplants containing either escs or inert material.9various measures of cardiac function were measured 1 month after the transplant. this design is typical of an intervention study. the analysis of such an experiment is an example of drawing inference about the di fference in two population means, \u00161\u0000\u00162, when the data are independent, i.e., not paired. the point estimate of the di fference,x1\u0000x2, is used to calculate a t-statistic that is the basis of confidence intervals and tests. 5.3.1 confidence interval for a difference of means figure 5.9 contains summary statistics for the 18 sheep.10percent change in heart pumping capacity was measured for each sheep. a positive value corresponds to increased pumping capacity, which generally suggests a stronger recovery from the heart attack. is there evidence for a potential treatment e ffect of administering stem cells? nx s escs 9 3.50 5.17 control 9 -4.33 2.76 figure 5.9: summary statistics of the embryonic stem cell study. 9menard c, et al., transplantation of cardiac-committed mouse embryonic stem cells to infarcted sheep myocardium: a preclinical 2005; 366:1005-12, doi https://doi.org/10.1016/s0140-6736(05)67380-1 10the data are accessible as the dataset stem.cells in the openintro rpackage. 248 chapter 5. inference for numerical data frequency −10% −5% 0% 5% 10% 15%embryonic stem cell transplant change in heart pumping function0123frequency −10% −5% 0% 5% 10% 15%0123control (no treatment) change in heart pumping function figure 5.10: histograms for both the embryonic stem cell group and the control group. higher values are associated with greater improvement. figure 5.10 shows that the distributions of percent change do not have any prominent outliers, which would indicate a deviation from normality; this suggests that each sample mean can be modeled using a t-distribution. additionally, the sheep in the study are independent of each other, and the sheep between groups are also independent. thus, the t-distribution can be used to model the difference of the two sample means. using the ttt-distribution for a difference in means thet-distribution can be used for inference when working with the standardized di fference of two means if (1) each sample meets the conditions for using the t-distribution and (2) the samples are independent. a confidence interval for a di fference of two means has the same basic structure as previously discussed confidence intervals: (x1\u0000x2)\u0006t? df\u0002se: the following formula is used to calculate the standard error of x1\u0000x2. since\u001bis typically unknown, the standard error is estimated by using sin place of\u001b. sex1\u0000x2=s \u001b2 1 n1+\u001b2 2 n2\u0019s s2 1 n1+s2 2 n2: in this setting, the t-distribution has a somewhat complicated formula for the degrees of freedom that is usually calculated with software.11an alternative approach uses the smaller of n1\u00001 andn2\u00001 as the degrees of freedom.12 11see section 5.6 for the formula. 12this technique for degrees of freedom is conservative with respect to a type 1 error; it is more di fficult to reject the null hypothesis using this approach for degrees of freedom. 5.3. two-sample test for independent data 249 distribution of a difference of sample means the sample di fference of two means, x1\u0000x2, can be modeled using the t-distribution and the standard error sex1\u0000x2=q s2 1 n1+s2 2 n2(5.8) when each sample mean can itself be modeled using a t-distribution and the samples are independent. to calculate the degrees of freedom without using software, use the smaller of n1\u00001 andn2\u00001. example 5.9 calculate and interpret a 95% confidence interval for the e ffect of escs on the change in heart pumping capacity of sheep following a heart attack. the point estimate for the di fference isx1\u0000x2=xesc\u0000xcontrol = 7:83. the standard error is: s s2 1 n1+s2 2 n2=r 5:172 9+2:762 9= 1:95: sincen1=n2= 9, usedf= 8;t? 8= 2:31 for a 95% confidence interval. alternatively, computer software can provide more accurate values: df= 12:225;t?= 2:174. the confidence interval is given by: (x1\u0000x2)\u0006t? df\u0002se! 7:83\u00062:31\u00021:95! (3:38;12:38): with 95% confidence, the average amount that escs improve heart pumping capacity lies between 3.38% to 12.38%.13the data provide evidence for a treatment e ffect of administering stem cells. 5.3.2 hypothesis tests for a difference in means is there evidence that newborns from mothers who smoke have a di fferent average birth weight than newborns from mothers who do not smoke? the dataset births contains data from a random sample of 150 cases of mothers and their newborns in north carolina over a year; there are 50 cases in the smoking group and 100 cases in the nonsmoking group.14 fage mage weeks weight sexbaby smoke 1 na 13 37 5.00 female nonsmoker 2 na 14 36 5.88 female nonsmoker 3 19 15 41 8.13 male smoker :::::::::::::::::: 150 45 50 36 9.25 female nonsmoker figure 5.11: four cases from the births dataset. 13from software, the confidence interval is (3.58, 12.08). 14this dataset is available in the openintro rpackage. 250 chapter 5. inference for numerical data example 5.10 evaluate whether it is appropriate to apply the t-distribution to the di fference in sample means between the two groups. since the data come from a simple random sample and consist of less than 10% of all such cases, the observations are independent. while each distribution is strongly skewed, the large sample sizes of 50 and 100 allow for the use of the t-distribution to model each mean separately. thus, the difference in sample means may be modeled using a t-distribution. newborn weights (lbs) from mothers who smoked0246810 newborn weights (lbs) from mothers who did not smoke0246810 figure 5.12: the top panel represents birth weights for infants whose mothers smoked. the bottom panel represents the birth weights for infants whose mothers who did not smoke. the distributions exhibit moderate-to-strong and strong skew, respectively. a hypothesis test can be conducted to evaluate whether there is a relationship between mother’s smoking status and average newborn birth weight. the null hypothesis represents the case of no difference between the groups, h0:\u0016ns\u0000\u0016s= 0, where\u0016nsrepresents the population mean of newborn birthweight for infants with mothers who did not smoke, and \u0016srepresents mean newborn birthweight for infants with mothers who smoked. under the alternative hypothesis, there is some difference in average newborn birth weight between the groups, ha:\u0016ns\u0000\u0016s,0. the hypotheses can also be written as h0:\u0016ns=\u0016sandha:\u0016ns,\u0016s. stating hypotheses for two-group data when testing a hypothesis about two independent groups, directly compare the two population means and state hypotheses in terms of \u00161and\u00162. – for a two-sided test, h0:\u00161=\u00162;ha:\u00161,\u00162. – for a one-sided test, either h0:\u00161=\u00162;ha:\u00161>\u00162orh0:\u00161=\u00162;ha:\u00161<\u00162. 5.3. two-sample test for independent data 251 in this setting, the formula for a t-statistic is: t=(x1\u0000x2)\u0000(\u00161\u0000\u00162) sex1\u0000x2=(x1\u0000x2)\u0000(\u00161\u0000\u00162)s s2 1 n1+s2 2 n2: under the null hypothesis of no di fference between the groups, h0:\u00161\u0000\u00162= 0, the formula simplifies to t=(x1\u0000x2)s s2 1 n1+s2 2 n2: example 5.11 using figure 5.13, conduct a hypothesis test to evaluate whether there is evidence that newborns from mothers who smoke have a di fferent average birth weight than newborns from mothers who do not smoke. the hypotheses are h0:\u00161=\u00162andha:\u00161,\u00162, where\u00161represents the average newborn birth weight for nonsmoking mothers and \u00162represents average newborn birth weight for mothers who smoke. let = 0:05. calculate the t-statistic: t=(x1\u0000x2)s s2 1 n1+s2 2 n2=7:18\u00006:78q 1:602 100+1:432 50= 1:54: approximate the degrees of freedom as 50 \u00001 = 49. the t-score of 1.49 falls between the first and second columns in the df= 49 row of the t-table, so the two-sided p-value is between 0.10 and 0.20.15 thisp-value is larger than the significance value, 0.05, so the null hypothesis is not rejected. there is insufficient evidence to state there is a di fference in average birth weight of newborns from north carolina mothers who did smoke during pregnancy and newborns from north carolina mothers who did not smoke during pregnancy. smoker nonsmoker mean 6.78 7.18 st. dev. 1.43 1.60 samp. size 50 100 figure 5.13: summary statistics for the births dataset. 15from r,df= 89:277 andp= 0:138. 252 chapter 5. inference for numerical data 5.3.3 the paired test vs. independent group test in the two-sample setting, students often find it di fficult to determine whether a paired test or an independent group test should be used. the paired test applies only in situations where there is a natural pairing of observations between groups, such as in the swim data. pairing can be obvious, such as the two measurements for each swimmer, or more subtle, such as measurements of respiratory function in twins, where one member of the twin pair is treated with an experimental treatment and the other with a control. in the case of two independent groups, there is no natural way to pair observations. a common error is to overlook pairing in data and assume that two groups are independent. the swimsuit data can be used to illustrate the possible harm in conducting an independent group test rather than a paired test. in section 5.2, the paired t-test showed a significant di fference in the swim velocities between swimmers wearing wetsuits versus regular swimsuits. suppose the analysis had been conducted without accounting for the fact that the measurements were paired. the mean and standard deviation for the 12 wet suit velocities are 1.51 and 0.14 (m/sec), respectively, and 1.43 and 0.14 (m/sec) for the 12 swim suit velocities. a two-group test statistic is: t=1:52\u00001:43p 0:142=12 + 0:142=12= 1:37: if the degrees of freedom are approximated as 11 = 12 \u00001, the two-sided p-value as calculated from software is 0.20. according to this method, the null hypothesis of equal mean velocities for the two suit types would not be rejected. it is not di fficult to show that the numerator of the paired test (the average of the within swimmer di fferences) and the numerator of the two-group test (the di fference of the average times for the two groups) are identical. the values of the test statistics di ffer because the denominators are different—specifically, the standard errors associated with each statistic are di fferent. for the paired test statistic, the standard error uses the standard deviation of the within pair di fferences (0.22) and has value 0 :022=p 12 = 0:006. the two-group test statistic combines the standard deviations for the original measurements and has valuep 0:142=12 + 0:142=12 = 0:06. the standard error for the two-group test is 10-fold larger than for the paired test. this striking di fference in the standard errors is caused by the much lower variability of the individual velocity di fferences compared to the variability of the original measurements. due to the correlation between swim velocities for a single swimmer, the di fferences in the two velocity measurements for each swimmer are consistently small, resulting in low variability. pairing has allowed for increased precision in estimating the di fference between groups. the swim suit data illustrates the importance of context, which distinguishes a statistical problem from a purely mathematical one. while both the paired and two-group tests are numerically feasible to calculate, without an apparent error, the context of the problem dictates that the correct approach is to use a paired test. guided practice 5.12 propose an experimental design for the embryonic stem cell study in sheep that would have required analysis with a paired t-test.16 16the experiment could have been done on pairs of siblings, with one assigned to the treatment group and one assigned to the control group. alternatively, sheep could be matched up based on particular characteristics relevant to the experiment; for example, sheep could be paired based on similar weight or age. note that in this study, a design involving two measurements taken on each sheep would be impractical. 5.3. two-sample test for independent data 253 5.3.4 case study: discrimination in developmental disability support section 1.7.1 presented an analysis of the relationship between age, ethnicity, and amount of expenditures for supporting developmentally disabled residents in the state of california, using the dds.discr dataset. when the variable ageis ignored, the expenditures per consumer is larger on average for white non-hispanics than hispanics, but figure 1.53 showed that average di fferences by ethnicity were much smaller within age cohorts. this section demonstrates the use of t-tests to conduct a more formal analysis of possible di fferences in expenditure by ethnicity, both overall (i.e., ignoring age) and within age cohorts. comparing expenditures overall when ignoring age, expenditures within the ethnicity groups hispanic and white non-hispanic show substantial right-skewing (figure 1.45). a transformation is advisable before conducting a t-test. as shown in figure 5.14, a natural log transformation e ffectively eliminates skewing. ethnicitylog expenditures (log(usd)) hispanic white not hispanic4681012 figure 5.14: a plot of log(expenditures) byethnicity . is there evidence of a di fference in mean expenditures by ethnic group? conduct a t-test of the null hypothesis h0:\u00161=\u00162versus the two-sided alternative ha:\u00161,\u00162, where\u00161is the population mean log expenditure in hispanics and \u00162is the population mean log expenditure in white non-hispanics. ethnicity nx s 1 hispanic 376 8.56 1.17 2 white non hispanic 401 9.47 1.35 figure 5.15: summary statistics for the transformed variable log(expenditures) in the dds.discr data. the summary statistics required to calculate the t-statistic are shown in figure 5.15. the t-statistic for the test is t=9:47\u00008:56p 1:352=401 + 1:172=376= 10:1: 254 chapter 5. inference for numerical data the degrees of freedom of the test can be approximated as 376 \u00001 = 375; the p-value can be calculated using a normal approximation. regardless of whether a tor normal distribution is used, the probability of a test statistic with absolute value larger than 10 is vanishingly small—the p-value is less than 0.001. when ignoring age, there is significant evidence of a di fference in mean expenditures between hispanics and white non-hispanics. it appears that on average, white nonhispanics receive a higher amount of developmental disability support from the state of california (x1<x2). however, as indicated in section 1.7.1, this is a misleading result. the analysis as conducted does not account for the confounding e ffect of age, which is associated with both expenditures and ethnicity. as individuals age, they typically require more support from the government. in this dataset, white non-hispanics tend to be older than hispanics; this di fference in age distribution contributes to the apparent di fference in expenditures between two groups. comparing expenditures within age cohorts one way to account for the e ffect of age is to compare mean expenditures within age cohorts. when comparing individuals of similar ages but di fferent ethnic groups, are the di fferences in mean expenditures larger than would be expected by chance alone? figure 1.52 shows that the age cohort 13-17 is the largest among the hispanic consumers, while the cohort 22-50 is the largest among white non-hispanics. this section will examine the evidence against the null hypothesis of no di fference in mean expenditures within these two cohorts. figure 5.16 shows that within both the age cohorts of 13-17 years and 22-50 years, the distribution of expenditures is reasonably symmetric; there is no need to apply a transformation before conducting a t-test. the skewing evident when age was ignored is due to the di ffering distributions of age within ethnicities. ethnicityexpenditures (usd) hispanic white not hispanic02000400060008000 (a) ethnicityexpenditures (usd) hispanic white not hispanic0100002000030000400005000060000 (b) figure 5.16: (a)a plot of expenditures byethnicity in the age cohort 13 - 17. (b) a plot of expenditures byethnicity in the age cohort 22 - 50. 5.3. two-sample test for independent data 255 figure 5.17 contains the summary statistics for computing the test statistic to compare expenditures in the two groups within this age cohort. the test statistic has value t= 0:318, with degrees of freedom 66. the two-sided p-value is 0.75. there is not evidence of a di fference between mean expenditures in hispanics and white non-hispanics ages 13-17. ethnicity nx s 1 hispanic 103 3955.28 938.82 2 white not hispanic 67 3904.36 1071.02 figure 5.17: summary statistics for expenditures , ages 13-17. the analysis of the age cohort 22 - 50 years shows the same qualitative result. the t-statistic calculated from the summary statistics in figure 5.18 has value t= 0:659 andp-value 0.51. just as in the 13-17 age cohort, there is insu fficient evidence to reject the null hypothesis of no di fference between the means. ethnicity n x s 1 hispanic 43 40924.12 6467.09 2 white not hispanic 133 40187.62 6081.33 figure 5.18: summary statistics for expenditures , ages 22 - 50. the inference-based analyses for these two age cohorts support the conclusions reached through the exploratory approach used in section 1.7.1—comparing individuals of similar ages shows that there are not large di fferences between mean expenditures for white non-hispanics versus hispanics. an analysis that accounts for age as a confounding variable does not suggest there is evidence of ethnic discrimination in developmental disability support provided by the state of california. 256 chapter 5. inference for numerical data 5.3.5 pooled standard deviation estimate occasionally, two populations will have standard deviations that are so similar that they can be treated as identical. for example, historical data or a well-understood biological mechanism may justify this strong assumption. in such cases, it can be more precise to use a pooled standard deviation to make inferences about the di fference in population means. the pooled standard deviation of two groups uses data from both samples to estimate the common standard deviation and standard error. if there are good reasons to believe that the population standard deviations are equal, an improved estimate of the group variances can be obtained by pooling the data from the two groups: s2 pooled=s2 1(n1\u00001) +s2 2(n2\u00001) n1+n2\u00002; wheren1andn2are the sample sizes, and s1ands2represent the sample standard deviations. in this setting, the t-statistic uses s2 pooledin place ofs2 1ands2 2in the standard error formula, and the degrees of freedom for the t\u0000statistic is the sum of the degrees of freedom for the two sample variances: df = (n1\u00001) + (n2\u00001) =n1+n2\u00002: thet-statistic for testing the null hypothesis of no di fference between population means becomes t=x1\u0000x2 spooledq 1 n1+1 n2: the formula for the two-sided confidence interval for the di fference in population means is (x1\u0000x2)\u0006t?\u0002spooledr 1 n1+1 n2; wheret?is the point on a t-distribution with n1+n2\u00002 degrees of freedom chosen according to the confidence coe fficient. the benefits of pooling the standard deviation are realized through obtaining a better estimate of the standard deviation for each group and using a larger degrees of freedom parameter for the tdistribution. both of these changes may permit a more accurate model of the sampling distribution ofx1\u0000x2, if the standard deviations of the two groups are indeed equal. in most applications, however, it is di fficult to verify the assumption of equal population standard deviations, and thus safer to use the methods discussed in sections 5.3.1 and 5.3.2. 5.4. power calculations for a difference of means 257 5.4 power calculations for a difference of means designing a study often involves many complex issues; perhaps the most important statistical issue in study design is the choice of an appropriate sample size. the power of a statistical test is the probability that the test will reject the null hypothesis when the alternative hypothesis is true; sample sizes are chosen to make that probability su fficiently large, typically between 80% and 90%. two competing considerations arise when choosing a sample size. the sample size should be sufficiently large to allow for important group di fferences to be detected in a hypothesis test. practitioners often use the term ‘detecting a di fference’ to mean correctly rejecting a null hypothesis, i.e., rejecting a null hypothesis when the alternative is true. if a study is so small that detecting a statistically significant di fference is unlikely even when there are potentially important di fferences, enrolling participants might be unethical, since subjects could potentially be exposed to a dangerous experimental treatment. however, it is also unethical to conduct studies with an overly large sample size, since more participants than necessary would be exposed to an intervention with uncertain value. additionally, collecting data is typically expensive and time consuming; it would be a waste of valuable resources to design a study with an overly large sample size. this section begins by illustrating relevant concepts in the context of a hypothetical clinical trial, where the goal is to calculate a su fficient sample size for being 80% likely to detect practically important e ffects.17afterwards, formulas are provided for directly calculating sample size, as well as references to software that can perform the calculations. 5.4.1 reviewing the concepts of a test example 5.13 a company would like to run a clinical trial with participants whose systolic blood pressures are between 140 and 180 mmhg. suppose previously published studies suggest that the standard deviation of patient blood pressures will be about 12 mmhg, with an approximately symmetric distribution.18what would be the approximate standard error for xtrmt\u0000xctrlif 100 participants were enrolled in each treatment group? the standard error is calculated as follows: sextrmt\u0000xctrl=s s2 trmt ntrmt+s2 ctrl nctrl=r 122 100+122 100= 1:70: this may be an imperfect estimate of sextrmt\u0000xctrl, since the standard deviation estimate of 12 mmhg from prior data may not be correct. however, it is su fficient for getting started, and making an assumption like this is often the only available option. 17while sample size planning is also important for observational studies, those techniques are not discussed here. 18in many studies like this one, each participant’s blood pressure would be measured at the beginning and end of the study, and the outcome measurement for the study would be the average di fference in blood pressure in each of the treatment groups. for this hypothetical study, we assume for simplicity that blood pressure is measured at only the end of the study, and that the randomization ensures that blood pressures at the beginning of the study are equal (on average) between the two groups. 258 chapter 5. inference for numerical data since the degrees of freedom are greater than 30, the distribution of xtrmt\u0000xctrlwill be approximately normal. under the null hypothesis, the mean is 0 and the standard deviation is 1.70 (from the standard error). −9 −6 −3 0 3 6 9 xtrmt−xctrlnull distribution figure 5.19: null distribution for the t-statistic in example 5.14. example 5.14 for what values of xtrmt\u0000xctrlwould the null hypothesis be rejected, using = 0:05? if the observed di fference is in the far left or far right tail of the null distribution, there is su fficient evidence to reject the null hypothesis. for = 0:05,h0is rejected if the di fference is in the lower 2.5% or upper 2.5% tail: lower 2.5%: for the normal model, this is 1.96 standard errors below 0, so any di fference smaller than\u00001:96\u00021:70 =\u00003:332 mmhg. upper 2.5%: for the normal model, this is 1.96 standard errors above 0, so any di fference larger than 1:96\u00021:70 = 3:332 mmhg. the boundaries of these rejection regions are shown below. note that if the new treatment is effective, mean blood pressure should be lower in the treatment group than in the control group; i.e., the di fference should be in the lower tail. −9 −6 −3 0 3 6 9 xtrmt−xctrlnull distribution reject h0do not reject h0reject h0 the next step is to perform some hypothetical calculations to determine the probability of rejecting the null hypothesis if the alternative hypothesis were true. 5.4. power calculations for a difference of means 259 5.4.2 computing the power for a 2-sample test if there is a real e ffect from an intervention, and the e ffect is large enough to have practical value, the probability of detecting that e ffect is referred to as the power . power can be computed for different sample sizes or di fferent e ffect sizes. there is no easy way to define when an e ffect size is large enough to be of value; this is not a statistical issue. for example, in a clinical trial, the scientifically significant e ffect is the incremental value of the intervention that would justify changing current clinical recommendations from an existing intervention to a new one. in such a setting, the e ffect size is usually determined from long discussions between the research team and study sponsors. suppose that for this hypothetical blood pressure medication study, the researchers are interested in detecting any e ffect on blood pressure that is 3 mmhg or larger than the standard medication. here, 3 mmhg is the minimum population e ffect size of interest. example 5.15 suppose the study proceeded with 100 patients per treatment group and the new drug does reduce average blood pressure by an additional 3 mmhg relative to the standard medication. what is the probability of detecting this e ffect? determine the sampling distribution for xtrmt\u0000xctrlwhen the true di fference is\u00003 mmhg; this has the same standard deviation of 1.70 as the null distribution, but the mean is shifted 3 units to the left. then, calculate the fraction of the distribution for xtrmt\u0000xctrlthat falls within the rejection region for the null distribution, as shown in figure 5.20. the probability of being in the left side of the rejection region ( x<\u00003:332) can be calculated by converting to a z-score and using either the normal probability table or statistical software.19 z=\u00003:332\u0000(\u00003) 1:7=\u00000:20!p(z\u0014\u00000:20) = 0:4207: the power for the test is about 42% when \u0016trmt\u0000\u0016ctrl=\u00003 mm/hg and each group has a sample size of 100. −9 −6 −3 0 3 6 9 xtrmt−xctrlnull distribution distribution with μtrmt−μctrl = −3 figure 5.20: the rejection regions are outside of the dotted lines. recall that the boundaries for = 0:05 were calculated to be \u00063:332 mmhg. 19the probability of being in the right side of the rejection region is negligible and can be ignored. 260 chapter 5. inference for numerical data 5.4.3 determining a proper sample size the last example demonstrated that with a sample size of 100 in each group, there is a probability of about 0.42 of detecting an e ffect size of 3 mmhg. if the study were conducted with this sample size, even if the new medication reduced blood pressure by 3 mmhg compared to the control group, there is a less than 50% chance of concluding that the medication is beneficial. studies with low power are often inconclusive, and there are important reasons to avoid such a situation: – participants were subjected to a drug for a study that may have little scientific value. – the company may have invested hundreds of millions of dollars in developing the new drug, and may now be left with uncertainty about its potential. – another clinical trial may need to be conducted to obtain a more conclusive answer as to whether the drug does hold any practical value, and that would require substantial time and expense. to ensure a higher probability of detecting a clinically important e ffect, a larger sample size should be chosen. what about a study with 500 patients per group? guided practice 5.16 calculate the power to detect a change of -3 mmhg using a sample size of 500 per group. recall that the standard deviation of patient blood pressures was expected to be about 12 mmhg.20 (a) determine the standard error. (b) identify the null distribution and rejection regions, as well as the alternative distribution when \u0016trmt\u0000\u0016ctrl=\u00003. (c) compute the probability of rejecting the null hypothesis. with a sample size of 500 per group, the power of the test is much larger than necessary. not only does this lead to a study that would be overly expensive and time consuming, it also exposes more patients than necessary to the experimental drug. sample sizes are generally chosen such that power is around 80%, although in some cases 90% is the target. other values may be reasonable for a specific context, but 80% and 90% are most commonly chosen as a good balance between high power and limiting the number of patients exposed to a new treatment (as well as reducing experimental costs). 20(a) the standard error will now be se=q 122 500+122 500= 0:76. (b) the null distribution, rejection boundaries, and alternative distribution are shown below. the rejection regions are the areas outside the two dotted lines at xtrmt\u0000xctrl\u00060:76\u00021:96 =\u00061:49. −9 −6 −3 0 3 6 9 xtrmt−xctrlnull distribution distribution with μtrmt−μctrl = −3 (c) compute the z-score and find the tail area, z=\u00001:49\u0000(\u00003) 0:76= 1:99!p(z\u00141:99) = 0:9767, which is the power of the test for a di fference of 3 mmhg. with 500 patients per group, the study would be 97.7% likely to detect an e ffect size of 3 mmhg. 5.4. power calculations for a difference of means 261 example 5.17 identify the sample size that would lead to a power of 80%. thez-score that defines a lower tail area of 0.80 is about z= 0:84. in other words, 0.84 standard errors from -3, the mean of the alternative distribution. −9 −6 −3 0 3 6 9 xtrmt−xctrlnull distribution distribution with μtrmt−μctrl = −3 0.84 se 1.96 se for = 0:05, the rejection region always extends 1.96 standard errors from 0, the center of the null distribution. the distance between the centers of the null and alternative distributions can be expressed in terms of the standard error: (0:84\u0002se) + (1:96\u0002se) = 2:8\u0002se: this quantity necessarily equals the minimum e ffect size of interest, 3 mmhg, which is the distance between -3 and 0. it is then possible to solve for n: 3 = 2:8\u0002se 3 = 2:8\u0002r 122 n+122 n n=2:82 32\u0002\u0010 122+ 122\u0011 = 250:88 the study should enroll at least 251 patients per group for 80% power. note that sample size should always be rounded up in order to achieve the desired power. even if the calculation had yielded a number closer to 250 (e.g., 250.25), the study should still enroll 251 patients per grou, since having 250 patients per group would result in a power lower than 80%. guided practice 5.18 suppose the targeted power is 90% and = 0:01. how many standard errors should separate the centers of the null and alternative distributions, where the alternative distribution is centered at the minimum e ffect size of interest? assume the test is two-sided.21 21find thez-score such that 90% of the distribution is below it: z= 1:28. next, find the cuto ffs for the rejection regions: \u00062:58. thus, the centers of the null and alternative distributions should be about 1 :28 + 2:58 = 3:86 standard errors apart. 262 chapter 5. inference for numerical data figure 5.21 shows the power for sample sizes from 20 participants to 5,000 participants when = 0:05 and the true di fference is -3 mmhg. while power increases with sample size, having more than 250-300 participants provides little additional value towards detecting an e ffect. sample size per grouppower 20 50100 200 5001000 2000 50000.00.20.40.60.81.0 figure 5.21: the curve shows the power for di fferent sample sizes in the context of the blood pressure example when the true di fference is -3. 5.4.4 formulas for power and sample size the previous sections have illustrated how power and sample size can be calculated from first principles, using the fundamental ideas behind distributions and testing. in practice, power and sample size calculations are so important that statistical software should be the method of choice; there are many commercially available and public domain programs for performing such calculations. however, hand calculations using formulas can provide quick estimates in the early stages of planning a study. use the following formula to calculate sample size for comparing two means, assuming each group will have nparticipants: n=(\u001b2 1+\u001b2 2)(z1\u0000 =2+z1\u0000 )2 \u00012: in this formula: –\u00161;\u00162;\u001b1;and\u001b2are the population means and standard deviations of the two groups. –\u0001=\u00161\u0000\u00162is the minimally important di fference that investigators wish to detect. – the null and alternative hypotheses are h0:\u0001= 0 (i.e., no di fference between the means) and ha:\u0001,0, i.e., a two-sided alternative. – the two-sided significance level is , andz1\u0000 =2is the point on a standard normal distribution with area 1\u0000 =2 to its left and =2 area to its right. – is the probability of incorrectly failing to reject h0for a specified value of \u0001; 1\u0000 is the power. the value z1\u0000 is the point on a standard normal distribution with area 1 \u0000 to its left. 5.4. power calculations for a difference of means 263 for a study with sample size nper group, where zis a normal random variable with mean 0 and standard deviation 1, power is given by: power =p0 bbbbbbbb@z<\u0000z1\u0000 =2+\u0001q \u001b2 1=n+\u001b2 2=n1 cccccccca: these formulas could have been used to do the earlier power and sample size calculations for the hypothetical study of blood pressure lowering medication. to calculate the sample size needed for 80% power in detecting a change of 3 mmhg, = 0:05, 1\u0000 = 0:80,\u0001= 3 mmhg, and \u001b1=\u001b2= 12 mmhg. the formula yields a sample size nper group of n=(122+ 122)(1:96 + 0:84)2 (\u00003:0)2= 250:88; which can be rounded up to 251. the formula for power can be used to verify the sample size of 251: power =p z<\u00001:96 +3p 122=251 + 122=251! =p(z<1:25) = 0:85: the calculated power is slightly larger than 80% because of the rounding to 251. the sample size calculations done before any data are collected are one of the most critical aspects of conducting a study. if an analysis is done incorrectly, it can be redone once the error is discovered. however, if data were collected for a sample size that is either too large or too small, it can be impossible to correct the error, especially in studies with human subjects. as a result, sample size calculations are nearly always done using software. for two-sample t-tests, the rfunction power.t.test is both freely available and easy to use. 264 chapter 5. inference for numerical data 5.5 comparing means with anova in some settings, it is useful to compare means across several groups. it might be tempting to do pairwise comparisons between groups; for example, if there are three groups ( a;b;c ), why not conduct three separate t-tests (avs.b,avs.c,bvs.c)? conducting multiple tests on the same data increases the rate of type i error, making it more likely that a di fference will be found by chance, even if there is no di fference among the population means. multiple testing is discussed further in section 5.5.3. instead, the methodology behind a t-test can be generalized to a procedure called analysis of variance (anov a) , which uses a single hypothesis test to assess whether the means across several groups are equal. strong evidence favoring the alternative hypothesis in anov a is described by unusually large di fferences among the group means. h0: the mean outcome is the same across all kgroups. in statistical notation, \u00161=\u00162=\u0001\u0001\u0001=\u0016k where\u0016irepresents the mean of the outcome for observations in category i. ha: at least one mean is di fferent. there are three conditions on the data that must be checked before performing anov a: 1) observations are independent within and across groups, 2) the data within each group are nearly normal, and 3) the variability across the groups is about equal. example 5.19 examine figure 5.22. compare groups i, ii, and iii. is it possible to visually determine if the differences in the group centers is due to chance or not? now compare groups iv, v, and vi. do the differences in these group centers appear to be due to chance? it is difficult to discern a di fference in the centers of groups i, ii, and iii, because the data within each group are quite variable relative to any di fferences in the average outcome. however, there appear to be di fferences in the centers of groups iv, v, and vi. for instance, group v appears to have a higher mean than that of the other two groups. the di fferences in centers for groups iv, v, and vi are noticeable because those di fferences are large relative to the variability in the individual observations within each group. outcome −101234 iiiiiiivvvi figure 5.22: side-by-side dot plot for the outcomes for six groups. 5.5. comparing means with anov a 265 5.5.1 analysis of variance (anova) and the fff-test the famuss dataset was introduced in chapter 1, section 1.2.2. in the famuss study, researchers examined the relationship between muscle strength and genotype at a location on the actn3 gene. the measure for muscle strength is percent change in strength in the non-dominant arm ( ndrm.ch ). is there a di fference in muscle strength across the three genotype categories ( cc,ct,tt)? guided practice 5.20 the null hypothesis under consideration is the following: \u0016cc=\u0016ct=\u0016tt. write the null and corresponding alternative hypotheses in plain language.22 figure 5.23 provides summary statistics for each group. a side-by-side boxplot for the change in non-dominant arm strength is shown in figure 5.24; figure 5.25 shows the q-q plots by each genotype. notice that the variability appears to be approximately constant across groups; nearly constant variance across groups is an important assumption that must be satisfied for using anov a. based on the q-q plots, there is evidence of moderate right skew; the data do not follow a normal distribution very closely, but could be considered to ’loosely’ follow a normal distribution.23it is reasonable to assume that the observations are independent within and across groups; it is unlikely that participants in the study were related, or that data collection was carried out in a way that one participant’s change in arm strength could influence another’s. cc ct tt sample size ( ni) 173 261 161 sample mean ( ̄xi) 48.89 53.25 58.08 sample sd ( si) 29.96 33.23 35.69 figure 5.23: summary statistics of change in non-dominant arm strength, split by genotype. genotypechange in non−dominant arm strength (%) cc ct tt050100150200250 figure 5.24: side-by-side box plot of the change in non-dominant arm strength for 595 participants across three groups. 22h0: the average percent change in non-dominant arm strength is equal across the three genotypes. ha: the average percent change in non-dominant arm strength varies across some (or all) groups. 23in a more advanced course, it can be shown that the anov a procedure still holds with deviations from normality when sample sizes are moderately large. additionally, a more advanced course would discuss appropriate transformations to induce normality. 266 chapter 5. inference for numerical data ●● ●● ● ● ●●● ● ●● ● ●● ● ●● ●●● ●●● ●● ●●●●● ●●● ●●● ●● ● ●●● ●● ●● ● ●● ●● ●● ●●●● ●● ●●●● ●●● ● ●●● ●● ●● ●● ●●●● ●● ● ●● ● ●● ●●● ●● ● ●●●● ●● ●●● ●●●● ●● ●●● ● ●● ● ●●● ●● ●● ●● ●● ●● ●●●● ● ●● ● ●● ●● ●●● ●● ● ●● ●●● ● ●●● ●●● ● ●● ● ●● ●● ● ●●● ● −2−1012050100150q−q for cc genotype theoretical quantilessample quantiles ●●● ● ● ●●● ●●● ●●● ● ●● ●●●●●●●●●● ●● ●● ●●●● ●● ● ●●●● ● ●●●● ●● ●●● ●●● ● ●●● ●●●●● ●● ●● ●● ●●● ●● ●● ●●● ● ● ●● ● ●●●● ● ●● ●●● ●●● ●● ●●● ● ● ●● ●● ●● ● ●● ●●●● ●●●●●● ● ●● ●●● ● ●● ●●● ● ●● ●● ●● ●●● ●● ●●●●● ●● ●● ● ●●● ●●● ●●●● ●●● ●●●●●● ●●● ●●● ●●● ● ●●●● ●● ●● ●●●●● ● ●●● ●●●● ●● ●● ● ●●●● ●●● ● ●● ●●●● ● ●●●●● ●●● ● ● ●● ●●● ●●●● ●● ● ●●● ●●● ● ●●● ● −3−2−10123050100150200250q−q for ct genotype theoretical quantilessample quantiles● ● ●●●●●● ● ●● ●●● ●● ●● ●● ●●● ●●●● ●●● ● ●●● ● ● ●● ●● ●●● ●●● ● ●● ●● ●●●● ●●●●● ●● ●● ●●● ●●● ●● ●●● ●●●● ●● ●●● ●● ●● ● ● ● ● ●●●●● ●●● ●●● ●● ● ●●●● ●● ●● ● ●●● ●● ●●● ● ●●●●● ●●● ● ●● ● ●●● ●● ●● ●● ●●● ●●● ●●●●● ●● ●●● −2−1012050100150200q−q for tt genotype theoretical quantilessample quantiles figure 5.25: q-q plots of the change in non-dominant arm strength for 595 participants across three groups. example 5.21 the largest di fference between the sample means is between the ccand ttgroups. consider again the original hypotheses: h0:\u0016cc=\u0016ct=\u0016tt ha: the average percent change in non-dominant arm strength ( \u0016i) varies across some (or all) groups. why might it be inappropriate to run the test by simply estimating whether the di fference of\u0016cc and\u0016ttis statistically significant at a 0.05 significance level? it is inappropriate to informally examine the data and decide which groups to formally test. this is a form of data fishing ; choosing the groups with the largest di fferences for the formal test will lead to an increased chance of incorrectly rejecting the null hypothesis (i.e., an inflation in the type i error rate). instead, all the groups should be tested using a single hypothesis test. analysis of variance focuses on answering one question: is the variability in the sample means large enough that it seems unlikely to be from chance alone? the variation between groups is referred to as the mean square between groups ( msg ); themsg is a measure of how much each group mean varies from the overall mean. let xrepresent the mean of outcomes across all groups, wherexiis the mean of outcomes in a particular group iandniis the sample size of group i. the mean square between groups is: msg =1 k\u00001kx i=1ni(xi\u0000x)2=1 dfgssg; wheressg is the sum of squares between groups ,pk i=1ni(xi\u0000x)2, and dfg=k\u00001 is the degrees of freedom associated with the msg when there are kgroups. 5.5. comparing means with anov a 267 under the null hypothesis, any observed variation in group means is due to chance and there is no real di fference between the groups. in other words, the null hypothesis assumes that the groupings are non-informative, such that all observations can be thought of as belonging to a single group. if this scenario is true, then it is reasonable to expect that the variability between the group means should be equal to the variability observed within a single group. the mean square error (mse )is a pooled variance estimate with associated degrees of freedom df e=n\u0000kthat provides a measure of variability within the groups. the mean square error is computed as: mse =1 n\u0000kkx i=1(ni\u00001)s2 i=1 dfesse; where thesse is the sum of squared errors ,niis the sample size of group i, andsiis the standard deviation of group i. under the null hypothesis that all the group means are equal, any di fferences among the sample means are only due to chance; thus, the msg andmse should also be equal. anov a is based on comparing the msg andmse . the test statistic for anov a, the f-statistic , is the ratio of the between-group variability to the within-group variability: f=msg mse: (5.22) example 5.23 calculate the f-statistic for the famuss data summarized in figure 5.23. the overall mean xacross all observations is 53.29. first, calculate the msg andmse . msg =1 k\u00001kx i=1ni( ̄xi\u0000 ̄x)2 =1 3\u00001[(173)(48:89\u000053:29)2+ (261)(53:25\u000053:29)2+ (161)(58:08\u000053:29)2] =3521:69 mse =1 n\u0000kkx i=1(ni\u00001)s2 i =1 595\u00003[(173\u00001)(29:962) + (261\u00001)(33:232) + (161\u00001)(35:692)] =1090:02 thef-statistic is the ratio: msg mse=3521:69 1090:02= 3:23: 268 chapter 5. inference for numerical data ap-value can be computed from the f-statistic using an f-distribution, which has two associated parameters: df 1and df 2. for thef-statistic in anov a, df 1= dfgand df 2= dfe. anf distribution with 2 and 592 degrees of freedom, corresponding to the f-statistic for the genotype and muscle strength hypothesis test, is shown in figure 5.26. 0123456 figure 5.26: an f-distribution with df 1= 2 and df 2= 592. the tail area greater thanf= 3:23 is shaded. the larger the observed variability in the sample means ( msg ) relative to the within-group variability ( mse ), the larger fwill be. larger values of frepresent stronger evidence against the null hypothesis. the upper tail of the distribution is used to compute a p-value, which is typically done using statistical software. example 5.24 thep-value corresponding to the test statistic is equal to about 0.04. does this provide strong evidence against the null hypothesis at significance level = 0:05? thep-value is smaller than 0.05, indicating the evidence is strong enough to reject the null hypothesis at a significance level of 0.05. the data suggest that average change in strength in the non-dominant arm varies by participant genotype. thefff-statistic and the fff-test analysis of variance (anov a) is used to test whether the mean outcome di ffers across two or more groups. anov a uses a test statistic f, which represents a standardized ratio of variability in the sample means relative to the variability within the groups. if h0is true and the model assumptions are satisfied, the statistic ffollows anfdistribution with parameters df1=k\u00001 and df 2=n\u0000k. the upper tail of the f-distribution is used to calculate the p-value. 5.5. comparing means with anov a 269 5.5.2 reading an anova table from software the calculations required to perform an anov a by hand are tedious and prone to human error. instead, it is common to use statistical software to calculate the f-statistic and associated p-value. the results of an anov a can be summarized in a table similar to that of a regression summary, which will be discussed in chapters 6 and 7. figure 5.27 shows an anov a summary to test whether the mean change in non-dominant arm strength varies by genotype. many of these values should look familiar; in particular, the f-statistic and p-value can be retrieved from the last two columns. df sum sq mean sq f value pr( >f) famuss$actn3.r577x 2 7043 3522 3.231 0.0402 residuals 592 645293 1090 figure 5.27: anov a summary for testing whether the mean change in nondominant arm strength varies by genotype at the actn3.r577x location on the actn3 gene. 5.5.3 multiple comparisons and controlling type i error rate rejecting the null hypothesis in an anov a analysis only allows for a conclusion that there is evidence for a di fference in group means. in order to identify the groups with di fferent means, it is necessary to perform further testing. for example, in the famuss analysis, there are three comparisons to make: cctoct,cctott, and cttott. while these comparisons can be made using two sample t-tests, it is important to control the type i error rate. one of the simplest ways to reduce the overall probability of identifying a significant di fference by chance in a multiple comparisons setting is to use the bonferroni correction procedure. in the bonferroni correction procedure, the p-value from a two-sample t-test is compared to a modified significance level, ?; ?= =k, wherekis the total number of comparisons being considered. for kgroups,k=k(k\u00001) 2. when calculating the t-statistic, use the pooled estimate of standard deviation between groups (which equalsp mse ); to calculate the p-value, use a tdistribution with df 2. it is typically more convenient to do these calculations using software. bonferroni correction the bonferroni correction suggests that a more stringent significance level is appropriate when conducting multiple tests: ?= =k wherekis the number of comparisons being considered. for kgroups,k=k(k\u00001) 2. 270 chapter 5. inference for numerical data example 5.25 the anov a conducted on the famuss dataset showed strong evidence of di fferences in the mean strength change in the non-dominant arm between the three genotypes. complete the three possible pairwise comparisons using the bonferroni correction and report any di fferences. use a modified significance level of ?= 0:05=3 = 0:0167. the pooled estimate of the standard deviation isp mse =p 1090:02 = 33:02. genotype ccversus genotype ct: t=x1\u0000x2 spooledq 1 n1+1 n2=48:89\u000053:25 33:02q 1 173+1 261=\u00001:35: this results in a p-value of 0.18 on df= 592. this p-value is larger than ?= 0:0167, so there is not evidence of a di fference in the means of genotypes ccand ct. genotype ccversus genotype tt: t=x1\u0000x2 spooledq 1 n1+1 n2=48:89\u000058:08 33:02q 1 173+1 161=\u00002:54: this results in a p-value of 0.01 on df= 592. this p-value is smaller than ?= 0:0167, so there is evidence of a di fference in the means of genotypes ccand tt. genotype ctversus genotype tt: t=x1\u0000x2 spooledq 1 n1+1 n2=53:25\u000058:08 33:02q 1 261+1 161=\u00001:46: this results in a p-value of 0.14 on df= 592. this p-value is larger than ?= 0:0167, so there is not evidence of a di fference in the means of genotypes ctand tt. in summary, the mean percent strength change in the non-dominant arm for genotype ctindividuals is not statistically distinguishable from those of genotype ccand ttindividuals. however, there is evidence that mean percent strength change in the non-dominant arm di ffers between individuals of genotype ccand ttare different. 5.5. comparing means with anov a 271 5.5.4 reading the results of pairwise ttt-tests from software statistical software can be used to calculate the p-values associated with each possible pairwise comparison of the groups in anov a. the results of the pairwise tests are summarized in a table that shows the p-value for each two-group test. figure 5.28 shows the p-values from the three possible two-group t-tests comparing change in non-dominant arm strengths between individuals with genotypes cc,ct, and tt. for example, the table indicates that when comparing mean change in non-dominant arm strength between ttand ccindividuals, the p-value is 0.01. this coheres with the calculations above, and these unadjusted p-values should be compared to ?= 0:0167. cc ct ct 0.18 tt 0.01 0.14 figure 5.28: unadjusted p-values for pairwise comparisons testing whether the mean change in non-dominant arm strength varies by genotype at the actn3.r577x location on actn3 gene. the use of statistical software makes it easier to apply corrections for multiple testing, such that it is not necessary to explicitly calculate the value of ?. figure 5.29 shows the bonferroniadjustedp-values from the three possible tests. when statistical software applies the bonferroni correction, the unadjusted p-value is multiplied by k, the number of comparisons, allowing for the values to be directly compared to , not ?. comparing an unadjusted p-value to =k is equivalent to comparing the quantity ( k\u0002p-value) to . cc ct ct 0.54 tt 0.03 0.43 figure 5.29: bonferroni-adjusted p-values for pairwise comparisons testing whether the mean change in non-dominant arm strength varies by genotype at theactn3.r577x location on actn3 gene. 272 chapter 5. inference for numerical data 5.6 notes the material in this chapter is particularly important. for many applications, t-tests and analysis of variance (anov a) are an essential part of the core of statistics in medicine and the life sciences. the comparison of two or more groups is often the primary aim of experiments both in the laboratory and in studies with human subjects. more generally, the approaches to interpreting and drawing conclusions from testing demonstrated in this chapter are used throughout the rest of the text and, indeed, in much of statistics. while it is important to master the details of the techniques of testing for di fferences in two or more groups, it is even more critical to not lose sight of the fundamental principles behind the tests. a statistically significant di fference in group means does not necessarily imply that group membership is the reason for the observed association. a significant association does not necessarily imply causation, even if it is highly significant; confounding variables may be involved. in most cases, causation can only be inferred in controlled experiments when interventions have been assigned randomly. it is also essential to carefully consider the context of a problem. for instance, students often find the distinction between paired and independent group comparisons confusing; understanding the problem context is the only reliable way to choose the correct approach. it is generally prudent to use the form of the t-test that does not assume equal standard deviations, but the power calculations described in section 5.4 assume models with equal standard deviations. the formulas are simpler when standard deviations are equal, and software is more widely available for that case. the di fferences in sample sizes are usually minor and less important than assumptions about target di fferences or the values of the standard deviations. if the standard deviations are expected to be very di fferent, then more specialized software for computing sample size and power should be used. the analysis done after the study has been completed should then use thet-test for unequal standard deviations. tests for significant di fferences are sometimes overused in science, with not enough attention paid to estimates and confidence intervals. confidence intervals for the di fference of two population means show a range of underlying di fferences in means that are consistent with the data, and often lead to insights not possible from only the test statistic and p-value. wide confidence intervals may show that a non-significant test is the result of high variability in the test statistic, perhaps caused by a sample size that was too small. conversely, a highly significant p-value may be the result of such a large sample size that the observed di fferences are not scientifically meaningful; that may be evident from confidence intervals with very narrow width. 5.6. notes 273 finally, the formula used to approximate degrees of freedom \u0017for the independent two-group t-test that does not assume equal variance is \u0017=h (s2 1=n1) + (s2 2=n2)i2 h (s2 1=n1)2=(n1\u00001) + (s2 2=n2)2=(n2\u00001)i; wheren1;s1are the sample size and standard deviation for the first sample, and n2;s2are the corresponding values for the second sample. since \u0017is routinely provided in the output from statistical software, there is rarely any need to calculate it by hand. the approximate formula df = min(n1\u00001;n2\u00001) always produces a smaller value for degrees of freedom and hence a larger p-value. the labs for this chapter are structured around particularly important problems in practice: comparing two groups, such as a treatment and control group (lab 1); assessing before starting a study whether a sample size is large enough to make it likely that important di fferences will be detected (lab 2); comparing more than two groups using analysis of variance (lab 3); controlling error rates when looking at many comparisons in a dataset (lab 4); and thinking about hypothesis testing in the larger context of reproducibility (lab 5). the first four labs provide guidance on how to conduct and interpret specific types of analyses. students may find the last lab particularly useful in understanding the distinction between a p-value and other probabilities relevant in an inferential setting, such as power. 274 chapter 5. inference for numerical data 5.7 exercises 5.7.1 single-sample inference with the ttt-distribution 5.1 identify the critical ttt.an independent random sample is selected from an approximately normal population with unknown standard deviation. find the degrees of freedom and the critical t-value (t?) for the given sample size and confidence level. (a)n= 6, cl = 90% (b)n= 21, cl = 98% (c)n= 29, cl = 95% (d)n= 12, cl = 99% 5.2 find the p-value, part i. an independent random sample is selected from an approximately normal population with an unknown standard deviation. find the p-value for the given sets of alternative hypothesis and test statistic, and determine if the null hypothesis would be rejected at = 0:05. (a)ha:\u0016>\u0016 0,n= 11,t= 1:91 (b)ha:\u0016<\u0016 0,n= 17,t=\u00003:45 (c)ha:\u0016,\u00160,n= 7,t= 0:83 (d)ha:\u0016>\u0016 0,n= 28,t= 2:13 5.3 cutoff values. the following are cuto ffvalues for the upper 5% of a t-distribution with either degrees of freedom 10, 50, or 100: 2.23, 1.98, and 2.01. identify which value belongs to which distribution and explain your reasoning. 5.4 find the p-value, part ii. an independent random sample is selected from an approximately normal population with an unknown standard deviation. find the p-value for the given sets of alternative hypothesis and test statistic, and determine if the null hypothesis would be rejected at = 0:01. (a)ha:\u0016>0:5,n= 26,t= 2:485 (b)ha:\u0016<3,n= 18,t= 0:5 5.5 working backwards, part i. a 95% confidence interval for a population mean, \u0016, is given as (18.985, 21.015). this confidence interval is based on a simple random sample of 36 observations. calculate the sample mean and standard deviation. assume that all conditions necessary for inference are satisfied. use thet-distribution in any calculations. 5.6 working backwards, part ii. a 90% confidence interval for a population mean is (65, 77). the population distribution is approximately normal and the population standard deviation is unknown. this confidence interval is based on a simple random sample of 25 observations. calculate the sample mean, the margin of error, and the sample standard deviation. 5.7. exercises 275 5.7 sleep habits of new yorkers. new york is known as \"the city that never sleeps\". a random sample of 25 new yorkers were asked how much sleep they get per night. statistical summaries of these data are shown below. do these data provide strong evidence that new yorkers sleep less than 8 hours a night on average? n ̄x s min max 25 7.73 0.77 6.17 9.78 (a) write the hypotheses in symbols and in words. (b) check conditions, then calculate the test statistic, t, and the associated degrees of freedom. (c) find and interpret the p-value in this context. drawing a picture may be helpful. (d) what is the conclusion of the hypothesis test? (e) if you were to construct a 90% confidence interval that corresponded to this hypothesis test, would you expect 8 hours to be in the interval? 5.8 heights of adults. researchers studying anthropometry collected body girth measurements and skeletal diameter measurements, as well as age, weight, height and gender, for 507 physically active individuals. the histogram below shows the sample distribution of heights in centimeters.24 height150 160 170 180 190 200020406080100 min 147.2 q1 163.8 median 170.3 mean 171.1 sd 9.4 q3 177.8 max 198.1 (a) what is the point estimate for the average height of active individuals? what about the median? (b) what is the point estimate for the standard deviation of the heights of active individuals? what about the iqr? (c) is a person who is 1m 80cm (180 cm) tall considered unusually tall? and is a person who is 1m 55cm (155cm) considered unusually short? explain your reasoning. (d) the researchers take another random sample of physically active individuals. would you expect the mean and the standard deviation of this new sample to be the ones given above? explain your reasoning. (e) the sample means obtained are point estimates for the mean height of all active individuals, if the sample of individuals is equivalent to a simple random sample. what measure do we use to quantify the variability of such an estimate? compute this quantity using the data from the original sample under the condition that the data are a simple random sample. 5.9 find the mean. you are given the following hypotheses: h0:\u0016= 60 ha:\u0016<60 we know that the sample standard deviation is 8 and the sample size is 20. for what sample mean would the p-value be equal to 0.05? assume that all conditions necessary for inference are satisfied. 24g. heinz et al. “exploring relationships in body dimensions”. in: journal of statistics education 11.2 (2003). 276 chapter 5. inference for numerical data 5.10ttt?vs.zzz?.for a given confidence level, t? dfis larger than z?. explain how t\u0003 dfbeing slightly larger than z\u0003affects the width of the confidence interval. 5.11 play the piano. georgianna claims that in a small city renowned for its music school, the average child takes less than 5 years of piano lessons. we have a random sample of 20 children from the city, with a mean of 4.6 years of piano lessons and a standard deviation of 2.2 years. (a) evaluate georgianna’s claim using a hypothesis test. (b) construct a 95% confidence interval for the number of years students in this city take piano lessons, and interpret it in context of the data. (c) do your results from the hypothesis test and the confidence interval agree? explain your reasoning. 5.12 auto exhaust and lead exposure. researchers interested in lead exposure due to car exhaust sampled the blood of 52 police o fficers subjected to constant inhalation of automobile exhaust fumes while working traffic enforcement in a primarily urban environment. the blood samples of these o fficers had an average lead concentration of 124.32 \u0016g/l and a sd of 37.74 \u0016g/l; a previous study of individuals from a nearby suburb, with no history of exposure, found an average blood level concentration of 35 \u0016g/l.25 (a) write down the hypotheses that would be appropriate for testing if the police o fficers appear to have been exposed to a higher concentration of lead. (b) explicitly state and check all conditions necessary for inference on these data. (c) test the hypothesis that the downtown police o fficers have a higher lead exposure than the group in the previous study. interpret your results in context. (d) based on your preceding result, without performing a calculation, would a 99% confidence interval for the average blood concentration level of police o fficers contain 35 \u0016g/l? (e) based on your preceding result, without performing a calculation, would a 99% confidence interval for this difference contain 0? explain why or why not. 5.13 car insurance savings. a market researcher wants to evaluate car insurance savings at a competing company. based on past studies he is assuming that the standard deviation of savings is $100. he wants to collect data such that he can get a margin of error of no more than $10 at a 95% confidence level. how large of a sample should he collect? 25wi mortada et al. “study of lead exposure from automobile exhaust as a risk for nephrotoxicity among tra ffic policemen.” in: american journal of nephrology 21.4 (2000), pp. 274–279. 5.7. exercises 277 5.7.2 two-sample test for paired data 5.14 air quality. air quality measurements were collected in a random sample of 25 country capitals in 2013, and then again in the same cities in 2014. we would like to use these data to compare average air quality between the two years. should we use a paired or non-paired test? explain your reasoning. 5.15 paired or not, part i. in each of the following scenarios, determine if the data are paired. (a) compare pre- (beginning of semester) and post-test (end of semester) scores of students. (b) assess gender-related salary gap by comparing salaries of randomly sampled men and women. (c) compare artery thicknesses at the beginning of a study and after 2 years of taking vitamin e for the same group of patients. (d) assess e ffectiveness of a diet regimen by comparing the before and after weights of subjects. 5.16 paired or not, part ii. in each of the following scenarios, determine if the data are paired. (a) we would like to know if intel’s stock and southwest airlines’ stock have similar rates of return. to find out, we take a random sample of 50 days, and record intel’s and southwest’s stock on those same days. (b) we randomly sample 50 items from target stores and note the price for each. then we visit walmart and collect the price for each of those same 50 items. (c) a school board would like to determine whether there is a di fference in average sat scores for students at one high school versus another high school in the district. to check, they take a simple random sample of 100 students from each high school. 5.17 global warming, part i. let’s consider a limited set of climate data, examining temperature di fferences in 1948 vs 2018. we sampled 197 locations from the national oceanic and atmospheric administration’s (noaa) historical data, where the data was available for both years of interest. we want to know: were there more days with temperatures exceeding 90°f in 2018 or in 1948?26the difference in number of days exceeding 90°f (number of days in 2018 - number of days in 1948) was calculated for each of the 197 locations. the average of these di fferences was 2.9 days with a standard deviation of 17.2 days. we are interested in determining whether these data provide strong evidence that there were more days in 2018 that exceeded 90°f from noaa’s weather stations. (a) is there a relationship between the observations collected in 1948 and 2018? or are the observations in the two groups independent? explain. (b) write hypotheses for this research in symbols and in words. (c) check the conditions required to complete this test. a histogram of the differences is given to the right. (d) calculate the test statistic and find the p-value. (e) use = 0:05 to evaluate the test, and interpret your conclusion in context. (f) what type of error might we have made? explain in context what the error means. (g) based on the results of this hypothesis test, would you expect a confidence interval for the average di fference between the number of days exceeding 90°f from 1948 and 2018 to include 0? explain your reasoning. differences in number of days−60−40−20020406001020304050 −60−40−200204060 26noaa, www.ncdc.noaa.gov/cdo-web/datasets, april 24, 2019. 278 chapter 5. inference for numerical data 5.18 high school and beyond, part i. the national center of education statistics conducted a survey of high school seniors, collecting test data on reading, writing, and several other subjects. here we examine a simple random sample of 200 students from this survey. side-by-side box plots of reading and writing scores as well as a histogram of the di fferences in scores are shown below. yscores read write20406080 differences in scores (read − write)−20 −10 010 20010203040 (a) is there a clear di fference in the average reading and writing scores? (b) are the reading and writing scores of each student independent of each other? (c) the average observed di fference in scores is ̄xread\u0000write =\u00000:545, and the standard deviation of the differences is 8.887 points. do these data provide convincing evidence of a di fference between the average scores on the two exams? conduct a hypothesis test; interpret your conclusions in context. (d) based on the results of this hypothesis test, would you expect a confidence interval for the average di fference between the reading and writing scores to include 0? explain your reasoning. 5.19 global warming, part ii. we considered the change in the number of days exceeding 90°f from 1948 and 2018 at 197 randomly sampled locations from the noaa database in exercise 5.17. the mean and standard deviation of the reported di fferences are 2.9 days and 17.2 days. (a) calculate a 90% confidence interval for the average di fference between number of days exceeding 90°f between 1948 and 2018. we’ve already checked the conditions for you. (b) interpret the interval in context. (c) does the confidence interval provide convincing evidence that there were more days exceeding 90°f in 2018 than in 1948 at noaa stations? explain. 5.20 high school and beyond, part ii. we considered the di fferences between the reading and writing scores of a random sample of 200 students who took the high school and beyond survey in exercise 5.18. the mean and standard deviation of the di fferences are ̄xread\u0000write =\u00000:545 and 8.887 points. (a) calculate a 95% confidence interval for the average di fference between the reading and writing scores of all students. (b) interpret this interval in context. (c) does the confidence interval provide convincing evidence that there is a real di fference in the average scores? explain. 5.7. exercises 279 5.21 gifted children. researchers collected a simple random sample of 36 children who had been identified as gifted in a large city. the following histograms show the distributions of the iq scores of mothers and fathers of these children. also provided are some sample statistics.27 mother's iq100 120 14004812 father's iq110 120 13004812 diff.−20 0 2004812 mother father di ff. mean 118.2 114.8 3.4 sd 6.5 3.5 7.5 n 36 36 36 (a) are the iqs of mothers and the iqs of fathers in this data set related? explain. (b) conduct a hypothesis test to evaluate if the scores are equal on average. make sure to clearly state your hypotheses, check the relevant conditions, and state your conclusion in the context of the data. 5.22 ddt exposure. suppose that you are interested in determining whether exposure to the organochloride ddt, which has been used extensively as an insecticide for many years, is associated with breast cancer in women. as part of a study that investigated this issue, blood was drawn from a sample of women diagnosed with breast cancer over a six-year period and a sample of healthy control subjects matched to the cancer patients on age, menopausal status, and date of blood donation. each woman’s blood level of dde (an important byproduct of ddt in the human body) was measured, and the di fference in levels for each patient and her matched control calculated. a sample of 171 such di fferences has mean d= 2:7 ng/ml and standard deviation sd= 15:9 ng/ml. di fferences were calculated as ddecancer\u0000ddecontrol . (a) test the null hypothesis that the mean blood levels of dde are identical for women with breast cancer and for healthy control subjects. what do you conclude? (b) would you expect a 95% confidence interval for the true di fference in population mean dde levels to contain the value 0? 5.23 blue-green eggshells. it is hypothesized that the blue-green color of the eggshells of many avian species represents an informational signal as to the health of the female that laid the eggs. to investigate this hypothesis, researchers conducted a study in which birds assigned to the treatment group were provided with supplementary food before and during laying; they predict that if eggshell coloration is related to female health at laying, females given supplementary food will lay more intensely blue-green eggs than control females. nests were paired according to when nest construction began, and the study examined 16 nest pairs. (a) the blue-green chroma (bgc) of eggs was measured on the day of laying; bgc refers to the proportion of total reflectance that is in the blue-green region of the spectrum, with a higher value representing a deeper blue-green color. in the food supplemented group, bgc chroma had x= 0:594 ands= 0:010; in the control group, bgc chroma had x= 0:586 ands= 0:009. a paired t-test resulted in t= 2:28 and p= 0:038. interpret the results in the context of the data. (b) in general, healthier birds are also known to lay heavier eggs. egg mass was also measured for both groups. in the food supplemented group, egg mass had x= 1:70 grams and s= 0:11 grams; in the control group, egg mass had x= 0:586 grams and s= 0:009 grams. the test statistic from a paired t-test was 2.64 withp-value 0.019. compute and interpret a 95% confidence interval for \u000e, the population mean difference in egg mass between the groups. 27f.a. graybill and h.k. iyer. regression analysis: concepts and applications . duxbury press, 1994, pp. 511–516. 280 chapter 5. inference for numerical data 5.7.3 two-sample test for independent data 5.24 diamond prices, part i. a diamond’s price is determined by various measures of quality, including carat weight. the price of diamonds increases as carat weight increases. while the di fference between the size of a 0.99 carat diamond and a 1 carat diamond is undetectable to the human eye, the price di fference can be substantial.28 0.99 carats 1 carat mean $ 44.51 $ 56.81 sd $ 13.32 $ 16.13 n 23 23 point price (in dollars) 0.99 carats 1 carat20406080 (a) use the data to assess whether there is a di fference between the average standardized prices of 0.99 and 1 carat diamonds. (b) construct a 95% confidence interval for the average di fference between the standardized prices of 0.99 and 1 carat diamonds. 5.25 friday the 13th, part i. in the early 1990’s, researchers in the uk collected data on tra ffic flow, number of shoppers, and tra ffic accident related emergency room admissions on friday the 13thand the previous friday, friday the 6th. the histograms below show the distribution of number of cars passing by a specific intersection on friday the 6thand friday the 13thfor many such date pairs. also given are some sample statistics, where the di fference is the number of cars on the 6thminus the number of cars on the 13th.29 friday the 6th120000 130000 14000001234 friday the 13th120000 130000 1400000123 difference0 2000 4000012345 6th13thdiff. ̄x128,385 126,550 1,835 s 7,259 7,664 1,176 n 10 10 10 (a) are there any underlying structures in these data that should be considered in an analysis? explain. (b) what are the hypotheses for evaluating whether the number of people out on friday the 6this different than the number out on friday the 13th? (c) check conditions to carry out the hypothesis test from part (b). (d) calculate the test statistic and the p-value. (e) what is the conclusion of the hypothesis test? (f) interpret the p-value in this context. (g) what type of error might have been made in the conclusion of your test? explain. 28h. wickham. ggplot2: elegant graphics for data analysis . springer new york, 2009. 29t.j. scanlon et al. “is friday the 13th bad for your health?” in: bmj 307 (1993), pp. 1584–1586. 5.7. exercises 281 5.26 egg volume. in a study examining 131 collared flycatcher eggs, researchers measured various characteristics in order to study their relationship to egg size (assayed as egg volume, in mm3). these characteristics included nestling sex and survival. a single pair of collared flycatchers generally lays around 6 eggs per breeding season; laying order of the eggs was also recorded. (a) is there evidence at the = 0:10 significance level to suggest that egg size di ffers between male and female chicks? if so, do heavier eggs tend to contain males or females? for male chicks, x= 1619:95,s= 127:54, andn= 80. for female chicks, x= 1584:20,s= 102:51, andn= 48. sex was only recorded for eggs that hatched. (b) construct a 95% confidence interval for the di fference in egg size between chicks that successfully fledged (developed capacity to fly) and chicks that died in the nest. from the interval, is there evidence of a size difference in eggs between these two groups? for chicks that fledged, x= 1605:87,s= 126:32, andn= 89. for chicks that died in the nest, x= 1606:91,s= 103:46,n= 42. (c) are eggs that are laid first a significantly di fferent size compared to eggs that are laid sixth? for eggs laid first,x= 1581:98,s= 155:95, andn= 22. for eggs laid sixth, x= 1659:62,s= 124:59, andn= 20. 5.27 friday the 13th, part ii. the friday the 13thstudy reported in exercise 5.25 also provides data on traffic accident related emergency room admissions. the distributions of these counts from friday the 6thand friday the 13thare shown below for six such paired dates along with summary statistics. you may assume that conditions for inference are met. friday the 6th5 10012 friday the 13th5 10012 difference−5 0012 6th13thdiff mean 7.5 10.83 -3.33 sd 3.33 3.6 3.01 n 6 6 6 (a) conduct a hypothesis test to evaluate if there is a di fference between the average numbers of tra ffic accident related emergency room admissions between friday the 6thand friday the 13th. (b) calculate a 95% confidence interval for the di fference between the average numbers of tra ffic accident related emergency room admissions between friday the 6thand friday the 13th. (c) the conclusion of the original study states, “friday 13th is unlucky for some. the risk of hospital admission as a result of a transport accident may be increased by as much as 52%. staying at home is recommended.” do you agree with this statement? explain your reasoning. 282 chapter 5. inference for numerical data 5.28 avian influenza, part i. in recent years, widespread outbreaks of avian influenza have posed a global threat to both poultry production and human health. one strategy being explored by researchers involves developing chickens that are genetically resistant to infection. in 2011, a team of investigators reported in science that they had successfully generated transgenic chickens that are resistant to the virus. as a part of assessing whether the genetic modification might be hazardous to the health of the chicks, hatch weights between transgenic chicks and non-transgenic chicks were collected. does the following data suggest that there is a di fference in hatch weights between transgenic and non-transgenic chickens? transgenic chicks (g) non-transgenic chicks (g) ̄x 45.14 44.99 s 3.32 4.57 n 54 54 5.29 chicken diet and weight, part i. chicken farming is a multi-billion dollar industry, and any methods that increase the growth rate of young chicks can reduce consumer costs while increasing company profits, possibly by millions of dollars. an experiment was conducted to measure and compare the e ffectiveness of various feed supplements on the growth rate of chickens. newly hatched chicks were randomly allocated into six groups, and each group was given a di fferent feed supplement. below are some summary statistics from this data set along with box plots showing the distribution of weights by feed type.30 weight (in grams) casein horsebean linseed meatmeal soybean sunflower100150200250300350400● ● ● mean sd n casein 323.58 64.43 12 horsebean 160.20 38.63 10 linseed 218.75 52.24 12 meatmeal 276.91 64.90 11 soybean 246.43 54.13 14 sunflower 328.92 48.84 12 (a) describe the distributions of weights of chickens that were fed linseed and horsebean. (b) do these data provide strong evidence that the average weights of chickens that were fed linseed and horsebean are di fferent? use a 5% significance level. (c) what type of error might we have committed? explain. (d) would your conclusion change if we used = 0:01? 5.30 fuel efficiency of manual and automatic cars, part i. each year the us environmental protection agency (epa) releases fuel economy data on cars manufactured in that year. below are summary statistics on fuel efficiency (in miles/gallon) from random samples of cars with manual and automatic transmissions manufactured in 2012. do these data provide strong evidence of a di fference between the average fuel e fficiency of cars with manual and automatic transmissions in terms of their average city mileage? assume that conditions for inference are satisfied.31 city mpg automatic manual mean 16.12 19.85 sd 3.58 4.51 n 26 26 city mpgautomatic manual152535 30chicken weights by feed type, from the datasets package in r.. 31u.s. department of energy, fuel economy data, 2012 datafile. 5.7. exercises 283 5.31 chicken diet and weight, part ii. casein is a common weight gain supplement for humans. does it have an e ffect on chickens? using data provided in exercise 5.29, test the hypothesis that the average weight of chickens that were fed casein is di fferent than the average weight of chickens that were fed soybean. if your hypothesis test yields a statistically significant result, discuss whether or not the higher average weight of chickens can be attributed to the casein diet. assume that conditions for inference are satisfied. 5.32 fuel efficiency of manual and automatic cars, part ii. the table provides summary statistics on highway fuel economy of cars manufactured in 2012 (from exercise 5.30). use these statistics to calculate a 98% confidence interval for the di fference between average highway mileage of manual and automatic cars, and interpret this interval in the context of the data.32 hwy mpg automatic manual mean 22.92 27.88 sd 5.29 5.01 n 26 26 hwy mpgautomatic manual152535 5.33 gaming and distracted eating. a group of researchers are interested in the possible e ffects of distracting stimuli during eating, such as an increase or decrease in the amount of food consumption. to test this hypothesis, they monitored food intake for a group of 44 patients who were randomized into two equal groups. the treatment group ate lunch while playing solitaire, and the control group ate lunch without any added distractions. patients in the treatment group ate 52.1 grams of biscuits, with a standard deviation of 45.1 grams, and patients in the control group ate 27.1 grams of biscuits, with a standard deviation of 26.4 grams. do these data provide convincing evidence that the average food intake (measured in amount of biscuits consumed) is di fferent for the patients in the treatment group? assume that conditions for inference are satisfied.33 5.34 placebos without deception. while placebo treatment can influence subjective symptoms, it is typically believed that patient response to placebo requires concealment or deception; in other words, a patient must believe that they are receiving an e ffective treatment in order to experience the benefits of being treated with an inert substance. researchers recruited patients su ffering from irritable bowel syndrome (ibs) to test whether placebo responses are neutralized by awareness that the treatment is a placebo. patients were randomly assigned to either the treatment arm or control arm. those in the treatment arm were given placebo pills, which were described as \"something like sugar pills, which have been shown in rigorous clinical testing to produce significant mind-body self-healing processes\". those in the control arm did not receive treatment. at the end of the study, all participants answered a questionnaire called the ibs global improvement scale (ibs-gis) which measures whether ibs symptoms have improved; higher scores are indicative of more improvement. at the end of the study, the 37 participants in the open placebo group had ibs-gis scores with x= 5:0 ands= 1:5, while the 43 participants in the no treatment group had ibs-gis scores with x= 3:9 ands= 1:3. based on an analysis of the data, summarize whether the study demonstrates evidence that placebos administered without deception may be an e ffective treatment for ibs. 32u.s. department of energy, fuel economy data, 2012 datafile. 33r.e. oldham-cooper et al. “playing a computer game during lunch a ffects fullness, memory for lunch, and later snack intake”. in: the american journal of clinical nutrition 93.2 (2011), p. 308. 284 chapter 5. inference for numerical data 5.35 prison isolation experiment, part i. subjects from central prison in raleigh, nc, volunteered for an experiment involving an “isolation” experience. the goal of the experiment was to find a treatment that reduces subjects’ psychopathic deviant t scores. this score measures a person’s need for control or their rebellion against control, and it is part of a commonly used mental health test called the minnesota multiphasic personality inventory (mmpi) test. the experiment had three treatment groups: (1) four hours of sensory restriction plus a 15 minute “therapeutic\" tape advising that professional help is available. (2) four hours of sensory restriction plus a 15 minute “emotionally neutral” tape on training hunting dogs. (3) four hours of sensory restriction but no taped message. forty-two subjects were randomly assigned to these treatment groups, and an mmpi test was administered before and after the treatment. distributions of the di fferences between pre and post treatment scores (pre - post) are shown below, along with some sample statistics. use this information to independently test the effectiveness of each treatment. make sure to clearly state your hypotheses, check conditions, and interpret results in the context of the data.34 treatment 10 20 400246 treatment 2−20 −10 0 10 20024 treatment 3−20 −10 0024 tr 1 tr 2 tr 3 mean 6.21 2.86 -3.21 sd 12.3 7.94 8.57 n 14 14 14 5.7.4 power calculations for a difference of means 5.36 email outreach efforts. a medical research group is recruiting people to complete short surveys about their medical history. for example, one survey asks for information on a person’s family history in regards to cancer. another survey asks about what topics were discussed during the person’s last visit to a hospital. so far, as people sign up, they complete an average of just 4 surveys, and the standard deviation of the number of surveys is about 2.2. the research group wants to try a new interface that they think will encourage new enrollees to complete more surveys, where they will randomize each enrollee to either get the new interface or the current interface. how many new enrollees do they need for each interface to detect an e ffect size of 0.5 surveys per enrollee, if the desired power level is 80%? 5.37 increasing corn yield. a large farm wants to try out a new type of fertilizer to evaluate whether it will improve the farm’s corn production. the land is broken into plots that produce an average of 1,215 pounds of corn with a standard deviation of 94 pounds per plot. the owner is interested in detecting any average difference of at least 40 pounds per plot. how many plots of land would be needed for the experiment if the desired power level is 90%? assume each plot of land gets treated with either the current fertilizer or the new fertilizer. 34prison isolation experiment, stat.duke.edu/resources/datasets/prison-isolation. 5.7. exercises 285 5.7.5 comparing means with anova 5.38 fill in the blank. when doing an anov a, you observe large di fferences in means between groups. within the anov a framework, this would most likely be interpreted as evidence strongly favoring the hypothesis. 5.39 chicken diet and weight, part iii. in exercises 5.29 and 5.31 we compared the e ffects of two types of feed at a time. a better analysis would first consider all feed types at once: casein, horsebean, linseed, meat meal, soybean, and sunflower. the anov a output below can be used to test for di fferences between the average weights of chicks on di fferent diets. df sum sq mean sq f value pr( >f) feed 5 231,129.16 46,225.83 15.36 0.0000 residuals 65 195,556.02 3,008.55 conduct a hypothesis test to determine if these data provide convincing evidence that the average weight of chicks varies across some (or all) groups. make sure to check relevant conditions. figures and summary statistics are shown below. weight (in grams) caseinhorsebean linseed meatmeal soybean sunflower100150200250300350400● ● ● mean sd n casein 323.58 64.43 12 horsebean 160.20 38.63 10 linseed 218.75 52.24 12 meatmeal 276.91 64.90 11 soybean 246.43 54.13 14 sunflower 328.92 48.84 12 5.40 teaching descriptive statistics. a study compared five di fferent methods for teaching descriptive statistics. the five methods were traditional lecture and discussion, programmed textbook instruction, programmed text with lectures, computer instruction, and computer instruction with lectures. 45 students were randomly assigned, 9 to each method. after completing the course, students took a 1-hour exam. (a) what are the hypotheses for evaluating if the average test scores are di fferent for the di fferent teaching methods? (b) what are the degrees of freedom associated with the f-test for evaluating these hypotheses? (c) suppose the p-value for this test is 0.0168. what is the conclusion? 286 chapter 5. inference for numerical data 5.41 coffee, depression, and physical activity. caffeine is the world’s most widely used stimulant, with approximately 80% consumed in the form of co ffee. participants in a study investigating the relationship between co ffee consumption and exercise were asked to report the number of hours they spent per week on moderate (e.g., brisk walking) and vigorous (e.g., strenuous sports and jogging) exercise. based on these data the researchers estimated the total hours of metabolic equivalent tasks (met) per week, a value always greater than 0. the table below gives summary statistics of met for women in this study based on the amount of coffee consumed.35 caffeinated co ffee consumption \u00141 cup/week 2-6 cups/week 1 cup/day 2-3 cups/day \u00154 cups/day total mean 18.7 19.6 19.3 18.9 17.5 sd 21.1 25.5 22.5 22.0 22.0 n 12,215 6,617 17,234 12,290 2,383 50,739 (a) write the hypotheses for evaluating if the average physical activity level varies among the di fferent levels of coffee consumption. (b) check conditions and describe any assumptions you must make to proceed with the test. (c) below is part of the output associated with this test. fill in the empty cells. df sum sq mean sq f value pr( >f) coffee xxxxx xxxxx xxxxx xxxxx 0.0003 residuals xxxxx 25,564,819 xxxxx total xxxxx 25,575,327 (d) what is the conclusion of the test? 5.42 student performance across discussion sections. a professor who teaches a large introductory statistics class (197 students) with eight discussion sections would like to test if student performance di ffers by discussion section, where each discussion section has a di fferent teaching assistant. the summary table below shows the average final exam score for each discussion section as well as the standard deviation of scores and the number of students in each section. sec 1 sec 2 sec 3 sec 4 sec 5 sec 6 sec 7 sec 8 ni 33 19 10 29 33 10 32 31 ̄xi92.94 91.11 91.80 92.45 89.30 88.30 90.12 93.35 si 4.21 5.58 3.43 5.92 9.32 7.27 6.93 4.57 the anov a output below can be used to test for di fferences between the average scores from the di fferent discussion sections. df sum sq mean sq f value pr( >f) section 7 525.01 75.00 1.87 0.0767 residuals 189 7584.11 40.13 conduct a hypothesis test to determine if these data provide convincing evidence that the average score varies across some (or all) groups. check conditions and describe any assumptions you must make to proceed with the test. 35m. lucas et al. “co ffee, caffeine, and risk of depression among women”. in: archives of internal medicine 171.17 (2011), p. 1571. 5.7. exercises 287 5.43 gpa and major. undergraduate students taking an introductory statistics course at duke university conducted a survey about gpa and major. the side-by-side box plots show the distribution of gpa among three groups of majors. also provided is the anov a output. gpa ● arts and humanities natural sciences social sciences2.73.03.33.63.9 df sum sq mean sq f value pr( >f) major 2 0.03 0.015 0.185 0.8313 residuals 195 15.77 0.081 (a) write the hypotheses for testing for a di fference between average gpa across majors. (b) what is the conclusion of the hypothesis test? (c) how many students answered these questions on the survey, i.e. what is the sample size? 5.44 work hours and education. the general social survey collects data on demographics, education, and work, among many other characteristics of us residents.36using anov a, we can consider educational attainment levels for all 1,172 respondents at once. below are the distributions of hours worked by educational attainment and relevant summary statistics that will be helpful in carrying out this analysis. educational attainment less than hs hs jr coll bachelor’s graduate total mean 38.67 39.6 41.39 42.55 40.85 40.45 sd 15.81 14.97 18.1 13.62 15.51 15.17 n 121 546 97 253 155 1,172 hours worked per week less than hs hs jr coll bachelor's graduate020406080 (a) write hypotheses for evaluating whether the average number of hours worked varies across the five groups. (b) check conditions and describe any assumptions you must make to proceed with the test. (c) below is part of the output associated with this test. fill in the empty cells. df sum sq mean sq f value pr( >f) degree xxxxx xxxxx 501.54 xxxxx 0.0682 residuals xxxxx 267,382 xxxxx total xxxxx xxxxx (d) what is the conclusion of the test? 36national opinion research center, general social survey, 2010. 288 chapter 5. inference for numerical data 5.45 true / false: anova, part i. determine if the following statements are true or false in anov a, and explain your reasoning for statements you identify as false. (a) as the number of groups increases, the modified significance level for pairwise tests increases as well. (b) as the total sample size increases, the degrees of freedom for the residuals increases as well. (c) the constant variance condition can be somewhat relaxed when the sample sizes are relatively consistent across groups. (d) the independence assumption can be relaxed when the total sample size is large. 5.46 child care hours. the china health and nutrition survey aims to examine the e ffects of the health, nutrition, and family planning policies and programs implemented by national and local governments.37 it, for example, collects information on number of hours chinese parents spend taking care of their children under age 6. the side-by-side box plots below show the distribution of this variable by educational attainment of the parent. also provided below is the anov a output for comparing average hours across educational attainment categories. child care hours primary school lower middle school upper middle school technical or vocational college050100150 df sum sq mean sq f value pr( >f) education 4 4142.09 1035.52 1.26 0.2846 residuals 794 653047.83 822.48 (a) write the hypotheses for testing for a di fference between the average number of hours spent on child care across educational attainment levels. (b) what is the conclusion of the hypothesis test? 37unc carolina population center, china health and nutrition survey, 2006. 5.7. exercises 289 5.47 prison isolation experiment, part ii. exercise 5.35 introduced an experiment that was conducted with the goal of identifying a treatment that reduces subjects’ psychopathic deviant t scores, where this score measures a person’s need for control or his rebellion against control. in exercise 5.35 you evaluated the success of each treatment individually. an alternative analysis involves comparing the success of treatments. the relevant anov a output is given below. df sum sq mean sq f value pr( >f) treatment 2 639.48 319.74 3.33 0.0461 residuals 39 3740.43 95.91 spooled = 9:793 ondf= 39 (a) what are the hypotheses? (b) what is the conclusion of the test? use a 5% significance level. (c) if in part (b) you determined that the test is significant, conduct pairwise tests to determine which groups are different from each other. if you did not reject the null hypothesis in part (b), recheck your answer. 5.48 true / false: anova, part ii. determine if the following statements are true or false, and explain your reasoning for statements you identify as false. if the null hypothesis that the means of four groups are all the same is rejected using anov a at a 5% significance level, then ... (a) we can then conclude that all the means are di fferent from one another. (b) the standardized variability between groups is higher than the standardized variability within groups. (c) the pairwise analysis will identify at least one pair of means that are significantly di fferent. (d) the appropriate to be used in pairwise comparisons is 0.05 / 4 = 0.0125 since there are four groups. 290 chapter 6 simple linear regression 6.1 examining scatterplots 6.2 estimating a regression line using least squares 6.3 interpreting a linear model 6.4 statistical inference with regression 6.5 interval estimates with regression 6.6 notes 6.7 exercises 291 the relationship between two numerical variables can be visualized using a scatterplot in the xy-plane. the predictor orexplanatory variable is plotted on the horizontal axis, while the response variable is plotted on the vertical axis.1 this chapter explores simple linear regression, a technique for estimating a straight line that best fits data on a scatterplot.2a line of best fit functions as a linear model that can not only be used for prediction, but also for inference. linear regression should only be used with data that exhibit linear or approximately linear relationships. for example, scatterplots in chapter 1 illustrated the linear relationship between height and weight in the nhanes data, with height as a predictor of weight. adding a best-fitting line to these data using regression techniques would allow for prediction of an individual’s weight based on their height. the linear model could also be used to investigate questions about the population-level relationship between height and weight, since the data are a random sample from the population of adults in the united states. not all relationships in data are linear. for example, the scatterplot in figure 1.28 of chapter 1 shows a highly non-linear relationship between between annual per capita income and life expectancy for 165 countries in 2011. relationships are called strong relationships if the pattern of the dependence between the predictor and response variables is clear, even if it is nonlinear as in figure 1.28. aweak relationship is one in which the points in the scatterplot are so di ffuse as to make it di fficult to discern any relationship. figure 1.29 in chapter 1 showed relationships progressing from weak to strong moving from left to right in the top and bottom panels. each of the relationships shown in the second panels from the left are moderate relationships . finally, changing the scale of measurement of one or both variables, such as changing age from age in years to age in months, simply stretches or compresses one or both axes and does not change the nature of the relationship. if a relationship is linear it will remain so, and with a simple change of scale, a nonlinear relationship will remain nonlinear. 1sometimes, the predictor variable is referred to as the independent variable, and the response variable referred to as the dependent variable. 2although the response variable in linear regression is necessarily numerical, the predictor variable can be numerical or categorical. 292 the next chapter covers multiple regression, a statistical model used to estimate the relationship between a single numerical response variable and several predictor variables. for labs, slides, and other resources, please visit www.openintro.org/book/biostat 6.1. examining scatterplots 293 6.1 examining scatterplots various demographic and cardiovascular risk factors were collected as a part of the prevention of renal and vascular end-stage disease (prevend) study, which took place in the netherlands. the initial study population began as 8,592 participants aged 28-75 years who took a first survey in 1997-1998.3participants were followed over time; 6,894 participants took a second survey in 2001-2003, and 5,862 completed the third survey in 2003-2006. in the third survey, measurement of cognitive function was added to the study protocol. data from 4,095 individuals who completed cognitive testing are in the prevend dataset, available in the rpackage oibiostat . as adults age, cognitive function changes over time, largely due to various cerebrovascular and neurodegenerative changes. it is thought that cognitive decline is a long-term process that may start as early as 45 years of age.4the ru fffigural fluency test (rfft) is one measure of cognitive function that provides information about cognitive abilities such as planning and the ability to switch between di fferent tasks. the test consists of drawing as many unique designs as possible from a pattern of dots, under timed conditions; scores range from 0 to 175 points (worst and best score, respectively). rfft scores for a random sample of 500 individuals are shown in figure 6.1, plotted against age at enrollment, which is measured in years. the variables ageand rfft are negatively associated; older participants tend to have lower cognitive function. there is an approximately linear trend observable in the data, which suggests that adding a line could be useful for summarizing the relationship between the two variables. it is important to avoid adding straight lines to non-linear data, such as in figure 1.28. 40 50 60 70 8020406080100120140rfft score●●● ● ● ●●●●● ● ●●● ●● ●●● ●●● ● ●● ●● ●● ●●●●●● ●●● ●●● ●● ●● ●● ●●●● ●●● ●●● ● ●● ●● ● ● ●● ●●● ● ●● ●● ●● ●●● ● ● ●● ● ●● ●●● ● ●● ● ●● ●●● ● ●●●● ●● ●●● ●●●● ●●● ●●● ● ●● ●● ●● ●● ● ● ●● ●● ●●● ●● ●●●●● ● ●● ● ● ●●● ●● ●● ●●●●● ● ●● ● ● ●●●● ●● ●● ●● ● ● ●● ● ● ● ●● ● ●● ● ●●●●● ● ●● ●● ●● ●● ●● ● ● ●●●● ●●● ● ● ●● ● ●● ●●● ●●●● ● ●●● ● ●●● ●● ● ●●● ● ● ●● ●●●● ●●●● ●●● ●● ●●●● ● ●● ●●● ● ●● ●● ●● ●● ● ●● ●● ●● ●● ●●●●● ●● ● ●●● ●● ●●● ●● ● ● ●●●● ● ●● ●● ● ●● ● ● ● ●● ●●●● ●● ● ●● ● ●● ●● ● ●● ● ●● ●● ●●● ●●●● ● ●● ●● ●● ●●● ●●● ●●● ●● ●● ● ●●●● ●● ●● ●● ● ●●●●● ●● ●● ● ●● ●● ●● ● ●●●● ●●● ●● ● ●● ●● ● ● ●●●● ●● ●●● ● ●●● ● ●●● ● ●● ●● ● ●● ●●●● ●●● ●● ● ● ●●● ●● ●●● ● ●● ●● ●●● ●● ●● ● ●● ●● ●● ●● ● ●● ● ●● ●● ●●●● ●● ● ●●● age (yrs) figure 6.1: a scatterplot showing agevs.rfft . age is the predictor variable, while rfft score is the response variable. 3participants were selected from the city of groningen on the basis of their urinary albumin excretion; urinary albumin excretion is known to be associated with abnormalities in renal function. 4joosten h, et al. cardiovascular risk profile and cognitive function in young, middle-aged, and elderly subjects. stroke. 2013;44:1543-1549, https://doi.org/10.1161/strokeaha.111.000496 294 chapter 6. simple linear regression the following conditions should be true in a scatterplot for a line to be considered a reasonable approximation to the relationship in the plot and for the application of the methods of inference discussed later in the chapter: 1 linearity. the data shows a linear trend. if there is a nonlinear trend, an advanced regression method should be applied; such methods are not covered in this text. occasionally, a transformation of the data will uncover a linear relationship in the transformed scale. 2 constant variability. the variability of the response variable about the line remains roughly constant as the predictor variable changes. 3 independent observations. the (x;y) pairs are independent; i.e., the value of one pair provides no information about other pairs. be cautious about applying regression to sequential observations in time ( time series data), such as height measurements taken over the course of several years. time series data may have a complex underlying structure, and the relationship between the observations should be accounted for in a model. 4 residuals that are approximately normally distributed. this condition can be checked only after a line has been fit to the data and will be explained in section 6.3.1, where the term residual is defined. in large datasets, it is su fficient for the residuals to be approximately symmetric with only a few outliers. this condition becomes particularly important when inferences are made about the line, as discussed in section 6.4. guided practice 6.1 figure 6.2 shows the relationship between clutch.volume and body.size in the frog data. the plot also appears as figure 1.26 in chapter 1. are the first three conditions met for linear regression?5 4.0 4.5 5.0 5.5 6.05001000150020002500clutch volume (mm3) ●●●●●●●●●●● ●●●●● ●● ● ●● ●● ●●●●●●●●● ●●● ●● ●●● ●● ●●●● ●●● ●● ●● ● ●●● ●●● ●●●●● ●●●●●● ●● ●●●●● ●●● ●●● ●●●● ●● ●● ●● ●● ● ●●●●● ●●●● ●●● ● ●●● ●●● ●●● ●●●● ●● ●●● ● female body size (cm) figure 6.2: a plot of clutch.volume versus body.size in the frog data. 5no. while the relationship appears linear and it is reasonable to assume the observations are independent (based on information about the frogs given in chapter 1), the variability in clutch.volume is noticeably less for smaller values of body.size than for larger values. 6.2. estimating a regression line using least squares 295 6.2 estimating a regression line using least squares figure 6.3 shows the scatterplot of age versus rfft score, with the least squares regression line added to the plot; this line can also be referred to as a linear model for the data. an rfft score can be predicted for a given age from the equation of the regression line: rfft = 137:55\u00001:26(age): the vertical distance between a point in the scatterplot and the predicted value on the regression line is the residual for the observation represented by the point; observations below the line have negative residuals, while observations above the line have positive residuals. the size of a residual is usually discussed in terms of its absolute value; for example, a residual of \u000013 is considered larger than a residual of 5. for example, consider the predicted rfft score for an individual of age 56. according to the linear model, this individual has a predicted score of 137 :550\u00001:261(56) = 66 :934 points. in the data, however, there is a participant of age 56 with an rfft score of 72; their score is about 5 points higher than predicted by the model (this observation is shown on the plot with a “ \u0002”). 40 50 60 70 8020406080100120140 age (yrs)rfft score figure 6.3: a scatterplot showing age(horizontal axis) vs. rfft (vertical axis) with the regression line added to the plot. three observations are marked in the figure; the one marked by a “+” has a large residual of about +38, the one marked by a “\u0002” has a small residual of about +5, and the one marked by a “ 4” has a moderate residual of about -13. the vertical dotted lines extending from the observations to the regression line represent the residuals. 296 chapter 6. simple linear regression residual: difference between observed and expected the residual of the ithobservation ( xi;yi) is the di fference of the observed response ( yi) and the response predicted based on the model fit ( byi): ei=yi\u0000byi the value byiis calculated by plugging xiinto the model equation. the least squares regression line is the line which minimizes the sum of the squared residuals for all the points in the plot. let ˆyibe the predicted value for an observation with value xifor the explanatory variable. the value ei=yi\u0000ˆyiis the residual for a data point ( xi;yi) in a scatterplot withnpairs of points. the least squares line is the line for which e2 1+e2 2+\u0001\u0001\u0001+e2 n (6.2) is smallest. for a general population of ordered pairs ( x;y), the population regression model is y= 0+ 1x+\": the term\"is a normally distributed ‘error term’ that has mean 0 and standard deviation \u001b. sincee(\") = 0, the model can also be written e(yjx) = 0+ 1x; where the notation e(yjx) denotes the expected value of ywhen the predictor variable has value x.6for the prevend data, the population regression line can be written as rfft = 0+ 1(age) +\";or ase(rfftjage) = 0+ 1(age): the term 0is the vertical intercept for the line (often referred to simply as the intercept) and 1is the slope. the notation b0andb1are used to represent the point estimates of the parameters 0and 1. the point estimates b0andb1are estimated from data; 0and 1are parameters from the population model for the regression line. b0;b1 sample estimates of 0, 1the regression line can be written as ˆy=b0+b1(x), where ˆyrepresents the predicted value of the response variable. the slope of the least squares line, b1, is estimated by b1=sy sxr; (6.3) whereris the correlation between the two variables, and sxandsyare the sample standard deviations of the explanatory and response variables, respectively. the intercept for the regression line is estimated by b0=y\u0000b1x: (6.4) typically, regression lines are estimated using statistical software. 6the error term \"can be thought of as a population parameter for the residuals ( e). while\"is a theoretical quantity that refers to the deviation between an observed value and e(yjx), a residual is calculated as the deviation between an observed value and the prediction from the linear model. 6.2. estimating a regression line using least squares 297 example 6.5 from the summary statistics displayed in figure 6.4 for prevend.samp , calculate the equation of the least-squares regression line for the prevend data. b1=sy sxr=27:40 11:60(\u00000:534) =\u00001:26 b0=y\u0000b1x= 68:40\u0000(\u00001:26)(54:82) = 137:55: the results agree with the equation shown at the beginning of this section: rfft = 137:55\u00001:26(age): age (yrs) rfft score mean x= 54:82y= 68:40 standard deviation sx= 11:60sy= 27:40 r=\u00000:534 figure 6.4: summary statistics for ageand rfft from prevend.samp . guided practice 6.6 figure 6.5 shows the relationship between height and weight in a sample from the nhanes dataset introduced in chapter 1. calculate the equation of the regression line given the summary statistics:x= 168:78;y= 83:83;sx= 10:29;sy= 21:04;r= 0:410.7 150 160 170 180 19050100150200weight (kg) ● ●●● ●● ●● ● ● ●● ●● ●● ●● ●● ●●● ● ●● ●●●● ●●● ● ●● ●● ●● ●●●●● ●● ●●● ●● ●●● ● ●●● ●● ●●●● ● ●●● ● ●●● ●● ●●● ● ●● ●● ●●● ● ● ● ●●● ●●● ●●● ●●● ●●●●●●● ●● ●● ● ●●●● ● ●●● ● ●●●● ● ●● ●● ● ● ●● ●● ●● ●● ●● ● ●● ● ●●● ●●●●● ● ●●● ● ●●●● ●● ●● ●●● ●● ● ●●● ● ● ●●● ● ●● ●●● ● ●● ●●● ● ● ● ●● ●● ● ●● ●●●● ●●●● ●● ●● ● ● ●● ●●● ● ●●●●● ●●● ● ●●● ●● ●● ●●● ●● ●●● ●●● ●●● ●● ●●● ●● ●● ● ●● ● ●●●● ●●● ●●●● ●● ●●●● ●●● ●● ● ● ●●● ● ●●● ●●● ●● ●● ●● ●● ● ●●● ●● ● ●● ●● ●●●●●● ●●● ● ●● ●● ●●●● ●● ●● ●● ●● ●● ● ● ●● ●● ● ●● ●● ●●● ● ●● ●●● ●●● ● ●● ● ●●●●●● ● ●● ●●● ●●● ●● ●● ●●● ●● ●● ● ●● ● ●●● ● ●● ●●●● ●●● ● ●●●● ● ●● ●● ● ●●●●● ●●● ●● ●● ●●●● ●● ● ●● ●●● ●● ● ●● ●● ● ●● ●●● ●●● ● ● ●●● ●●● ●●● ● ●●●● ● ●● ● ●● ●● ●● ●● height (cm) figure 6.5: a plot of height versus weight innhanes.samp.adult.500 , with a leastsquares regression line guided practice 6.7 predict the weight in pounds for an adult who is 5 feet, 11 inches tall. 1 cm = .3937 in; 1 lb = 0.454 kg.8 7the equation of the line is weight =\u000057:738+0:839(height ), where height is in centimeters and weight is in kilograms. 85 feet, 11 inches equals 71 =:3937 = 180:34 centimeters. from the regression equation, the predicted weight is \u000057:738+ 0:839(180:34) = 93:567 kilograms. in pounds, this weight is 93 :567=0:454 = 206:280. 298 chapter 6. simple linear regression 6.3 interpreting a linear model a least squares regression line functions as a statistical model that can be used to estimate the relationship between an explanatory and response variable. while the calculations for constructing a regression line are relatively simple, interpreting the linear model is not always straightforward. in addition to discussing the mathematical interpretation of model parameters, this section also addresses methods for assessing whether a linear model is an appropriate choice, interpreting categorical predictors, and identifying outliers. the slope parameter of the regression line specifies how much the line rises (positive slope) or declines (negative slope) for one unit of change in the explanatory variable. in the prevend data, the line decreases by 1.26 points for every increase of 1 year. however, it is important to clarify that rfft score tends to decrease as age increases, with average rfft score decreasing by 1.26 points for each additional year of age. as visible from the scatter of the data around the line, the line does not perfectly predict rfft score from age; if this were the case, all the data would fall exactly on the line. when interpreting the slope parameter, it is also necessary to avoid phrasing indicative of a causal relationship, since the line describes an association from data collected in an observational study. from these data, it is not possible to conclude that increased age causes a decline in cognitive function.9 mathematically, the intercept on the vertical axis is a predicted value on the line when the explanatory variable has value 0. in biological or medical examples, 0 is rarely a meaningful value of the explanatory variable. for example, in the prevend data, the linear model predicts a score of 137.55 when age is 0—however, it is nonsensical to predict an rfft score for a newborn infant. in fact, least squares lines should never be used to extrapolate values outside the range of observed values. since the prevend data only includes participants between ages 36 and 81, it should not be used to predict rfft scores for people outside that age range. the nature of a relationship may change for very small or very large values of the explanatory variable; for example, if participants between ages 15 and 25 were studied, a di fferent relationship between age and rfft scores might be observed. even making predictions for values of the explanatory variable slightly larger than the minimum or slightly smaller than the maximum can be dangerous, since in many datasets, observations near the minimum or maximum values (of the explanatory variable) are sparse. linear models are useful tools for summarizing a relationship between two variables, but it is important to be cautious about making potentially misleading claims based on a regression line. the following subsection discusses two commonly used approaches for examining whether a linear model can reasonably be applied to a dataset. 6.3.1 checking residuals from a linear model recall that there are four assumptions that must be met for a linear model to be considered reasonable: linearity, constant variability, independent observations, normally distributed residuals. in the prevend data, the relationship between rfft score and age appears approximately linear, and it is reasonable to assume that the data points are independent. to check the assumptions of constant variability around the line and normality of the residuals, it is helpful to consult residual plots and normal probability plots (section 3.3.7).10 9similarly, avoid language such as increased age leads to orproduces lower rfft scores. 10while simple arithmetic can be used to calculate the residuals, the size of most datasets makes hand calculations impractical. the plots here are based on calculations done in r. 6.3. interpreting a linear model 299 examining patterns in residuals there are a variety of residual plots used to check the fit of a least squares line. the plots shown in this text are scatterplots in which the residuals are plotted on the vertical axis against predicted values from the model on the horizontal axis. other residual plots may instead show values of the explanatory variable or the observed response variable on the horizontal axis. when a least squares line fits data very well, the residuals should scatter about the horizontal line y= 0 with no apparent pattern. figure 6.6 shows three residual plots from simulated data; the plots on the left show data plotted with the least squares regression line, and the plots on the right show residuals on the yaxis and predicted values on the x-axis. a linear model is a particularly good fit for the data in the first row, where the residual plot shows random scatter above and below the horizontal line. in the second row, the original data cycles below and above the regression line; this nonlinear pattern is more evident in the residual plot. in the last row, the variability of the residuals is not constant; the residuals are slightly more variable for larger predicted values. x predict(g)g$residuals x predict(g)g$residuals g$residuals figure 6.6: sample data with their best fitting lines (left) and their corresponding residual plots (right). 300 chapter 6. simple linear regression figure 6.7 shows a residual plot from the estimated linear model rfft = 137 :55\u00001:26(age). while the residuals show scatter around the line, there is less variability for lower predicted rfft scores. a data analyst might still decide to use the linear model, with the knowledge that predictions of high rfft scores may not be as accurate as for lower scores. reading a residual plot critically can reveal weaknesses about a linear model that should be taken into account when interpreting model results. more advanced regression methods beyond the scope of this text may be more suitable for these data. 40 50 60 70 80 90−60−40−200204060 predicted rfft scoreresiduals figure 6.7: residual plot for the model in figure 6.3 using prevend.samp . example 6.8 figure 6.8 shows a residual plot for the model predicting weight from height using the sample of 500 adults from the nhanes data, nhanes.samp.adult.500 . assess whether the constant variability assumption holds for the linear model. the residuals above the line are more variable, taking on more extreme values than those below the line. larger than expected residuals imply that there are many large weights that are underpredicted; in other words, the model is less accurate at predicting relatively large weights. 70 80 90 100050100 predicted weightresiduals figure 6.8: a residual plot from the linear model for height versus weight in nhanes.samp.adult.500 . 6.3. interpreting a linear model 301 checking normality of the residuals the normal probability plot, introduced in section 3.3.7, is best suited for checking normality of the residuals, since normality can be di fficult to assess using histograms alone. figure 6.9 shows both the histogram and normal probability plot of the residuals after fitting a least squares regression to the age versus rfft data. residuals−60 −200204060 theoretical quantilessample quantiles −3−2−10123−60−40−200204060 figure 6.9: a histogram and normal probability plot of the residuals from the linear model for rfft versus age in prevend.samp . the normal probability plot shows that the residuals are nearly normally distributed, with only slight deviations from normality in the left and right tails. guided practice 6.9 figure 6.10 shows a histogram and normal probability plot for the linear model to predict weight from height in nhanes.samp.adult.500 . evaluate the normality of the residuals.11 residuals−60−40−200204060●● ●●●● ●●● ● ●● ●● ● ●●● ●●●● ● ● ●● ●●●● ●● ● ● ●● ●●● ● ●●●●● ●● ●●● ●● ●●● ● ●●●●● ●●●● ●●●● ● ●●● ●●●●● ● ● ●●● ●●● ● ●● ●●● ●●● ●●● ●●● ●● ●●●●● ●● ●● ● ●●●● ● ●●●● ●●●● ● ●● ●●● ● ●● ●● ●● ●● ●● ● ●● ● ●●● ●●●●● ● ●●● ● ●● ●● ●● ●● ●●● ●● ●●●● ● ● ●●● ● ●● ●●● ● ●● ●● ● ● ●● ●● ●●● ●● ●●● ● ●●●● ●● ●● ●● ●●● ●● ● ●● ●●● ●●● ● ●●● ●● ●●●●● ●● ● ●● ●●● ●●● ●● ● ●●●● ●● ●●● ● ● ●●●● ●● ●● ●● ●● ● ●●● ● ●● ●●● ● ●●● ● ●●● ●● ● ●●●● ●● ●● ●●●● ●● ● ●● ●● ●● ●●●● ●●● ●●● ●● ●●●● ●● ●● ●●●● ●● ● ●●● ●●● ●● ●● ●●● ● ●●●●● ●●● ●●● ● ●●●● ●● ● ●● ●●● ●● ● ●● ●● ● ●● ●● ●● ● ●● ● ● ●●● ●● ●●● ●●● ● ●● ●●● ● ●● ●● ● ●●●●● ●●●●● ●● ●●●● ●● ● ●● ●● ● ●●● ●● ● ●● ●● ●●●●●● ● ● ●●● ●●● ●●● ● ●●● ●● ●● ●●● ●● ●● ●● theoretical quantilessample quantiles −3−2−10123050100 figure 6.10: a histogram and normal probability plot of the residuals from the linear model for height versus weight in nhanes.samp.adult.500 . 11the data are roughly normal, but there are deviations from normality in the tails, particularly the upper tail. there are some relatively large observations, which is evident from the residual plot shown in figure 6.8. 302 chapter 6. simple linear regression 6.3.2 using r2to describe the strength of a fit the correlation coe fficientrmeasures the strength of the linear relationship between two variables. however, it is more common to measure the strength of a linear fit using r2, which is commonly written as r2in the context of regression.12 the quantity r2describes the amount of variation in the response that is explained by the least squares line. while r2can be easily calculated by simply squaring the correlation coe fficient, it is easier to understand the interpretation of r2by using an alternative formula: r2=variance of predicted y-values variance of observed y-values: it is possible to show that r2can also be written r2=s2 y\u0000s2 residuals s2y: in the linear model predicting rfft scores from age, the predicted values on the least squares line are the values of rfft that are ’explained’ by the linear model. the variability of the residuals about the line represents the remaining variability after the prediction; i.e., the variability unexplained by the model. for example, if a linear model perfectly captured all the data, then the variance of the predicted y-values would be equal to the variance of the observed y-values, resulting in r2= 1. in the linear model for rfft , the proportion of variability explained is r2=s2 rfft\u0000s2 residuals s2 rfft=750:52\u0000536:62 750:52=213:90 750:52= 0:285; about 29%. this is equal to the square of the correlation coe fficient,r2=\u00000:5342= 0:285. sincer2in simple linear regression is simply the square of the correlation coe fficient between the predictor and the response, it does not add a new tool to regression. it becomes much more useful in models with several predictors, where it has the same interpretation as the proportion of variability explained by a model but is no longer the square of any one of the correlation coe fficients between the individual responses and the predictor. those models are discussed in chapter 7. guided practice 6.10 in the nhanes data, the variance of weight is 442:53 kg2and the variance of the residuals is 368.1. what proportion of the variability in the data is explained by the model?13 guided practice 6.11 if a linear model has a very strong negative relationship with a correlation of -0.97, how much of the variation in the response is explained by the explanatory variable?14 12in software output, r2is usually labeled r-squared . 13about 16.8%:s2 weight\u0000s2 residuals s2 weight=442:53\u0000368:1 442:53=74:43 442:53= 0:168 14aboutr2= (\u00000:97)2= 0:94 or 94% of the variation is explained by the linear model. 6.3. interpreting a linear model 303 6.3.3 categorical predictors with two levels although the response variable in linear regression is necessarily numerical, the predictor variable may be either numerical or categorical. this section explores the association between a country’s infant mortality rate and whether or not 50% of the population has access to adequate sanitation facilities. the world development indicators (wdi) is a database of country-level variables (i.e., indicators) recording outcomes for a variety of topics, including economics, health, mortality, fertility, and education.15the dataset wdi.2011 contains a subset of variables on 165 countries from the year 2011.16the infant mortality rate in a country is recorded as the number of deaths in the first year of life per 1,000 live births. access to sanitation is recorded as the percentage of the population with adequate disposal facilities for human waste. due to the availability of death certificates, infant mortality is measured reasonably accurately throughout the world. however, it is more difficult to obtain precise measurements of the percentage of a population with access to adequate sanitation facilities; instead, considering whether half the population has such access may be a more reliable measure. the analysis presented here is based on 163 of the 165 countries; the values for access to sanitation are missing for new zealand and turkmenistan. figure 6.11(a) shows that infant mortality rates are highly right-skewed, with a relatively small number of countries having high infant mortality rates. in 13 countries, infant mortality rates are higher than 70 deaths per thousand live births. figure 6.11(b) shows infant mortality after a log transformation; the following analysis will use the more nearly symmetric transformed version of inf.mortality . infant mortality (deaths/1,000 live births) 0103050709011001020304050 (a) log (infant mortality)1 2 3 4 50102030 (b) figure 6.11: (a) histogram of infant mortality, measured in deaths per 1,000 live births in the first year of life. (b) histogram of the log-transformed infant mortality. 15http://data.worldbank.org/data-catalog/world-development-indicators 16the data were collected by a harvard undergraduate in the statistics department, and are accessible via the oibiostat package. 304 chapter 6. simple linear regression figure 6.12 shows a scatterplot of log( inf.mortality ) against the categorical variable for sanitation access, coded 1if at least 50% of the population has access to adequate sanitation, and 0otherwise. since there are only two values of the predictor, the values of infant mortality are stacked above the two predictor values 0 and 1.17 access to sanitation0 (low)1 (high)1234log(infant mortality) figure 6.12: country-level infant mortality rates, divided into low access ( x= 0) and high access ( x= 1) to sanitation. the least squares regression line is also shown. the least squares regression line has the form log(inf.mortality ) =b0+b1(sanit.access ): (6.12) the estimated least squares regression line has intercept and slope parameters of 4.018 and -1.681, respectively. while the scatterplot appears unlike those for two numerical variables, the interpretation of the parameters remains unchanged. the slope, -1.681, is the estimated change in the logarithm of infant mortality when the categorical predictor changes from low access to sanitation facilities to high access. the intercept term 4.018 is the estimated log infant mortality for the set of countries where less than 50% of the population has access to adequate sanitation facilities ( sanit.access = 0). 17typically, side-by-side boxplots are used to display the relationship between a numerical variable and a categorical variable. in a regression context, it can be useful to use a scatterplot instead, in order to see the variability around the regression line. 6.3. interpreting a linear model 305 using the model in equation 6.12, the prediction equation can be written log(inf.mortality ) = 4:018\u00001:681( sanit.access ): exponentiating both sides of the equation yields inf.mortality =e4:018\u00001:681( sanit.access ): when sanit.access = 0, the equation simplifies to e4:018= 55:590 deaths among 1,000 live births; this is the estimated infant mortality rate in the countries with low access to sanitation facilities. when sanit.access = 1, the estimated infant mortality rate is e4:018\u00001:681(1)=e2:337= 10:350 deaths per 1,000 live births. the infant mortality rate drops by a factor of 0 :186; i.e., the mortality rate in the high access countries is approximately 20% of that in the low access countries.18 example 6.13 check the assumptions of constant variability around the regression line and normality of the residuals in the model for the relationship between the transformed infant mortality variable and access to sanitation variable. residual plots are shown in figure 6.13. while the normal probability plot does show that the residuals are approximately normally distributed, the residual plot reveals that variability is far from constant around the two predictors. another method for assessing the relationship between the two groups is advisable; this is discussed further in section 6.4. 2.5 3.0 3.5 4.0−1012 predicted log(infant mortality)residuals (a) residuals−2−1012 theoretical quantilessample quantiles −2−1012−1012 (b) figure 6.13: (a) residual plot of log(inf.mortality) and sanit.access . (b) histogram and normal probability plot of the residuals. 6.3.4 outliers in regression depending on their position, data points in a scatterplot have varying degrees of contribution to the estimated parameters of a regression line. points that are at particularly low or high values of the predictor ( x) variable are said to have high leverage , and have a large influence on the estimated intercept and slope of the regression line; observations with xvalues closer to the center of the distribution of xdo not have a large e ffect on the slope. 18when examining event rates in public health, associations are typically measured using rate ratios rather than rate differences. 306 chapter 6. simple linear regression a data point in a scatterplot is considered an outlier in regression if its value for the response (y) variable does not follow the general linear trend in the data. outliers that sit at extreme values of the predictor variable (i.e., have high leverage) have the potential to contribute disproportionately to the estimated parameters of a regression line. if an observation does have a strong e ffect on the estimates of the line, such that estimates change substantially when the point is omitted, the observation is influential . these terms are formally defined in advanced regression courses. this section examines the relationship between infant mortality and number of doctors, using data for each state and the district of columbia.19infant mortality is measured as the number of infant deaths in the first year of life per 1,000 live births, and number of doctors is recorded as number of doctors per 100,000 members of the population. figure 6.14 shows scatterplots with infant mortality on the y-axis and number of doctors on the x-axis. one point in figure 6.14(a), marked in red, is clearly distant from the main cluster of points. this point corresponds to the district of columbia, where there were approximately 807.2 doctors per 100,000 members of the population, and the infant mortality rate was 11.3 per 1,000 live births. since 807.2 is a high value for the predictor variable, this observation has high leverage. it is also an outlier; the other points exhibit a downward sloping trend as the number of doctors increases, but this point, with an unusually high y-value paired with a high x-value, does not follow the trend. figure 6.14(b) illustrates that the dc observation is influential. not only does the observation simply change the numerical value of the slope parameter, it reverses the direction of the linear trend; the regression line fitted with the complete dataset has a positive slope, but the line re-fitted without the dc observation has a negative slope. the large number of doctors per population is due to the presence of several large medical centers in an area with a population that is much smaller than a typical state. it seems natural to ask whether or not an influential point should be removed from a dataset, but that may not be the right question. instead, it is usually more important to assess whether the influential point might be an error in the data, or whether it belongs in the dataset. in this case, the district of columbia has certain characteristics that may make comparisons with other states inappropriate; this is one argument in favor of excluding the dc observation from the data. generally speaking, if an influential point arises from random sampling from a large population and is not a data error, it should be left in the dataset, since it probably represents a small subset of the population from which the data were sampled. guided practice 6.14 once the influential dc point is removed, assess whether it is appropriate to use linear regression on these data by checking the four assumptions behind least squares regression: linearity, constant variability, independent observations, and approximate normality of the residuals. refer to the residual plots shown in figure 6.15.20 19data are from the statistical abstract of the united states, published by the us census bureau. data are for 2010, and available as census.2010 in the oibiostat package. 20the scatterplot in figure 6.14(b) does not show any nonlinear trends. similarly, figure 6.15(a) does not indicate any nonlinear trends or noticeable di fference in the variability of the residuals, although it does show that there are relatively few observations for low values of predicted infant mortality. from figure 6.15(b), the residuals are approximately normally distributed. infant mortality across the states reflects a complex mix of di fferent levels of income, access to health care, and individual state initiatives in health care; these and other state-specific features probably act independently across the states, although there is some dependence from federal influence such as funding for pre-natal care. overall, independence seems like a reasonable assumption. 6.3. interpreting a linear model 307 200300400500600700800567891011 doctors (per 100,000 members of pop.)infant mortality (a) 200 250 300 350 400 4505678910 doctors (per 100,000 members of pop.)infant mortality (b) figure 6.14: (a) plot including district of columbia data point. (b) plot without influential district of columbia data point. 5.5 6.0 6.5 7.0 7.5−2−10123 predicted infant mortalityresiduals (a) residuals−4−2 024 theoretical quantilessample quantiles −2−1012−2−10123 (b) figure 6.15: (a) residual plot of inf.mortality and doctors . (b) histogram and normal probability plot of the residuals. 308 chapter 6. simple linear regression 6.4 statistical inference with regression the previous sections in this chapter have focused on linear regression as a tool for summarizing trends in data and making predictions. these numerical summaries are analogous to the methods discussed in chapter 1 for displaying and summarizing data. regression is also used to make inferences about a population. the same ideas covered in chapters 4 and 5 about using data from a sample to draw inferences about population parameters apply with regression. previously, the goal was to draw inference about the population parameter \u0016; in regression, the population parameter of interest is typically the slope parameter 1. inference about the intercept term is rare, and limited to the few problems where the vertical intercept has scientific meaning.21 inference in regression relies on the population linear model for the relationship between an explanatory variable xand a response variable ygiven by y= 0+ 1x+\"; (6.15) where\"is assumed to have a normal distribution with mean 0 and standard deviation \u001b(\"\u0018 n(0;\u001b)). this population model specifies that a response yhas value 0+ 1xplus a random term that pushes ysymmetrically above or below the value specified by the line.22 the set of ordered pairs ( xi;yi) used when fitting a least squares regression line are assumed to have been sampled from a population in which the relationship between the explanatory and response variables follows equation 6.15. under this assumption, the slope and intercept values of the least squares regression line, b0andb1, are estimates of the population parameters 0and 1;b0andb1have sampling distributions, just as xdoes when thought of as an estimate of a population mean \u0016. a more advanced treatment of regression would demonstrate that the sampling distribution of b1is normal with mean e(b1) = 1and standard deviation \u001bb1=\u001bpp(xi\u0000x)2: the sampling distribution of b0has meane(b0) = 0and standard deviation \u001bb0=\u001bs 1 n+x2 p(xi\u0000x)2: in both of these expressions, \u001bis the standard deviation of \". hypothesis tests and confidence intervals for regression parameters have the same basic form as tests and intervals about population means. the test statistic for a null hypothesis h0: 1= 0 1 about a slope parameter is t=b1\u0000 0 1 s.e.(b1); where the formula for s.e.( b1) is given below. in this setting, thas at-distribution with n\u00002 degrees of freedom, where nis the number of ordered pairs used to estimate the least squares line. 21in some applications of regression, the predictor xis replaced by x\u0003=x\u0000x. in that case, the vertical intercept is the value of the line when x\u0003= 0, orx=x. 22sincee(\") = 0, this model can also be written as y\u0018n(\u0016x), with\u0016x=e(y) = 0+ 1x. the term\"is the population model for the observed residuals eiin regression. 6.4. statistical inference with regression 309 typically, hypothesis testing in regression involves tests of whether the xandyvariables are associated; in other words, whether the slope is significantly di fferent from 0. in these settings, the null hypothesis is that there is no association between the explanatory and response variables, or h0: 1= 0 = 0 1, in which case t=b1 s.e.(b1): the hypothesis is rejected in favor of the two-sided alternative ha: 1,0 with significance level whenjtj\u0015t? df, wheret? dfis the point on a t-distribution with n\u00002 degrees of freedom that has =2 area to its right (i.e., when p\u0014 ). a two-sided confidence interval for 1is given by b1\u0006s.e.(b1)\u0002t? df: tests for one-sided alternatives and one-sided confidence intervals make the usual adjustments to the rejection rule and confidence interval, and p-values are interpreted just as in chapters 4 and 5. formulas for calculating standard errors statistical software is typically used to obtain t-statistics and p-values for inference with regression, since using the formulas for calculating standard error can be cumbersome. the standard errors of b0andb1used in confidence intervals and hypothesis tests replace \u001b withs, the standard deviation of the residuals from a fitted line. formally, s=spe2 i n\u00002=rp(yi\u0000ˆyi)2 n\u00002: (6.16) the terms2is often called the mean squared error from the regression, and sthe root mean squared error. the two standard errors are s.e.(b1) =spp(xi\u0000x)2and s.e.( b0) =ss 1 n+x2 p(xi\u0000x)2: 310 chapter 6. simple linear regression example 6.17 is there evidence of a significant association between number of doctors per 100,000 members of the population in a state and infant mortality rate? the numerical output that rreturns is shown in figure 6.16.23 the question implies that the district of columbia should not be included in the analysis. the assumptions for applying a least squares regression have been verified in exercise 6.14. whenever possible, formal inference should be preceded by a check of the assumptions for regression. the null and alternative hypotheses are h0: 1= 0 andha: 1,0: the estimated slope of the least squares line is -0.0068, with standard error 0.0028. the t-statistic equals -2.40, and the probability that the absolute value of a t-statistic with 50 \u00002 = 48 degrees of freedom is smaller than \u00002:40 or larger than 2 :40 is 0.021. sincep= 0:021<0:05, the data support the alternative hypothesis that the number of physicians is associated with infant mortality at the 0.05 significance level. the sign of the slope implies that the association is negative; states with more doctors tend to have lower rates of infant mortality. estimate std. error t value pr( >jtj) (intercept) 8.5991 0.7603 11.31 0.0000 doctors per 100,000 -0.0068 0.0028 -2.40 0.0206 figure 6.16: summary of regression output from rfor the model predicting infant mortality from number of doctors, using the census.2010 dataset. care should be taken in interpreting the above results. the r2for the model is 0.107; the model explains only about 10% of the state-to-state variability in infant mortality, which suggests there are several other factors a ffecting infant mortality that are not accounted for in the model.24 additionally, an important implicit assumption being made in this example is that data from the year 2010 are representative; in other words, that the relationship between number of physicians and infant mortality is constant over time, and that the data from 2010 can be used to make inference about other years. note that it would be incorrect to make claims of causality from these data, such as stating that an additional 100 physicians (per 100,000 residents) would lead to a decrease of 0.68 in the infant mortality rate. guided practice 6.18 calculate a 95% two-sided confidence interval for the slope parameter 1in the state-level infant mortality data.25 23other software packages, such as stata or minitab, provide similar information but with slightly di fferent labeling. 24calculations of the r2value are not shown here. 25thet?value for at-distribution with 48 degrees of freedom is 2.01, and the standard error of b1is 0.0028. the 95% confidence interval is \u00000:0068\u00062:01(0:0028) = (-0.0124, -0.0012). 6.4. statistical inference with regression 311 connection to two-group hypothesis testing conducting a regression analysis with a numerical response variable and a categorical predictor with two levels is analogous to conducting a two-group hypothesis test. for example, section 6.3.3 shows a regression model that compares the average infant mortality rate in countries with low access to sanitation facilities versus high access.26in other words, the purpose of the analysis is to compare mean infant mortality rate between the two groups: countries with low access versus countries with high access. recall that the slope parameter b1is the difference between the means of log(mortality rate). a test of the null hypothesis h0: 1= 0 in the context of a categorical predictor with two levels is a test of whether the two means are di fferent, just as for the two-group null hypothesis, h0:\u00161=\u00162. when the pooled standard deviation assumption (section 5.3.5) is used, the t-statistic and p-value from a two-group hypothesis test are equivalent to that returned from a regression model. figure 6.17 shows the routput from a regression model in the wdi.2011 data, in which sanit.access = 1 for countries where at least 50% of the population has access to adequate sanitation and 0 otherwise. the abbreviated routput from two-group t-tests are shown in figure 6.18. the version of the t-test that does not assume equal standard deviations and uses non-integer degrees of freedom is often referred to as the welch test. estimate std. error t value pr( >jtj) (intercept) 4.0184 0.1100 36.52 < 0.001 high access -1.6806 0.1322 -12.72 0.001 figure 6.17: regression of log(infant mortality) versus sanitation access. test df t value pr(>|t|) two-groupt-test 161 12.72 <0:001 welch two-group t-test 155.82 17.36 <0:001 figure 6.18: results from the independent two-group t-test, under di ffering assumptions about standard deviations between groups, for mean log(infant mortality) between sanitation access groups. the sign of the t-statistic di ffers because for the two-group test, the di fference in mean log(infant mortality) was calculated by subtracting the mean in the high access group from the mean in the low access group; in the regression model, the negative sign reflects the reduction in mean log(infant mortality) when changing from low access to high access. since the t-distribution is symmetric, the two-sided p-value is equal. in this case, pis a small number less than 0.001, as calculated from a t-distribution with 163 \u00002 = 161 degrees of freedom (recall that 163 countries are represented in the dataset). the degrees of freedom for the pooled two-group test and linear regression are equivalent. example 6.13 showed that the constant variability assumption does not hold for these data. as a result, it might be advisable for a researcher interested in comparing the infant mortality rates between these two groups to conduct a two-group hypothesis test without using the pooled standard deviation assumption. since this test uses a di fferent formula for calculating the standard error of the di fference in means, the t-statistic is di fferent; additionally, the degrees of freedom are not equivalent. in this particular example, there is not a noticeable e ffect on thep-value. 26recall that a log transformation was used on the infant mortality rate. 312 chapter 6. simple linear regression 6.5 interval estimates with regression section 6.4 introduced interval estimates for regression parameters, such as the population slope 1. an estimated regression line can also be used to construct interval estimates for the regression line itself and to calculate prediction intervals for a new observation. 6.5.1 confidence intervals as initially discussed in section 6.2, the estimated regression line for the association between rfft score and age from the 500 individuals in prevend.samp is rfft = 137:55\u00001:26(age): figure 6.19 shows the summary output from rwhen the regression model is fit. ralso provides the value of r2as 0.285 and the value of s, the estimated standard deviation of the residuals, as 23.2. estimate std. error t value pr( >jtj) (intercept) 137.55 5.02 27.42 0.000 age -1.26 0.09 -14.09 0.000 df= 498 figure 6.19: summary of regression output from rfor the model predicting rfft score from age, using the prevend.samp dataset. a confidence interval for the slope parameter 1is centered at the point estimate b1, with a width based on the standard error for the slope. for this model, the 95% confidence interval for age is\u00001:26\u0006(1:96)(0:09) = (\u00001:44;\u00001:09) years.27with 95% confidence, each additional year of age is associated with between a 1.1 and 1.4 point lower rfft score. a confidence interval can also be calculated for a specific point on a least squares line. consider a specific value of the predictor variable, x\u0003, such as 60 years of age. at age 60 years, the predicted value of rfft score is 137 :55\u00001:26(60) = 61 :95 points. the fitted line suggests that individuals from this population who are 60 years of age score, on average, about 62 points on the rfft. each point on the estimated regression line represents the predicted average rfft score for a certain age. more generally, the population model for a regression line is e(yjx) = 0+ 1x, and at a value x\u0003of the predictor x, the fitted regression line e(yjx\u0003) =b0+b1x\u0003 estimates the mean of yfor members of the population with predictor value x\u0003. thus, each point on a fitted regression line represents a point estimate for e(yjx\u0003). the corresponding interval estimate for e(yjx\u0003) measures the uncertainty in the estimated mean of yat predictor value x\u0003, just as how an interval estimate for the population slope 1represents the uncertainty around b1. 27the critical value 1.96 is used here because at degrees of freedom 498, the t-distribution is very close to a normal distribution. from software, t? 0:975;df=498= 1:9647. 6.5. interv al estimates with regression 313 the confidence interval for e(yjx\u0003) is computed using the standard error of the estimated mean of the regression model at a value of the predictor: s.e.(e(yjx\u0003)) =s s2 1 n+(x\u0003\u0000x)2 p(xi\u0000x)2! =ss 1 n+(x\u0003\u0000x)2 p(xi\u0000x)2: in this expression, sis given by equation 6.16, the usual estimate of \u001b, the standard deviation of the error term \u000fin the population linear model y= 0+ 1x+\u000f. the standard error of an estimated mean in regression is rarely calculated by hand; with all but the smallest datasets, the calculations are long and best left to software. when necessary, it can be calculated from basic features of the data and summary statistics. consider computing a 95% confidence interval for e(rfftjage= 60). – the sample size is n= 500. –s= 23:2 appears in the regression output. – the sample mean xof the predictor is age= 54:8 years. – (x\u0003\u0000x)2is the squared distance between the predictor value of interest and the sample mean of the predictors: (60 \u000054:8)2= 27:04. – the sump(xi\u0000x)2is the numerator in the calculation of the variance of the predictor, and equals (n\u00001)var(x) = (499)(134 :4445) = 67;088. using these values, the standard error of the estimated mean rfft score at age 60 is s.e.(e(rfftjage= 60)) = 23:2r 1 500+27:04 67;088= 1:14: thus, a 95% confidence interval for the estimated mean is 61 :95\u0006(1:96)(1:14) = (59:72;64:18) points. with 95% confidence, the interval (59.72, 64.18) points contains the average rfft score of a 60-year-old individual. it is also possible to calculate approximate confidence intervals for the estimated mean at a specific value of a predictor. when x\u0003=x, the second term in the square root will be 0, and the standard error of the estimated mean at the average value xwill have the simple form s=pn. for values close to x, approximating the standard error as s=pnis often su fficient. in the prevend data, 60 years is reasonably close to the average age 54.8 years, and the approximate value of the standard error is 23 :2=p 500 = 1:03. for values x\u0003that are more distant from the mean, the second term in the square root cannot be reasonably ignored. the approximate form of the standard error for the mean at a predictor value, s=pn, makes it easier to see that for large n, the standard error approaches 0; thus, the confidence interval narrows as sample size increases, allowing the estimates to become more precise. this behavior is identical to the confidence interval for a simple mean, as one would expect. it is possible to show algebraically that the confidence intervals at any value of the predictor become increasingly narrow as the sample size increases. 314 chapter 6. simple linear regression 6.5.2 prediction intervals after fitting a regression line, a prediction interval is used to estimate a range of values for a new observation of the response variable ywith predictor value x\u0003; that is, an observation not in the data used to estimate the line. the point estimate dyjx\u0003=b0+b1x\u0003is the same ase(yjx\u0003), but the corresponding interval estimate is wider than a confidence interval for the mean. the width of the interval reflects both the uncertainty in the estimate of the mean,e(yjx\u0003), and the inherent variability of the response variable. the standard error for a predicted value dyjx\u0003 at predictor x\u0003is s.e.(dyjx\u0003) =s s2+s2 1 n+(x\u0003\u0000x)2 p(xi\u0000x)2! =ss 1 +1 n+(x\u0003\u0000x)2 p(xi\u0000x)2: the increased variability when estimating yjx\u0003versuse(yjx\u0003) is accounted for by the additional s2 term inside the square root. the standard error for a prediction can also be calculated from summary statistics; the calculation is similar to that for the standard error for a mean. from the values of the summary statistics, s.e.( rfftjage= 60) = 23:2r 1 +1 500+27:04 67;088= 23:23: the 95% prediction interval is 61 :95\u0006(1:96)(23:23) = (16:42;107:68) points. these data and the model suggest that with 95% confidence, a newly selected 60-year-old will score between 16 and 108 points on the rfft. this interval is wider than the confidence interval for the mean rfft score (at age 60 years). just as with confidence intervals, an approximate prediction interval for a predictor near the average of the predictors can be constructed by considering the case when x\u0003=xand the standard error reduces to sp 1 + 1=n. this approximate standard error shows why prediction intervals are wider than confidence intervals and do not become narrower as sample size increases. for large sample sizes, the term 1 =nis close to 0, and the standard error is close to s, the standard deviation of the residuals about the line. even when the mean is estimated perfectly, a prediction interval will reflect the variability in the data (specifically, the variability in the response variable). 40 50 60 70 8020406080100120140rfft score age (yrs) figure 6.20: a scatterplot showing rfft versus age, with the regression line in blue. the confidence intervals are marked by solid red lines, while the prediction intervals are shown in dashed red lines. 6.5. interv al estimates with regression 315 figure 6.20 visually demonstrates confidence intervals and prediction intervals in the prevend example; the regression line is shown in blue, while the 95% confidence intervals and prediction intervals for each value of age are shown in red. at any value of age, the width of the interval estimate at that value is represented by the vertical distance between the two red lines. for example, the width of the confidence interval at age 60 years is represented by the distance between the points (60, 59.72) and (60, 64.18); the solid red lines pass through the upper and lower confidence bounds calculated in the earlier example. similarly, the dashed red lines that represent prediction intervals pass through (60, 16.42) and (60, 107.68), the 95% upper and lower bounds for the predicted rfft score of a 60-year-old. the plot shows how the confidence intervals are most narrow at the mean age, 54.8 years, and become wider at values of age further from the mean. the prediction intervals are always wider than the confidence intervals. while the mean can be estimated with relative precision along the regression line, prediction intervals reflect the scatter of rfft scores about the line (which is directly related to the inherent variability of rfft scores within the study participants). while larger sample sizes can lead to narrower confidence intervals, the width of the prediction intervals will remain essentially unchanged unless the sampling scheme is changed in a way that reduces the variability of the response. a sample of individuals restricted to ages 60 - 70 years, for example, would be expected to have less variable rfft scores, which would allow for narrower prediction intervals. the distinction between confidence and prediction intervals is important and often overlooked. a clinical practitioner interested in the expected outcomes of a test generally should rely on confidence intervals for the mean along a regression line. the prevend data suggest that 60 year olds will score on average about 62 points on the test, and the average score is between 59.7 points and 62.2 points. when the rfft is administered to a new 60 year old, however, the likely range of responses will be between 16.4 and 107.7. the prediction interval is wide because the scores on the test are quite variable. 316 chapter 6. simple linear regression 6.6 notes this chapter provides only an introduction to simple linear regression; the next chapter, chapter 7, expands on the principles of simple regression to models with more than one predictor variable. when fitting a simple regression, be sure to visually assess whether the model is appropriate. nonlinear trends or outliers are often obvious in a scatterplot with the least squares line plotted. if outliers are evident, the data source should be consulted when possible, since outliers may be indicative of errors in data collection. it is also important to consider whether observed outliers belong to the target population of inference, and assess whether the outliers should be included in the analysis. there are several variants of residual plots used for model diagnostics. the ones shown in section 6.3.1, which plot the predicted values on the horizontal axis, easily generalize to settings with multiple predictors, since there is always a single predicted value even when there is more than one predictor. if the only model used is a simple regression, plotting residuals against predictor values may make it easier to identify a case with a notable residual. additionally, data analysts will sometimes plot residuals against case number of the predictor, since runs of large or small residuals may indicate that adjacent cases are correlated. ther2statistic is widely used in the social sciences, where the unexplained variability in the data is typically much larger than the variability captured or explained by a model. it is important to be aware of what information r2does and does not provide. even though a model may have a low proportion of explained variability, regression coe fficients in the model can still be highly statistically significant. the r2should not be interpreted as a measure of the quality of the fit of the model. it is possible for r2to be large even when the data do not show a linear relationship. linear regression models are often estimated after an investigator has noticed a linear relationship in data, and experienced investigators can often guess correctly that regression coe fficients will be significant before calculating a p-value. unlike with two-sample hypothesis tests, regression models are rarely specified in advance at the design stage. in practice, it is best to be skeptical about a small p-value in a regression setting, and wait to see whether the observed statistically significant relationship can be confirmed in an independent dataset. the issue of model validation and assessing whether results of a regression analysis will generalize to other datasets is often discussed at length in advanced courses. in more advanced texts, substantial attention is devoted to the subtleties of fitting straight line models. for instance, there are strategies for adjusting an analysis when one or more of the assumptions for regression do not hold. there are also specific methods to numerically assess the leverage or influence that each observation has on a fitted model. lab 1 explores the relationship between cognitive function and age in adults by fitting and interpreting a straight line to these variables in the prevend dataset, in addition to discussing the statistical model for least squares regression and residual plots used to assess the assumptions for linear regression. the lab is a useful reminder that least squares regression is much more than the mechanics of finding a line that best fits a dataset. lab 2 uses simulated data to explore the quantityr2. lab 3 explores the use of binary categorical predictor variables in regression and shows how two-sample t-tests can be calculated using linear regression, in addition to introducing inference in a regression context. categorical predictor variables are common in medicine and the life sciences. 6.7. exercises 317 6.7 exercises 6.7.1 examining scatterplots 6.1 identify relationships, part i. for each of the six plots, identify the strength of the relationship (e.g. weak, moderate, or strong) in the data and whether fitting a linear model would be reasonable. ●● ●● ●●●● ● ● ● ●●● ●● ●● ●● ●●● ●●● ● ●● ● ●● ●●● ●● ● ● ●● ●● ●● ●● ●● ●● ●● ●● ● ● ●● ● ● ●●● ●● ●● ● ●●● ● ●● ● ●● ●●●● ●●● ●● ●●● ●●● ●● ●●●● ●●● ●● ●●● ●●● ●●●●● ●●● ● ●● (a) ●●● ●●● ●● ● ●● ● ●● ●●● ●● ●●●● ●● ●● ●●● ●● ●●●●● ●●●●●●● ● ●●● ● ● ●●● ● ● ●● ● ●● ●●● ●● ●● ●●● ●● ●●● ●● ●●●●● ● ●●● ● ●● ●● ●●●● ●●● ●● ●● ●●●●● ●●●●●● ●●● ●● ●● ● (b) ●● ●● ● ●● ●● ●●● ● ● ●●● ●●●● ●●● ●●● ●● ●● ●●● ● ●●● ●● ●● ●●● ●●● ●●● ●● ●●●● ● ●●●●● ●● ● ●● ●● ●● ●● ● ●● ●● ●●● ●● ●●● ● ●● ●● ●● ● ●● ●● ● ●● ●●●●● ● ●●● ●●● ●● ●●● ●● (c) ● ● ●● ●●●● ●●●●● ●●● ● ●● ●●● ●● ● ●● ●● ● ● ●● ●● ●● ●● ●● ●● ●●●●● ● ●● ●● ●●● ●●● ●●● ●● ● ●●● ●●● ● ●● ● ●● ● ●●● ●● ●● ●●● ●●● ●● ●● ● ●● ●● ●● ●●● ●● ●●● ●●● ● ●● ● ● ●●● (d) ●●● ●●● ●● ●● ●●● ●● ● ●●● ●●● ●● ●●● ●●●● ● ●● ●●●● ●●● ● ●●●●● ●● ●●● ●●● ●● ●●●● ●● ● ●●●● ● ●● ●● ● ● ●●●● ●● ●● ●● ●●●● ●●● ●● ● ● ●●●● ●●● ●● ●●● ●● ● ●● ●● ● ●● ● ●● (e) ●● ●●● ●● ● ●● ●●● ●●● ●● ●● ● ● ●● ●● ●●●● ● ● ●● ●● ●● ●●● ●● ●● ● ● ●●● ● ● ●●●●● ●● ●●● ● ●●● ● ●●●● ●●●●● ●● ● ● ●● ● ●●●●●● ● ●●● ●● ●● ● ●●● ●●●● ●● ● ● ●●●● ●●● ●●● ●● (f) 6.2 identify relationships, part ii. for each of the six plots, identify the strength of the relationship (e.g. weak, moderate, or strong) in the data and whether fitting a linear model would be reasonable. ●● ●● ●●●● ● ● ● ●●● ●● ●● ●● ●●● ●●● ● ●● ● ●● ●●● ●● ● ● ●● ●●●● ●● ●● ●●●● ●● ● ● ●● ● ● ●●● ●● ●● ● ●●● ● ●● ● ●● ●●●● ●●● ●● ●●● ●●● ●● ●●●● ●●● ●● ●●● ●●● ●●●●● ●●● ● ●●●● ● ●●● ●● ● ●● ● ●● ●●● ●● ● (a) ●●● ●● ●● ●●● ●● ●●●●● ●●●●●●● ● ●●● ● ● ●●● ● ● ●●● ●● ●●● ●● ●● ●●● ●● ●●● ●● ●●●●● ● ●●● ● ●● ●● ●●●● ●●● ●● ●● ●●●●● ●●●●●● ●●● ●● ●● ●●● ●●●●● ●●●●●●● ●●● ●●●● ●●●●●● ●●●● ●●●●●●● ●● (b) ●● ●●● ●●● ●●● ●● ●●●● ●●●●●● ●● ● ●● ●● ●● ●● ● ●● ●●●●● ●●●●● ● ●● ●● ●● ● ●● ●● ● ●● ●●●●● ●●●● ●●● ●● ●●● ●●● ● ●● ●●●● ●●●●● ●●● ● ●● ●●● ●● ●●● ●● ● ● ●● ●● ●● ●● ●● ●● ●●●●● ● ●● ●● ●●● ●●● ● (c) ●● ●● ● ●●● ●●● ● ●● ● ●● ● ●●● ●● ●● ●●● ●●● ●● ●● ● ●● ●● ●● ●●● ●● ●●● ●●● ● ●● ● ● ●●●●●● ●●● ●● ●● ●●● ●● ● ●●● ●●● ●● ● ●● ●●●● ● ●● ●●●● ●●● ● ●●●●● ●● ●●● ●●● ●● ●●● ● ●● ● ●●●● ● ●● ●● ● ● ●●●● ● (d) ● ●● ●● ●●●● ● ●● ●● ● ● ●●●● ●●● ●● ●●● ●● ● ●● ●● ● ●● ● ●● ●● ●●● ●● ● ●● ●●● ●●● ●● ●● ● ● ●● ●● ●●●● ● ● ●● ●● ●● ●●● ●● ●● ● ● ●●● ● ● ●●●●● ●● ● ●● ● ●●● ● ●●●● ●●●●● ●● ● ● ●● ● ●●●●●● ● ●●● ●● ●● ● ●● (e) ● ●●●● ●● ● ●●●●● ●●● ●●● ●●● ● ●● ● ●● ● ●●● ●● ●●●● ●● ●● ●● ●● ●● ● ●●● ●●● ●●● ●●● ●●●● ● ●●●● ●●● ● ●●● ● ● ●● ●● ●●● ●● ● ●● ● ●● ●●● ● ●●● ●● ● ● ●● ● ●●●● ● ●● ●● ● ● ●● ●● ●● ●● ●●●● ● ●●● ●●● ●●● (f) 318 chapter 6. simple linear regression 6.7.2 estimating a regression line using least squares 6.3 body measurements, part i. researchers studying anthropometry collected body girth measurements and skeletal diameter measurements, as well as age, weight, height and gender for 507 physically active individuals.28the scatterplot below shows the relationship between height and shoulder girth (over deltoid muscles), both measured in centimeters. (a) describe the relationship between shoulder girth and height. (b) how would the relationship change if shoulder girth was measured in inches while the units of height remained in centimeters? 90100 110 120 130150160170180190200 shoulder girth (cm)height (cm) 6.4 body measurements, part ii. the scatterplot below shows the relationship between weight measured in kilograms and hip girth measured in centimeters from the data described in exercise 6.3. (a) describe the relationship between hip girth and weight. (b) how would the relationship change if weight was measured in pounds while the units for hip girth remained in centimeters? 80 90100 110 120 130406080100 hip girth (cm)weight (kg) 28g. heinz et al. “exploring relationships in body dimensions”. in: journal of statistics education 11.2 (2003). 6.7. exercises 319 6.5 over-under, part i. suppose we fit a regression line to predict the shelf life of an apple based on its weight. for a particular apple, we predict the shelf life to be 4.6 days. the apple’s residual is -0.6 days. did we over or under estimate the shelf-life of the apple? explain your reasoning. 6.6 over-under, part ii. suppose we fit a regression line to predict the number of incidents of skin cancer per 1,000 people from the number of sunny days in a year. for a particular year, we predict the incidence of skin cancer to be 1.5 per 1,000 people, and the residual for this year is 0.5. did we over or under estimate the incidence of skin cancer? explain your reasoning. 6.7 murders and poverty, part i. the following regression output is for predicting annual murders per million from percentage living in poverty in a random sample of 20 metropolitan areas. estimate std. error t value pr( >jtj) (intercept) -29.901 7.789 -3.839 0.001 poverty% 2.559 0.390 6.562 0.000 s= 5:512 r2= 70:52% r2 adj= 68:89% (a) write out the linear model. (b) interpret the intercept. (c) interpret the slope. (d) calculate the correlation coe fficient. ●●● ●● ●●● ●●●● ●● ●● ●● ●● percent in povertyannual murders per million 14% 18% 22% 26%10203040 6.8 cats, part i. the following regression output is for predicting the heart weight (in g) of cats from their body weight (in kg). the coe fficients are estimated using a dataset of 144 domestic cats. estimate std. error t value pr( >jtj) (intercept) -0.357 0.692 -0.515 0.607 body wt 4.034 0.250 16.119 0.000 s= 1:452 r2= 64:66% r2 adj= 64:41% (a) write out the linear model. (b) interpret the intercept. (c) interpret the slope. (d) calculate the correlation coe fficient. body weight (kg)heart weight (g) 2.0 2.5 3.0 3.5 4.05101520 320 chapter 6. simple linear regression 6.9 age and rfft score, part i. a linear model fit to rfft scores and age for 500 randomly sampled individuals from the prevend data has equation rfft = 137:55\u00001:26(age). (a) interpret the slope and intercept values in the context of the data; i.e., explain the linear model in terms that a non-statistician would understand. comment on whether the intercept value has any interpretive meaning in this setting. (b) based on the linear model, how much does rfft score di ffer, on average, between an individual who is 60 years old versus an individual who is 50 years old? (c) according to the linear model, what is the average rfft score for an individual who is 70 years old? (d) examine figure 6.1. is it valid to use the linear model to estimate rfft score for an individual who is 20 years old? explain your answer. 6.10 guppies, part i. guppies are small, brightly colored tropical fish often seen in freshwater fish aquariums. a study was conducted in 147 male guppies to examine the relationship between coloration and heterozygosity; heterozygosity refers to the condition of having di fferent alleles at a given genetic locus. the guppies were randomly sampled from a river in the wild. in an initial stage of the study, researchers examined whether length and height are linearly associated. the mean length is 1261.21 cm, with standard deviation 95.62 cm. the mean height is 201.75 cm, with standard deviation 20.68. the correlation between length and height is 0.85. (a) from a visual inspection, does it seem like the line is a reasonable fit for the data? (b) write the equation of the regression line for predicting length from height. (c) estimate the predicted mean length of a guppy with height 180 cm. 160180200220240260280110012001300140015001600 height (cm)length (cm) 6.7. exercises 321 6.7.3 interpreting a linear model 6.11 visualize the residuals. the scatterplots shown below each have a superimposed regression line. if we were to construct a residual plot (residuals versus x) for each, describe what those plots would look like. ● ●● ●●●●●●●●● ●●● ●●●● ●● ● ●●●● ● ●●● ●● ●● ●●●●●● ●● ●●●● ●●●●●●●● ●●● ● ●●●● ●●●●● ●● ●●●●● ●● ●● ●●●●● ●●● ●● ●● ●●●● ●● ●● ●● (a) ●●● ●●● ●● ●●● ●● ●● ●● ●● ●●●●●● ●● ●● ●● ●● ● ●●●● ● ●●● ●●●● ●● ●●● ●●● ● ●● ●● ●● ●● ●● ●●●● ●●●●●●● ●●●● ●●●● ●● ●●●●●●●●●●●●●● (b) 6.12 trends in the residuals. shown below are two plots of residuals remaining after fitting a linear model to two di fferent sets of data. describe important features and determine if a linear model would be appropriate for these data. explain your reasoning. ●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●●● ●●●●● ●●●●●●● ●●●●● ●●●● ●●●●●●●●● ●●● ●●●● ●●●●●●● ●● ●●●● ●●● ● ●●● ●●● ●● ●●● ●● ●●●●● ●●●●● ● ● ●●● ● ●●● ●● ●●● ●●●● ●●● ●●●●● ●●●●●●● ●● ●●●● ●●● ●● ●●● ●● ●● ●● ●●● ● ●● ●● ●● ●● ●●● ●●●● ●●●● ●●● ● ●●● ●● ●● ● ●● ● ●● ●● ●●● ●● ●●● ●●●●● ●● ●● ●● ● ●●● ●● ●● ● ● ●● ● ● ●●● ●●● ● ●● ●●●● ● ● ●● ●●●● ●● ●● ●●● ●● ●● ●●●● ●● ● ●●● ●● ● ●●● ●● ● ● ●● ● ●● ●● (a) ●●●● ● ●●●● ●●●●● ●●● ●● ● ●●● ●●● ●●●● ●●● ●● ●●●● ●●●● ●●● ●●●●●● ●●●●●●● ●● ●● ● ●●●●●●●●●●● ●●●●● ●● ●● ●●● ● ●●●●●●● ●● ●● ●● ●●●● ●●● ● ●●●● ●●● ●●●● ●●● ●● ●●●●● ● ●●● ●●● ●● ●●●●●●● ●●●● ●●● ●● ● ●●● ●● ●●●●● ●● ●● ●●●●● ●●●●●●● ● ●● ●● ●●●●● ●●● ●● ●●●● ●●● ●●●●●●● ●●● ●●●●● ●● ● ●●●● ●●●● ●●●●●● ●●●●● ●●●●●●●●● ●● ●● ● ●●● ● ●●●● ●●●● ●●● ●●●● ● ●● ●● ●●●●● ●● ●● ●●● ●● ●●●●● ●● (b) 6.13 guppies, part ii. exercise 6.10 showed a plot of length versus height for 147 male guppies with a least squares regression line. (a) identify two points that have relatively high leverage and discuss whether these points seem to be particularly influential. (b) based on the plot, comment on whether it is appropriate to use r2as a metric for describing the strength of the model fit. (c) ther2for this model is 0.718. interpret this value in the context of the data. 322 chapter 6. simple linear regression 6.14 nutrition at starbucks, part i. the scatterplot below shows the relationship between the number of calories and amount of carbohydrates (in grams) starbucks food menu items contain.29since starbucks only lists the number of calories on the display items, we are interested in predicting the amount of carbs a menu item has based on its calorie content. caloriescarbs (grams) 10020030040050020406080 caloriesresiduals 100200300400500−20020 residuals−40 −20 0 20 400510152025 (a) describe the relationship between number of calories and amount of carbohydrates (in grams) that starbucks food menu items contain. (b) in this scenario, what are the explanatory and response variables? (c) why might we want to fit a regression line to these data? (d) do these data meet the conditions required for fitting a least squares line? 6.15 nutrition at starbucks, part ii. exercise 6.14 introduced a data set on nutrition information on starbucks food menu items. based on the scatterplot and the residual plot provided, describe the relationship between the protein content and calories of these menu items, and determine if a simple linear model is appropriate to predict amount of protein from the number of calories. caloriesprotein (grams) 100 200 300 400 5000102030 −20020 29source: starbucks.com, collected on march 10, 2011, www.starbucks.com/menu/nutrition. 6.7. exercises 323 6.16 body measurements, part iii. exercise 6.3 introduces data on shoulder girth and height of a group of individuals. the mean shoulder girth is 107.20 cm with a standard deviation of 10.37 cm. the mean height is 171.14 cm with a standard deviation of 9.41 cm. the correlation between height and shoulder girth is 0.67. (a) write the equation of the regression line for predicting height. (b) interpret the slope and the intercept in this context. (c) calculate r2of the regression line for predicting height from shoulder girth, and interpret it in the context of the application. (d) a randomly selected student from your class has a shoulder girth of 100 cm. predict the height of this student using the model. (e) the student from part (d) is 160 cm tall. calculate the residual, and explain what this residual means. (f) a one year old has a shoulder girth of 56 cm. would it be appropriate to use this linear model to predict the height of this child? 6.17 outliers, part i. identify the outliers in the scatterplots shown below, and determine what type of outliers they are. explain your reasoning. (a) (b) (c) 6.18 outliers, part ii. identify the outliers in the scatterplots shown below and determine what type of outliers they are. explain your reasoning. (a) (b) (c) 324 chapter 6. simple linear regression 6.19 guppies, part iii. the residual plots below are for the linear model fit in exercise 6.10 predicting length from height for 147 male guppies. height(cm)residuals 160180200220240260280−20020 −2 −1 0 1 2−100−50050100150q−q plot of residuals theoretical quantilessample quantiles −2 −1 0 1 2−100−50050100150 (a) from a plot of residual values versus predicted values, are the assumptions of linearity and constant variability satisfied? explain your answer. (b) is it reasonable to assume that the observations were independent, based on the description of the study? explain your answer. (c) are the residuals approximately normally distributed? explain your answer. 6.20 guppies, part iv. multilocus heterozygosity (mlh) is reflective of genetic quality; according to sexual selection research, it is thought that sexual ornamentation functions as a visual indicator of fitness. by selecting males with features such as bright coloration, females can improve the chances of reproductive success. male guppies are covered in a mixture of colored spots; orange coloration is consistently preferred by females. heterozygosity was assessed by genotyping 9 loci and calculating the proportion of loci that are heterozygous. the research question of interest is whether mlh and orange color are linearly associated. 0.2 0.3 0.4 0.5 0.6 0.7 0.80.050.100.150.20 mlhorange area relative to body size 0.2 0.3 0.4 0.5 0.6 0.7 0.8−0.050.000.050.10 relative orange arearesiduals −2 −1 0 1 2−0.050.000.050.10q−q plot of residuals theoretical quantilessample quantiles −2 −1 0 1 2−0.050.000.050.10 (a) based on the plot of mlh versus relative orange area, describe the nature of the association in language accessible to a general audience. (b) comment on whether the assumptions of linearity and constant variability are reasonably met. (c) comment on whether the residuals are approximately normally distributed. 6.7. exercises 325 6.21 diamond prices, part ii. exercise 5.24 introduced data on the price of diamonds based on whether a diamond is 0.99 carats or 1 carat. based on the summary statistics, write an estimated model equation predicting price from a binary indicator of carat weight. be sure to clearly define the variables used in the model. 0.99 carats 1 carat mean $ 44.51 $ 56.81 sd $ 13.32 $ 16.13 n 23 23 6.22 avian influenza, part ii. exercise 5.28 introduced data from an analysis investigating whether hatch weights between transgenic and non-transgenic chicks di ffer. transgenic chicks (g) non-transgenic chicks (g) ̄x 45.14 44.99 s 3.32 4.57 n 54 54 (a) write an estimated least squares regression line for a model predicting hatch weight from chick type, where non-transgenic chicks are the reference group; i.e., the group for which the binary predictor takes on value 0. (b) write an estimated least squares regression line for a model predicting hatch weight from chick type, where transgenic chicks are the reference group. 326 chapter 6. simple linear regression 6.7.4 statistical inference with regression 6.23 body measurements, part iv. the scatterplot and least squares summary below show the relationship between weight measured in kilograms and height measured in centimeters of 507 physically active individuals. height (cm)weight (kg) 150 175 200507090110 estimate std. error t value pr( >jtj) (intercept) -105.0113 7.5394 -13.93 0.0000 height 1.0176 0.0440 23.13 0.0000 (a) describe the relationship between height and weight. (b) write the equation of the regression line. interpret the slope and intercept in context. (c) do the data provide strong evidence that an increase in height is associated with an increase in weight? state the null and alternative hypotheses, report the p-value, and state your conclusion. (d) the correlation coe fficient for height and weight is 0.72. calculate r2and interpret it in context. 6.24 beer and blood alcohol content. many people believe that gender, weight, drinking habits, and many other factors are much more important in predicting blood alcohol content (bac) than simply considering the number of drinks a person consumed. here we examine data from sixteen student volunteers at ohio state university who each drank a randomly assigned number of cans of beer. these students were evenly divided between men and women, and they di ffered in weight and drinking habits. thirty minutes later, a police o fficer measured their blood alcohol content (bac) in grams of alcohol per deciliter of blood.30the scatterplot and regression table summarize the findings. ● ●● ● ●● ● ● ●●●● ●● ●● 2 4 6 80.050.100.15 cans of beerbac (grams / deciliter) estimate std. error t value pr( >jtj) (intercept) -0.0127 0.0126 -1.00 0.3320 beers 0.0180 0.0024 7.48 0.0000 (a) describe the relationship between the number of cans of beer and bac. (b) write the equation of the regression line. interpret the slope and intercept in context. (c) do the data provide strong evidence that drinking more cans of beer is associated with an increase in blood alcohol? state the null and alternative hypotheses, report the p-value, and state your conclusion. (d) the correlation coe fficient for number of cans of beer and bac is 0.89. calculate r2and interpret it in context. (e) suppose we visit a bar, ask people how many drinks they have had, and also take their bac. do you think the relationship between number of drinks and bac would be as strong as the relationship found in the ohio state study? 30j. malkevitch and l.m. lesser. for all practical purposes: mathematical literacy in today’s world . wh freeman & co, 2008. 6.7. exercises 327 6.25 husbands and wives, part i. the scatterplot below summarizes husbands’ and wives’ heights in a random sample of 170 married couples in britain, where both partners’ ages are below 65 years. summary output of the least squares fit for predicting wife’s height from husband’s height is also provided in the table. husband's height (in inches)wife's height (in inches) 60 65 70 7555606570 estimate std. error t value pr( >jtj) (intercept) 43.5755 4.6842 9.30 0.0000 height_husband 0.2863 0.0686 4.17 0.0000 (a) is there strong evidence that taller men marry taller women? state the hypotheses and include any information used to conduct the test. (b) write the equation of the regression line for predicting wife’s height from husband’s height. (c) interpret the slope and intercept in the context of the application. (d) given that r2= 0:09, what is the correlation of heights in this data set? (e) you meet a married man from britain who is 5’9\" (69 inches). what would you predict his wife’s height to be? how reliable is this prediction? (f) you meet another married man from britain who is 6’7\" (79 inches). would it be wise to use the same linear model to predict his wife’s height? why or why not? (g) is there statistically significant evidence of an association between husband height and wife height based on these data? explain your answer. (h) would you expect a 95% confidence interval for husband height to contain 0? explain your answer. 6.26 helmets and lunches. the scatterplot shows the relationship between socioeconomic status measured as the percentage of children in a neighborhood receiving reduced-fee lunches at school ( lunch ) and the percentage of bike riders in the neighborhood wearing helmets ( helmet ). the average percentage of children receiving reduced-fee lunches is 30.8% with a standard deviation of 26.7% and the average percentage of bike riders wearing helmets is 38.8% with a standard deviation of 16.9%. (a) if ther2for the least-squares regression line for these data is 72%, what is the correlation between lunch and helmet ? (b) calculate the slope and intercept for the least-squares regression line for these data. (c) interpret the intercept of the least-squares regression line in the context of the application. (d) interpret the slope of the least-squares regression line in the context of the application. (e) what would the value of the residual be for a neighborhood where 40% of the children receive reduced-fee lunches and 40% of the bike riders wear helmets? interpret the meaning of this residual in the context of the application. ●●● ●● ● ●●● ●●● rate of receiving a reduced−fee lunch0%20%40%60%80%0%20%40%60%rate of wearing a helmet 328 chapter 6. simple linear regression 6.27 husbands and wives, part ii. exercise 6.25 presents a scatterplot displaying the relationship between husbands’ and wives’ ages in a random sample of 170 married couples in britain, where both partners’ ages are below 65 years. given below is summary output of the least squares fit for predicting wife’s age from husband’s age. husband's age (in years)wife's age (in years) 20 40 60204060 estimate std. error t value pr( >jtj) (intercept) 1.5740 1.1501 1.37 0.1730 age_husband 0.9112 0.0259 35.25 0.0000 df= 168 (a) we might wonder, is the age di fference between husbands and wives consistent across ages? if this were the case, then the slope parameter would be 1= 1. use the information above to evaluate if there is strong evidence that the di fference in husband and wife ages di ffers for di fferent ages. (b) write the equation of the regression line for predicting wife’s age from husband’s age. (c) interpret the slope and intercept in context. (d) given that r2= 0:88, what is the correlation of ages in this data set? (e) you meet a married man from britain who is 55 years old. what would you predict his wife’s age to be? how reliable is this prediction? (f) you meet another married man from britain who is 85 years old. would it be wise to use the same linear model to predict his wife’s age? explain. 6.28 guppies, part v. exercise 6.20 introduced a linear model for predicting relative orange area from proportion of loci that are heterozygous (mlh). relative orange area refers to the percentage of the body that is orange (rather than a di fferent color). estimate std. error t value pr( >jtj) (intercept) 0.037 0.010 3.68 0.0033 mlh 0.051 0.018 2.77 0.0063 df= 145 (a) write the estimated model equation. (b) what is the predicted mean relative orange area for a guppy that is heterozygous at 8 out of 9 loci? (c) based on the linear model, how much does mean relative orange area di ffer between a guppy that is heterozygous at 2 loci versus 4 loci (out of 9 total)? (d) conduct a hypothesis test to determine whether relative orange area is significantly associated with mlh. do the results suggest that more elaborate sexual ornaments are associated with increased heterozygosity? explain. (e) compute and interpret a 95% confidence interval for the slope parameter 1. 6.7. exercises 329 6.29 age and rfft score, part ii. the following regression output is for predicting rfft score of 500 randomly sampled individuals from the prevend data based on age (years). estimate std. error t value pr( >jtj) (intercept) 137.55 5.02 27.42 0.000 age -1.26 0.09 -14.09 0.000 df= 498 (a) do these data provide statistically significant evidence at the = 0:01 significance level that age is associated with rfft score? state the null and alternative hypotheses, report the relevant p-value, and state your conclusion. (b) compute and interpret a 99% confidence interval for the population slope. 6.30 avian influenza, part iii. exercise 5.28 introduced data from an analysis investigating whether hatch weights between transgenic and non-transgenic chicks di ffer. based on the results from conducting the twogroup test, explain whether the 95% confidence interval for the 1parameter in a model predicting hatch weight from a group indicator would contain 0. 6.7.5 interval estimates with regression 6.31 husbands and wives, part iii. exercise 6.27 introduces data from a random sample of 170 married couples in britain, where both partners’ ages are below 65 years, and fits a model predicting wife’s age from husband’s age. wife’s age has a mean of 40.68 years, with standard deviation 11.41 years. husband’s age has a mean of 42.92 years, with standard deviation 11.76 years. from software, the residual standard error is s= 3:95. (a) use the summary statistics to calculate a 95% confidence interval for the average age of wives whose husbands are 55 years old. (b) you meet a married man from britain who is 55 years old. predict his wife’s age and give a 95% prediction interval for her age. (c) repeat parts (a) and (b) using the approximate formulas for the appropriate standard errors. 6.32 guppies, part vi. the relationship between length and height for 147 male guppies was introduced in exercise 6.10, which used the summary statistics to calculate the equation of the least squares line for length as a function of height and estimate the mean length of an adult male guppy with height 180 cm. the estimated residual standard error from this model is s= 50:93. (a) use the summary statistics given in exercise 6.10 to construct a 95% confidence interval for the estimated mean length when height is 180 cm. (b) use a prediction interval based on the summary statistics to estimate the lengths for a new 180 cm guppy that would be more than two standard deviations above and below the estimated mean. (c) use the approximate formulas for the standard error for a mean and for a prediction to recalculate the intervals in parts (a) and (b). 330 chapter 7 multiple linear regression 7.1 introduction to multiple linear regression 7.2 simple versus multiple regression 7.3 evaluating the fit of a multiple regression model 7.4 the general multiple linear regression model 7.5 categorical predictors with several levels 7.6 reanalyzing the prevend data 7.7 interaction in regression 7.8 model selection for explanatory models 7.9 the connection between anova and regression 7.10 notes 7.11 exercises 331 in most practical settings, more than one explanatory variable is likely to be associated with a response. this chapter discusses how the ideas behind simple linear regression can be extended to a model with multiple predictor variables. there are several applications of multiple regression. one of the most common applications in a clinical setting is estimating an association between a response variable and primary predictor of interest while adjusting for possible confounding variables. sections 7.1 and 7.2 introduce the multiple regression model by examining the possible association between cognitive function and the use of statins after adjusting for potential confounders. section 7.8 discusses another application of multiple regression—constructing a model that e ffectively explains the observed variation in the response variable. the other sections in the chapter outline general principles of multiple regression, including the statistical model, methods for assessing quality of model fit, categorical predictors with more than two levels, interaction, and the connection between anov a and regression. the methods used to conduct hypothesis tests and construct confidence intervals for regression coe fficients extend naturally from simple to multiple linear regression, so the section on the statistical model for multiple regression can be treated as optional. for labs, slides, and other resources, please visit www.openintro.org/book/biostat 332 chapter 7. multiple linear regression 7.1 introduction to multiple linear regression statins are a class of drugs widely used to lower cholesterol. there are two main types of cholesterol: low density lipoprotein (ldl) and high density lipoprotein (hdl).1research suggests that adults with elevated ldl may be at risk for adverse cardiovascular events such as a heart attack or stroke. in 2013, a panel of experts commissioned by the american college of cardiology and the american heart association recommended that statin therapy be considered in individuals who either have any form of atherosclerotic cardiovascular disease2or have ldl cholesterol levels \u0015190 mg/dl, individuals with type ii diabetes ages 40 to 75 with ldl between 70 to 189 mg/dl, and non-diabetic individuals ages of 40 to 75 with a predicted probability of future clogged arteries of at least 0.075.3 health policy analysts have estimated that if the new guidelines were to be followed, almost half of americans ages 40 to 75 and nearly all men over 60 would be prescribed a statin. however, some physicians have raised the question of whether treatment with a statin might be associated with an increased risk of cognitive decline.4, 5older adults are at increased risk for cardiovascular disease, but also for cognitive decline. a study by joosten, et al. examined the association of statin use and other variables with cognitive ability in an observational cohort of 4,095 participants from the netherlands who were part of the larger prevend study introduced in section 6.1.6the analyses presented in this chapter are based on a random sample of 500 participants from the cohort.7 the investigators behind the joosten study anticipated an issue in the analysis—statins are used more often in older adults than younger adults, and older adults su ffer a natural cognitive decline. age is a potential confounder in this setting. if age is not accounted for in the analysis, it may seem that cognitive decline is more common among individuals prescribed statins, simply because those prescribed statins are simply older and more likely to have reduced cognitive ability than those not prescribed statins. 1total cholesterol level is the sum of ldl and hdl levels. 2i.e., arteries thickening and hardening with plaque 3stone nj, et al. 2013 acc/aha guideline on the treatment of blood cholesterol to reduce atherosclerotic cardiovascular risk in adults circulation. 2014;129:s1-s45. doi: 10.1161/01.cir.0000437738.63853.7a 4muldoon, matthew f., et al. randomized trial of the e ffects of simvastatin on cognitive functioning in hypercholesterolemic adults. the american journal of medicine 117.11 (2004): 823-829. 5king, deborah s., et al. cognitive impairment associated with atorvastatin and simvastatin. pharmacotherapy: the journal of human pharmacology and drug therapy 23.12 (2003): 1663-1667. 6joosten h, visser st, van eersel me, gansevoort rt, bilo hjg, et al. (2014) statin use and cognitive function: population-based observational study with long-term follow- up. plos one 9(12): e115755. doi:10.1371/ journal.pone.0115755 7the random sample is accessible as prevend.samp in the oibiostat rpackage. 7.1. introduction to multiple linear regression 333 40 50 60 70 8020406080100120140rfft score age (yrs) figure 7.1: a scatterplot showing agevs. rfft inprevend.samp . statin users are represented with red points; participants not using statins are shown as blue points. figure 7.1 visually demonstrates why age is a potential confounder for the association between statin use and cognitive function, where cognitive function is measured via the ru fffigural fluency test (rfft). scores range from 0 (worst) to 175 (best). the blue points indicate individuals not using statins, while red points indicate statin users. first, it is clear that age and statin use are associated, with statin use becoming more common as age increases; the red points are more prevalent on the right side of the plot. second, it is also clear that age is associated with lower rfft scores; ignoring the colors, the point cloud drifts down and to the right. however, a close inspection of the plot suggests that for ages in relatively small ranges (e.g., ages 50-60), statin use may not be strongly associated with rfft score—there are approximately as many red dots with low rfft scores as with high rfft scores in a given age range. in other words, for subsets of participants with approximately similar ages, statin use may not be associated with rfft. multiple regression provides a way to estimate the association of statin use with rfft while adjusting for age; i.e., accounting for the underlying relationship between age and statin use. 334 chapter 7. multiple linear regression 7.2 simple versus multiple regression a simple linear regression model can be fit for an initial examination of the association between statin use and rfft score, e(rfft) = 0+ statin (statin): rfft scores in prevend.samp are approximately normally distributed, ranging between approximately 10 and 140, with no obvious outliers (figure 7.2(a)). the least squares regression line shown in figure 7.2(b) has a negative slope, which suggests a possible negative association. rfft scoresfrequency 20406080100120140010203040506070 (a) statin use0 (no)1 (yes)20406080100120140 rfft score (b) figure 7.2: (a) histogram of rfft scores. (b) scatterplot of rfft score versus statin use in prevend.samp . the variable statin is coded 1for statin users, and 0 otherwise. figure 7.3 gives the parameter estimates of the least squares line, and indicates that the association between rfft score and statin use is highly significant. on average, statin users score approximately 10 points lower on the rfft. however, even though the association is statistically significant, it is potentially misleading since the model does not account for the underlying relationship between age and statin use. the association between age and statin use visible from figure 7.1 is even more apparent in figure 7.4, which shows that the median age of statin users is about 10 years higher than the median age of individuals not using statins. estimate std. error t value pr( >jtj) (intercept) 70.7143 1.3808 51.21 0.0000 statin -10.0534 2.8792 -3.49 0.0005 figure 7.3: rsummary output for the simple regression model of rfft versus statin use in prevend.samp . 7.2. simple versus multiple regression 335 statin useage (years) 0 14050607080 figure 7.4: boxplot of age by statin use in prevend.samp . the variable statin is coded 1for statin users, and 0otherwise. multiple regression allows for a model that incorporates both statin use and age, e(rfft) = 0+ statin (statin) + age(age): in statistical terms, the association between rfft and statin is being estimated after adjusting for age. this is an example of one of the more important applications of multiple regression: estimating an association between a response variable and primary predictor of interest while adjusting for possible confounders. in this setting, statin use is the primary predictor of interest. the principles and assumptions behind the multiple regression model are introduced more formally in section 7.4, along with the method used to estimate the coe fficients. figure 7.5 shows the parameter estimates for the model from r. estimate std. error t value pr( >jtj) (intercept) 137.8822 5.1221 26.92 0.0000 statin 0.8509 2.5957 0.33 0.7432 age -1.2710 0.0943 -13.48 0.0000 figure 7.5: rsummary output for the multiple regression model of rfft versus statin use and age in prevend.samp . 336 chapter 7. multiple linear regression example 7.1 using the parameter estimates in figure 7.5, write the prediction equation for the linear model. how does the predicted rfft score for a 67-year-old not using statins compare to that of an individual of the same age who does use statins? the equation of the linear model is rfft = 137:8822 + 0:8509(statin)\u00001:2710(age): the predicted rfft score for a 67-year-old not using statins ( statin = 0 ) is rfft = 137:8822 + (0:8509)(0)\u0000(1:2710)(67) = 52 :7252: the predicted rfft score for a 67-year-old using statins ( statin = 1 ) is rfft = 137:8822 + (0:8509)(1)\u0000(1:2710)(67) = 53 :5761: the two calculations di ffer only by the value of the coe fficient statin , 0.8509.8thus, for two individuals who are the same age, the model predicts that rfft score will be 0.8509 higher in the individual taking statins; statin use is associated with a small increase in rfft score. example 7.2 suppose two individuals are both taking statins; one individual is 50 years of age, while the other is 60 years of age. compare their predicted rfft scores. from the model equation, the coe fficient of age ageis -1.2710; an increase in one unit of age (i.e., one year) is associated with a decrease in rfft score of -1.2710, when statin use is the same. thus, the individual who is 60 years of age is predicted to have an rfft score that is about 13 points lower ((\u00001:2710)(10) =\u000012:710) than the individual who is 50 years of age. this can be confirmed numerically: the predicted rfft score for a 50-year-old using statins is rfft = 137:8822 + (0:8509)(1)\u0000(1:2710)(50) = 75 :1831: the predicted rfft score for a 60-year-old using statins is rfft = 137:8822 + (0:8509)(1)\u0000(1:2710)(60) = 62 :4731: the scores di ffer by 62:4731\u000075:1831 =\u000012:710: 8in most cases, predictions do not need to be calculated to so many significant digits, since the coe fficients are only estimates. this example uses the additional precision to illustrate the role of the coe fficients. 7.2. simple versus multiple regression 337 guided practice 7.3 what does the intercept represent in this model? does the intercept have interpretive value?9 as in simple linear regression, t-statistics can be used to test hypotheses about the slope coe fficients; for this model, the two null hypotheses are h0: statin = 0 andh0: age= 0. thep-values for the tests indicate that at significance level = 0:05, the association between rfft score and statin use is not statistically significant, but the association between rfft score and age is significant. in a clinical setting, the interpretive focus lies on reporting the nature of the association between the primary predictor and the response and specifying which confounders have been adjusted for. the results of the analysis might be summarized as follows— although the use of statins appeared to be associated with lower rfft scores when no adjustment was made for possible confounders, statin use is not significantly associated with rfft score in a regression model that adjusts for age. the results shown in figure 7.5 do not provide information about either the quality of the model fit or its value as a prediction model. the next section describes the residual plots that can be used to check model assumptions and the use of r2to estimate how much of the variability in the response variable is explained by the model. there is an important aspect of these data that should not be overlooked. the data do not come from a study in which participants were followed as they aged; i.e., a longitudinal study. instead, this study was a cross-sectional study, in which patient age, statin use, and rfft score were recorded for all participants during a short time interval. while the results of the study support the conclusion that older patients tend to have lower rfft scores, they cannot be used to conclude that scores decline with age in individuals; there were no repeated measurements of rfft taken as individual participants aged. older patients come from an earlier birth cohort, and it is possible, for instance, that younger participants have more post-secondary school education or better health practices generally; such a cohort e ffect may have some explanatory e ffect on the observed association. the details of how a study is designed and how data are collected should always be taken into account when interpreting study results. 9the intercept represents an individual with value 0for both statin and age; i.e., an individual not using statins with age of 0 years. it is not reasonable to predict rfft score for a newborn, or to assess statin use; the intercept is meaningless and has no interpretive value. 338 chapter 7. multiple linear regression 7.3 evaluating the fit of a multiple regression model 7.3.1 using residuals to check model assumptions the assumptions behind multiple regression are essentially the same as the four assumptions listed in section 6.1 for simple linear regression. the assumption of linearity is extended to multiple regression by assuming that when only one predictor variable changes, it is linearly related to the change in the response variable. assumption 2 becomes the slightly more general assumption that the residuals have approximately constant variance. assumptions 3 and 4 do not change; it is assumed that the observations on each case are independent and the residuals are approximately normally distributed. since it is not possible to make a scatterplot of a response variable against several simultaneous predictors, residual plots become even more essential as tools for checking modeling assumptions. to assess the linearity assumption, examine plots of residuals against each of the predictors. these plots might show an nonlinear trend that could be corrected with a transformation. the scatterplot of residual values versus age in figure 7.6 shows no apparent nonlinear trends. it is not necessary to assess linearity against a categorical predictor, since a line drawn through two points (i.e., the means of the two groups) is necessarily linear. 40 50 60 70 80−60−40−200204060residual●● ●● ● ●●●●● ●● ●● ●● ●●● ●●● ● ●● ●●●● ● ●●●●● ● ●● ●● ●●● ●● ●● ● ●●● ●●● ●●● ● ●● ●● ● ● ● ● ● ●● ● ●● ●● ●● ● ●●● ● ●● ● ●● ●●●● ●● ● ●● ●●● ● ● ●●● ●● ●●● ●●●● ●● ● ●● ●● ● ●●●● ●●●● ●●● ●● ●●● ●● ●●●●● ● ●● ● ●●●● ●●● ●● ●● ●● ● ●●● ● ●●●● ●● ●● ●● ●● ●● ● ●● ● ● ● ●● ● ●●● ●● ● ●● ●● ●● ● ●●●● ● ●●● ● ●●●● ● ●● ● ●● ●●● ● ●●● ●●●● ● ● ●● ●● ●●●● ●● ●● ● ●●● ●● ●● ●●● ●● ●●●● ● ●● ● ●● ●●● ●● ●● ●●● ●● ●● ●● ●●● ● ●●● ●● ● ●● ●●● ●● ● ●● ● ● ● ●●● ● ●● ●● ● ●●● ● ● ●● ● ●●● ●●● ●● ●●● ●● ● ●●● ●● ● ● ●● ●● ●●● ● ●● ●● ●● ●●● ●●● ●●● ●●●● ● ● ●● ● ●● ●● ●● ● ● ●●●● ●● ●● ● ●●●● ●● ● ●● ●● ● ●● ●● ● ●● ●●● ●●●●● ● ● ●●● ● ●●● ●●●● ●●●●●● ●● ●● ●● ●●● ●● ● ● ●●● ●● ●●● ●●● ●●●●● ●● ●● ●●● ●●● ● ●● ● ●● ●●● ●● ●●●● ●●● ●●● age (yrs) figure 7.6: residuals versus age in the model for rfft vs statins and age in the prevend data. 7.3. ev aluating the fit of a multiple regression model 339 since each case has one predicted value and one residual, regardless of the number of predictors, residuals can still be plotted against predicted values to assess the constant variance assumption. the scatterplot in the left panel of figure 7.7 shows that the variance of the residuals is slightly smaller for lower predicted values of rfft, but is otherwise approximately constant. just as in simple regression, normal probability plots can be used to check the normality assumption of the residuals. the normal probability plot in the right panel of figure 7.7 shows that the residuals from the model are reasonably normally distributed, with only slight departures from normality in the tails. 405060708090−60−40−200204060 predicted valueresidual●● ●● ● ●●●●● ●● ●● ●● ●●● ●●● ● ●● ●●●● ● ●●●●● ● ●● ●● ●●● ●● ●● ● ●●● ●●● ●●● ● ●● ●● ● ● ● ● ● ●●● ●● ●● ●● ● ●●● ● ●● ● ●● ●●●● ●● ● ●● ●●● ● ●●●● ●● ●●● ●●●● ●● ● ●● ●● ● ●●●● ●●●● ●●● ●● ●●● ●● ●●●●● ● ●● ● ●●●● ●●● ●● ●● ●● ● ●●● ● ●●●● ●● ●● ●● ●● ●● ● ●● ● ● ● ●● ● ●●● ●● ● ●● ●● ●● ● ●●●● ● ●●● ● ●●●● ● ●● ● ●● ●●● ● ●●● ●●●● ● ● ●● ●● ●●●● ●●●● ● ●●● ●● ●● ●●● ●● ●●●● ● ●● ● ●● ●●● ●● ●● ●●● ●● ●● ●● ●●● ● ●●● ●●● ●● ●●● ●●● ●● ● ● ● ●●● ● ●● ●● ● ●●● ● ● ●● ● ●●● ●●● ●● ●●● ●● ● ●●● ●● ● ● ●● ●● ●●● ● ●● ●● ●● ●●● ●●● ●●● ●●●● ● ● ●● ● ●● ●● ●● ● ● ●●●● ●● ●● ● ●●●● ●● ●●●●● ● ●● ●● ● ●● ●●● ●●●●● ● ● ●●● ● ●●● ●●●● ●●●●●● ●● ●● ●● ●●● ●● ● ● ●●● ●● ●●● ●●● ●●●●● ●● ●● ●●● ●●● ● ●● ●●● ●●● ● ● ●●●● ●●● ●●● theoretical quantilessample quantiles −3−2−10123−60−40−200204060 figure 7.7: residual plots from the linear model for rfft versus statin use and age in prevend.samp . 340 chapter 7. multiple linear regression example 7.4 section 1.7 featured a case study examining the evidence for ethnic discrimination in the amount of financial support o ffered by the state of california to individuals with developmental disabilities. although an initial look at the data suggested an association between expenditures and ethnicity , further analysis suggested that age is a confounding variable for the relationship. a multiple regression model can be fit to these data to model the association between expenditures , age, and ethnicity in a subset that only includes data from hispanics and white non-hispanics. two residual plots from the model fit for e(expenditures) = 0+ ethnicity (ethnicity) + age(age) are shown in figure 7.8. from these plots, assess whether a linear regression model is appropriate for these data. the model assumptions are clearly violated. the residual versus fitted plot shows obvious patterns; the residuals do not scatter randomly about the y= 0 line. additionally, the variance of the residuals is not constant around the y= 0 line. as shown in the normal probability plot, the residuals show marked departures from normality, particularly in the upper tail; although this skewing may be partially resolved with a log transformation, the patterns in the residual versus fitted plot are more problematic. recall that a residual is the di fference between an observed value and expected value; for an observationi, the residual equals yi\u0000ˆyi. positive residuals occur when a model’s predictions are smaller that the observed values, and vice versa for negative residuals. in the residual versus fitted plot, it can be seen that in the middle range of predicted values, the model consistently under-predicts expenditures; on the upper and lower ends, the model over-predicts. this is a particularly serious issue with the model fit. a single linear regression model is not appropriate for these data. for a more detailed examination of the model residuals, refer to chapter 7, lab 2. with some subsetting according to age cohort, it can be reasonable to use linear regression for modeling these data. 020000 60000−20000−100000100002000030000residual● ●●●●●● ●●●● ●●● ●● ● ●● ●●● ●● ●●● ●● ● ●●● ●●● ● ●●● ●● ●● ●● ●● ●●● ●● ●●●●●● ●● ●●● ● ●●● ● ●● ● ●● ●● ●●●● ●●●● ● ●●●● ● ● ●● ●● ●● ●●●●● ● ●●● ● ●● ●●●● ●●● ● ●●● ● ● ●● ● ●●● ●● ●● ●● ● ●● ●● ●●●●●●● ●●●● ●● ●● ● ●● ●● ●●● ● ●●● ●●● ●● ●● ●● ●●●●● ●●●● ●●●● ●● ●●●● ●●● ● ●●● ● ● ● ●●●●● ●● ●● ●●●● ● ●●● ● ●●●● ●●● ●● ●● ●● ● ●●●●●● ●●● ●●● ● ●●●●● ●●● ●● ●●●●● ●● ●● ●● ●●● ●●●● ●●● ●●● ●●● ●●● ●●● ●●●● ● ●● ● ●●● ● ● ● ●● ● ●●● ● ●●● ●●●● ● ● ● ●● ●●● ●●●● ●●● ●●● ●●●●● ●●● ●●●● ● ●●● ●●● ●●●● ● ●●●● ●●● ● ●●● ●●● ●● ●●●● ●● ● ●●●● ●●●● ●● ●●● ●● ●● ● ● ●●● ●● ●●● ●●●● ●● ●●●● ● ● ●●● ● ● ●● ●●●● ●● ●● ●● ●●● ●● ●● ●●● ●● ● ●●● ●● ●●●●●● ●● ● ●●●● ● ● ●●●●● ●●●● ●●●● ●●●● ● ● ●●● ● ●● ●● ●●●●● ●● ●● ●● ●● ●● ●● ● ●● ● ●● ●● ● ●● ● ●● ●●● ● ● ●● ● ●● ●●●● ●● ●● ● ●●●● ● ●●● ● ●● ●●● ● ● ● ● ●● ●● ● ● ●●● ●● ●●● ● ● ● ●●●● ● ● ● ●●● ●●● ●● ●● ● ●●● ●● ●●● ●● ●● ●●●● ● ●● ●● ●● ●● ●●● ●● ●● ●●● ●● ●●●● ●●● ●● ●● ●●● ● ●● ● ●●●● ●● ●●●● ●●●●● ● ●● ●● ● ●●●● ●●●● ●●● ●●● ● ● ●●● ●● ● ●●● ● ●●● ●●● ●●● ●●● ●●● ● ●● ● ●●●●● ●● ● ●● ●● ● ●●● ●● ●●● ● ●●●●● ● ● ●● ● ●● ● ● ●● ● ●● ● ●●●● ● ●●● ●● sample quantiles −3−10123−20000−100000100002000030000 theoretical quantiles figure 7.8: residual versus fitted values plot and residual normal probability plot from the linear model for expenditures versus ethnicity and age for a subset ofdds.discr . 7.3. ev aluating the fit of a multiple regression model 341 7.3.2 using r2r2r2and adjusted r2r2r2with multiple regression section 6.3.2 provided two definitions of the r2statistic—it is the square of the correlation coefficientrbetween a response and the single predictor in simple linear regression, and equivalently, it is the proportion of the variation in the response variable explained by the model. in statistical terms, the second definition can be written as r2=var(yi)\u0000var(ei) var(yi)= 1\u0000var(ei) var(yi); whereyiandeidenote the response and residual values for the ithcase. the first definition cannot be used in multiple regression, since there is a correlation coefficient between each predictor and the response variable. however, since there is a single set of residuals, the second definition remains applicable. althoughr2can be calculated directly from the equation, it is rarely calculated by hand since statistical software includes r2as a standard part of the summary output for a regression model.10 in the model with response rfft and predictors statin and age,r2= 0:2852. the model explains almost 29% of the variability in rfft scores, a considerable improvement over the model with statin alone (r2= 0:0239). adding a variable to a regression model always increases the value of r2. sometimes that increase is large and clearly important, such as when age is added to the model for rfft scores. in other cases, the increase is small, and may not be worth the added complexity of including another variable. the adjusted r-squared is often used to balance predictive ability with complexity in a multiple regression model. like r2, the adjusted r2is routinely provided in software output. adjusted r2as a tool for model assessment the adjusted r2is computed as r2 adj= 1\u0000var(ei)=(n\u0000p\u00001) var(yi)=(n\u00001)= 1\u0000var(ei) var(yi)\u0002n\u00001 n\u0000p\u00001; wherenis the number of cases used to fit the model and pis the number of predictor variables in the model. essentially, the adjusted r2imposes a penalty for including additional predictors that do not contribute much towards explaining the observed variation in the response variable. the value of the adjusted r2in the model with both statin and ageis 0.2823, which is essentially the same as ther2value of 0.2852. the additional predictor ageconsiderably increases the strength of the model, resulting in only a small penalty to the r2value. while the adjusted r2is useful as a statistic for comparing models, it does not have an inherent interpretation like r2. students often confuse the interpretation of r2and adjusted r2; while the two are similar, adjusted r2isnotthe proportion of variation in the response variable explained by the model. the use of adjusted r2for model selection will be discussed in section 7.8. 10inrand other software, r2is typically labeled ’multiple r-squared’. 342 chapter 7. multiple linear regression 7.4 the general multiple linear regression model this section provides a compact summary of the multiple regression model and contains more mathematical detail than most other sections; the next section, section 7.5, discusses categorical predictors with more than two levels. the ideas outlined in this section and the next are illustrated with an extended analysis of the prevend data in section 7.6. 7.4.1 model parameters and least squares estimation for multiple regression, the data consist of a response variable yandpexplanatory variables x1;x2;:::;xp. instead of the simple regression model y= 0+ 1x+\"; multiple regression has the form y= 0+ 1x1+ 2x2+ 3x3+\u0001\u0001\u0001+ pxp+\"; or equivalently e(y) = 0+ 1x1+ 2x2+ 3x3+\u0001\u0001\u0001+ pxp; since the normally distributed error term \"is assumed to have mean 0. each predictor xihas an associated coe fficient i. in simple regression, the slope coe fficient captures the change in the response variable yassociated with a one unit change in the predictor x. in multiple regression, the coe fficient jof a predictor xjdenotes the change in the response variable yassociated with a one unit change in xjwhen none of the other predictors change; i.e., each coefficient in multiple regression plays the role of a slope, as long as the other predictors are not changing. multiple regression can be thought of as the model for the mean of the response yin a population where the mean depends on the values of the predictors, rather than being constant. for example, consider a setting with two binary predictors such as statin use and sex; the predictors partition the population into four subgroups, and the four predicted values from the model are estimates of the mean in each of the four groups. 7.4. the general multiple linear regression model 343 guided practice 7.5 figure 7.9 shows an estimated regression model for rfft with predictors statin and gender , where gender is coded 0 for males and 1 for females.11based on the model, what are the estimated mean rfft scores for the four groups defined by these two categorical predictors?12 estimate std. error t value pr( >jtj) (intercept) 70.4068 1.8477 38.11 0.0000 statin -9.9700 2.9011 -3.44 0.0006 gender 0.6133 2.4461 0.25 0.8021 figure 7.9: rsummary output for the multiple regression model of rfft versus statin use and sex in prevend.samp . datasets for multiple regression have ncases, usually indexed algebraically by i, whereitakes on values from 1 to n; 1 denotes the first case in the dataset and ndenotes the last case. the dataset prevend.samp containsn= 500 observations. algebraic representations of the data must indicate both the case number and the predictor in the set of ppredictors. for case iin the dataset, the variablexijdenotes predictor xj; the response for case iis simplyyi, since there can only be one response variable. the dataset prevend.samp has many possible predictors, some of which are examined later in this chapter. the analysis in section 7.2 used p= 2 predictors, statin and age. just as in chapter 2, upper case letters are used when thinking of data as a set of random observations subject to sampling from a population, and lower case letters are used for observed values. in a dataset, it is common for each row to contain the information on a single case; the observations in row iof a dataset with ppredictors can be written as ( yi;xi1;xi2;:::;xip). for any given set of estimates b1;b2;:::;bpand predictors xi1;xi2;:::;xip, predicted values of the response can be calculated using ˆyi=b0+b1xi1+b2xi2+\u0001\u0001\u0001+bpxip; whereb0;b1;:::;bpare estimates of the coe fficients 0; 1;:::; pobtained using the principle of least squares estimation. as in simple regression, each prediction has an associated residual, which is the di fference between the observed value yiand the predicted value ˆyi, orei=yi\u0000ˆyi. the least squares estimate of the model is the set of estimated coe fficientsb0;b1;:::bpthat minimizes e2 1+e2 2+\u0001\u0001\u0001e2n. explicit formulas for the estimates involve advanced matrix theory, but are rarely used in practice. instead, estimates are calculated using software such as as r, stata, or minitab. 11until recently, it was common practice to use gender to denote biological sex. gender is di fferent than biological sex, but this text uses the original names in published datasets. 12the prediction equation for the model is rfft = 70:41\u00009:97(statin) + 0 :61(gender). both statin and gender can take on values of either 0or1; the four possible subgroups are statin non-user / male ( 0,0), statin non-user / female ( 0,1), statin user / male ( 1,0), statin user / female ( 1,1). predicted rfft scores for these groups are 70.41, 71.02, 60.44, and 61.05, respectively. 344 chapter 7. multiple linear regression 7.4.2 hypothesis tests and confidence intervals usingt-tests for individual coe fficients the test of the null hypothesis h0: k= 0 is a test of whether the predictor xkis associated with the response variable. when a coe fficient of a predictor equals 0, the predicted value of the response does not change when the predictor changes; i.e., a value of 0 indicates there is no association between the predictor and response. due to the inherent variability in observed data, an estimated coefficientbkwill almost never be 0 even when the model coe fficient kis. hypothesis testing can be used to assess whether the estimated coe fficient is significantly di fferent from 0 by examining the ratio of the estimated coe fficient to its standard error. when the assumptions of multiple regression hold, at least approximately, this ratio has a t-distribution with n\u0000(p+ 1) =n\u0000p\u00001 degrees of freedom when the model coe fficient is 0. the formula for the degrees of freedom follows a general rule that appears throughout statistics—the degrees of freedom for an estimated model is the number of cases in the dataset minus the number of estimated parameters. there are p+1 parameters in the multiple regression model, one for each of theppredictors and one for the intercept. sampling distributions of estimated coefficients suppose ˆy=b0+b1xi+b2xi+\u0001\u0001\u0001+bpxi is an estimated multiple regression model from a dataset with nobservations on the response and predictor variables, and let bkbe one of the estimated coe fficients. under the hypothesis h0: k= 0, the standardized statistic bk s.e.(bk) has at-distribution with n\u0000p\u00001 degrees of freedom. this sampling distribution can be used to conduct hypothesis tests and construct confidence intervals. testing a hypothesis about a regression coefficient a test of the two-sided hypothesis h0: k= 0 vs.ha: k,0 is rejected with significance level when jbkj s.e.(bk)>t? df; wheret? dfis the point on a t-distribution with n\u0000p\u00001 degrees of freedom and area (1 \u0000 =2) in the left tail. 7.4. the general multiple linear regression model 345 for one-sided tests, t? dfis the point on a t-distribution with n\u0000p\u00001 degrees of freedom and area (1\u0000 ) in the left tail. a one-sided test of h0againstha: k>0 rejects when the standardized coefficient is greater than t? df; a one-sided test of h0againstha: k<0 rejects when the standardized coefficient is less than t? df. confidence intervals for regression coefficient a two-sided 100(1 \u0000 )% confidence interval for the model coe fficient kis bk\u0006s.e.(bk)\u0002t? df: all statistical software packages provide an estimate sof the standard deviation of the residuals\u000f. thef-statistic for an overall test of the model when all the model coe fficients are 0, the predictors in the model, considered as a group, are not associated with the response; i.e., the response variable is not associated with any linear combination of the predictors. the f-statistic is used to test this null hypothesis of no association, using the following idea. the variability of the predicted values about the overall mean response can be estimated by msm =p i(ˆyi\u0000y)2 p: in this expression, pis the number of predictors and is the degrees of freedom of the numerator sum of squares (derivation not given here). the term msm is called the model sum of squares because it reflects the variability of the values predicted by the model ( ˆyi) about the mean ( y) response.13 in an extreme case, msm will have value 0 when all the predicted values coincide with the overall mean; in this scenario, a model would be unnecessary for making predictions, since the average of all observations could be used to make a prediction. the variability in the residuals can be measured by mse =p i(yi\u0000ˆyi)2 n\u0000p\u00001: mse is called the mean square of the errors since residuals are the observed ‘errors’, the di fferences between predicted and observed values. when msm is small compared to mse, the model has captured little of the variability in the data, and the model is of little or no value. the f-statistic is given by f=msm mse: the formula is not used for calculation, since the numerical value of the f-statistic is a routine part of the output of regression software. 13it turns out that yis also the mean of the predicted values. 346 chapter 7. multiple linear regression thefff-statistic in regression thef-statistic in regression is used to test the null hypothesis h0: 1= 2=\u0001\u0001\u0001= p= 0 against the alternative that at least one of the coe fficients is not 0. under the null hypothesis, the sampling distribution of the f-statistic is an f-distribution with parameters ( p;n\u0000p\u00001), and the null hypothesis is rejected if the value of the f-statistic is in the right tail of the distribution of the sampling distribution with area , where is the significance level of the test. thef-test is inherently one-sided—deviations from the null hypothesis of any form will push the statistic to the right tail of the f-distribution. the p-value from the right tail of the f-distribution should never be doubled. students also sometimes make the mistake of assuming that if the null hypothesis of the f-test is rejected, all coe fficients must be non-zero, instead of at least one. a significant p-value for the f-statistic suggests that the predictor variables in the model, when considered as a group, are associated with the response variable. in practice, it is rare for the f-test not to reject the null hypothesis, since most regression models are used in settings where a scientist has prior evidence that at least some of the predictors are useful. confidence and prediction intervals the confidence and prediction intervals discussed in section 6.5 can be extended to multiple regression. predictions based on specific values of the predictors are made by evaluating the estimated model at those values, and both confidence intervals for the mean and prediction intervals for a new observation are constructed using the corresponding standard errors. the formulas for standard errors in the multiple predictor setting are beyond the scope of this text, and there are no simple approximate formulas that can be calculated by hand. they are always computed in software. figure 7.5 shows the estimated regression model used to examine the association of age and statin use with rfft score in prevend. as shown in example 7.5, the predicted rfft score for a 67-year-old statin user is 57.6 points. software can be used to show that a 95% confidence interval for the mean rfft score for 67-year-old statin users is (49.2, 58.9) points, while a 95% prediction interval for the rfft score of a particular 67-year statin user is (7.8, 99.4) points. just as with simple linear regression, the prediction interval is wider than the confidence interval for the mean because it accounts for both variability in the estimated mean and variability in a new observation of the response, rfft score. 7.5. categorical predictors with several levels 347 7.5 categorical predictors with several levels in the initial model fit with the prevend data, the variable statin is coded 0if the participant was not using statins, and coded 1if the participant was a statin user. the category coded 0 is referred to as the reference category; in this model, statin non-users ( statin = 0 ) are the reference category. the estimated coe fficient statin is the change in the average response between the reference category and the category statin = 1 . since the variable statin is categorical, the numerical codes 0and 1are simply labels for statin non-users and users. the labels can be specified more explicitly in software. for example, in r, categorical variables can be coded as factors; the levels of the variable are displayed as text (such as \"nonuser\" or \"user\"), while the data remain stored as integers. the routput with the variable statin.factor is shown in figure 7.10, where 0corresponds to the label \"nonuser\" and 1corresponds to \"user\". the predictor variable is now labeled statin.factoruser ; the estimate -10.05 is the change in mean rfft from the \"nonuser\" (reference) category to the \"user\" category. note how the reference category is not explicitly labeled; instead, it is contained within the intercept. estimate std. error t value pr( >jtj) (intercept) 70.7143 1.3808 51.21 0.0000 statin.factoruser -10.0534 2.8792 -3.49 0.0005 figure 7.10: rsummary output for the simple regression model of rfft versus statin use in prevend.samp , with statin converted to a factor called statin.factor that has levels nonuser and user . for a categorical variable with two levels, estimates from the regression model remain the same regardless of whether the categorical predictor is treated as numerical or not. a \"one unit change\" in the numerical sense corresponds exactly to the switch between the two categories. however, this is not true for categorical variables with more than two levels. this idea will be explored with the categorical variable education , which indicates the highest level of education that an individual completed in the dutch educational system: primary school, lower secondary school, higher secondary education, or university education. in the prevend dataset, educational level is coded as either 0,1,2, or3, where 0denotes at most a primary school education, 1a lower secondary school education, 2a higher secondary education, and 3a university education. figure 7.11 shows the distribution of rfft by education level; rfft scores tend to increase as education level increases. in a regression model with a categorical variable with more than two levels, one of the categories is set as the reference category, just as in the setting with two levels for a categorical predictor. the remaining categories each have an estimated coe fficient, which corresponds to the estimated change in response relative to the reference category. 348 chapter 7. multiple linear regression education levelrfft score 0 1 2 3020406080100120140 figure 7.11: box plots for rfft score by education level in prevend.samp . example 7.6 is rfft score associated with educational level? interpret the coe fficients from the following model. figure 7.12 provides the routput for the regression model of rfft versus educational level in prevend.samp . the variable education has been converted to education.factor , which has levels primary ,lowersecond ,highersecond , and univ . it is clearest to start with writing the model equation: rfft = 40:94 + 14:78(edulowersecond) + 32 :13(eduhighersecond) + 44 :96(eduuniv) each of the predictor levels can be thought of as binary variables that can take on either 0or1, where only one level at most can be a 1and the rest must be 0, with 1corresponding to the category of interest. for example, the predicted mean rfft score for individuals in the lower secondary group is given by rfft = 40:94 + 14:78(1) + 32:13(0) + 44:96(0) = 55:72: the value of the lowersecond coefficient, 14.78, is the change in predicted mean rfft score from the reference category primary to the lowersecond category. participants with a higher secondary education scored approximately 32.1 points higher on the rfft than individuals with only a primary school education, and have estimated mean rfft score 40:94 + 32:13 = 73:07:those with a university education have estimated mean rfft score 40 :94 + 44:96 = 85:90. the intercept value, 40.94, corresponds to the estimated mean rfft score for individuals who at most completed primary school. from the regression equation, rfft = 40:94 + 14:78(0) + 32:13(0) + 44:96(0) = 40:94: thep-values indicate that the change in mean score between participants with only a primary school education and any of the other categories is statistically significant. 7.5. categorical predictors with several levels 349 estimate std. error t value pr( >jtj) (intercept) 40.9412 3.2027 12.78 0.0000 education.factorlowersecond 14.7786 3.6864 4.01 0.0001 education.factorhighersecond 32.1335 3.7631 8.54 0.0000 education.factoruniv 44.9639 3.6835 12.21 0.0000 figure 7.12: rsummary output for the regression model of rfft versus educational level in prevend.samp , with education converted to a factor called education.factor that has levels primary ,lowersecond ,highersecond , and univ . example 7.7 suppose that the model for predicting rfft score from educational level is fitted with education , using the original numerical coding with 0,1,2, and 3; theroutput is shown in figure 7.13. what does this model imply about the change in mean rfft between groups? explain why this model is flawed. according to this model, the change in mean rfft between groups increases by 15.158 for any one unit change in education . for example, the change in means between the groups coded 0and 1 is necessarily equal to the change in means between the groups coded 2and 3, since the predictor changes by 1 in both cases. it is unreasonable to assume that the change in mean rfft score when comparing the primary school group to the lower secondary group will be equal to the di fference in means between the higher secondary group and university group. the numerical codes assigned to the groups are simply short-hand labels, and are assigned arbitrarily. as a consequence, this model would not provide consistent results if the numerical codes were altered; for example, if the primary school group and lower secondary group were relabeled such that the predictor changes by 2, the estimated di fference in mean rfft would change. estimate std. error t value pr( >jtj) (intercept) 41.148 2.104 19.55 0.0000 education 15.158 1.023 14.81 0.0000 figure 7.13: rsummary output for the simple regression model of rfft versus educational level in prevend.samp , where education is treated as a numerical variable. note that it would be incorrect to fit this model; figure 7.12 shows the results from the correct approach. categorical variables can be included in multiple regression models with other predictors, as is shown in the next section. section 7.9 discusses the connection between anov a and regression models with only one categorical predictor. 350 chapter 7. multiple linear regression 7.6 reanalyzing the prevend data the earlier models fit to examine the association between cognitive ability and statin use showed that considering statin use alone could be misleading. while older participants tended to have lower rfft scores, they were also more likely to be taking statins. age was found to be a confounder in this setting—is it the only confounder? potential confounders are best identified by considering the larger scientific context of the analysis. for the prevend data, there are two natural candidates for potential confounders: education level and presence of cardiovascular disease. the use of medication is known to vary by education levels, often because individuals with more education tend to have higher incomes and consequently, better access to health care; higher educational levels are associated with higher rfft scores, as shown by model 7.12. individuals with cardiovascular disease are often prescribed statins to lower cholesterol; cardiovascular disease can lead to vascular dementia and cognitive decline. figure 7.14 contains the result of a regression of rfft with statin use, adding the possible confounders age, educational level, and presence of cardiovascular disease. the variables statin , education and cvdhave been converted to factors, and ageis a continuous predictor. the coe fficient for statin use shows the importance of adjusting for confounders. in the initial model for rfft that only included statin use as a predictor, statin use was significantly associated with decreased rfft scores. after adjusting for age, statins were no longer significantly associated with rfft scores, but the model suggested that statin use could be associated with increased rfft scores. this final model suggests that, after adjusting for age, education, and the presence of cardiovascular disease, statin use is associated with an increase in rfft scores of approximately 4.7 points. the p-value for the slope coe fficient for statin use is 0.056, which suggests moderately strong evidence of an association (significant at = 0:10, but not = 0:05). estimate std. error t value pr( >jtj) (intercept) 99.0351 6.3301 15.65 0.0000 statin.factoruser 4.6905 2.4480 1.92 0.0559 age -0.9203 0.0904 -10.18 0.0000 education.factorlowersecond 10.0883 3.3756 2.99 0.0029 education.factorhighersecond 21.3015 3.5777 5.95 0.0000 education.factoruniv 33.1246 3.5471 9.34 0.0000 cvd.factorpresent -7.5665 3.6516 -2.07 0.0388 figure 7.14: rsummary output for the multiple regression model of rfft versus statin use, age, education, and presence of cardiovascular disease in prevend.samp . ther2for the model is 0.4355; a substantial increase from the model with only statin use and age as predictors, which had an r2of 0.2852. the adjusted r2for the model is 0.4286, close to the r2value, which suggests that the additional predictors increase the strength of the model enough to justify the additional complexity. 7.6. reanalyzing the prevend data 351 figure 7.15 shows a plot of residuals vs predicted rfft scores from the model in figure 7.14 and a normal probability plot of the residuals. these plots show that the model fits the data reasonably well. the residuals show a slight increase in variability for larger predicted values, and the normal probability plot shows the residuals depart slightly from normality in the extreme tails. model assumptions never hold exactly, and the possible violations shown in this figure are not sufficient reasons to discard the model. 20406080100−60−40−200204060 predicted valueresidual●● ●● ● ●●●● ● ●● ●● ●● ● ●● ●●● ● ●● ●●● ● ●●●●●● ●●● ● ●●●● ●● ●● ● ●● ● ●●● ●●● ● ●● ●● ●● ● ●●●●● ●● ●● ●●● ●●● ●●●● ●● ●●●● ●●● ● ●●●● ● ● ●●● ●● ●●● ●● ●● ●●● ●●● ● ● ● ●●● ●●●● ●●● ●● ●●● ●● ●●●● ● ●●● ● ●●●●●● ● ●●●● ●● ● ●●● ● ● ●●● ● ● ●● ●● ●● ●● ● ●● ●●● ●● ● ●●● ●● ●●● ●● ●● ● ●● ●● ● ●●● ● ●●●● ● ●● ●●●●●● ●●●● ● ●●● ●● ●● ●● ● ●●●● ●●●● ●● ● ●● ●● ●●● ●● ●●●● ● ●● ● ●● ●●● ●● ●● ● ●● ●● ●● ●● ●●● ● ●●● ●●● ●● ● ●● ● ●● ●●● ● ● ●●● ● ●●●● ● ●● ● ● ● ●● ●● ●● ●●● ●● ●●● ●● ● ●● ●● ●● ● ●●● ●●●● ● ●● ●● ●● ●●● ●●● ●●● ●●●● ●● ●●● ●●● ●● ●● ● ●●●● ●● ●● ● ●● ●● ●● ●●●●● ● ●●●● ● ●● ●●● ●● ● ●● ●● ●●● ● ●●● ●●●● ●● ●●●● ●● ●●●● ●● ● ●● ●● ●● ● ●● ●●● ●●● ●●●● ● ●●●● ●●● ● ●● ● ●● ●●● ●●● ●● ● ●●● ●● ●● ●● theoretical quantilessample quantiles −3−2−10123−60−40−200204060 figure 7.15: a histogram and normal probability plot of the residuals from the linear model for rfft vs. statin use, age, educational level and presence of cardiovascular disease in the prevend data. it is quite possible that even the model summarized in figure 7.14 is not the best one to understand the association of cognitive ability with statin use. there be other confounders that are not accounted for. possible predictors that may be confounders but have not been examined are called residual confounders . residual confounders can be other variables in a dataset that have not been examined, or variables that were not measured in the study. residual confounders exist in almost all observational studies, and represent one of the main reasons that observational studies should be interpreted with caution. a randomized experiment is the best way to eliminate residual confounders. randomization ensures that, at least on average, all predictors are not associated with the randomized intervention, which eliminates one of the conditions for confounding. a randomized trial may be possible in some settings; there have been many randomized trials examining the effect of using statins. however, in many other settings, such as a study of the association of marijuana use and later addiction to controlled substances, randomization may not be possible or ethical. in those instances, observational studies may be the best available approach. 352 chapter 7. multiple linear regression 7.7 interaction in regression an important assumption in the multiple regression model y= 0+ 1x1+ 2x2+:::+ pxp+\" is that when one of the predictor variables xjchanges by 1 unit and none of the other variables change, the predicted response changes by j, regardless of the values of the other variables. a statistical interaction occurs when this assumption is not true, such that the relationship of one explanatory variable xjwith the response depends on the particular value(s) of one or more other explanatory variables. interaction is most easily demonstrated in a model with two predictors, where one of the predictors is categorical and the other is numerical.14consider a model that might be used to predict total cholesterol level from age and diabetes status (either diabetic or non-diabetic): e(totchol) = 0+ 1(age) + 2(diabetes): (7.8) figure 7.16 shows the routput for a regression estimating model 7.8, using data from a sample of 500 adults from the nhanes dataset ( nhanes.samp.adult.500 ). total cholesterol ( totchol ) is measured in mmol/l, ageis recorded in years, and diabetes is a factor level with the levels no (non-diabetic) and yes(diabetic) where 0corresponds to noand 1corresponds to yes. estimate std. error t value pr( >jtj) (intercept) 4.8000 0.1561 30.75 0.0000 age 0.0075 0.0030 2.47 0.0137 diabetesyes -0.3177 0.1607 -1.98 0.0487 figure 7.16: regression of total cholesterol on age and diabetes, using nhanes.samp.adult.500 . 14interaction e ffects between numerical variables and between more than two variables can be complicated to interpret. a more complete treatment of interaction is best left to a more advanced course; this text will only examine interaction in the setting of models with one categorical variable and one numerical variable. 7.7. interaction in regression 353 example 7.9 using the output in figure 7.16, write the model equation and interpret the coe fficients for age and diabetes. how does the predicted total cholesterol for a 60-year-old individual compare to that of a 50-year-old individual, if both have diabetes? what if both individuals do not have diabetes? totchol = 4:80 + 0:0075(age)\u00000:32(diabetesyes ) the coe fficient for age indicates that with each increasing year of age, predicted total cholesterol increases by 0.0075 mmol/l. the coe fficient for diabetes indicates that diabetics have an average total cholesterol that is 0.32 mmol/l lower than non-diabetic individuals. if both individuals have diabetes, then the change in predicted total cholesterol level can be determined directly from the coe fficient for age. an increase in one year of age is associated with a 0.0075 increase in total cholesterol; thus, an increase in ten years of age is associated with 10(0:0075) = 0:075 mmol/l increase in predicted total cholesterol. the calculation does not di ffer if both individuals are non-diabetic. according to the model, the relationship between age and total cholesterol remains the same regardless of the values of the other variable in the model. example 7.10 using the output in figure 7.16, write two separate model equations: one for diabetic individuals and one for non-diabetic individuals. compare the two models. for non-diabetics ( diabetes = 0 ), the linear relationship between average cholesterol and age is totchol = 4 :80 + 0:0075(age)\u00000:32(0) = 4:80 + 0:0075(age): for diabetics ( diabetes = 1 ), the linear relationship between average cholesterol and age is totchol = 4 :80 + 0:0075(age)\u00000:32(1) = 4:48 + 0:0075(age): the lines predicting average cholesterol as a function of age in diabetics and non-diabetics are parallel, with the same slope and di fferent intercepts. while predicted total cholesterol is higher overall in non-diabetics (as indicated by the higher intercept), the rate of change in predicted average total cholesterol by age is the same for both diabetics and non-diabetics. this relationship can be expressed directly from the model equation 7.8. for non-diabetics, the population regression line is e(totchol) = 0+ 1(age). for diabetics, the line is e(totchol) = 0+ 1(age)+ 2= 0+ 2+ 1(age). the lines have the same slope 1but intercepts 0and 0+ 2. 354 chapter 7. multiple linear regression however, a model that assumes the relationship between cholesterol and age does not depend on diabetes status might be overly simple and potentially misleading. figure 7.17(b) shows a scatterplot of total cholesterol versus age where the least squares models have been fit separately for non-diabetic and diabetic individuals. the blue line in the plot is estimated using only non-diabetic individuals, while the red line was fit using data from diabetic individuals. the lines are not parallel, and in fact, have slopes with di fferent signs. the plot suggests that among non-diabetics, age is positively associated with total cholesterol. among diabetics, however, age is negatively associated with total cholesterol. 20 30 40 50 60 70 803456789total choelsterol (mmol/l) age (yrs) (a) 20 30 40 50 60 70 803456789total cholesterol (mmol/l) age (yrs) (b) figure 7.17: scatterplots of total cholesterol versus age in nhanes.samp.adult.500 , where blue represents non-diabetics and red represents diabetics. plot (a) shows the model equations written out in example 7.10, estimated from the entire sample of 500 individuals. plot (b) shows least squares models that are fit separately; coe fficients of the blue line are estimated using only data from non-diabetics, while those of the red line are estimated using only data from diabetics. 7.7. interaction in regression 355 with the addition of another parameter (commonly referred to as an interaction term), a linear regression model can be extended to allow the relationship of one explanatory variable with the response to vary based on the values of other variables in the model. consider the model e(totchol) = 0+ 1(age) + 2(diabetes) + 3(diabetes\u0002age): (7.11) the interaction term allows the slope of the association with age to di ffer by diabetes status. among non-diabetics ( diabetes = 0), the model reduces to the earlier one, e(totchol) = 0+ 1(age): among the diabetic participants, the model becomes e(totchol) = 0+ 1(age) + 2+ 3(age) = 0+ 2+ ( 1+ 3)(age): unlike in the original model, the slopes of the population regression lines for non-diabetics and diabetics are now di fferent: 1versus 1+ 3. figure 7.18 shows the routput for a regression estimating model 7.11. in r, the syntax age:diabetesyes represents the (age \u0002diabetes) interaction term. estimate std. error t value pr( >jtj) (intercept) 4.6957 0.1597 29.40 0.0000 age 0.0096 0.0031 3.10 0.0020 diabetesyes 1.7187 0.7639 2.25 0.0249 age:diabetesyes -0.0335 0.0123 -2.73 0.0067 figure 7.18: regression of total cholesterol on age and diabetes with an interaction term, using nhanes.samp.adult.500 example 7.12 using the output in figure 7.18, write the overall model equation, the model equation for nondiabetics, and the model equation for diabetics. the overall model equation is totchol = 4 :70 + 0:0096(age) + 1 :72(diabetesyes)\u00000:034(age\u0002diabetesyes) : for non-diabetics ( diabetes = 0 ), the linear relationship between average cholesterol and age is totchol = 4 :70 + 0:0096(age) + 1 :72(0)\u00000:034(age\u00020) = 4:70 + 0:0096(age): for diabetics ( diabetes = 1 ), the linear relationship between average cholesterol and age is totchol = 4 :70 + 0:0096(age) + 1 :72(1)\u00000:034(age\u00021) = 6:42\u00000:024(age): 356 chapter 7. multiple linear regression the estimated equations for non-diabetic and diabetic individuals show the same qualitative behavior seen in figure 7.17(b), where the slope is positive in non-diabetics and negative in diabetics. however, note that the lines plotted in the figure were estimated from two separate model fits on non-diabetics and diabetics; in contrast, the equations from the interaction model are fit using data from all individuals. it is more e fficient to model the data using a single model with an interaction term than working with subsets of the data.15additionally, using a single model allows for the calculation of at-statistic and p-value that indicates whether there is statistical evidence of an interaction. the pvalue for the age:diabetes interaction term is significant at the = 0:05 level. thus, the estimated model suggests there is strong evidence for an interaction between age and diabetes status when predicting total cholesterol. residual plots can be used to assess the quality of the model fit. figure 7.19 shows that the residuals have roughly constant variance in the region with the majority of the data (predicted values between 4.9 and 5.4 mmol/l). however, there are more large positive residuals than large negative residuals, which suggests that the model tends to underpredict; i.e., predict values of totchol that are smaller than the observed values.16figure 7.20 shows that the residuals do not fit a normal distribution in the tails. in the right tails, the sample quantiles are larger than the theoretical quantiles, implying that there are too many large residuals. the left tail is a better fit; however, there are too few large negative residuals since the sample quantiles in the left tail are closer to 0 than the theoretical quantiles. 4.6 4.8 5.0 5.2 5.4 5.6−2−101234 predicted valueresidual figure 7.19: a scatterplot of residuals versus predicted values in the model for total cholesterol that includes age, diabetes status, and the interaction of age and diabetes status. 15in more complex settings, such as those with potential interaction between several variables or between two numerical variables, it may not be clear how to subset the data in a way that reveals interactions. this is another advantage to using an interaction term and single model fit to the entire dataset. 16recall that model residuals are calculated as yi\u0000ˆyi; i.e., totchol i\u0000totcholi. 7.7. interaction in regression 357 residuals−3−2−10123 theoretical quantilessample quantiles −3−2−10123−2−101234 figure 7.20: a histogram of the residuals and a normal probability plot of the residuals from the linear model for total cholesterol versus age, diabetes status, and the interaction of age and diabetes status. it is also important to note that the model explains very little of the observed variability in total cholesterol—the multiple r2of the model is 0.032. while the model falls well short of perfection, it may be reasonably adequate in applied settings. in the setting of a large study, such as one to examine factors a ffecting cholesterol levels in adults, a model like the one discussed here is typically a starting point for building a more refined model. given these results, a research team might proceed by collecting more data. regression models are commonly used as tools to work towards understanding a phenomenon, and rarely represent a ’final answer’. there are some important general points that should not be overlooked when interpreting this model. the data cannot be used to infer causality; the data simply show associations between total cholesterol, age, and diabetes status. each of the nhanes surveys are cross-sectional; they are administered to a sample of us residents with various ages and other demographic features during a relatively short period of time. no single individual has had his or her cholesterol levels measured over a period of many years, so the model slope for diabetes is not indicative of an individual’s cholesterol level declining (or increasing) with age. finally, the interpretation of a model often requires additional contextual information that is relevant to the study population but not captured in the dataset. what might explain increased age being associated with lower cholesterol for diabetics, but higher cholesterol for non-diabetics? the guidelines for the use of cholesterol-lowering statins suggest that these drugs should be prescribed more often in older individuals, and even more so in diabetic individuals. it is a reasonable speculation that the interaction between age and diabetes status seen in the nhanes data is a result of more frequent statin use in diabetic individuals. 358 chapter 7. multiple linear regression 7.8 model selection for explanatory models previously, multiple regression modeling was shown in the context of estimating an association while adjusting for possible confounders. another application of multiple regression is explanatory modeling, in which the goal is to construct a model that explains the observed variation in the response variable. in this context, there is no pre-specified primary predictor of interest; explanatory modeling is concerned with identifying predictors associated with the response. it is typically desirable to have a small model that avoids including variables which do not contribute much towards the r2. the intended use of a regression model influences the way in which a model is selected. approaches to model selection vary from those based on careful study of a relatively small set of predictors to purely algorithmic methods that screen a large set of predictors and choose a final model by optimizing a numerical criterion. algorithmic selection methods have gained popularity as researchers have been able to collect larger datasets, but the choice of an algorithm and the optimization criterion require more advanced material and are not covered here. this section illustrates model selection in the context of a small set of potential predictors using only the tools and ideas that have been discussed earlier in this chapter and in chapter 6. generally, model selection for explanatory modeling follows these steps: 1.data exploration. using numerical and graphical approaches, examine both the distributions of individual variables and the relationships between variables. 2.initial model fitting. fit an initial model with the predictors that seem most highly associated with the response variable, based on the data exploration. 3.model comparison. work towards a model that has the highest adjusted r2. – fit new models without predictors that were either not statistically significant or only marginally so and compare the adjusted r2between models; drop variables that decrease the adjusted r2. – if the initial set of variables is relatively small, it is prudent to add variables not in the initial model and check the adjusted r2; add variables that increase the adjusted r2. – examine whether interaction terms may improve the adjusted r2. 4.model assessment. use residual plots to assess the fit of the final model. the process behind model selection will be illustrated with a case study in which a regression model is built to examine the association between the abundance of forest birds in a habitat patch and features of a patch. 7.8. model selection for explanatory models 359 abundance of forest birds: introduction habitat fragmentation is the process by which a habitat in a large contiguous space is divided into smaller, isolated pieces; human activities such as agricultural development can result in habitat fragmentation. smaller patches of habitat are only able to support limited populations of organisms, which reduces genetic diversity and overall population fitness. ecologists study habitat fragmentation to understand its e ffect on species abundance. the forest.birds dataset in the oibiostat package contains a subset of the variables from a 1987 study analyzing the e ffect of habitat fragmentation on bird abundance in the latrobe valley of southeastern victoria, australia.17 the dataset consists of the following variables, measured for each of the 57 patches. –abundance : average number of forest birds observed in the patch, as calculated from several independent 20-minute counting sessions. –patch.area : patch area, measured in hectares. 1 hectare is 10,000 square meters and approximately 2.47 acres. –dist.nearest : distance to the nearest patch, measured in kilometers. –dist.larger : distance to the nearest patch larger than the current patch, measured in kilometers. –altitude : patch altitude, measured in meters above sea level. –grazing.intensity : extent of livestock grazing, recorded as either \"light\", \"less than average\", \"average\", \"moderately heavy\", or \"heavy\". –year.of.isolation : year in which the patch became isolated due to habitat fragmentation. –yrs.isolation : number of years since patch became isolated due to habitat fragmentation.18 the following analysis is similar to analyses that appear in logan (2011)19and quinn & keough (2002).20in the approach here, the grazing intensity variable is treated as a categorical variable; logan and quinn & keough treat grazing intensity as a numerical variable, with values 1-5 corresponding to the categories. the implications of these approaches are discussed at the end of the section. 17loyn, r.h. 1987. \"e ffects of patch area and habitat on bird abundances, species numbers and tree health in fragmented victorian forests.\" printed in nature conservation: the role of remnants of native vegetation. saunders da, arnold gw, burbridge aa, and hopkins ajm eds. surrey beatty and sons, chipping norton, nsw, 65-77, 1987. 18the loyn study completed data collection in 1983; yrs.isolation = 1983\u0000year.of.isolation . 19logan, m., 2011. biostatistical design and analysis using r: a practical guide. john wiley & sons, ch. 9. 20quinn, g.p . and keough, m.j., 2002. experimental design and data analysis for biologists. cambridge university press, ch. 6. 360 chapter 7. multiple linear regression data exploration the response variable for the model is abundance . numerical summaries calculated from software show that abundance ranges from 1.5 to 39.6. figure 7.21 shows that the distribution of abundance is bimodal, with modes at small values of abundance and at between 25 and 30 birds. the median (21.0) and mean (19.5) are reasonably close, which confirms the distribution is near enough to symmetric to be used in the model without a transformation. the boxplot confirms that the distribution has no outliers. abundancefrequency 0 10 20 30 40051015 (a) abundance 01020304050 (b) figure 7.21: a histogram (a) and boxplot (b) of abundance in the forest.birds data. there are six potential predictors in the model; the variable year.of.isolation is only used to calculate the more informative variable yrs.isolation . the plots in figure 7.22 reveal rightskewing in patch.area ,dist.nearest ,dist.larger , and yrs.isolation ; these might benefit from a log transformation. the variable altitude is reasonably symmetric, and the predictor grazing.factor is categorical and so does not take transformations. figure 7.23 shows the distributions of log.patch.area , log.dist.nearest ,log.dist.larger , and log.yrs.isolation , which were created through a natural log transformation of the original variables. all four are more nearly symmetric. these will be more suitable for inclusion in a model than the untransformed versions. 7.8. model selection for explanatory models 361 areafrequency 05001000 15000510152025 altitudefrequency 1001502002500510152025 dist.nearestfrequency 0500 1000 15000510152025 dist.largerfrequency 010002000300040000510152025 yrs.isolationfrequency 0204060801000510152025 light average heavy grazing.intensity0510152025 figure 7.22: histograms and a barplot for the potential predictors of abundance . log(area)frequency −2024680510152025 log(dist.nearest)frequency 345670510152025 log(dist.larger)frequency 3456780510152025 log(yrs.isolation)frequency 2.02.53.03.54.04.50510152025 figure 7.23: histograms of the log-transformed versions of patch.area , dist.nearest ,dist.larger , and yrs.isolation . 362 chapter 7. multiple linear regression ascatterplot matrix can be useful for visualizing the relationships between the predictor and response variables, as well as the relationships between predictors. each subplot in the matrix is a simple scatterplot; all possible plots are shown, except for the plots of a variable versus itself. the variable names are listed along the diagonal of the matrix, and the diagonal divides the matrix into symmetric plots. for instance, the first plot in the first row shows abundance on the vertical axis and log.area on the horizontal axis; the first plot in the first column shows abundance on the horizontal axis and log.area on the vertical axis. note that for readability, grazing.intensity appears with values 1 - 5, with 1 denoting \"light\" and 5 denoting \"heavy\" grazing intensity. the plots in the first row of figure 7.24 show the relationships between abundance and the predictors.21there is a strong positive association between abundance with log.area , and a strong negative association between abundance andlog.yrs.isolation . the variables log.dist.near.patch and log.dist.larger seem weakly positively associated with abundance . there is high variance of abundance and somewhat similar centers for the first four categories, but abundance does clearly tend to be lower in the \"high grazing\" category versus the others. abundance −20246 45678 2.02.53.03.54.04.5 01030−226 log.area log.dist.nearest 4567468 log.dist.larger altitude 1002002.03.04.0 log.yrs.isolation010203040 4567 100150200250 1234512345 grazing.intensity figure 7.24: scatterplot matrix of abundance and the possible predictors: log.area ,log.dist.near.patch ,log.dist.larger.patch ,altitude , log.yrs.isolation , and grazing.intensity . 21traditionally, the response variable (i.e., the dependent variable) is plotted on the vertical axis; as a result, it seems more natural to look at the first row where abundance is on they-axis. it is equally valid, however, to assess the association ofabundance with the predictors from the plots in the first column. 7.8. model selection for explanatory models 363 the variables log.dist.nearest and log.dist.larger appear strongly associated; a model may only need one of the two, as they may be essentially \"redundant\" in explaining the variability in the response variable.22in this case, however, since both are only weakly associated with abundance , both may be unnecessary in a model. a numerical approach confirms some of the features observable from the scatterplot matrix. figure 7.25 shows the correlations between pairs of numerical variables in the dataset. correlations between abundance and log.area and between abundance and log.yrs.isolation are relatively high, at 0.74 and -0.48, respectively. in contrast, the correlation between abundance and the two variables log.dist.nearest and log.dist.larger are much smaller, at 0.13 and 0.12. additionally, the two potential predictors log.dist.nearest and log.dist.larger have a relatively high correlation of 0.60. abundance log.area log.dist.nearest log.dist.larger altitude log.yrs.isolation abundance 1.00 0.74 0.13 0.12 0.39 -0.48 log.area 0.74 1.00 0.30 0.38 0.28 -0.25 log.dist.nearest 0.13 0.30 1.00 0.60 -0.22 0.02 log.dist.larger 0.12 0.38 0.60 1.00 -0.27 0.15 altitude 0.39 0.28 -0.22 -0.27 1.00 -0.29 log.yrs.isolation -0.48 -0.25 0.02 0.15 -0.29 1.00 figure 7.25: a correlation matrix for the numerical variables in forest.birds . initial model fitting based on the data exploration, the initial model should include the variables log.area ,altitude , log.yrs.isolation , and grazing.intensity ; a summary of this model is shown in figure 7.26. the r2and adjusted r2for this model are, respectively, 0.728 and 0.688. the model explains about 73% of the variability in abundance . estimate std. error t value pr( >jtj) (intercept) 14.1509 6.3006 2.25 0.0293 log.area 3.1222 0.5648 5.53 0.0000 altitude 0.0080 0.0216 0.37 0.7126 log.yrs.isolation 0.1300 1.9193 0.07 0.9463 grazing.intensityless than average 0.2967 2.9921 0.10 0.9214 grazing.intensityaverage -0.1617 2.7535 -0.06 0.9534 grazing.intensitymoderately heavy -1.5936 3.0350 -0.53 0.6019 grazing.intensityheavy -11.7435 4.3370 -2.71 0.0094 figure 7.26: initial model: regression of abundance onlog.area ,altitude , log.yrs.isolation and grazing.intensity . two of the variables in the model are not statistically significant at the = 0:05 level: altitude and log.yrs.isolation . only one of the categories of grazing.intensity (heavy grazing) is highly significant. 22typically, the predictor that is less strongly correlated with the response variable is the one that is \"redundant\" and will be statistically insignificant when included in a model with the more strongly correlated predictor. this is not always the case, and depends on the other variables in the model. 364 chapter 7. multiple linear regression model comparison first, fit models excluding the predictors that were not statistically significant: altitude and log.yrs.isolation . models excluding either variable have adjusted r2of 0.69, and a model excluding both variables has an adjusted r2of 0.70, a small but noticeable increase from the initial model. this suggests that these two variables can be dropped. at this point, the working model includes only log.area and grazing.intensity ; this model has r2= 0:727 and is shown in figure 7.27. estimate std. error t value pr( >jtj) (intercept) 15.7164 2.7674 5.68 0.0000 log.area 3.1474 0.5451 5.77 0.0000 grazing.intensityless than average 0.3826 2.9123 0.13 0.8960 grazing.intensityaverage -0.1893 2.5498 -0.07 0.9411 grazing.intensitymoderately heavy -1.5916 2.9762 -0.53 0.5952 grazing.intensityheavy -11.8938 2.9311 -4.06 0.0002 figure 7.27: working model: regression of abundance onlog.area and grazing.intensity . it is prudent to check whether the two distance-related variables that were initially excluded might increase the adjusted r2, even though this seems unlikely. when either or both of these variables are added, the adjusted r2decreases from 0.70 to 0.69. thus, these variables are not added to the working model. in this working model, only one of the coe fficients associated with grazing intensity is statistically significant; when compared to the baseline grazing category (light grazing), heavy grazing is associated with a reduced predicted mean abundance of 11.9 birds (assuming that log.area is held constant). individual categories of a categorical variable cannot be dropped, so a data analyst has the choice of leaving the variable as is, or collapsing the variable into fewer categories. for this model, it might be useful to collapse grazing intensity into a two-level variable, with one category corresponding to the original classification of heavy, and another category corresponding to the other four categories; i.e., creating a version of grazing intensity that only has the levels \"heavy\" and \"not heavy\". this is supported by the data exploration; a plot of abundance versus grazing.intensity shows that the centers of the distributions of abundance in the lowest four grazing intensity categories are roughly similar, relative to the center in the heavy grazing category. the model with the binary version of grazing intensity, grazing.binary , is shown in figure 7.28. the model with grazing.binary has adjusted r2= 0:71, which is slightly larger than 0.70 in the more complex model with grazing.intensity ; the model explains 72% of the variability in abundance (r2= 0:724). incorporating an interaction term did not improve the model; adding a parameter for the interaction between log.area and grazing.binary decreased the adjusted r2to 0.709. thus, the model shown in figure 7.28 is the final model. estimate std. error t value pr( >jtj) (intercept) 15.3736 1.4507 10.60 0.0000 log.area 3.1822 0.4523 7.04 0.0000 grazing.binaryheavy -11.5783 1.9862 -5.83 0.0000 figure 7.28: final model: regression of abundance on log.area and grazing.binary . 7.8. model selection for explanatory models 365 model assessment the fit of a model can be assessed using various residual plots. figure 7.29 shows a histogram and normal probability plot of the residuals for the final model. both show that the residuals follow the shape of a normal density in the middle range (between -10 and 10) but fit less well in the tails. there are too many large positive and large negative values) residuals. residuals−15 −5051015 theoretical quantilessample quantiles −2−1012−15−10−50510 figure 7.29: histogram and normal probability plot of residuals in the model for abundance with predictors log.area and grazing.binary . figure 7.30 gives a more detailed look at the residuals, plotting the residuals against predicted values and against the two predictors in the model, log.area and grazing.level . recall that residual values closer to 0 are indicative of a more accurate prediction; positive values occur when the predicted value from the model is smaller than the observed value, and vice versa for negative values. residuals are a measure of the prediction error of a model. 10203040−15−10−50510 predicted valueresidual●●●●● ●● ● ●●● ●● ●● ●● ●● ●● ● ●● ●● ●●●● ●●● ● ●● ●●● ● ●●● ●●●●● ●●● ●●● ●● −20246−15−10−50510residual log.areanot heavy heavy−15−10−50510residual grazing.binary figure 7.30: scatterplots of residuals versus predicted values and residuals versuslog.area , and a side-by-side boxplot of residuals by grazing.binary . in the middle plot, red points correspond to values where grazing level is \"heavy\" and blue points correspond to \"not heavy\". 366 chapter 7. multiple linear regression in the left plot, the large positive and large negative residuals visible from figure 7.29 are evident; the large positive residuals occur across the range of predicted values, while the large negative residuals occur around 20 (predicted birds). the middle plot shows that the large positive and negative residuals occur at intermediate values of log.area ; i.e., for values of log.area between 0 and 4, or equivalently for values of area between exp(0) = 1 and exp(4) = 54 :5 hectares. in the same range, there are also relatively accurate predictions; most residuals are between -5 and 5. both the middle plot and the right plot show that the prediction error is smaller for patches with heavy grazing than for patches where grazing intensity was between \"light\" and \"moderately heavy\". patches with heavy grazing are represented with red points; note how the red points mostly cluster around the y= 0 line, with the exception of one outlier with a residual value of about 10. conclusions the relatively large r2for the final model (0.72) suggests that patch area and extent of grazing (either heavy or not) explain a large amount of the observed variability in bird abundance. of the features measured in the study, these two are the most highly associated with bird abundance. larger area is associated with an increase in abundance; when grazing intensity does not change, the model predicts an increase in average abundance by 3.18 birds for every one unit increase in log area (or equivalently, when area is increased by a factor of exp(1) = 2 :7). a patch with heavy grazing is estimated to have a mean abundance of about 11.58 birds lower than a patch that has not been heavily grazed. the residual plots imply that the final model may not be particularly accurate. for most observations, the predictions are accurate between \u00065 birds, but there are several instances of overpredictions as high as around 10 and under-predictions of about 15. additionally, the accurate and inaccurate predictions occur at similar ranges of of log.area ; if the model only tended to be inaccurate at a specific range, such as for patches with low area, it would be possible to provide clearer advice about when the model is unreliable. the residuals plots do suggest that the model is more reliable for patches with heavy grazing, although there is a slight tendency towards overprediction. based on these results, the ecologists might decide to proceed by collecting more data. currently, the model seems to adequately explain the variability in bird abundance for patches that have been heavily grazed, but perhaps there are additional variables that are associated with bird abundance, especially in patches that are not heavily grazed. adding these variables might improve model residuals, in addition to raising r2. final considerations might a model including all the predictor variables be better than the final model with only log.area and grazing.binary ? the model is shown in figure 7.31. the r2for this model is 0.729 and the adjusted r2is 0.676. while the r2is essentially the same as for the final model, the adjustedr2is noticeably lower. the residual plots in figure 7.32 do not indicate that this model is an especially better fit, although the residuals are slightly closer to normality. there would be little gained from using the larger model. in fact, there is an additional reason to avoid the larger model. when building regression models, it is important to consider that the complexity of a model is limited by sample size (i.e., the number of observations in the data). attempting to estimate too many parameters from a small dataset can produce a model with unreliable estimates; the model may be ’overfit’, in the sense that it fits the data used to build it particularly well, but will fail to generalize to a new set of data. methods for exploring these issues are covered in more advanced regression courses. 7.8. model selection for explanatory models 367 estimate std. error t value pr( >jtj) (intercept) 10.8120 9.9985 1.08 0.2852 log.area 2.9720 0.6587 4.51 0.0000 log.dist.near.patch 0.1390 1.1937 0.12 0.9078 log.dist.larger.patch 0.3496 0.9301 0.38 0.7087 altitude 0.0117 0.0233 0.50 0.6169 log.yrs.isolation 0.2155 1.9635 0.11 0.9131 grazing.intensityless than average 0.5163 3.2631 0.16 0.8750 grazing.intensityaverage 0.1344 2.9870 0.04 0.9643 grazing.intensitymoderately heavy -1.2535 3.2000 -0.39 0.6971 grazing.intensityheavy -12.0642 4.5657 -2.64 0.0112 figure 7.31: full model: regression of abundance on all 6 predictors in forest.birds . 010203040−15−10−50510 predicted valueresidual●● ●●● ●● ● ●●● ●● ●● ●● ●● ●● ●●● ●● ●●●● ●●● ● ●● ●●● ● ●●●● ●●●● ●●● ●●● ●● theoretical quantilessample quantiles −2−1012−15−10−50510 figure 7.32: residual plots for the full model of abundance that includes all predictors. a general rule of thumb is to avoid fitting a model where there are fewer than 10 observations per parameter; e.g., to fit a model with 3 parameters, there should be at least 30 observations in the data. in a regression context, all of the following are considered parameters: an intercept term, a slope term for a numerical predictor, a slope term for each level of a categorical predictor, and an interaction term. in forest.birds , there are 56 cases, but fitting the full model involves estimating 10 parameters. the rule of thumb suggests that for these data, a model can safely support at most 5 parameters. as mentioned earlier, other analyses of forest.birds have treated grazing.intensity as a numerical variable with five values. one advantage to doing so is to produce a more stable model; only one slope parameter needs to be estimated, rather than four. however, treating grazing.intensity as a numerical variable requires assuming that any one unit change is associated with the same change in population mean abundance ; under this assumption, a change between \"light\" and \"less than average\" (codes 1to2) is associated with the same change in population mean abundance as between \"moderately heavy\" to \"heavy\" (codes 4to5) grazing. previous model fitting has shown that this assumption is not supported by the data, and that changes in mean abundance between adjacent levels in grazing intensity are not constant. in this text, it is our recommendation that categorical variables should not be treated as numerical variables. 368 chapter 7. multiple linear regression 7.9 the connection between anova and regression regression with categorical variables and anov a are essentially the same method, but with some important di fferences in the information provided by the analysis. earlier in this chapter, the strength of the association between rfft scores and educational level was assessed with regression. figure 7.33 shows the results of an anov a to analyze the di fference in rfft scores between education groups. df sum sq mean sq f value pr( >f) as.factor(education) 3 115040.88 38346.96 73.30 0.0000 residuals 496 259469.32 523.12 figure 7.33: summary of anov a of rfft by education levels in this setting, the f-statistic is used to test the null hypothesis of no di fference in mean rfft score by educational level against the alternative that at least two of the means are di fferent. the f-statistic is 73.3 and highly significant. thef-statistic can also be calculated for regression models, although it has not been shown in the regression model summaries in this chapter. in regression, the f-statistic tests the null hypothesis that all regression coe fficients are equal to 0 against the alternative that least one of the coefficients is not equal to 0. although the phrasing of the hypotheses in anov a versus regression may seem di fferent initially, they are equivalent. consider the regression model for predicting rfft from educational level—each of the coe fficients in the model is an estimate of the di fference in mean rfft for a particular education level versus the baseline category of education = 0. a significant f-statistic indicates that at least one of the coe fficients is not zero; i.e., that at least one of the mean levels of rfft di ffers from the baseline category. if all the coe fficients were to equal zero, then the differences between the means would be zero, implying all the mean rfft levels are equal. it is reasonable, then, that the f-statistic associated with the rfft versus education regression model is also 73.3. the assumptions behind the two approaches are identical. both anov a and linear regression assume that the groups are independent, that the observations within each group are independent, that the response variable is approximately normally distributed, and that the standard deviations of the response are the same across the groups. the regression approach provides estimates of the mean at the baseline category (the intercept) and the di fferences of the means between each category and the baseline, along with a tstatistic and p-value for each comparison. from regression output, it is easy to calculate all the estimated means; to do the same with anov a requires calculating summary statistics for each group. additionally, diagnostic plots to check model assumptions are generally easily accessible in most computing software. 7.9. the connection between anov a and regression 369 why use anov a at all if fitting a linear regression model seems to provide more information? a case can be be made that the most important first step in analyzing the association between a response and a categorical variable is to compute and examine the f-statistic for evidence of any effect, and that only when the f-statistic is significant does it become appropriate to proceed to examine the nature of the di fferences. anov a displays the f-statistic prominently, emphasizing its importance. it is available in regression output, but may not always be easy to locate; the focus of regression is on the significance of the individual coe fficients. anov a has traditionally been used in carefully designed experiments. there are complex versions of anov a that are appropriate for experiments in which several di fferent factors are set at a range of levels. more complex versions of anov a are beyond the scope of this text and are covered in more advanced books. section 5.5 discussed the use of bonferroni corrections when testing hypotheses about pairwise di fferences among the group means when conducting anov a. in principle, bonferroni corrections can be applied in regression with categorical variables, but that is not often done. in designed experiments in which anov a has historically been used, the goal was typically to show definitively that a categorical predictor, often a treatment or intervention, was associated with a response variable so that the treatment could be adopted for clinical use. in experiments where the predictor can be manipulated by a scientist and cases are randomized to one of several levels of a predictor, the association can be interpreted as causal. it can be particularly important to control type i error probabilities in those settings. regression is often thought of as an exploratory technique, used in observational studies to discover associations that can be explored in further studies. strict control of type i error probabilities may be less critical in such settings. at the introductory level, anov a is useful in that it provides more direct access to type i error control and pairwise comparisons with t-tests. in practice, with the use of techniques not covered in this text, any analysis done via the anov a approach can also be approached with regression modeling. 370 chapter 7. multiple linear regression 7.10 notes this chapter and the previous chapter cover only the basic principles behind linear regression, and are meant to provide useful tools for getting started with data analysis. this section summarizes the most important ideas in the chapter and makes reference to some related topics that have not been discussed in detail. important ideas keep a clear view of the purpose. is the goal of constructing the model to understand the relationship between the response and a particular predictor after adjusting for confounders? or is the goal to understand the joint association between a response and a set of predictors? avoid rushing into model fitting. before fitting models, examine the data. assess whether the response variable has an approximate normal distribution, or at least a symmetric distribution; a log transformation will often produce approximate normality. examine the relationships between the response and predictors, as well as the relationships between predictors; check for nonlinear trends or outliers. remember the context of the problem. context is important at each stage of a regression analysis. the best approach for constructing a model from a small number of potential predictors is based on considering the context of the problem and including predictors that have either been shown in the past to be associated with the response or for which there is a plausible working hypothesis about association with the response. when interpreting coe fficients, consider whether the model results cohere with the underlying biological or medical context. critically examine residual plots. all models are approximations, so it is not necessary to be concerned about relatively minor violations of assumptions; residual plots are seldom as well behaved as those for the prevend data. in some cases, like with the california dds data, residual plots show obvious major violations. with intermediate cases such as in the forest.birds plots, examine the plots closely and provide a detailed assessment of where the model seems less reliable. 7.10. notes 371 related topics stepwise model selection. many introductory texts recommend using “stepwise” regression. forward stepwise regression adds predictors one by one according to a set criterion (usually by smallestp-value). backward stepwise regression eliminates variables one by one from a larger model until a criterion is met. stepwise methods can be useful, and are usually automated in statistical software. however, there are weaknesses—the final models are data-dependent and chance alone can lead to spurious variables being included. in very large datasets, stepwise regression can lead to substantially incorrect models. prediction models. an application of regression not discussed in this chapter is predictive modeling, in which the goal is to construct a model that best predicts outcomes. the focus is on overall predictive accuracy; significance of individual coe fficients is less important. evaluating a model’s predictive accuracy involves advanced methods such as cross-validation, in which the original data sample is divided into a training set and a test set, similar to the approach used with the golub leukemia data in chapter 1. prediction models are typically built from large datasets, using automated model selection procedures like stepwise regression. prediction intervals. predicted values from regression have an inherent uncertainty because model parameters are only estimates. there are two types of interval estimates used with prediction: confidence intervals for a predicted mean response from a set of values for the predictors, and prediction intervals that show the variability in the predicted value for a new response (i.e., for a case not in the dataset) given a set of values for the predictor variables. prediction intervals are wider than confidence intervals for a predicted mean because prediction intervals are subject to both the variability in a predicted mean response and the variability of an individual observation about its mean. controlling type i error in regression. control of type i error probabilities becomes more critical in regression models with very large numbers of potential predictors. datasets containing measurements on genetic data often contain large numbers of potential predictors for a response for many cases; a stricter significance level is used to maintain an overall error rate of = 0:05. for example, in genome-wide association studies, the accepted \"genome-wide significance rate\" for an individual marker to be considered significantly associated with an outcome is 5\u000210\u00008. because there are so many tools available in multiple regression, this chapter has a larger collection of labs than most other chapters. lab 1 introduces the multiple regression model, illustrating one its most common uses—estimating an association between a response variable and predictor of interest while adjusting for possible confounding. lab 2 discusses the residual plots used to check assumptions for multiple regression and introduces adjusted r2using the california dds dataset initially introduced in chapter 1. lab 3 explores how the association between a response variable and categorical predictors with more than two levels can be be estimated using multiple regression. this topic extends the earlier material in chapter 6, lab 4. lab 4 introduces the concept of a statistical interaction using the nhanes dataset, examining whether the association between bmi and age among women is different than that among men. multiple regression is often used to examine associations between response variables and a small set of pre-specified predictors. it can also be used to explore and select models between a response variable and a set of candidate predictors. lab 5 discusses explanatory modeling, in which the goal is to construct a model that e ffectively explains the observed variation in the response variable. 372 chapter 7. multiple linear regression 7.11 exercises 7.11.1 introduction to multiple linear regression there are not currently exercises available for this section. 7.11.2 simple versus multiple regression 7.1 prevend, part i. the summary table below shows the results of a multiple regression model of rfft score versus statin use and age. estimate std. error t value pr( >jtj) (intercept) 137.8822 5.1221 26.92 0.0000 statin 0.8509 2.5957 0.33 0.7432 age -1.2710 0.0943 -13.48 0.0000 in a clinical setting, the interpretive focus lies on reporting the nature of the association between the primary predictor and the response, while specifying which potential confounders have been adjusted for. briefly respond to a clinician who is concerned about a possible association between statin use and decreased cognitive function, based on the above analysis. 7.2 prevend, part ii. can the results of the analysis in exercise 7.1 be used to conclude that as one ages, one’s cognitive function (as measured by rfft score) declines? explain your answer. 7.3 baby weights, part i. the child health and development studies investigate a range of topics. one study considered all pregnancies between 1960 and 1967 among women in the kaiser foundation health plan in the san francisco east bay area. the variable smoke is coded 1 if the mother is a smoker, and 0 if not. the variable parity is 1 if the child is the first born, and 0 otherwise. the summary table below shows the results of a linear regression model for predicting the average birth weight of babies, measured in ounces, based on the smoking status of the mother and whether the child is the first born.23 estimate std. error t value pr( >jtj) (intercept) 123.57 0.72 172.75 0.0000 smoke -8.96 1.03 -8.68 0.0000 parity -1.98 1.15 -1.72 0.0859 (a) write the equation of the regression model. (b) interpret the model slopes in the context of the data. (c) calculate the estimated di fference in mean birth weight for two infants born to non-smoking mothers, if one is first born and the other is not. (d) calculate the estimated di fference in mean birth weight for two infants born to mothers who are smokers, if one is first born and the other is not. (e) calculate the predicted mean birth weight for a first born baby born to a mother who is not a smoker. 23child health and development studies, baby weights data set. 7.11. exercises 373 7.4 wolbachia, part i. wolbachia is a microbial symbiont estimated to be hosted by about 40% of all arthropod species, transmitted primarily from females to their o ffspring through the eggs. researchers conducted a study on a wasp species to understand the e ffect of wolbachia on the lifetime reproductive success of an insect host. they estimated the realized lifetime reproductive success of female wasps by collecting them soon after they die naturally in the field, counting the number of eggs remaining in their ovaries and quantifying wolbachia density in their body. in the first stage of the experiment, researchers estimated potential reproductive success by collecting female wasps as they emerged from eggs then dissecting them to count the number of eggs in their ovaries. these data were used to create a predictive model for initial number of eggs based on tibia length (an indicator of body size) and wolbachia density. tibia length was measured in \u0016m, and wolbachia density in units of -ddct. estimate std. error t value pr( >jtj) (intercept) -18.82 27.26 -0.69 0.497 wolbachia 1.77 1.07 -1.65 0.111 tibia 0.357 0.15 2.38 0.0258 (a) write the model equation. (b) interpret the model coe fficients in the context of the data. (c) predict mean initial egg count for a wasp with tibia length of 171.4286 \u0016mandwolbachia density of -3.435 -ddct. 7.11.3 evaluating the fit of a multiple regression model 7.5 baby weights, part iii. we considered the variables smoke and parity , one at a time, in modeling birth weights of babies in exercise 7.3. a more realistic approach to modeling infant weights is to consider all possibly related variables at once. other variables of interest include length of pregnancy in days ( gestation ), mother’s age in years ( age), mother’s height in inches ( height ), and mother’s pregnancy weight in pounds (weight ). below are three observations from this data set. bwt gestation parity age height weight smoke 1 120 284 0 27 62 100 0 2 113 282 0 33 64 135 0 :::::::::::::::::::::::: 1236 117 297 0 38 65 129 0 the summary table below shows the results of a regression model for predicting the average birth weight of babies based on all of the variables included in the data set. estimate std. error t value pr( >jtj) (intercept) -80.41 14.35 -5.60 0.0000 gestation 0.44 0.03 15.26 0.0000 parity -3.33 1.13 -2.95 0.0033 age -0.01 0.09 -0.10 0.9170 height 1.15 0.21 5.63 0.0000 weight 0.05 0.03 1.99 0.0471 smoke -8.40 0.95 -8.81 0.0000 (a) write the equation of the regression model that includes all of the variables. (b) interpret the slopes of gestation and agein this context. (c) the coe fficient for parity is different than in the linear model shown in exercise 7.3. why might there be a difference? (d) calculate the residual for the first observation in the data set. (e) the variance of the residuals is 249.28, and the variance of the birth weights of all babies in the data set is 332.57. calculate the r2and the adjusted r2. note that there are 1,236 observations in the data set. 374 chapter 7. multiple linear regression 7.6 absenteeism, part i. researchers interested in the relationship between absenteeism from school and certain demographic characteristics of children collected data from 146 randomly sampled students in rural new south wales, australia, in a particular school year. below are three observations from this data set. eth sex lrn days 1 0 1 1 2 2 0 1 1 11 ::::::::::::::: 146 1 0 0 37 the summary table below shows the results of a linear regression model for predicting the average number of days absent based on ethnic background ( eth: 0 - aboriginal, 1 - not aboriginal), sex ( sex: 0 - female, 1 - male), and learner status ( lrn: 0 - average learner, 1 - slow learner).24 estimate std. error t value pr( >jtj) (intercept) 18.93 2.57 7.37 0.0000 eth -9.11 2.60 -3.51 0.0000 sex 3.10 2.64 1.18 0.2411 lrn 2.15 2.65 0.81 0.4177 (a) write the equation of the regression model. (b) interpret each one of the slopes in this context. (c) calculate the residual for the first observation in the data set: a student who is aboriginal, male, a slow learner, and missed 2 days of school. (d) the variance of the residuals is 240.57, and the variance of the number of absent days for all students in the data set is 264.17. calculate the r2and the adjusted r2. note that there are 146 observations in the data set. 24w. n. venables and b. d. ripley. modern applied statistics with s . fourth edition. data can also be found in the r mass package. new york: springer, 2002. 7.11. exercises 375 7.7 baby weights, part vi. exercise 7.5 presents a regression model for predicting the average birth weight of babies based on length of gestation, parity, height, weight, and smoking status of the mother. use the following plots to assess whether the assumptions for linear regression are reasonably met. discuss your reasoning. residuals−60−40−200204060050100150200250300 fitted valuesresiduals 80 120 160−40040 order of collectionresiduals 0 400 800 1200−40040 length of gestationresiduals 150 200 250 300 350−40040 parityresiduals 0 1−40040 height of motherresiduals 55 60 65 70−40040 weight of motherresiduals 100 150 200 250−40040 smokeresiduals 0 1−40040 376 chapter 7. multiple linear regression 7.8 toxemia and birth weight. a model was fit for a random sample of 100 low birth weight infants born in two teaching hospitals in boston, massachusetts, regressing birthweight on the predictors gestational age and toxemia status. the condition toxemia, also known as preeclampsia, is characterized by high blood pressure and protein in urine by the 20thweek of pregnancy; left untreated, toxemia can be life-threatening. birth weight was measured in grams and gestational age measured in weeks. estimate std. error t value pr( >jtj) (intercept) -1286.200 234.918 -5.475 0.0000 toxemiayes -206.591 51.078 -4.045 0.0001 gestage 84.048 8.251 10.188 0.0000 24 26 28 30 32 34−600−400−2000200400residual vs gestage gestage (wks)residual 800 1000 1200 1400−600−400−2000200400residual vs fitted predicted birthweight (g)residual −2 −1 0 1 2−600−400−2000200400normal q−q plot theoretical quantilessample quantiles (a) write the model equation. (b) interpret the coe fficients of the model, and comment on whether the intercept has a meaningful interpretation. (c) predict the average birth weight for an infant born to a mother diagnosed with toxemia with gestational age 31 weeks. (d) evaluate whether the assumptions for linear regression are reasonably satisfied. (e) a simple regression model with only toxemia status as a predictor had r2= 0:0001 andr2 adj= 0:010; in this model, the slope estimate for toxemia status is 7.785, with p= 0:907. the simple regression model and multiple regression model disagree regarding the nature of the association between birth weight and toxemia. briefly explain a potential reason behind the discrepancy. which model do you prefer for understanding the relationship between birth weight and toxemia, and why? 7.9 multiple regression fact checking. determine which of the following statements are true and false. for each statement that is false, explain why it is false. (a) suppose a numerical variable xhas a coe fficient ofb1= 2:5 in the multiple regression model. suppose also that the first observation has x1= 7:2, the second observation has a value of x1= 8:2, and these two observations have the same values for all other predictors. then the predicted value of the second observation will be 2.5 higher than the prediction of the first observation based on the multiple regression model. (b) if a regression model’s first variable has a coe fficient ofb1= 5:7, then if we are able to influence the data so that an observation will have its x1be 1 larger than it would otherwise, the value y1for this observation would increase by 5.7. (c) suppose we fit a multiple regression model based on a data set of 472 observations. we also notice that the distribution of the residuals includes some skew but does not include any particularly extreme outliers. because the residuals are not nearly normal, we should not use this model and require more advanced methods to model these data. 7.11. exercises 377 7.11.4 the general multiple linear regression model 7.10 cherry trees. timber yield is approximately equal to the volume of a tree, however, this value is difficult to measure without first cutting the tree down. instead, other variables, such as height and diameter, may be used to predict a tree’s volume and yield. researchers wanting to understand the relationship between these variables for black cherry trees collected data from 31 such trees in the allegheny national forest, pennsylvania. height is measured in feet, diameter in inches (at 54 inches above ground), and volume in cubic feet.25 estimate std. error t value pr( >jtj) (intercept) -57.99 8.64 -6.71 0.00 height 0.34 0.13 2.61 0.01 diameter 4.71 0.26 17.82 0.00 (a) calculate a 95% confidence interval for the coe fficient of height, and interpret it in the context of the data. (b) one tree in this sample is 79 feet tall, has a diameter of 11.3 inches, and is 24.2 cubic feet in volume. determine if the model overestimates or underestimates the volume of this tree, and by how much. 7.11 gpa. a survey of 55 duke university students asked about their gpa, number of hours they study at night, number of nights they go out, and their gender. summary output of the regression model is shown below. note that male is coded as 1. estimate std. error t value pr( >jtj) (intercept) 3.45 0.35 9.85 0.00 studyweek 0.00 0.00 0.27 0.79 sleepnight 0.01 0.05 0.11 0.91 outnight 0.05 0.05 1.01 0.32 gender -0.08 0.12 -0.68 0.50 (a) calculate a 95% confidence interval for the coe fficient of gender in the model, and interpret it in the context of the data. (b) would you expect a 95% confidence interval for the slope of the remaining variables to include 0? explain 7.12 trait inheritance of high blood pressure. one research question of public health interest is to determine the extent to which high blood pressure is a genetic phenomenon. in 20 families, the systolic blood pressure of the mother, father, and first-born child in the family were measured (in units of mm hg). a multiple linear regression model using y= child’s blood pressure, x1= mother’s blood pressure, and x2= father’s blood pressure led to the following estimate of a least squares line: e(y) =\u000015:69 + 0:415x1+ 0:423x2. the standard errors associated with b0,b1, andb2, respectively, are 23.65, 0.125, and 0.119. the least squares fit producedr2= 0:597 andmse = 113:8. (a) what proportion of the variability of a child’s systolic blood pressure is explained by this model? (b) does the least squares line indicate statistically significant associations between each of the parent’s systolic blood pressures and that of the child? explain your answer. (c) what is the predicted systolic blood pressure for a child whose mother’s and father’s systolic blood pressure is 125 mm hg and 140 mm hg, respectively? (d) a colleague tells you that something must be wrong with your model because your fitted intercept is negative, but blood pressures are never negative. how do you respond? (e) briefly describe three di fferent plots for assessing the appropriateness or fit of the above regression model. 25d.j. hand. a handbook of small data sets . chapman & hall/crc, 1994. 378 chapter 7. multiple linear regression 7.13 wolbachia, part ii. exercise 7.4 introduced a study about wolbachia and reproductive success in a wasp host. the following table shows the model coe fficients for a model predicting the number of eggs laid over a lifetime from the predictor variables wolbachia density and tibia length. a higher number of eggs laid over a lifetime is indicative of greater reproductive success. the model has r2= 0:314 and degrees of freedom 34. thef-statistic is 7.782, with p-value 0.0016. estimate std. error t value pr( >jtj) (intercept) -17.88 28.63 -0.62 0.537 wolbachia 4.28 1.25 3.42 0.002 tibia 0.272 0.16 1.69 0.010 (a) write the model equation. (b) interpret the slope coe fficient of wolbachia . (c) assess the evidence for whether wolbachia is beneficial for its host in nature, based on these data. (d) compute and interpret a 95% confidence interval for the population slope of wolbachia . (e) interpret the significance of the f-statistic. 7.14 difficult encounters, part i. a study was conducted at a university outpatient primary care clinic in switzerland to identify factors associated with di fficult doctor-patient encounters. the data consist of 527 patient encounters, conducted by the 27 medical residents employed at the clinic. after each encounter, the attending physician completed two questionnaires: the di fficult doctor patient relationship questionnaire (ddprq-10) and the patient’s vulnerability grid (pvg). a higher score on the ddprq-10 indicates a more di fficult encounter. the maximum possible score is 60 and encounters with score 30 and higher are considered di fficult. a model was fit for the association of ddprq-10 score with features of the attending physician: age, sex, and years of training. the model has f-statistic of 0.23 on 3 and 286 degrees of freedom, with p-value 0.876. estimate std. error t value pr( >jtj) (intercept) 30.594 2.886 10.601 0.0000 age -0.016 0.104 -0.157 0.876 sexm -0.535 0.781 -0.686 0.494 yrs.train 0.096 0.215 0.445 0.656 (a) as a group, are these physician features useful for predicting ddprq-10 score? (b) is there evidence of a significant association between ddprq-10 score and any of the physician features? 7.11. exercises 379 7.11.5 categorical predictors with several levels 7.15 prison isolation experiment, part iii. exercises 5.35 and 5.47 introduced an experiment conducted with the goal of identifying a treatment that reduces subjects’ psychopathic deviant t scores on the mmpi test. exercise 5.35 evaluated the success of each individual treatment, and in exercise 5.47, anov a was used to compare the success of the three treatments. this exercise uses multiple regression to examine the intervention e ffect. for this problem, a treatment variable (labeled treatment ) has been constructed with three levels: (1)therapeutic for sensory restriction plus the 15 minute \"therapeutic\" tape advising that professional help is available. (2)neutral for sensory restriction plus a 15 minute \"emotionally neutral\" tap on training hunting dogs. (3)absent for sensory restriction but no taped message. forty-two subjects were randomly assigned to these treatment groups, and an mmpi test was administered before and after the treatment. investigators hoped that the interventions would lower mmpi scores. the table below shows the result of a multiple regression in rwhere the response variable trt.effect is the change in mmpi score (pre-intervention - post-intervention) and the predictor variable is treatment . estimate std. error t value pr( >jtj) (intercept) -3.2143 2.6174 -1.23 0.2268 treatmentneutral 6.0714 3.7015 1.64 0.1090 treatmenttherapeutic 9.4286 3.7015 2.55 0.0149 in this model, the residual standard error is 9.79, the f-statistic is 3.33 with 2 and 39 degrees of freedom; p(f2;39>3:33) = 0:0461. (a) interpret the meaning of a positive value for trt.effect versus a negative value. (b) write the estimated model equation. (c) calculate the predicted value for trt.effect for a patient in the neutral tape group. (d) does the intercept have a meaningful interpretation in this model? (e) what is the interpretation of the two slope coe fficients in the regression model? (f) describe the tested hypotheses that correspond to each of the p-values in the last column of the table. 7.16 poverty and educational level. this question uses data from 500 randomly selected adults in the larger nhanes dataset. poverty is measured as a ratio of family income to poverty guidelines. smaller numbers indicate more poverty, and ratios of 5 or larger were recorded as 5. the education variable indicates the highest level of education achieved: either 8th grade, 9 - 11th grade, high school, some college, or college grad. estimate std. error t value pr( >jtj) (intercept) 1.4555 0.2703 5.38 0.0000 education9 - 11th grade 0.9931 0.3302 3.01 0.0028 educationhigh school 1.0900 0.3113 3.50 0.0005 educationsome college 1.4943 0.2976 5.02 0.0000 educationcollege grad 2.4948 0.2958 8.43 0.0000 in this model, the residual standard error is 1.46, the f-statistic is 28.09 with 4 and 456 degrees of freedom;p(f4;456>28:09)<0:0001. (a) write the estimated model equation. (b) calculate the predicted poverty ratio for an individual who at most completed high school. (c) interpret the estimated intercept value. (d) interpret the slope coe fficient for educationcollege grad , and describe the tested hypotheses that correspond to the p-value for this slope coe fficient. (e) assess whether educational level, overall, is associated with poverty. be sure to include any relevant numerical evidence as part of your answer. 380 chapter 7. multiple linear regression 7.17 prison isolation experiment, part iv. exercise 7.15 used regression to examine the e ffect of three interventions on prisoner mmpi scores. the response variable in the regression was trt.effect , the change in mmpi score (pre-intervention - post-intervention). instead of estimating the intervention e ffect through the change in scores, suppose one is interested in predicting a post-intervention score based on the pre-intervention score for an individual and a particular intervention. (a) the table below shows an alternative regression model that can be fit to the data. in this model, the response variable is the post-intervention mmpi value ( post , not shown explicitly in the table) and the predictors are the pre-intervention score ( pre) and the treatment, coded as in problem 7.15. write the estimated equation for this model. estimate std. error t value pr( >jtj) (intercept) 28.4053 12.2949 2.31 0.0264 pre 0.6593 0.1628 4.05 0.0002 treatmentneutral -5.7307 3.5545 -1.61 0.1152 treatmenttherapeutic -9.7450 3.5540 -2.74 0.0093 (b) in this model, describe in general terms the association of the pre-intervention and post-intervention scores. (c) does the pre-intervention score appear to be an important predictor of a post intervention score? (d) what is the predicted post-intervention score for an individual with a pre-intervention score of 73 and receiving no tape after the isolation? (e) explain the interpretation of the coe fficients for coe fficient of treatmentneutral . is there strong statistical evidence that it is an important predictor? 7.18 resilience, part i. the american psychological association defines resilience as \"the process of adapting well in the face of adversity, trauma, tragedy, threats, or even significant sources of stress\". studies have suggested that resilience is an important factor in contributing to how medical students perceive their quality of life and educational environment. survey data were collected from 1,350 students across 25 medical schools. at each school, 54 students were randomly selected to participate in the study. participants completed questionnaires measuring resilience, quality of life, perception of educational environment, depression symptoms, and anxiety symptoms. the following regression model was fit to analyze the relationship between resilience and depressive symptoms. resilience was categorized as: very low, low, moderately low, moderately high, high, and very high. depressive symptoms were measured on a scale of 0 to 63 points, with higher scores indicating either more numerous or more severe depressive symptoms; this questionnaire is called the beck depression inventory (bdi). in this model, the residual standard error is 5.867, the f-statistic is 118.1 with 5 and 1344 degrees of freedom;p(f5;1344>118:1)<0:0001. estimate std. error t value pr( >jtj) (intercept) 4.9754 0.4118 12.08 0.0000 reshigh 2.2936 0.4987 4.60 0.0000 resmodhigh 4.3005 0.5181 8.30 0.0000 resmodlow 6.7108 0.5938 11.30 0.0000 reslow 9.6538 0.7458 12.94 0.0000 resverylow 15.6453 0.7518 20.81 0.0000 (a) describe the overall trend in language accessible to someone who has not taken a statistics course. (b) does the intercept have a meaningful interpretation? explain your answer. (c) compare the predicted mean bdi score for someone with low resilience to that of someone with very low resilience. (d) + (e) continue to the next page for parts (d) and (e). 7.11. exercises 381 (d) assess whether level of resilience, overall, is associated with depressive symptoms as measured by bdi score. be sure to include any relevant numerical evidence as part of your answer. (e) a model was fitted predicting bdi score from resilience, with the categories numerically coded from 1to 6, with 1being very high resilience and 6being very low resilience. this model has a single slope estimate of 2.76 with p-value < 0.0001. i. using this model, compare the predicted mean bdi score for someone with low resilience to that of someone with very low resilience. compare this answer to the one from part (c). ii. what does this model imply about the change in mean bdi score between groups? iii. explain why this model is flawed. 7.11.6 reanalyzing the prevend data there are not currently exercises available for this section. 7.11.7 interaction in regression 7.19 prison isolation experiment, part v. exercise 7.17 used regression to predict a post-intervention score based on pre-intervention score and a particular intervention. the following table shows a model incorporating interaction between pre-intervention score and intervention. estimate std. error t value pr( >jtj) (intercept) -17.5790 17.7090 -0.99 0.3275 pre 1.2813 0.2376 5.39 0.0000 treatmentneutral 67.7518 30.6168 2.21 0.0333 treatmenttherapeutic 64.4183 24.2124 2.66 0.0116 pre:treatmentneutral -0.9890 0.4082 -2.42 0.0206 pre:treatmenttherapeutic -1.0080 0.3266 -3.09 0.0039 (a) write the model equation. (b) interpret the model coe fficients. (c) write a separate model equation for each intervention group. (d) do these data suggest that there is a statistically significant di fference in association between pre- and post-intervention scores by treatment group? explain your answer. 7.20 vitamin d. a study was conducted to evaluate vitamin d status among schoolchildren in thailand. exposure to sunlight allows the body to produce serum 25( oh)d, which is a marker of vitamin d status; serum level is measured in units of nmol/l and having serum level below 50 nmol/l is indicative of vitamin d deficiency. the following model was fit to predict serum 25( oh)dlevel from age, sex, and their interaction. estimate std. error t value pr( >jtj) (intercept) 97.7709 4.7732 20.48 0.0000 age -3.0156 0.4774 -6.32 0.0000 sexm -16.2848 7.0740 -2.30 0.0217 age:sexm 2.9369 0.7054 4.16 0.0000 (a) write the model equation. (b) interpret the model coe fficients. (c) is there statistically significant evidence that the association between serum 25( oh)dlevel and age di ffers by sex? explain your answer. 382 chapter 7. multiple linear regression 7.21 prevend, part iii. exercise 7.1 showed a multiple regression model predicting rfft score from statin use and age. for this problem, an interaction term is added between statin use and age. estimate std. error t value pr( >jtj) (intercept) 140.2031 5.6209 24.94 0.0000 statin -13.9720 15.0113 -0.93 0.3524 age -1.3149 0.1040 -12.65 0.0000 statin:age 0.2474 0.2468 1.00 0.3166 (a) write the model equation. (b) interpret the model coe fficients. (c) is there statistically significant evidence that the association between rfft score and age di ffers by whether someone is a statin user? explain your answer. 7.22 antibiotic consumption, part i. antibiotic resistance represents a major public health challenge. overuse of antibiotics in clinical settings is thought to be a major contributor to increased antibiotic resistance. a study was conducted across several regions in china to investigate the impact of a 2011 law prohibiting over-the-counter (otc) sales of antibiotics in private pharmacies. the study team collected data on average monthly antibiotic consumption in 621 counties, in addition to information on socioeconomic determinants such as percentage of population illiterate. the following model was fit to investigate whether the relationship between monthly antibiotic consumption and percentage of population (over 25 years of age) with an advanced degree di ffers between counties that are located in a metropolitan area and those that are not. estimate std. error t value pr( >jtj) (intercept) 0.8482 0.3311 2.56 0.0107 metroyes 2.3035 0.9612 2.40 0.0169 edu 0.5711 0.0319 17.90 0.0000 metroyes:edu -0.1838 0.0752 -2.44 0.0148 (a) interpret the model coe fficients, including any relevant inferential results. (b) make a prediction of average monthly antibiotic consumption for a county in a metropolitan area where 10% of the population over 25 years old has an advanced degree. 7.11.8 model selection for explanatory variables 7.23 baby weights, part vii. suppose the starting point for model selection for the birth weight data were the full model, with all variables. the table below shows the adjusted r2for the full model as well as the adjustedr2values for all models with one fewer predictor variable. based on examining the table from exercise 7.5 and the following table, identify which variable, if any, should be removed from the model first. explain your answer. model adjusted r2 1 full model 0.2541 2 no gestation 0.1031 3 no parity 0.2492 4 no age 0.2547 5 no height 0.2311 6 no weight 0.2536 7 no smoking status 0.2072 7.11. exercises 383 7.24 absenteeism, part ii. suppose the starting point for model selection for the absenteeism data were the full model, with all variables. the table below shows the adjusted r2for the full model as well as the adjusted r2values for all models with one fewer predictor variable. based on examining the table from exercise 7.6 and the following table, identify which variable, if any, should be removed from the model first. explain your answer. model adjusted r2 1 full model 0.0701 2 no ethnicity -0.0033 3 no sex 0.0676 4 no learner status 0.0723 7.25 baby weights, part viii. exercise 7.5 shows a regression model for predicting the average birth weight of babies based on all variables included in the dataset: length of pregnancy in days ( gestation ), mother’s age in years ( age), mother’s height in inches ( height ), and mother’s pregnancy weight in pounds ( weight ). the following plots show the relationships between the response and the numerical predictor variables, in addition to the relationships between the response and two categorical predictor variables. not first born first born6080100120140160180birthweight (oz) birth ordernonsmoker smoker6080100120140160180birthweight (oz) mother smoking status bwt 150200250300350 55606570 60100160150250350 gestation age 152535455565 height6080100120140160180 15202530354045 100 200100150200250 weight (a) examine the relationship between the response variable and the predictor variables. describe what you see. which predictor variables seem like they would be useful to include in an initial model? (b) identify any predictors that seem related to each other. 384 chapter 7. multiple linear regression 7.26 antibiotic consumption, part ii. exercise 7.22 introduced a study conducted about antibiotic consumption in china. one aim of the study was to develop a prediction model for predicting monthly average antibiotic consumption based on county-level data. the following plots show the association between monthly average antibiotic consumption and four potential predictor variables: proportion female inhabitants ( female ), average life expectancy in years ( lifeexp ), proportion of population illiterate ( illiterate ), and population density in 1,000 people / km2(popdensity ). 4648505254010203040consumption vs female percentage female popconsumption (did) 72737475767778010203040consumption vs life expectancy life expectancy (yrs)consumption (did) 5 10 15010203040consumption vs percent illiterate percentage illiterateconsumption (did) 024681012010203040consumption vs pop density density (1,000 people per sq km)consumption (did) (a) summarize what you see. (b) identify any predictor variables that might benefit from a natural log transformation and briefly justify your choices. 7.11. exercises 385 7.11.9 the connection between anova and regression 7.27 prison isolation experiment, part vi. problem 7.15 used a regression model to examine the e ffect of the interventions on possibly reducing psychopathic deviant t scores on prisoners. the regression model is shown in the problem statement. (a) the value of the f-statistic is 3.33 with 2 and 39 degrees of freedom and p(f2;39>3:33) = 0:0461. in terms of the variables in the regression model, state the null hypothesis that corresponds to the f-statistic. (b) describe the relationship between the coe fficients from the linear model and the usual summary statistics for the three sets of di fference scores. (c) explain why the null hypothesis in this regression model is equivalent to the null hypothesis when these data were analyzed using anov a in problem 5.47. (d) explain whether the assumptions for this regression model di ffer from those used in anov a. 7.28 resilience, part ii. exercise 7.18 shows a regression model for the association of bdi score with resilience level. (a) in terms of the variables in the regression model, state the null hypothesis that corresponds to the fstatistic. (b) describe the relationship between the coe fficients from the linear model and the usual summary statistics for the six sets of bdi scores. (c) explain why the null hypothesis used in this regression model is equivalent to the null hypothesis that would be used if these data were analyzed with an anov a approach. 386 chapter 8 inference for categorical data 8.1 inference for a single proportion 8.2 inference for the difference of two proportions 8.3 inference for two or more groups 8.4 chi-square tests for the fit of a distribution 8.5 outcome-based sampling: case-control studies 8.6 notes 8.7 exercises 387 previous chapters discussed methods of inference for numerical data; in this chapter, those methods are extended to categorical data, such as binomial proportions or data in two-way tables. while various details of the methods may change, such as the calculations for a test statistic or the distributions used to find a p-value, the core ideas and principles behind inference remain the same. categorical data arise frequently in medical research because disease outcomes and patient characteristics are often recorded in natural categories such as types of treatment received, whether or not disease advanced to a later stage, or whether or not a patient responded initially to a treatment. in the simplest settings, a binary outcome (yes/no, success/failure, etc) is recorded for a single group of participants, in hopes of learning more about the population from which the participants were drawn. the binomial distribution is often used for the statistical model in this setting, and inference about the binomial probability of success provides information about a population proportion p. in more complex settings, participant characteristics are recorded in a categorical variable with two or more levels, and the outcome or response variable itself has two or more levels. in these instances, data are usually summarized in two-way tables with two or more rows and two or more columns. as with all methods of inference, it is important to understand how the data were collected and whether the data may be viewed as a random sample from a well-identified population, at least approximately. this issue is at least as important as the formulas for test statistics and confidence intervals, and is often overlooked. be careful about the notation in this chapter—since pis the standard notation for a population proportion and for a probability, pdoes double duty in this chapter as a population parameter and significance level. for labs, slides, and other resources, please visit www.openintro.org/book/biostat 388 chapter 8. inference for categorical data 8.1 inference for a single proportion advanced melanoma is an aggressive form of skin cancer that until recently was almost uniformly fatal. in rare instances, a patient’s melanoma stopped progressing or disappeared altogether when the patient’s immune system successfully mounted a response to the cancer. those observations led to research into therapies that might trigger an immune response in cancer. some of the most notable successes have been in melanoma, particularly with two new therapies, nivolumab and ipilimumab.1 a 2013 report in the new england journal of medicine by wolchok et al. reported the results of a study in which patients were treated with both nivolumab and ipilimumab.2fifty-three patients were given the new regimens concurrently, and the response to therapy could be evaluated in 52 of the 53. of the 52 evaluable patients, 21 (40%) experienced a response according to commonly accepted criteria. in previous studies, the proportion of patients responding to one of these agents was 30% or less. how might one compare the new data to past results? the data from this study are binomial data, with success defined as a response to therapy. suppose the number of patients who respond in a study like this is represented by the random variablex, wherexis binomial with parameters n(the number of trials, where each trial is represented by a patient) and p(the unknown population proportion of response). from formulas discussed in chapter 3, the mean of xisnpand the standard deviation of xisp np(1\u0000p). inference about pis based on the sample proportion ˆp, where ˆp=x=n. in this case, ˆp= 21=52 = 0:404. if the sample proportion is nearly normally distributed, the normal approximation to the binomial distribution can be used to conduct inference; this method is commonly used. when x does not have an approximately normal distribution, exact inference can based on the binomial distribution for x. both the normal approximation and exact methods are covered in this chapter. 8.1.1 inference using the normal approximation asample proportion can be described as a sample mean. if each success in the melanoma data is represented as a 1and each failure as a 0, then the sample proportion is the mean of the 52 numerical outcomes: ˆp=0 + 1 + 1 +\u0001\u0001\u0001+ 0 52= 0:404: the distribution of ˆpis nearly normal when the distribution of successes and failures is not too strongly skewed. 1the -mab su ffix in these therapies stands for monoclonal antibody, a therapeutic agent made by identical immune cells that are all clones of a unique parent cell from a patient. 2n engl j med 2013;369:122-33. doi: 10.1056/nejmoa1302369 8.1. inference for a single proportion 389 conditions for the sampling distribution of ˆpˆpˆpbeing nearly normal the sampling distribution for ˆp, calculated from a sample of size nfrom a population with a success proportion p, is nearly normal when 1. the sample observations are independent and 2. at least 10 successes and 10 failures are expected in the sample, i.e. np\u001510 andn(1\u0000p)\u0015 10. this is called the success-failure condition . if these conditions are met, then the sampling distribution of ˆpis approximately normal with meanpand standard error seˆp=r p(1\u0000p) n: (8.1)ˆp sample proportion p population proportion when conducting inference, the population proportion pis unknown. thus, to construct a confidence interval, the sample proportion ˆpcan be substituted for pto check the success-failure condition and compute the standard error. in a hypothesis test, p0is substituted for p. confidence intervals for a proportion when using the normal approximation to the sampling distribution of ˆp, a confidence interval for a proportion has the same structure as a confidence interval for a mean; it is centered at the point estimate, with a margin of error calculated from the standard error and appropriate z?value. the formula for a 95% confidence interval is ˆp\u00061:96r ˆp(1\u0000ˆp) n: example 8.2 using the normal approximation, construct an approximate 95% confidence interval for the response probability for patients with advanced melanoma who were administered the combination of nivolumab and ipilimumab. the independence and success-failure assumptions should be checked first. since the outcome of one patient is unlikely to influence that of other patients, the observations are independent. the success-failure condition is satisfied since nˆp= (52)(:404) = 21>10 andnˆp(1\u0000ˆp) = (52)(:596) = 31>10. the point estimate for the response probability, based on a sample of size n= 52, is ˆp= 0:404. for a 95% confidence interval, z?= 1:96. the standard error is estimated as:q ˆp(1\u0000ˆp) n=q (0:404)(1\u00000:404) 52= 0:068. the confidence interval is 0:404\u00061:96(0:068)!(0:27;0:54) the approximate 95% confidence interval for p, the population response probability of melanoma patients to the combination of these new drugs, is (0.27, 0.54) or (27%, 54%). 390 chapter 8. inference for categorical data guided practice 8.3 in new york city on october 23rd, 2014, a doctor who had recently been treating ebola patients in guinea went to the hospital with a slight fever and was subsequently diagnosed with ebola. soon after, a survey conducted by the marist poll, an organization with a carefully designed methodology for drawing random samples from identified populations, found that 82% of new yorkers favored a \"mandatory 21-day quarantine for anyone who has come in contact with an ebola patient.\"3a) verify that the sampling distribution of ˆpis nearly normal. b) construct a 95% confidence interval forp, the proportion of new york adults who supported a quarantine for anyone who has come into contact with an ebola patient.4 did the participants in the melanoma trial constitute a random sample? patients who participate in clinical trials are unlikely to be a random sample of patients with the disease under study since the patients or their physicians must be aware of the trial, and patients must be well enough to travel to a major medical center and be willing to receive an experimental therapy that may have serious side e ffects. investigators in the melanoma trial were aware that the observed proportion of patients responding in a clinical trial may be di fferent than the hypothetical response probability in the population of patients with advanced melanoma. study teams try to minimize these systematic differences by following strict specifications for deciding whether patients are eligible for a study. however, there is no guarantee that the results observed in a sample will be replicated in the general population. small, initial studies in which there is no control group, like the one described here, are early steps in exploring the value of a new therapy and are used to justify further study of a treatment when the results are substantially di fferent than expected. the largest observed response rate in previous trials of 30% was close to the lower bound of the confidence interval from the study (27%, 54%), so the results were considered adequate justification for continued research on this treatment. hypothesis testing for a proportion just as with inference for population means, confidence intervals for population proportions can be used when deciding whether to reject a null hypothesis. it is useful in most settings, however, to calculate the p-value for a test as a measure of the strength of the evidence contradicting the null hypothesis. when using the normal approximation for the distribution of ˆpto conduct a hypothesis test, one should always verify that ˆpis nearly normal under h0by checking the independence and success-failure conditions. since a hypothesis test is based on the distribution of the test statistic under the null hypothesis, the success-failure condition is checked using the null proportion p0, not the estimate ˆp. according to the normal approximation to the binomial distribution, the number of successes inntrials is normally distributed with mean np0and standard deviationp np(1\u0000p0). this approximation is valid when np0andn(1\u0000p0) are both at least 10.5 3poll id ny141026 on maristpoll.marist.edu. 4a) the poll is based on a simple random sample and consists of fewer than 10% of the adult population of new york, which makes independence a reasonable assumption. the success-failure condition is satisfied since, 1042(0 :82)>5 and 1042(1\u00000:82)>5. b) 0:82\u00061:96q 0:82(1\u00000:82) 1042!(0:796;0:844). 5the normal approximation to the binomial distribution was discussed in section 3.2 of chapter 3. 8.1. inference for a single proportion 391 under the null hypothesis, the sample proportion ˆp=x=n is approximately distributed as n0 bbbbb@p0;r p0(1\u0000p0) n1 ccccca: the test statistic zfor the null hypothesis h0:p=p0based on a sample of size nis z=point estimate - null value se =ˆp\u0000p0q (p0)(1\u0000p0) n: example 8.4 suppose that out of a cohort of 120 patients with stage 1 lung cancer at the dana-farber cancer institute (dfci) treated with a new surgical approach, 80 of the patients survive at least 5 years, and suppose that national cancer institute statistics indicate that the 5-year survival probability for stage 1 lung cancer patients nationally is 0.60. do the data collected from 120 patients support the claim that the dfci population treated with this new form of surgery has a di fferent 5-year survival probability than the national population? let = 0:10, since this is an early study of the new surgery. test the hypothesis h0:p= 0:60 versus the alternative, ha:p,0:60, using = 0:10. if we assume that the outcome of one patient at dfci does not influence the outcome of other patients, the independence condition is met, and the success-failure condition is satisfied since (120)(0 :60) = 80>5 and (120)(1\u00000:60) = 40>5:the test statistic is the z-score of the point estimate: z=point estimate - null value se=0:67\u00000:60q (0:60)(1\u00000:60) 120= 1:57: thep-value is the probability that a standard normal variable is larger than 1.57 or smaller than -1.57,p(jzj>1:57) = 0:12 ; since the p-value is greater than 0.10, there is insu fficient evidence to rejecth0in favor ofha. there is not convincing evidence that the survival probability at dfci differs from the national survival probability. had a more traditional 0.05 significance level been used, the data would be even less convincing. example 8.5 using the data from the study in advanced melanoma, use the normal approximation to the sampling distribution of ˆpto test the null hypothesis that the response probability to the novel combined therapy is 30% against a one-sided alternative that the response proportion is greater than 30%. let = 0:10. the test statistic has value z= (0:404\u00000:30)=p (0:30)(0:70)=52 = 1:64: the one-sided p-value isp(z\u00151:64) = 0:05; there is su fficient evidence to reject the null hypothesis at = 0:10. this is an example of where a two-sided test and a one-sided test yield di fferent conclusions. 392 chapter 8. inference for categorical data guided practice 8.6 one of the questions on the national health and nutrition examination survey (introduced in chapter 5) asked participants whether they participated in moderate or vigorous intensity sports, fitness, or recreational activities. in a random sample of 135 adults, 76 answered \"yes\" to the question. based on this evidence, are a majority of american adults physically active?6 8.1.2 inference using exact methods when the normal approximation to the distribution of ˆpmay not be accurate, inference is based on exact binomial probabilities. calculating confidence intervals and p-values based on the binomial distribution can be done by hand, with tables of the binomial distribution, or (more easily and accurately) with statistical software. the logic behind computing a p-value is discussed here, but the formulas for a confidence interval are complicated and are not shown. thep-value for a hypothesis test corresponds to the sum of the probabilities of all events that are as or more extreme than the sample result. let xbe a binomial random variable with parameters nandp0, where ˆp=x=nandxis the observed number of events. if ˆp\u0014p0, then the one-tail probability equals p(x\u0014x); if ˆp>p 0, then the one-tail probability equals p(x\u0015x). these probabilities are calculated using the approaches from chapter 3. two-tailed probabilities are calculated by doubling the appropriate one-tailed value. example 8.7 in 2009, the fda oncology drug advisory committee (odac) recommended that the drug avastin be approved for use in glioblastoma, a form of brain cancer. tumor shrinkage after taking a drug is called a response; out of 85 patients, 24 exhibited a response. historically, response probabilities for brain cancer drugs were approximately 0.05, or about 5%. assess whether there is evidence that the response probability for avastin is di fferent from previous drugs. h0:p= 0:05;ha:p,0:05. let = 0:05. the independence condition is satisfied, but the success-failure condition is not, since np0= (85)(0:05) = 4:25<5, so this is a setting where exact binomial probabilities should be used to calculate ap-value. the sample proportion ˆpequalsx=n= 24=85 = 0:28. since ˆp>p 0, calculate the two-sided p-value from 2\u0002p(x\u001524), wherex\u0018binom(85;0:05). calculating the p-value is best done in software; the rcommand pbinom returns a value of 5 :3486\u0002 10\u000012.7 thep-value is highly significant and suggests that the response probability for avastin is higher than for previous brain cancer drugs. the fda sta ffconsidered this evidence su fficiently strong to justify approval for the use of the drug, even though the fda normally requires evidence from two independently conducted randomized trials. 6the observations are independent. check success-failure: np0=n(1\u0000p0) = 135(0:5)>10.h0:p= 0:5;ha:p>0:5. calculate the z-score:z=0:56\u00000:50q 0:5(1\u00000:5) 135= 1:39. thep-value is 0.08. since the p-value is larger than 0.05, there is insu fficient evidence to reject h0; there is not convincing evidence that a majority of americans are physically active, although the data suggest that may be the case. 72*pbinom(q = 23, size = 85, p = 0.05, lower.tail = false) 8.1. inference for a single proportion 393 guided practice 8.8 medical consultants assist patients with all aspects of an organ donation surgery, with the goal of reducing the possibility of complications during the medical procedure and recovery. to attract customers, one consultant noted that while the usual proportion of complications in liver donation surgeries in the united states is about 10%, only 3 out of her 62 clients experienced complications with liver donor surgeries. is there evidence to suggest that the proportion of complications in her patients is lower than the national average?8 8.1.3 choosing a sample size when estimating a proportion whenever possible, a sample size for a study should be estimated before data collection begins. section 5.4 explored the calculation of sample sizes that allow a hypothesis test comparing two groups to have adequate power. when estimating a proportion, preliminary sample size calculations are often done to estimate a sample size large enough to make the margin of error min a confidence interval su fficiently small for the interval to be useful. recall that the margin of error mis the term that is added to and subtracted from the point estimate. statistically, this means estimating a sample size nso that the sample proportion is within some margin of error mof the actual proportion with a certain level of confidence. when the normal approximation is used for a binomial proportion, a sample size su fficiently large to have a margin of error of mwill satisfy m= (z?)(s.e.( ˆp)) =z?r (p)(1\u0000p) n: algebra can be used to show that the above equation implies n=(z?)2(p)(1\u0000p) m2: in some settings a preliminary estimate for pcan be used to calculate n. when no estimate is available, calculus can be used to show that p(1\u0000p) has its largest value when p= 0:50, and that conservative value for pis often used to ensure that nis sufficiently large regardless of the value of the unknown population proportion p. in that case, nsatisfies n\u0015(z?)2(0:50)(1\u00000:50) m2=(z?)2 4m2: 8assume that the 62 patients in her dataset may be viewed as a random sample from patients receiving a donated liver. the sample proportion ˆp= 3=62 = 0:048. under the null hypothesis, the expected number of complications is 62(0 :10) = 6:2, so the normal approximation may not be accurate and it is best to use exact binomial probabilities. since ˆp\u0014p0, find thepvalue by calculating p(x\u00143) whenxhas a binomial distribution with parameters n= 62;p= 0:10:p(x\u00143) = 0:121. there is not su fficient evidence to suggest that the proportion of complications among her patients is lower than the national average. 394 chapter 8. inference for categorical data example 8.9 donor organs for organ transplant are scarce. studies are conducted to explore whether the population of eligible organs can be expanded. suppose a research team is studying the possibility of transplanting lungs from hepatitis c positive individuals; recipients can be treated with one of the new drugs that cures hepatitis c. preliminary studies in organ transplant are often designed to estimate the probability of a successful organ graft 6 months after the transplant. how large should a study be so that the 95% confidence interval for the probability of a successful graft at 6 months is no wider than 20%? a confidence interval no wider than 20% has a margin of error of 10%, or 0.10. using the conservative value p = 0.50, n=(1:96)2 (4)(0:102)= 96:04: sample sizes are always rounded up, so the study should have 97 patients. since the study will likely yield a value ˆpdifferent from 0.50, the final margin of error will be smaller than\u00060:10. when the confidence coe fficient is 95%, 1.96 can replaced by 2 and the sample size formula reduces to n= 1=m2: this remarkably simple formula is often used by practitioners for a quick estimate of sample size. guided practice 8.10 a 2015 estimate of congress’ approval rating was 19%.9using this estimate, how large should an additional survey be to produce a margin of error of 0.04 with 95% confidence?10 9www.gallup.com/poll/183128/five-months-gop-congress-approval-remains-low.aspx 10apply the formula 1:96\u0002r p(1\u0000p) n\u00191:96\u0002r 0:19(1\u00000:19) n\u00140:04!n\u0015369:5: a sample size of 370 or more would be reasonable. 8.2. inference for the difference of two proportions 395 8.2 inference for the difference of two proportions just as inference can be done for the di fference of two population means, conclusions can also be drawn about the di fference of two population proportions: p1\u0000p2. 8.2.1 sampling distribution of the difference of two proportions the normal model can be applied to ˆp1\u0000ˆp2if the sampling distribution for each sample proportion is nearly normal and if the samples are independent random samples from the relevant populations. conditions for the sampling distribution of ˆp1\u0000ˆp2to be approximately normal the difference ˆp1\u0000ˆp2tends to follow a normal model when – each of the two samples are random samples from a population, – the two samples are independent of each other, and – each sample proportion follows (approximately) a normal model. this condition is satisfied when n1p1;n1(1\u0000p1);n2p2andn2(1\u0000p2) are all\u001510. the standard error of the di fference in sample proportions is seˆp1\u0000ˆp2=q se2 ˆp1+se2 ˆp2=s p1(1\u0000p1) n1+p2(1\u0000p2) n2; (8.11) wherep1andp2are the population proportions, and n1andn2are the two sample sizes. 396 chapter 8. inference for categorical data 8.2.2 confidence intervals for p1\u0000p2p1\u0000p2p1\u0000p2 when calculating confidence intervals for a di fference of two proportions using the normal approximation to the binomial, the two sample proportions are used to verify the success-failure condition and to compute the standard error. example 8.12 the way a question is phrased can influence a person’s response. for example, pew research center conducted a survey with the following question:11 as you may know, by 2014 nearly all americans will be required to have health insurance. [people who do not buy insurance will pay a penalty] while [people who cannot afford it will receive financial help from the government]. do you approve or disapprove of this policy? for each randomly sampled respondent, the statements in brackets were randomized: either they were kept in the order given above, or the order of the two statements was reversed. figure 8.1 shows the results of this experiment. calculate and interpret a 90% confidence interval of the difference in the probability of approval of the policy. first the conditions for the use of a normal model must be verified. the pew research center uses sampling methods that produce random samples of the us population (at least approximately) and because each group was a simple random sample from less than 10% of the population, the observations are independent, both within the samples and between the samples. the success-failure condition also holds for each sample, so the normal model can be used for confidence intervals for the difference in approval proportions. the point estimate of the di fference in support, where ˆp1 corresponds to the original ordering and ˆp2to the reversed ordering: ˆp1\u0000ˆp2= 0:47\u00000:34 = 0:13: the standard error can be computed from equation (8.11) using the sample proportions: se\u0019r 0:47(1\u00000:47) 771+0:34(1\u00000:34) 732= 0:025: for a 90% confidence interval, z?= 1:65: point estimate\u0006z?\u0002se! 0:13\u00061:65\u00020:025! (0:09;0:17): with 90% confidence, the proportion approving the 2010 health care law ranged between 9% and 17% depending on the phrasing of the question. the pew research center interpreted this modestly large di fference as an indication that for most of the public, opinions were still fluid on the health insurance mandate. the law eventually passed as the a ffordable health care act (aca). sample size ( ni) approve (%) disapprove (%) other original ordering 771 47 49 3 reversed ordering 732 34 63 3 figure 8.1: results for a pew research center poll where the ordering of two statements in a question regarding healthcare were randomized. 11www.people-press.org/2012/03/26/public-remains-split-on-health-care-bill-opposed-to-mandate. sample sizes for each polling group are approximate. 8.2. inference for the difference of two proportions 397 8.2.3 hypothesis testing for p1\u0000p2p1\u0000p2p1\u0000p2 hypothesis tests for p1\u0000p2are usually testing the null hypothesis of no di fference between p1 andp2; i.e.h0:p1\u0000p2= 0. under the null hypothesis, ˆp1\u0000ˆp2is normally distributed with mean 0 and standard deviationq p(1\u0000p)(1 n1+1 n2), where under the null hypothesis p=p1=p2. sincepis unknown, an estimate is used to compute the standard error of ˆp1\u0000ˆp2;pcan be estimated by ˆp, the weighted average of the sample proportions ˆp1and ˆp2: ˆp=n1ˆp1+n2ˆp2 n1+n2=x1+x2 n1+n2; wherex1is the number of observed events in the first sample and x2is the number of observed events in the second sample. this pooled proportion ˆpis also used to check the success-failure condition. the test statistic zfor testingh0:p1=p2versusha:p1,p2equals: z=ˆp1\u0000ˆp2q ˆp(1\u0000ˆp)\u00101 n1+1 n2\u0011: 398 chapter 8. inference for categorical data example 8.13 the use of screening mammograms for breast cancer has been controversial for decades because the overall benefit on breast cancer mortality is uncertain. several large randomized studies have been conducted in an attempt to estimate the e ffect of mammogram screening. a 30-year study to investigate the e ffectiveness of mammograms versus a standard non-mammogram breast cancer exam was conducted in canada with 89,835 female participants.12during a 5-year screening period, each woman was randomized to either receive annual mammograms or standard physical exams for breast cancer. during the 25 years following the screening period, each woman was screened for breast cancer according to the standard of care at her health care center. at the end of the 25 year follow-up period, 1,005 women died from breast cancer. the results by intervention are summarized in figure 8.2. assess whether the normal model can be used to analyze the study results. since the participants were randomly assigned to each group, the groups can be treated as independent, and it is reasonable to assume independence of patients within each group. participants in randomized studies are rarely random samples from a population, but the investigators in the canadian trial recruited participants using a general publicity campaign, by sending personal invitation letters to women identified from general population lists, and through contacting family doctors. in this study, the participants can reasonably be thought of as a random sample. the pooled proportion ˆpis ˆp=x1+x2 n1+n2=500 + 505 500 + 44;425 + 505 + 44 ;405= 0:0112: checking the success-failure condition for each group: ˆp\u0002nmgm = 0:0112\u000244,925 = 503 (1 \u0000ˆp)\u0002nmgm = 0:9888\u000244,925 = 44,422 ˆp\u0002nctrl= 0:0112\u000244,910 = 503 (1 \u0000ˆp)\u0002nctrl= 0:9888\u000244,910 = 44,407 all values are at least 10. the normal model can be used to analyze the study results. death from breast cancer? yes no mammogram 500 44,425 control 505 44,405 figure 8.2: summary results for the mammogram study. 12miller ab. 2014. twenty five year follow-up for breast cancer incidence and mortality of the canadian national breast screening study: randomised screening trial . bmj 2014;348:g366 doi: 10.1136/bmj.g366 8.2. inference for the difference of two proportions 399 example 8.14 do the results from the study provide convincing evidence of a di fference in the proportion of breast cancer deaths between women who had annual mammograms during the screening period versus women who received annual screening with physical exams? the null hypothesis is that the probability of a breast cancer death is the same for the women in the two groups. if group 1 represents the mammogram group and group 2 the control group, h0:p1=p2andha:p1,p2. let = 0:05. calculate the test statistic z: z=0:01113\u00000:01125q (0:0112)(1\u00000:0112)\u00101 44;925+1 44;910\u0011=\u00000:17: the two-sided p-value ispjzj\u00150:17 = 0:8650, which is greater than 0.05. there is insu fficient evidence to reject the null hypothesis; the observed di fference in breast cancer death rates is reasonably explained by chance. evaluating medical treatments typically requires accounting for additional evidence that cannot be evaluated from a statistical test. for example, if mammograms are much more expensive than a standard screening and do not o ffer clear benefits, there is reason to recommend standard screenings over mammograms. this study also found that a higher proportion of diagnosed breast cancer cases in the mammogram screening arm (3250 in the mammogram group vs 3133 in the physical exam group), despite the nearly equal number of breast cancer deaths. the investigators inferred that mammograms may cause over-diagnosis of breast cancer, a phenomenon in which a breast cancer diagnosed with mammogram and subsequent biopsy may never become symptomatic. the possibility of over-diagnosis is one of the reasons mammogram screening remains controversial. 400 chapter 8. inference for categorical data example 8.15 calculate a 95% confidence interval for the di fference in proportions of deaths from breast cancer from the canadian study. the independence and random sampling conditions have already been discussed. the success failure condition should be checked for each sample, since this is not a hypothesis testing context (i.e., there is no null hypothesis). for the mammogram group, ˆp1= 0:01113;n1ˆp1= (0:1113)(44;925) = 500 andn1(1\u0000ˆp1) = 39;925:it is easy to show that the success failure condition is holds for the control group as well. the point estimate for the di fference in the probability of death is ˆp1\u0000ˆp2= 0:01113\u00000:01125 =\u00000:00012; or 0.012%. the standard error for the estimated di fference uses the individual estimates of the probability of a death: se\u0019r 0:01113(1\u00000:01113) 44;925+0:01125(1\u00000:01125) 44;910= 0:0007: the 95% confidence interval is given by \u00000:00012\u0006(1:96)(0:0007) = (\u00000:0015;0:0013): with 95% confidence, the di fference in the probability of death is between -0.15% and 0.13%. as expected from the large p-value, the confidence interval contains the null value 0. 8.3. inference for two or more groups 401 8.3 inference for two or more groups the comparison of the proportion of breast cancer deaths between the two groups can also be approached using a two-way contingency table, which contains counts for combinations of outcomes for two variables. the results for the mammogram study in this format are shown in figure 8.3. previously, the main question of interest was stated as, \"is there evidence of a di fference in the proportion of breast cancer deaths between the two screening groups?\" if the probability of a death from breast cancer does not depend the method of screening, then screening method and outcome are independent. thus, the question can be re-phrased: \"is there evidence that screening method is associated with outcome?\" hypothesis testing in a two-way table assesses whether the two variables of interest are associated (i.e., not independent). the approach can be applied to settings with two or more groups and for responses that have two or more categories. the observed number of counts in each table cell are compared to the number of expected counts , where the expected counts are calculated under the assumption that the null hypothesis of no association is true. a 2test of significance is based on the di fferences between observed and expected values in the cells. death from bc yes no total mammogram 500 44,425 44,925 control 505 44,405 44,910 total 1,005 88,830 89,835 figure 8.3: results of the mammogram study, as a contingency table with marginal totals. guided practice 8.16 formulate hypotheses for a contingency-table approach to analyzing the mammogram data.13 8.3.1 expected counts if type of breast cancer screening had no e ffect on outcome in the mammogram data, what would the expected results be? recall that if two events aandbare independent, then p(a\\b) =p(a)p(b). letarepresent assignment to the mammogram group and bthe event of death from breast cancer. under independence, the number of individuals out of 89,835 that are expected to be in the mammogram screening group and die from breast cancer equals: (89;835)p(a)p(b) = (89;835)\u001244;925 89;835\u0013\u00121;005 89;835\u0013 = 502:6: note that the quantities 44,925 and 1,005 are the row and column totals corresponding to the upper left cell of figure 8.3, and 89,835 is the total number nof observations in the table. a general formula for computing expected counts for any cell can be written from the marginal totals and the total number of observations. 13h0: there is no association between type of breast cancer screening and death from breast cancer. ha: there is an association between type of breast cancer screening and death from breast cancer. 402 chapter 8. inference for categorical data computing expected counts in a two-way table to calculate the expected count for the ithrow andjthcolumn, compute expected countrowi;colj=(rowitotal)\u0002(columnjtotal) table total: example 8.17 calculate expected counts for the data in figure 8.3. e1;1=44;925\u00021;005 89;835= 502:6e1;2=44;925\u000288;830 89;835= 44;422:4 e2;1=2;922\u00021;005 89;835= 502:4e2;2=7;078\u000288;830 89;835= 44;407:6 death from bc yes no total mammogram 500 (502.6) 44,425 (44,422.4) 44,925 control 505 (502.4) 44,405 (44,407.6) 44,910 total 1,005 88,830 89,835 figure 8.4: results of the mammogram study, with (expected counts) . the expected counts should also sum to the row and column totals; this can be a useful check for accuracy. example 8.18 if a newborn is hiv+, should he or she be treated with nevirapine (nvp) or a more expensive drug, lopinarvir (lpv)? in this setting, success means preventing virologic failure; i.e., growth of the virus. a randomized study was conducted to assess whether there is an association between treatment and outcome.14of the 147 children administered nvp, about 41% experienced virologic failure; of the 140 children administered lpv, about 19% experienced virologic failure. construct a table of observed counts and a table of expected counts. convert the proportions to count data: 41% of 147 is approximately 60, and 19% of 140 is approximately 27. the observed results are given in figure 8.5. calculate the expected counts for each cell: e1;1=87\u0002147 287= 44:6e1;2=87\u0002140 287= 42:4 e2;1=200\u0002147 287= 102:4e2;2=200\u0002140 287= 97:6 the expected counts are summarized in figure 8.6. 14violari a, et al. n engl j med 2012; 366:2380-2389 doi: 10.1056/nejmoa1113249 8.3. inference for two or more groups 403 nvp lpv total virologic failure 60 27 87 stable disease 87 113 200 total 147 140 287 figure 8.5: observed counts for the hiv study. nvp lpv total virologic failure 44.6 42.4 87 stable disease 102.4 97.6 200 total 147 140 287 figure 8.6: expected counts for the hiv study. 8.3.2 the 2 2 2test statistic previously, test statistics have been constructed by calculating the di fference between a point estimate and a null value, then dividing by the standard error of the point estimate to standardize the difference. the 2statistic is based on a di fferent idea. in each cell of a table, the di fference observed -expected is a measure of the discrepancy between what was observed in the data and what should have been observed under the null hypothesis of no association. if the row and column variables are highly associated, that di fference will be large. two adjustments are made to the differences before the final statistic is calculated. first, since both positive and negative di fferences suggest a lack of independence, the di fferences are squared to remove the e ffect of the sign. second, cells with larger counts may have larger discrepancies by chance alone, so the squared di fferences in each cell are scaled by the number expected in the cell under the hypothesis of independence. the final 2statistic is the sum of these standardized squared di fferences, where the sum has one term for each cell in the table. the 2test statistic is calculated as: 2 chi-square test statistic 2=x all cells(observed\u0000expected)2 expected: the theory behind the 2test and its sampling distribution relies on the same normal approximation to the binomial distribution that was introduced earlier. the cases in the dataset must be independent and each expected cell count should be at least 10. the second condition can be relaxed in tables with more than 4 cells. conditions for the 2 2 2test two conditions that must be checked before performing a 2test: independence. each case that contributes a count to the table must be independent of all the other cases in the table. sample size. each expected cell count must be greater than or equal to 10. for tables larger than 2\u00022, it is appropriate to use the test if no more than 1/5 of the expected counts are less than 5, and all expected counts are greater than 1. 404 chapter 8. inference for categorical data example 8.19 for the mammogram data, check the conditions for the 2test and calculate the 2test statistic. independence is a reasonable assumption, since individuals have been randomized to either the treatment or control group. each expected cell count is greater than 10. 2=x all cells(observed\u0000expected)2 expected =(500\u0000502:6)2 502:6+(44;425\u000044;422:4)2 44;422:4+(505\u0000502:4)2 502:4+(44;405\u000044;407:6)2 44;407:6 = 0:02: guided practice 8.20 for the hiv data, check the conditions for the 2test and calculate the 2test statistic.15 8.3.3 calculating ppp-values for a 2 2 2distribution the chi-square distribution is often used with data and statistics that are positive and rightskewed. the distribution is characterized by a single parameter, the degrees of freedom. figure 8.7 demonstrates three general properties of chi-square distributions as the degrees of freedom increases: the distribution becomes more symmetric, the center moves to the right, and the variability increases. 0 5 10 15 20 25degrees of freedom 2 4 9 figure 8.7: three chi-square distributions with varying degrees of freedom. 15independence holds, since this is a randomized study. the expected counts are greater than 10. 2=(60\u000044:6)2 44:6+ (27\u000042:4)2 42:4+(87\u0000102:4)2 102:4+(113\u000097:6)2 97:6= 14:7: 8.3. inference for two or more groups 405 the 2statistic from a contingency table has a sampling distribution that approximately follows a 2distribution with degrees of freedom df= (r\u00001)(c\u00001), whereris the number of rows andcis the number of columns. either statistical software or a table can be used to calculate pvalues from the 2distribution. the chi-square table is partially shown in figure 8.8, and a more complete table is presented in appendix b.3 on . this table is very similar to the t-table: each row provides values for distributions with di fferent degrees of freedom, and a cut-o ffvalue is provided for specified tail areas. one important di fference from the t-table is that the 2table only provides upper tail values. upper tail 0.3 0.2 0.1 0.05 0.02 0.01 0.005 0.001 df 1 1.07 1.64 2.71 3.84 5.41 6.63 7.88 10.83 2 2.41 3.22 4.61 5.99 7.82 9.21 10.60 13.82 3 3.66 4.64 6.25 7.81 9.84 11.34 12.84 16.27 4 4.88 5.99 7.78 9.49 11.67 13.28 14.86 18.47 5 6.06 7.29 9.24 11.07 13.39 15.09 16.75 20.52 6 7.23 8.56 10.64 12.59 15.03 16.81 18.55 22.46 7 8.38 9.80 12.02 14.07 16.62 18.48 20.28 24.32 figure 8.8: a section of the chi-square table. a complete table is in appendix b.3 on . example 8.21 calculate an approximate p-value for the mammogram data, given that the 2statistic equals 0.02. assess whether the data provides convincing evidence of an association between screening group and breast cancer death. the degrees of freedom in a 2 \u00022 table is 1, so refer to the values in the first column of the probability table. the value 0.02 is less than 1.07, so the p-value is greater than 0.3. the data do not provide convincing evidence of an association between screening group and breast cancer death. this supports the conclusions from example 8.14, where the p-value was calculated to be 0.8650 and is visualized in figure 8.9. 0 1 2 3 4 5 figure 8.9: the p-value for the mammogram data is shaded on the 2distribution withdf= 1.the shaded area is to the right of x = 0.02. guided practice 8.22 calculate an approximate p-value for the hiv data. assess whether the data provides convincing evidence of an association between treatment and outcome at the = 0:01 significance level.16 16the 2statistic is 14.7. for degrees of freedom 1, the tail area beyond 14.7 is smaller than 0.001. there is evidence to suggest that treatment is not independent of outcome. 406 chapter 8. inference for categorical data 8.3.4 interpreting the results of a 2 2 2test if thep-value from a 2test is small enough to provide evidence to reject the null hypothesis of no association, it is important to explore the results further to understand direction of the observed association. this is done by examining the residuals, the standardized di fferences of the observed -expected , for each cell. instead of using squared di fferences, the residuals are based on the differences themselves, and the standardizing or scaling factor isp expected. calculating residuals can be particularly helpful for understanding the results from large tables. for each cell in a table, the residual equals: observed\u0000expectedp expected: residuals with a large magnitude contribute the most to the 2statistic. if a residual is positive, the observed value is greater than the expected value, and vice versa for a negative residual. 8.3. inference for two or more groups 407 example 8.23 in the famuss study introduced in chapter 1, researchers measured a variety of demographic and genetic characteristics for about 1,300 participants, including data on race and genotype at a specific locus on the actn3 gene (figure 8.10). is there evidence of an association between genotype and race? first, check the assumptions for applying a 2test. it is reasonable to assume independence, since it is unlikely that any participants were related to each other. none of the expected counts, as shown in figure 8.11, are less than 5. h0: race and genotype are independent. ha: race and genotype are not independent. let = 0:05. calculate the 2statistic: 2=x all cells(observed\u0000expected)2 expected =(16\u00007:85)2 7:85+(6\u000011:84)2 11:84+:::+(5\u00006:22)2 6:22 = 19:4: calculate the p-value: for a table with 3 rows and 5 columns, the 2statistic is distributed with (3\u00001)(5\u00001) = 8 degrees of freedom. from the table, a 2value of 19.4 corresponds to a tail area between 0.01 and 0.02. thus, there is su fficient evidence to reject the null hypothesis of independence between race and genotype. thep-value can be obtained using the rfunction pchisq (pchisq(19.4, df = 8, lower.tail = false) ), which returns a value of 0.012861. to further explore the di fferences in genotype distribution between races, calculate residuals for each cell (figure 8.12). the largest residuals are in the first row; there are many more african americans with the cc genotype than expected under independence, and fewer with the ct genotype than expected. the residuals in the second row indicate a similar trend for asians, but with a less pronounced di fference. these results suggest further directions for research; a future study could enroll a larger number of african american and asian participants to examine whether the observed trend holds with a more representative sample. geneticists might also be interested in exploring whether this genetic di fference between populations has an observable phenotypic e ffect. cc ct tt sum african american 16 6 5 27 asian 21 18 16 55 caucasian 125 216 126 467 hispanic 4 10 9 23 other 7 11 5 23 sum 173 261 161 595 figure 8.10: observed counts for race and genotype data from the famuss study. 408 chapter 8. inference for categorical data cc ct tt sum african am 7.85 11.84 7.31 27.00 asian 15.99 24.13 14.88 55.00 caucasian 135.78 204.85 126.36 467.00 hispanic 6.69 10.09 6.22 23.00 other 6.69 10.09 6.22 23.00 sum 173.00 261.00 161.00 595.00 figure 8.11: expected counts for race and genotype data from the famuss study. cc ct tt sum african am 2.91 -1.70 -0.85 0.00 asian 1.25 -1.25 0.29 0.00 caucasian -0.93 0.78 -0.03 0.00 hispanic -1.04 -0.03 1.11 0.00 other 0.12 0.29 -0.49 0.00 sum 0.00 0.00 0.00 0.00 figure 8.12: residuals for race and genotype data from the famuss study. example 8.24 in guided practice 8.22, the p-value was found to be smaller than 0.001, suggesting that treatment is not independent of outcome. does the evidence suggest that infants should be given nevirapine or lopinarvir? in a 2\u00022 table, it is relatively easy to directly compare observed and expected counts. for nevirapine, more infants than expected experienced virologic failure (60 > 44.6), while fewer than expected reached a stable disease state (87 < 102.4). for lopinarvir, fewer infants than expected experienced virologic failure (27 < 42.4), and more infants than expected reached a stable disease state (113 > 97.6) (figure 8.13). the outcomes for infants on lopinarvir are better than for those on nevirapine; combined with the results of the significance test, the data suggest that lopinarvir is associated with better treatment outcomes. nvp lpv total virologic failure 6044.6 2742.4 87 stable disease 87102.4 113 97.6 200 total 147 140 287 figure 8.13: observed and (expected) counts for the hiv study. guided practice 8.25 confirm the conclusions reached in example 8.24 by analyzing the residuals.17 17r1;1=(44:6\u000060)p 44:6= 2:31;r1;2=(42:4\u000027)p 27=\u00002:37;r2;1=(87\u0000102:4)p 102:4=\u00001:53;r2;2=(113\u000097:6)p 97:6= 1:56. the positive residuals for the upper left and lower right cells indicate that more infants than expected experienced virologic failure on nvp and stable disease on lpv; vice versa for the upper right and lower left cells. the larger magnitude of the residuals for the two nvp cells indicates that most of the discrepancy between observed and expected counts is for outcomes related to nvp . 8.3. inference for two or more groups 409 guided practice 8.26 chapter 1 started with the discussion of a study examining whether exposure to peanut products reduce the rate of a child developing peanut allergies. children were randomized either to the peanut avoidance or the peanut consumption group; at 5 years of age, each child was tested for peanut allergy using an oral food challenge (ofc). the results of the ofc are reproduced in figure 8.14; failing the food challenge indicates an allergic reaction. assess whether there is evidence for exposure to peanut allergy reducing the chance of developing peanut allergies.18 fail ofc pass ofc sum peanut avoidance 36 227 263 peanut consumption 5 262 267 sum 41 489 530 figure 8.14: leap study results. 8.3.5 fisher’s exact test if sample sizes are too small, the 2distribution does not yield accurate p-values for assessing independence of the row and column variables in a table. when expected counts in a table are less than 10, fisher’s exact test is often used to calculate exact levels of significance. this test is usually applied to 2\u00022 tables. it can be applied to larger tables, but the logic behind the test is complex and the calculations involved are computationally intensive, so this section covers only 2 \u00022 tables. clostridium di fficileis a bacterium that causes inflammation of the colon. antibiotic treatment is typically not e ffective, particularly for patients who experience multiple recurrences of infection. infusion of feces from healthy donors has been reported as an e ffective treatment for recurrent infection. a randomized trial was conducted to compare the e fficacy of donor-feces infusion versus vancomycin, the antibiotic typically prescribed to treat c. difficileinfection. the results of the trial are shown in figure 8.15.19a brief calculation shows that all of the expected cell counts are less than 10, so the 2test should not be used as a test for association. under the null hypothesis, the probabilities of cure in the fecal infusion and vancomycin groups are equal; i.e., individuals in one group are just as likely to be cured as individuals in the other group. suppose the probability that an individual is cured, given that he or she was assigned to the fecal infusion group, is p1and the probability an individual is cured in the vancomycin group isp2. researchers were interested in testing the null hypothesis h0:p1=p2. cured uncured sum fecal infusion 13 3 16 vancomycin 4 9 13 sum 17 12 29 figure 8.15: fecal infusion study results. 18the assumptions for conducting a 2test are satisfied. calculate a 2test statistic: 24.29. the associated p-value is 8:3\u000210\u00007. there is evidence to suggest that treatment group is not independent of outcome. specifically, a residual analysis shows that in the peanut avoidance group, more children than expected failed the ofc; in the peanut consumption group, more children than expected passed the ofc. 19these results correspond to the number of patients cured after the first infusion of donor feces and the number of patients cured in the vancomycin-alone group. 410 chapter 8. inference for categorical data thep-value is the probability of observing results as or more extreme than those observed in the study under the assumption that the null hypothesis is true. previously discussed methods for significance testing have relied on calculating a test statistic associated with a defined sampling distribution, then obtaining p-values from tail areas on the distribution. fisher’s exact test uses a similar approach, but introduces a new sampling distribution. thep-value for fisher’s exact test is calculated by adding together the individual conditional probabilities of obtaining each table that is as or more extreme than the one observed, under the null hypothesis and given that the marginal totals are considered fixed. – when the row and column totals are held constant, the value of any one cell in the table determines the rest of the entries. for example, if the marginal sums in figure 8.15 are known, along with the value in one cell (e.g., the upper right equals 3), it is possible to calculate the values in the other three cells. thus, when marginal totals are considered fixed, each table represents a unique set of results. – extreme tables are those which contradict the null hypothesis of p1=p2. in the fecal infusion group, under the null hypothesis of no di fference in the population proportion cured, one would expect16\u000217 29= 9:38 cured individuals. the 13 observed cured individuals is extreme in the direction of more being cured than expected under the null hypothesis. an extreme result in the other direction would be, for instance, 1 cured patient in the fecal infusion group and 16 in the vancomycin group. example 8.27 of the 17 patients cured, 13 were in the fecal infusion group and 4 were in the vancomycin group. assume that the marginal totals are fixed (i.e., 17 patients were cured, 12 were uncured, and 16 patients were in the fecal infusion group, while 13 were in the vancomycin group). enumerate all possible sets of results that are more extreme than what was observed, in the same direction. the observed results show a case of ˆp1>ˆp2; results that are more extreme consist of cases where more than 13 cured patients were in the fecal infusion group. under the assumption that the total number of cured patients is constant at 17 and that only 16 patients were assigned to the fecal infusion group (out of 29 patients total), more extreme results are represented by cases where 14, 15, or 16 cured patients were in the fecal infusion group. the following tables illustrate the unique combinations of values for the 4 table cells corresponding to those extreme results. cured uncured sum fecal infusion 14 2 16 vancomycin 3 10 13 sum 17 12 29 cured uncured sum fecal infusion 15 1 16 vancomycin 2 11 13 sum 17 12 29 cured uncured sum fecal infusion 16 0 16 vancomycin 1 12 13 sum 17 12 29 8.3. inference for two or more groups 411 calculating a one-sided ppp-value suppose that researchers were interested in testing the null hypothesis against the one-sided alternative,ha:p1> p 2. to calculate the one-sided p-value, sum the probabilities of each table representing results as or more extreme than those observed; specifically, sum the probabilities of observing figure 8.15 and the tables in example 8.27. cured uncured sum fecal infusion a b a +b vancomycin c d c +d suma+c b +d n figure 8.16: general layout of data in fecal infusion study. the probability of observing a table with cells a;b;c;d given fixed marginal totals a+b,c+d, a+c, andb+dfollows the hypergeometric distribution. the hypergeometric distribution was introduced in section 3.5.3. p(a;b;c;d ) = hgeom( a+b;c+d;a+c) =\u0000a+b a\u0001\u0000c+d c\u0001 \u0000n a+c\u0001=(a+b)! (c+d)! (a+c)! (b+d)! a!b!c!d!n!: example 8.28 calculate the probability of observing figure 8.15, assuming the margin totals are fixed. p(13;3;4;9) =\u000016 13\u0001\u000013 4\u0001 \u000029 17\u0001=16! 13! 17! 12! 13! 3! 4! 9! 29!= 7:71\u000210\u00003: the value 0.0077 represents the probability of observing 13 cured patients out of 16 individuals in the fecal infusion group and 1 cured in the vancomycin group, given that there are a total of 29 patients and 17 were cured overall. 412 chapter 8. inference for categorical data example 8.29 evaluate the statistical significance of the observed data in figure 8.15 using the one-sided alternativeha:p1>p2. calculate the probability of the tables from example 8.27. generally, the formula for these tables is p(a;b;c;d ) =\u0000a+b a\u0001\u0000c+d c\u0001 \u0000n a+c\u0001=\u000016 a\u0001\u000013 c\u0001 \u000029 17\u0001; since the marginal totals from figure 8.15 are fixed. the value aranges from 14, 15, 16, while c ranges from 3, 2, 1. p(14;2;3;10) =\u000016 14\u0001\u000013 3\u0001 \u000029 17\u0001= 6:61\u000210\u00004 p(15;1;2;11) =\u000016 15\u0001\u000013 2\u0001 \u000029 17\u0001= 2:40\u000210\u00005 p(16;0;1;12) =\u000016 16\u0001\u000013 1\u0001 \u000029 17\u0001= 2:51\u000210\u00007 the probability of the observed table is 7 :71\u000210\u00003, as calculated in the previous example. the one-sided p-value is the sum of these table probabilities: (7 :71\u000210\u00003) + (6:61\u000210\u00004) + (2:40\u0002 10\u00005) + (2:51\u000210\u00007) = 0:0084: the results are significant at the = 0:05 significance level. there is evidence to support the onesided alternative that the proportion of cured patients in the fecal infusion group is higher than the proportion of cured patients in the vancomycin group. however, it is important to note that two-sided alternatives are the standard in medical literature. conducting a two-sided test would be especially desirable when evaluating a treatment which lacks randomized trials supporting its efficacy, such as donor-feces infusion. calculating a two-sided ppp-value there are various methods for calculating a two-sided p-value in the fisher’s exact test setting. when the test is calculated by hand, the most common way to calculate a two-sided p-value is to double the smaller of the one-sided p-values. one other common method used by various statistical computing packages such as ris to classify \"more extreme\" tables as all tables with probabilities less than that of the observed table, in both directions. the two-sided p-value is the sum of probabilities for the qualifying tables. that approach is illustrated in the next example. 8.3. inference for two or more groups 413 example 8.30 evaluate the statistical significance of the observed data in figure 8.15 using the two-sided alternativeha:p1,p2. identify tables that are more extreme in the other direction of the observed result, i.e. where the proportion of cured patients in the vancomycin group are higher than in the fecal infusion group. start with the most extreme cases and calculate probabilities until a table has a p-value higher than 7:71\u000210\u00003, the probability of the observed table. the most extreme result in the ˆp1<ˆp2direction would be if all patients in the vancomycin group were cured; then 13 of the cured patients would be in the vancomycin group and 4 would be in the fecal transplant group. this table has probability 3 :5\u000210\u00005. cured uncured sum fecal infusion 4 12 16 vancomycin 13 0 13 sum 17 12 29 continue enumerating tables by decreasing the number of cured patients in the vancomycin group. the table with 5 cured patients in the fecal infusion group has probability 1 :09\u000210\u00003. cured uncured sum fecal infusion 5 11 16 vancomycin 12 1 13 sum 17 12 29 the table with 6 cured patients in the fecal infusion group has probability 0.012. this value is greater than 7 :71\u000210\u00003, so it will not be part of the sum to calculate the two-sided p-value. cured uncured sum fecal infusion 6 10 16 vancomycin 11 2 13 sum 17 12 29 as calculated in the previous example, the one-sided p-value is 0:0084. thus, the two-sided p-value for these data equals 0 :0084 + (3:5\u000210\u00005) + (1:09\u000210\u00003) = 0:0095. the results are significant at the = 0:01 significance level, and there is evidence to support the e fficacy of donor-feces infusion as a treatment for recurrent c. difficileinfection. 414 chapter 8. inference for categorical data 8.4 chi-square tests for the fit of a distribution the 2test can also be used to examine the appropriateness of hypothesized distribution for a dataset, most commonly when a set of observations falls naturally into categories as in the examples discussed in this section. as with testing in the two-way table setting, expected counts are calculated based on the assumption that the hypothesized distribution is correct, and the statistic is based on the discrepancies between observed and expected counts. the 2sampling distribution for the test statistic is reasonably accurate when each expected count is at least 5 and follows a 2 distribution with k\u00001 degrees of freedom, where kis the number of categories. some guidelines recommend that no more than 1/5 of the cells have expected counts less than 5, but the stricter requirement that all cells have expected counts greater than 5 is safer. when used in this setting, the 2test is often called a ‘goodness-of-fit’ test , a term that is often misunderstood. small p-values of the test suggest evidence that a hypothesized distribution is not a good model, but non-significant p-values do not imply that the hypothesized distribution is the best model for the data, or even a good one. in the logic of hypothesis testing, failure to reject a null hypothesis cannot be viewed as evidence that the null hypothesis is true. example 8.31 the participants in the famuss study were volunteers at a university, and so did not come from a random sample of the us population. the participants may not be representative of the general united states population. the 2test can be used to test the null hypothesis that the participants are racially representative of the general population. figure 8.17 shows the number observed by racial category in famuss and the proportions of the us population in each of those categories.20 under the null hypothesis, the sample proportions should equal the population proportions. for example, since african americans are 0.128 of the general proportion, (0 :128)(595) = 76 :16 african americans would be expected in the sample. the rest of the expected counts are shown in figure 8.18. since each expected count is greater than or equal to 5, the 2distribution can be used to calculate ap-value for the test. 2=x all cells(observed\u0000expected)2 expected =(27\u000076:16)2 76:16+(55\u00005:95)2 5:95+(467\u0000478:38)2 478:38+(46\u000034:51)2 34:51 = 440:18: there are 3 degrees of freedom, since k= 4. the 2statistic is extremely large, and the associated tail area is smaller than 0.001. there is more than su fficient evidence to reject the null hypothesis that the sample is representative of the general population. a comparison of the observed and expected values (or the residuals) indicates that the largest discrepancy is with the over-representation of asian participants. 20the us census bureau considers hispanic as a classification separate from race, on the basis that hispanic individuals can be any race. in order to facilitate the comparison with the famuss data, participants identified as \"hispanic\" have been merged with the \"other\" category. 8.4. chi-square tests for the fit of a distribution 415 race african american asian caucasian other total famuss 27 55 467 46 595 us census 0.128 0.01 0.804 0.058 1.00 figure 8.17: representation by race in the famuss study versus the general population. race african american asian caucasian other total observed 27 55 467 46 595 expected 76.16 5.95 478.38 34.51 595 figure 8.18: actual and expected counts in the famuss data. example 8.32 according to mendelian genetics, alleles segregate independently; if an individual is heterozygous for a gene and has alleles aandb, then the alleles have an equal chance of being passed to an offspring. under this framework, if two individuals with genotype abmate, then their o ffspring are expected to exhibit a 1:2:1 genotypic ratio; 25% of the o ffspring will be aa, 50% will be ab, and 50% will be bb. the term \"segregation distortion\" refers to a deviation from expected mendelian frequencies. at a specific gene locus in the plant arabidopsis thaliana , researchers have observed 84 aaindividuals, 233 abindividuals, and 134 bbindividuals. is there evidence of segregation disorder at this locus? conduct the test at = 0:0001 to account for multiple testing, since the original study examined approximately 250 locations across the genome. the mendelian proportions are 25%, 50%, and 25%. thus, the expected counts in a group of 451 individuals are: 112.75 aa, 225.50ab, and 112.75 bb. no expected count is less than 5. 2=x all cells(observed\u0000expected)2 expected =(84\u0000112:75)2 112:75+(233\u0000225:50)2 225:50+(134\u0000112:75)2 112:75 = 11:59: there are 2 degrees of freedom, since k= 3. thep-value is between 0.005 and 0.001, which is greater than = 0:0001. there is insu fficient evidence to reject the null hypothesis that the o ffspring ratios correspond to expected mendelian frequencies; i.e., there is not evidence of segregation distortion at this locus. 416 chapter 8. inference for categorical data 8.5 outcome-based sampling: case-control studies 8.5.1 introduction the techniques so far in this chapter have often relied on the assumption that the data were collected using random sampling from a population. when cases come from a random sample, the sample proportion of observations with a particular outcome should accurately estimate the population proportion, given that the sample size is large enough. when studying rare outcomes, however, moderate sized samples may contain few or none of the outcomes. persistent pulmonary hypertension of the newborn (pphn) is a dangerous condition in which the blood vessels in the lungs of a newborn do not relax immediately after birth, leading to inadequate oxygenation. the condition is rare, occurring in about 1.9 per 1,000 live births, so it is di fficult to study using random sampling. in the early 2000s, anecdotal evidence began to accumulate that the risk of the condition might be increased if the mother of the newborn had been taking a particular medication for depression, a selective serotonin reuptake inhibitor (ssri) during the third trimester of pregnancy or even as early as during week 20 of the pregnancy. one design for studying the issue would enroll two cohorts of women, one in which women were taking ssris for depression and one in which they were not. however, if the chance of pphn was 1.9/1,000 in newborns of a control cohort of 1,000 women, then the probability of observing no cases of pphn is about 0.15. if the probability of pphn is elevated among infants born to women taking ssris, such as to 3.0/1,000, the chance of observing no cases among 1,000 women is approximately 0.05. precise measures of the probability of pphn occurring would require very large cohorts. an alternative design for studies like this reverses the sampling scheme so that the two cohorts are determined by outcome, rather than exposure; a cohort with the condition and a cohort without the condition are sampled, then exposure to a possible cause is recorded. to apply this design for studying pphn, a registry of live births could be used to sort births by presence or absence of pphn. the number in each group in which the mother had been taking ssris could then be recorded (based on medical records). such a design would have the advantage of su fficient numbers of cases with and without pphn, but it has other limitations which will be discussed later in this section. traditionally, these studies have been called case-control studies because of the original sampling of individuals with and without a condition. more generally, it is an example of outcomedependent sampling. 8.5. outcome-based sampling: case-control studies 417 8.5.2 2 2 2tests of association in case-control studies in 2006, chambers, et. al reported a case-control study examining the association of ssri use and persistent pulmonary hypertension in newborns.21the study team enrolled 337 women whose infants su ffered from pphn and 836 women with similar characteristics but whose infants did not have pphn. among the women whose infants had pphn, 14 had taken an ssri after week 20 of the pregnancy. in the cohort of women whose infants did not have pphn, 6 had been taking the medication after week 20. in the subset of women who had been taking an ssri, the infants are considered ‘exposed’ to the medication. the data from the study are summarized in figure 8.19. pphn present yes no total ssri exposed 14 6 20 ssri unexposed 323 830 1153 total 337 836 1173 figure 8.19: ssri exposure vs observed number of pphn cases in newborns. the sample of women participating in the study are clearly not a random sample drawn from women who had recently given birth; they were identified according to the disease status of their infants. in this sample, the proportion of newborns with pphn (337/1173 = 28.7%) is much higher than the disease prevalence in the general population. even so, the concept of independence between rows and columns under a null hypothesis of no association still holds. if ssri use had no e ffect on the occurrence of pphn, then the proportions of mothers taking ssris among the pphn and non-pphn infants should be about the same. in other words, the null hypothesis of equal ssri use among mothers with/without pphn a ffected infants is the hypothesis of no association between ssri use and pphn. the test of independence can be conducted using the approach introduced earlier in the chapter. the expected counts shown in figure 8.20 suggest that the p-value from a 2test may not be accurate; under the null hypothesis, the expected number of pphn cases in the ssri exposed group is less than 10. pphn present yes no total ssri exposed 5.80 14.20 20 ssri unexposed 331.20 811.80 1153 total 337 836 1173 figure 8.20: ssri exposure vs expected number of pphn cases in newborn. thep-value from fisher’s exact test is <0:001 (0:00014, to be precise), so the evidence is strong that ssri exposure and pphn are associated. fisher’s exact test is often used in studies of rare conditions or exposures since one or more expected cell counts are typically less than 10. 21n engl j med 2006;354:579-87. 418 chapter 8. inference for categorical data 8.5.3 estimates of association in case-control studies for data in a 2\u00022 table, correct point estimates of association depend on the mechanism used to gather the data. in the example of a clinical trial of nevirapine versus lopinarvir discussed in section 8.3.1, the population proportion of children who would experience virologic failure after treatment with one of the drugs can be estimated by the observed proportion of virologic failures while on that drug. for nevirapine, the proportion of children with virologic failure is 60/147 = 0.41, while for lopinarvir the proportion is 27/140 = 0.19. the di fference in outcome between the two groups can be summarized by the di fference in these proportions. the proportion experiencing virologic failure when treated with nevirapine was 0.12 larger in nevirapine (0.41 - 0.29), so if the two drugs were to be used in a large population, approximately 12% more children treated with nevirapine would experience virologic failure as compared to lopinarvir. the confidence intervals discussed in section 8.2.2 can be used to express the uncertainty in this estimate. since the proportion of virologic failures can be estimated from the trial data, the relative risk of virologic failure can also be used to estimate the association between treatment and virologic failure. relative risk is the ratio of two proportions, and was introduced in section 1.6.2. the relative risk of virologic failure with nevirapine versus lopinarvir is 0 :41=0:19 = 2:16. children treated with nevirapine are estimated to be more than twice as likely to experience virologic failure. statistically, the population parameter for the relative risk in the study of hiv+is a ratio of conditional probabilities: p(virologic failurejtreatment with nevirapine) p(virologic failurejtreatment with lopinarvir): in a study like the pphn case-control study, the natural population parameter of interest would be the relative risk of pphn for infants exposed to an ssri during after week 20 of gestation compared to those who were not exposed. however, in the design of this study, participating mothers were sampled and grouped according to whether their infants did or did not su ffer from pphn, rather than assigned to either ssri exposure or non-exposure. relative risk of pphn from exposure to ssri cannot be estimated from the data because it is not possible to estimate p(pphnjssri exposure) and p(pphnjno ssri exposure). in case-control studies, association is estimated using odds and odds ratios rather than relative risk. the odds of ssri exposure among the cases are given by the fraction odds cases =p(ssri exposurejpphn) p(no ssri exposure jpphn)=14=337 323=337=14 323: the odds of ssri exposure among the controls are given by the fraction odds controls =p(ssri exposurejno pphn) p(no ssri exposure jno pphn)=6=836 830=836=6 830: the ratio of the odds, the odds ratio, compares the odds of exposure among the cases to the odds of exposure among the controls: or exposure, cases vs. controls =odds cases odds controls=14=323 6=830=(14)(830) (323)(6)= 6:00: 8.5. outcome-based sampling: case-control studies 419 a population odds ratio of, for example, 1.5, implies that the odds of exposure in cases are 50% larger than the odds of exposure in controls. for this study, the odds ratio of 6.00 implies that the odds of ssri exposure in infants with pphn are 6 times as large as the odds of exposure in infants without pphn. epidemiologists describe this odds ratio as the odds of exposure given presence of pphn compared to the odds of exposure given absence of pphn. an or greater than 1 suggests that the exposure may be a risk factor for the disease or condition under study. epidemiologists also use the term relative odds as a synonym for odds ratio. surprisingly, the odds ratio of exposure comparing cases to controls is equivalent to the odds ratio of disease comparing exposed to unexposed.22with a specific example, it is easy to see how the fraction for the odds ratios are numerically equivalent: or disease, exposed versus unexposed =odds exposed odds unexposed=14=6 323=830=(14)(830) (6)(323)= 6:00: despite the apparently restrictive nature of the case-control sampling design, the odds ratio of interest, the odds ratio for disease given exposure, can be estimated from case-control data. epidemiologists rely on one additional result, called the rare disease assumption. when a disease is rare, the odds ratio for the disease given exposure is approximately equal to the relative risk of the disease given exposure. these identities are the reason case-control studies are widely used in settings in which a disease is rare: it allows for the relative risk of disease given exposure to be estimated, even if the study design is based on sampling cases and controls then measuring exposure. in a general 2\u00022 table of exposure versus disease status (figure 8.21) the odds ratio for disease given exposure status is the ad=bc . disease status present absent total exposed a b a +b unexposed c d c +d total a+c b +d n figure 8.21: exposure vs disease status. in the pphn case-control data, the odds ratio for pphn given ssri exposure status is (14)(830) =(6)(323) = 6:00. because pphn is a rare condition, the risk of pphn among infants exposed to an ssri is estimated to be approximately 6 times that of the risk among unexposed infants. infants exposed to an ssri are 600% more likely to su ffer from pphn. it can be shown that the p-value used in a test of no association (between exposure and disease) is also thep-value for a test of the null hypothesis that the odds ratio is 1. 22this result can be shown through bayes’ rule. 420 chapter 8. inference for categorical data 8.6 notes two-way tables are often used to summarize data from medical research studies, and entire texts have been written about methods of analysis for these tables. this chapter covers only the most basic of those methods. until recently, fisher’s exact test could only be calculated for 2 \u00022 tables with small cell counts. research has produced faster algorithms for enumerating tables and calculating p-values, and the computational power of recent desktop and laptop computers now make it possible to calculate the fisher test on nearly any 2 \u00022 table. there are also versions of the test that can be calculated on tables with more than 2 rows and/or columns. the practical result for data analysts is that the sample size condition for the validity of the 2test can be made more restrictive. this chapter recommends using the 2test only when cell counts in a 2 \u00022 table are greater than 10; some approaches recommend cell counts larger than 10. for many years, introductory textbooks recommended using a modified version of the 2test, called the fisher-yates test, which adjusted the value of the statistic in small sample sizes to increase the accuracy of the 2sampling distribution in calculating p-values. the fisher-yates version of the test is no longer used as often because of the widespread availability of the fisher test. the fisher test is not without controversy, at least in the theoretical literature. conditioning on the row and column totals allows the calculation of a p-value from the hypergeometric distribution, but in principle restricts inference to the set of tables with the same row and column values. in practice, this is less serious than it may seem. for tables of moderate size, the p-values from the 2and fisher tests are nearly identical and for tables with small counts, the fisher test guarantees that the type i error will be no larger than the specified value of . in small sample sizes, some statisticians argue that the fisher-yates correction is preferable to the fisher test because of the discrete nature of the hypergeometric distribution. in small tables, for example, an observed p-value of 0.04 may be the largest value that is less than 0.05, such that the type i error of the test in that situation is 0.04, not 0.05. section 8.5.3 does not show the derivation that the odds ratio estimated from a case-control is the same as that from a cohort study. it is long and algebraically more complex than other derivations shown in the text, but it is a direct application of bayes’ rule, applied to each term in the fraction that defines population odds ratio. the two labs for this chapter examine methods of inference for the success probability in binomial data then generalizes inference for binomial proportions to two-way contingency tables. lab 2 also discusses measures of association in two-by-two tables. the datasets in the labs are similar to datasets that arise frequently in medical statistics. lab 1 assesses the evidence for a treatment e ffect in a single uncontrolled trial of a new drug for melanoma and whether outcomes in stage 1 lung cancer are di fferent among patients treated at dana-farber cancer institute compared to population based statistics. in lab 2, students analyze a dataset from a published clinical trial examining the benefit of using a more expensive but potentially more e ffective drug to treat hivpositive infants. 8.7. exercises 421 8.7 exercises 8.7.1 inference for a single proportion 8.1 vegetarian college students. suppose that 8% of college students are vegetarians. determine if the following statements are true or false, and explain your reasoning. (a) the distribution of the sample proportions of vegetarians in random samples of size 60 is approximately normal since n\u001530. (b) the distribution of the sample proportions of vegetarian college students in random samples of size 50 is right skewed. (c) a random sample of 125 college students where 12% are vegetarians would be considered unusual. (d) a random sample of 250 college students where 12% are vegetarians would be considered unusual. (e) the standard error would be reduced by one-half if we increased the sample size from 125 to 250. 8.2 young americans, part i. about 77% of young adults think they can achieve the american dream. determine if the following statements are true or false, and explain your reasoning.23 (a) the distribution of sample proportions of young americans who think they can achieve the american dream in samples of size 20 is left skewed. (b) the distribution of sample proportions of young americans who think they can achieve the american dream in random samples of size 40 is approximately normal since n\u001530. (c) a random sample of 60 young americans where 85% think they can achieve the american dream would be considered unusual. (d) a random sample of 120 young americans where 85% think they can achieve the american dream would be considered unusual. 8.3 gender equality. the general social survey asked a random sample of 1,390 americans the following question: “on the whole, do you think it should or should not be the government’s responsibility to promote equality between men and women?” 82% of the respondents said it “should be”. at a 95% confidence level, this sample has 2% margin of error. based on this information, determine if the following statements are true or false, and explain your reasoning.24 (a) we are 95% confident that between 80% and 84% of americans in this sample think it’s the government’s responsibility to promote equality between men and women. (b) we are 95% confident that between 80% and 84% of all americans think it’s the government’s responsibility to promote equality between men and women. (c) if we considered many random samples of 1,390 americans, and we calculated 95% confidence intervals for each, 95% of these intervals would include the true population proportion of americans who think it’s the government’s responsibility to promote equality between men and women. (d) in order to decrease the margin of error to 1%, we would need to quadruple (multiply by 4) the sample size. (e) based on this confidence interval, there is su fficient evidence to conclude that a majority of americans think it’s the government’s responsibility to promote equality between men and women. 23a. vaughn. “poll finds young adults optimistic, but not about money”. in: los angeles times (2011). 24national opinion research center, general social survey, 2018. 422 chapter 8. inference for categorical data 8.4 elderly drivers. the marist poll published a report stating that 66% of adults nationally think licensed drivers should be required to retake their road test once they reach 65 years of age. it was also reported that interviews were conducted on 1,018 american adults, and that the margin of error was 3% using a 95% confidence level.25 (a) verify the margin of error reported by the marist poll. (b) based on a 95% confidence interval, does the poll provide convincing evidence that more than 70% of the population think that licensed drivers should be required to retake their road test once they turn 65? 8.5 fireworks on july 4th.a local news outlet reported that 56% of 600 randomly sampled kansas residents planned to set o fffireworks on july 4th. determine the margin of error for the 56% point estimate using a 95% confidence level.26 8.6 life rating in greece. greece has faced a severe economic crisis since the end of 2009. a gallup poll surveyed 1,000 randomly sampled greeks in 2011 and found that 25% of them said they would rate their lives poorly enough to be considered “su ffering”.27 (a) describe the population parameter of interest. what is the value of the point estimate of this parameter? (b) check if the conditions required for constructing a confidence interval based on these data are met. (c) construct a 95% confidence interval for the proportion of greeks who are “su ffering\". (d) without doing any calculations, describe what would happen to the confidence interval if we decided to use a higher confidence level. (e) without doing any calculations, describe what would happen to the confidence interval if we used a larger sample. 8.7 study abroad. a survey on 1,509 high school seniors who took the sat and who completed an optional web survey shows that 55% of high school seniors are fairly certain that they will participate in a study abroad program in college.28 (a) is this sample a representative sample from the population of all high school seniors in the us? explain your reasoning. (b) let’s suppose the conditions for inference are met. even if your answer to part (a) indicated that this approach would not be reliable, this analysis may still be interesting to carry out (though not report). construct a 90% confidence interval for the proportion of high school seniors (of those who took the sat) who are fairly certain they will participate in a study abroad program in college, and interpret this interval in context. (c) what does “90% confidence\" mean? (d) based on this interval, would it be appropriate to claim that the majority of high school seniors are fairly certain that they will participate in a study abroad program in college? 25marist poll, road rules: re-testing drivers at age 65?, march 4, 2011. 26survey usa, news poll #19333, data collected on june 27, 2012. 27gallup world, more than one in 10 “su ffering\" worldwide, data collected throughout 2011. 28studentpoll, college-bound students’ interests in study abroad and other international learning activities, january 2008. 8.7. exercises 423 8.8 legalization of marijuana, part i. the general social survey asked 1,578 us residents: “do you think the use of marijuana should be made legal, or not?” 61% of the respondents said it should be made legal.29 (a) is 61% a sample statistic or a population parameter? explain. (b) construct a 95% confidence interval for the proportion of us residents who think marijuana should be made legal, and interpret it in the context of the data. (c) a critic points out that this 95% confidence interval is only accurate if the statistic follows a normal distribution, or if the normal model is a good approximation. is this true for these data? explain. (d) a news piece on this survey’s findings states, “majority of americans think marijuana should be legalized.” based on your confidence interval, is this news piece’s statement justified? 8.9 national health plan, part i. akaiser family foundation poll for us adults in 2019 found that 79% of democrats, 55% of independents, and 24% of republicans supported a generic “national health plan”. there were 347 democrats, 298 republicans, and 617 independents surveyed.30 (a) a political pundit on tv claims that a majority of independents support a national health plan. do these data provide strong evidence to support this type of statement? (b) would you expect a confidence interval for the proportion of independents who oppose the public option plan to include 0.5? explain. 8.10 legalize marijuana, part ii. as discussed in exercise 8.8, the general social survey reported a sample where about 61% of us residents thought marijuana should be made legal. if we wanted to limit the margin of error of a 95% confidence interval to 2%, about how many americans would we need to survey? 8.11 national health plan, part ii. exercise 8.9 presents the results of a poll evaluating support for a generic “national health plan” in the us in 2019, reporting that 55% of independents are supportive. if we wanted to estimate this number to within 1% with 90% confidence, what would be an appropriate sample size? 8.12 acetaminophen and liver damage. it is believed that large doses of acetaminophen (the active ingredient in over the counter pain relievers like tylenol) may cause damage to the liver. a researcher wants to conduct a study to estimate the proportion of acetaminophen users who have liver damage. for participating in this study, he will pay each subject $20 and provide a free medical consultation if the patient has liver damage. (a) if he wants to limit the margin of error of his 98% confidence interval to 2%, what is the minimum amount of money he needs to set aside to pay his subjects? (b) the amount you calculated in part (a) is substantially over his budget so he decides to use fewer subjects. how will this a ffect the width of his confidence interval? 8.13 college smokers. we are interested in estimating the proportion of students at a university who smoke. out of a random sample of 200 students from this university, 40 students smoke. (a) calculate a 95% confidence interval for the proportion of students at this university who smoke, and interpret this interval in context. (reminder: check conditions.) (b) if we wanted the margin of error to be no larger than 2% at a 95% confidence level for the proportion of students who smoke, how big of a sample would we need? 29national opinion research center, general social survey, 2018. 30kaiser family foundation, the public on next steps for the aca and proposals to expand coverage, data collected between jan 9-14, 2019. 424 chapter 8. inference for categorical data 8.14 2010 healthcare law. on june 28, 2012 the u.s. supreme court upheld the much debated 2010 healthcare law, declaring it constitutional. a gallup poll released the day after this decision indicates that 46% of 1,012 americans agree with this decision. at a 95% confidence level, this sample has a 3% margin of error. based on this information, determine if the following statements are true or false, and explain your reasoning.31 (a) we are 95% confident that between 43% and 49% of americans in this sample support the decision of the u.s. supreme court on the 2010 healthcare law. (b) we are 95% confident that between 43% and 49% of americans support the decision of the u.s. supreme court on the 2010 healthcare law. (c) if we considered many random samples of 1,012 americans, and we calculated the sample proportions of those who support the decision of the u.s. supreme court, 95% of those sample proportions will be between 43% and 49%. (d) the margin of error at a 90% confidence level would be higher than 3%. 8.15 oral contraceptive use, part i. in a study of 100 randomly sampled 18 year-old women in an inner city neighborhood, 15 reported that they were taking birth control pills. (a) can the normal approximation to the binomial distribution be used to calculate a confidence interval for proportion of women using birth control pills in this neighborhood? explain your answer. (b) compute an approximate 95% confidence interval for the population proportion of women age 18 in this neighborhood taking birth control pills. (c) does the interval from part (b) support the claim that, for the young women in this neighborhood, the percentage who use birth control is not significantly di fferent from the national average of 5%? justify your answer. 8.16 oral contraceptive use, part ii. suppose that the study were repeated in a di fferent inner city neighborhood and that out of 50 randomly sampled 18-year-old women, 6 reported that they were taking birth control pills. the researchers would like to assess the evidence that the proportion of 18-year-old women using birth control pills in this neighborhood is greater than the national average of 5%. (a) can the normal approximation to the binomial distribution be used to conduct a hypothesis test of the null hypothesis that the proportion of women using birth control pills in this neighborhood is equal to 0.05? explain your answer. (b) state the hypotheses for the analysis of interest and compute the p-value. (c) interpret the results from part (b) in the context of the data. 31gallup, americans issue split decision on healthcare ruling, data collected june 28, 2012. 8.7. exercises 425 8.7.2 inference for the difference of two proportions 8.17 social experiment, part i. a “social experiment\" conducted by a tv program questioned what people do when they see a very obviously bruised woman getting picked on by her boyfriend. on two di fferent occasions at the same restaurant, the same couple was depicted. in one scenario the woman was dressed “provocatively” and in the other scenario the woman was dressed “conservatively”. the table below shows how many restaurant diners were present under each scenario, and whether or not they intervened. scenario provocative conservative total interveneyes 5 15 20 no 15 10 25 total 20 25 45 explain why the sampling distribution of the di fference between the proportions of interventions under provocative and conservative scenarios does not follow an approximately normal distribution. 8.18 heart transplant success. the stanford university heart transplant study was conducted to determine whether an experimental heart transplant program increased lifespan. each patient entering the program was o fficially designated a heart transplant candidate, meaning that he was gravely ill and might benefit from a new heart. patients were randomly assigned into treatment and control groups. patients in the treatment group received a transplant, and those in the control group did not. the table below displays how many patients survived and died in each group.32 control treatment alive 4 24 dead 30 45 suppose we are interested in estimating the di fference in survival rate between the control and treatment groups using a confidence interval. explain why we cannot construct such an interval using the normal approximation. what might go wrong if we constructed the confidence interval despite this problem? 8.19 national health plan, part iii. exercise 8.9 presents the results of a poll evaluating support for a generically branded “national health plan” in the united states. 79% of 347 democrats and 55% of 617 independents support a national health plan. (a) calculate a 95% confidence interval for the di fference between the proportion of democrats and independents who support a national health plan ( pd\u0000pi), and interpret it in this context. we have already checked conditions for you. (b) true or false: if we had picked a random democrat and a random independent at the time of this poll, it is more likely that the democrat would support the national health plan than the independent. 8.20 sleep deprivation, ca vs. or, part i. according to a report on sleep deprivation by the centers for disease control and prevention, the proportion of california residents who reported insu fficient rest or sleep during each of the preceding 30 days is 8.0%, while this proportion is 8.8% for oregon residents. these data are based on simple random samples of 11,545 california and 4,691 oregon residents. calculate a 95% confidence interval for the di fference between the proportions of californians and oregonians who are sleep deprived and interpret it in context of the data.33 32b. turnbull et al. “survivorship of heart transplant data”. in: journal of the american statistical association 69 (1974), pp. 74–80. 33cdc, perceived insu fficient rest or sleep among adults — united states, 2008. 426 chapter 8. inference for categorical data 8.21 remdesivir in covid-19. remdesivir is an antiviral drug previously tested in animal models infected with coronaviruses like sars and mers. as of may 2020, remdesivir has temporary approval from the fda for use in severely ill covid-10 patients. a randomized controlled trial conducted in china enrolled 236 patients with severe covid-19; 158 were assigned to receive remdesivir and 78 to receive a placebo. in the remdesivir group, 103 patients showed clinical improvement; in the placebo group, 45 patients showed clinical improvement. (a) conduct a formal comparison of the clinical improvement rates and summarize your findings. (b) report and interpret an appropriate confidence interval. 8.22 sleep deprivation, ca vs. or, part ii. exercise 8.20 provides data on sleep deprivation rates of californians and oregonians. the proportion of california residents who reported insu fficient rest or sleep during each of the preceding 30 days is 8.0%, while this proportion is 8.8% for oregon residents. these data are based on simple random samples of 11,545 california and 4,691 oregon residents. (a) conduct a hypothesis test to determine if these data provide strong evidence the rate of sleep deprivation is different for the two states. (reminder: check conditions) (b) it is possible the conclusion of the test in part (a) is incorrect. if this is the case, what type of error was made? 8.23 gender and color preference. a study asked 1,924 male and 3,666 female undergraduate college students their favorite color. a 95% confidence interval for the di fference between the proportions of males and females whose favorite color is black ( pmale\u0000pfemale ) was calculated to be (0.02, 0.06). based on this information, determine if the following statements are true or false, and explain your reasoning for each statement you identify as false.34 (a) we are 95% confident that the true proportion of males whose favorite color is black is 2% lower to 6% higher than the true proportion of females whose favorite color is black. (b) we are 95% confident that the true proportion of males whose favorite color is black is 2% to 6% higher than the true proportion of females whose favorite color is black. (c) 95% of random samples will produce 95% confidence intervals that include the true di fference between the population proportions of males and females whose favorite color is black. (d) we can conclude that there is a significant di fference between the proportions of males and females whose favorite color is black and that the di fference between the two sample proportions is too large to plausibly be due to chance. (e) the 95% confidence interval for ( pfemale\u0000pmale) cannot be calculated with only the information given in this exercise. 34l ellis and c ficek. “color preferences according to gender and sexual orientation”. in: personality and individual differences 31.8 (2001), pp. 1375–1379. 8.7. exercises 427 8.24 prenatal vitamins and autism. researchers studying the link between prenatal vitamin use and autism surveyed the mothers of a random sample of children aged 24 - 60 months with autism and conducted another separate random sample for children with typical development. the table below shows the number of mothers in each group who did and did not use prenatal vitamins during the three months before pregnancy (periconceptional period).35 autism autism typical development total periconceptional no vitamin 111 70 181 prenatal vitamin vitamin 143 159 302 total 254 229 483 (a) state appropriate hypotheses to test for independence of use of prenatal vitamins during the three months before pregnancy and autism. (b) complete the hypothesis test and state an appropriate conclusion. (reminder: verify any necessary conditions for the test.) (c) a new york times article reporting on this study was titled “prenatal vitamins may ward o ffautism\". do you find the title of this article to be appropriate? explain your answer. additionally, propose an alternative title.36 8.25 sleep deprived transportation workers. the national sleep foundation conducted a survey on the sleep habits of randomly sampled transportation workers and a control sample of non-transportation workers. the results of the survey are shown below.37 transportation professionals truck train bus/taxi/limo control pilots drivers operators drivers less than 6 hours of sleep 35 19 35 29 21 6 to 8 hours of sleep 193 132 117 119 131 more than 8 hours 64 51 51 32 58 total 292 202 203 180 210 conduct a hypothesis test to evaluate if these data provide evidence of a di fference between the proportions of truck drivers and non-transportation workers (the control group) who get less than 6 hours of sleep per day, i.e. are considered sleep deprived. 8.26 an apple a day keeps the doctor away. a physical education teacher at a high school wanting to increase awareness on issues of nutrition and health asked her students at the beginning of the semester whether they believed the expression “an apple a day keeps the doctor away”, and 40% of the students responded yes. throughout the semester she started each class with a brief discussion of a study highlighting positive e ffects of eating more fruits and vegetables. she conducted the same apple-a-day survey at the end of the semester, and this time 60% of the students responded yes. can she used a two-proportion method from this section for this analysis? explain your reasoning. 35r.j. schmidt et al. “prenatal vitamins, one-carbon metabolism gene variants, and risk for autism”. in: epidemiology 22.4 (2011), p. 476. 36r.c. rabin. “patterns: prenatal vitamins may ward o ffautism”. in: new york times (2011). 37national sleep foundation, 2012 sleep in america poll: transportation workers’ sleep, 2012. 428 chapter 8. inference for categorical data 8.7.3 inference for two or more groups 8.27 true or false, part i. determine if the statements below are true or false. for each false statement, suggest an alternative wording to make it a true statement. (a) the chi-square distribution, just like the normal distribution, has two parameters, mean and standard deviation. (b) the chi-square distribution is always right skewed, regardless of the value of the degrees of freedom parameter. (c) the chi-square statistic is always positive. (d) as the degrees of freedom increases, the shape of the chi-square distribution becomes more skewed. 8.28 true or false, part ii. determine if the statements below are true or false. for each false statement, suggest an alternative wording to make it a true statement. (a) as the degrees of freedom increases, the mean of the chi-square distribution increases. (b) if you found 2= 10 withdf= 5 you would fail to reject h0at the 5% significance level. (c) when finding the p-value of a chi-square test, we always shade the tail areas in both tails. (d) as the degrees of freedom increases, the variability of the chi-square distribution decreases. 8.29 quitters. does being part of a support group a ffect the ability of people to quit smoking? a county health department enrolled 300 smokers in a randomized experiment. 150 participants were assigned to a group that used a nicotine patch and met weekly with a support group; the other 150 received the patch and did not meet with a support group. at the end of the study, 40 of the participants in the patch plus support group had quit smoking while only 30 smokers had quit in the other group. (a) create a two-way table presenting the results of this study. (b) answer each of the following questions under the null hypothesis that being part of a support group does not affect the ability of people to quit smoking, and indicate whether the expected values are higher or lower than the observed values. i. how many subjects in the “patch + support\" group would you expect to quit? ii. how many subjects in the “patch only\" group would you expect to not quit? 8.30 parasitic worm. lymphatic filariasis is a disease caused by a parasitic worm. complications of the disease can lead to extreme swelling and other complications. here we consider results from a randomized experiment that compared three di fferent drug treatment options to clear people of the this parasite, which people are working to eliminate entirely. the results for the second year of the study are given below:38 clear at year 2 not clear at year 2 three drugs 52 2 two drugs 31 24 two drugs annually 42 14 (a) set up hypotheses for evaluating whether there is any di fference in the performance of the treatments, and also check conditions. (b) statistical software was used to run a chi-square test, which output: x2= 23:7 df= 2 p-value = 7.2e-6 use these results to evaluate the hypotheses from part (a), and provide a conclusion in the context of the problem. 38christopher king et al. “a trial of a triple-drug treatment for lymphatic filariasis”. in: new england journal of medicine 379 (2018), pp. 1801–1810. 8.7. exercises 429 8.31 prevend, part iv. in the prevend data, researchers measured various features of study participants, including data on statin use and highest level of education attained. a two-way table of education level and statin use is shown below. primary lowersec uppersec univ sum nonuser 31 111 107 136 385 user 20 46 27 22 115 sum 51 157 134 158 500 (a) set up hypotheses for evaluating whether there is an association between statin use and educational level. (b) check assumptions required for an analysis of these data. (c) statistical software was used to conduct a 2test: the test statistic is 19.054, with p-value 0.0027. summarize the conclusions in context of the data, and be sure to comment on the direction of association. 8.32 diabetes and unemployment. a gallup poll surveyed americans about their employment status and whether or not they have diabetes. the survey results indicate that 1.5% of the 47,774 employed (full or part time) and 2.5% of the 5,855 unemployed 18-29 year olds have diabetes.39 (a) create a two-way table presenting the results of this study. (b) state appropriate hypotheses to test for di fference in proportions of diabetes between employed and unemployed americans. (c) the sample di fference is about 1%. if we completed the hypothesis test, we would find that the p-value is very small (about 0), meaning the di fference is statistically significant. use this result to explain the difference between statistically significant and practically significant findings. 8.33 tb treatment. tuberculosis (tb) is an infectious disease caused by the mycobacterium tuberculosis bacteria. active tb can be cured by adhering to a treatment regimen of several drugs for 6-9 months. a major barrier to eliminating tb worldwide is failure to adhere to treatment; this is known as defaulting from treatment. a study was conducted in thailand to identify factors associated with default from treatment. the study results indicate that out of 54 diabetic participants, 0 defaulted from treatment; out of 1,180 non-diabetic participants, 54 defaulted from treatment. participants were recruited at health centers upon diagnosis of tb. (a) create a two-way table presenting the results of this study. (b) state appropriate hypotheses to test for di fference in proportions of treatment default between diabetics and non-diabetics. (c) check assumptions. you may use a less stringent version of the success-failure condition: the expected number of successes per group should be greater than or equal to 5 (rather than 10). (d) formally test whether the proportion of patients who default from treatment di ffers between diabetics and non-diabetics. summarize your findings. 39gallup wellbeing, employed americans in better health than the unemployed, data collected jan. 2, 2011 - may 21, 2012. 430 chapter 8. inference for categorical data 8.34 coffee and depression. researchers conducted a study investigating the relationship between caffeinated co ffee consumption and risk of depression in women. they collected data on 50,739 women free of depression symptoms at the start of the study in the year 1996, and these women were followed through 2006. the researchers used questionnaires to collect data on ca ffeinated co ffee consumption, asked each individual about physician- diagnosed depression, and also asked about the use of antidepressants. the table below shows the distribution of incidences of depression by amount of ca ffeinated co ffee consumption.40 caffeinated co ffee consumption \u00141 2-6 1 2-3 \u00154 cup/week cups/week cup/day cups/day cups/day total clinical yes 670 373 905 564 95 2,607 depression no 11,545 6,244 16,329 11,726 2,288 48,132 total 12,215 6,617 17,234 12,290 2,383 50,739 (a) what type of test is appropriate for evaluating if there is an association between co ffee intake and depression? (b) write the hypotheses for the test you identified in part (a). (c) calculate the overall proportion of women who do and do not su ffer from depression. (d) identify the expected count for the highlighted cell, and calculate the contribution of this cell to the test statistic. (e) the test statistic is 2= 20:93. what is the p-value? (f) what is the conclusion of the hypothesis test? (g) one of the authors of this study was quoted on the nytimes as saying it was “too early to recommend that women load up on extra co ffee\" based on just this study.41do you agree with this statement? explain your reasoning. 8.35 mosquito nets and malaria. this problem examines a hypothetical prospective study about an important problem in the developing world: the use of mosquito nets to prevent malaria in children. the nets are typically used to protect children from mosquitoes while sleeping. suppose that in a large region of an african country, 100 households with one child are randomized to receive free mosquito nets for the child in the household and 100 households with one child are randomized to a control group where families do not receive the nets. you are given the following information: – in the 100 households receiving the nets, 22 children became infected with malaria. – in the 100 households without the nets, 30 children became infected with malaria. – the 200 families selected to participate in the study may be regarded as a random sample from the families in the region, so the 100 families in each group may be regarded as random samples from the population. – malaria among children is common in this region, with a prevalence of approximately 25%. (a) write down the 2 \u00022 contingency table that corresponds to the data from the trial, labeling the table clearly and including the row and column totals. (b) under the hypothesis of no association between use of a mosquito net and malaria infection, calculate the expected number of infected children among 100 families who did receive a net. (c) the 2statistic for this 2 \u00022 table is 1.66. use this information to conduct a test of the null hypothesis of no effect of the use of a mosquito net on malaria infection in children. (d) compute and interpret the estimated relative risk of malaria infection, comparing the households without a net to those with a net. 40m. lucas et al. “co ffee, caffeine, and risk of depression among women”. in: archives of internal medicine 171.17 (2011), p. 1571. 41a. o’connor. “co ffee drinking linked to less depression in women”. in: new york times (2011). 8.7. exercises 431 8.36 health care fraud. most errors in billing insurance providers for health care services involve honest mistakes by patients, physicians, or others involved in the health care system. however, fraud is a serious problem. the national health care anti-fraud association estimates that approximately $68 billion is lost to health care fraud each year. often when fraud is suspected, an audit of randomly selected billings is conducted. the selected claims are reviewed by experts and each claim is classified as allowed or not allowed. the claims not allowed are considered to be potentially fraudulent. in general, the distribution of claims is highly skewed such that the majority of claims filed are small claims and only a few are large claims. since simple random sampling would likely be overwhelmed by small claims, claims chosen for auditing are sampled in a stratified way: a set number of claims are sampled from each category of claim size: small, medium, and large. here are data from an audit that used stratified sampling from three strata based on the claim size (i.e., monetary amount of the claim). stratum sampled claims not allowed small 100 10 medium 50 17 large 20 4 (a) can these data be used to estimate the proportion of large claims for which fraud might be expected? (b) can these data be used to estimate the proportion of possibly fraudulent claims that are large claims? (c) construct a 2 \u00023 contingency table of counts for these data and include the marginal totals, with the rows being the classification of claims and the columns being the size of the claim. (d) calculate the expected number of claims that would not be allowed among the large claims, under the hypothesis of no association of between size of claim and the claim not being allowed. (e) is the use of the chi-square statistic justified for these data? (f) a chi-square test of no association between size of claim and whether it was allowed has value 12.93. how many degrees of freedom does the chi-square statistic have and what is the p-value for a test of no association? (g) compute the 2residuals. based on the residuals, interpret the findings in the context of the data. 8.37 anxiety. psychologists conducted an experiment to investigate the e ffect of anxiety on a person’s desire to be alone or in the company of others (schacter 1959; lehmann 1975). a group of 30 individuals were randomly assigned into two groups; one group was designated the \"high anxiety\" group and the other the \"low anxiety\" group. those in the high-anxiety group were told that in the \"upcoming experiment\", they would be subjected to painful electric shocks, while those in the low-anxiety group were told that the shocks would be mild and painless.42all individuals were informed that there would be a 10 minute wait before the experiment began, and that they could choose whether to wait alone or with other participants. the following table summarizes the results: wait together wait alone sum high-anxiety 12 5 17 low-anxiety 4 9 13 sum 16 14 30 (a) under the null hypothesis of no association, what are the expected cell counts? (b) under the assumption that the marginal totals are fixed and the null hypothesis is true, what is the probability of the observed set of results? (c) enumerate the tables that are more extreme than what was observed, in the same direction. (d) conduct a formal test of association for the results and summarize your findings. let = 0:05. 42individuals were not actually subjected to electric shocks of any kind 432 chapter 8. inference for categorical data 8.38 salt intake and cvd. suppose we are interested in investigating the relationship between high salt intake and death from cardiovascular disease (cvd). one possible study design is to identify a group of highand low-salt users then follow them over time to compare the relative frequency of cvd death in the two groups. in contrast, a less expensive study design is to look at death records, identify cvd deaths from noncvd deaths, collect information about the dietary habits of the deceased, then compare salt intake between individuals who died of cvd versus those who died of other causes. this design is called a retrospective design. suppose a retrospective study is done in a specific county of massachusetts; data are collected on men ages 50-54 who died over a 1-month period. of 35 men who died from cvd, 5 had a diet with high salt intake before they died, while of the 25 men who died from other causes, 2 had a diet with high salt intake. these data are summarized in the following table. cvd death non-cvd death total high salt diet 5 2 7 low salt diet 30 23 53 total 35 25 60 (a) under the null hypothesis of no association, what are the expected cell counts? (b) of the 35 cvd deaths, 5 were in the high salt diet group and 30 were in the low salt diet group. under the assumption that the marginal totals are fixed, enumerate all possible sets of results (i.e., the table counts) that are more extreme than what was observed, in the same direction. (c) calculate the probability of observing each set of results from part (b). (d) evaluate the statistical significance of the observed data with a two-sided alternative. let = 0:05. summarize your results. 8.7.4 chi-square tests for the fit of a distribution 8.39 open source textbook. a professor using an open source introductory statistics book predicts that 60% of the students will purchase a hard copy of the book, 25% will print it out from the web, and 15% will read it online. at the end of the semester he asks his students to complete a survey where they indicate what format of the book they used. of the 126 students, 71 said they bought a hard copy of the book, 30 said they printed it out from the web, and 25 said they read it online. (a) state the hypotheses for testing if the professor’s predictions were inaccurate. (b) how many students did the professor expect to buy the book, print the book, and read the book exclusively online? (c) this is an appropriate setting for a chi-square test. list the conditions required for a test and verify they are satisfied. (d) calculate the chi-squared statistic, the degrees of freedom associated with it, and the p-value. (e) based on the p-value calculated in part (d), what is the conclusion of the hypothesis test? interpret your conclusion in this context. 8.40 barking deer. microhabitat factors associated with foraging sites of barking deer in hainan island, china were examined. in this region, woods make up 4.8% of the land, cultivated grass plots make up 14.7%, and deciduous forests make up 39.6%. of the 426 sites where the deer forage, 4 were categorized as woods, 16 as cultivated grass plots, and 61 as deciduous forests. the table below summarizes these data.43 woods cultivated grassplot deciduous forests other total 4 16 61 345 426 (a) write the hypotheses for testing if barking deer prefer to forage in certain habitats over others. (b) check if the assumptions and conditions required for testing these hypotheses are reasonably met. (c) do these data provide convincing evidence that barking deer prefer to forage in certain habitats over others? conduct an analysis and summarize your findings. 43liwei teng et al. “forage and bed sites characteristics of indian muntjac (muntiacus muntjak) in hainan island, china”. in: ecological research 19.6 (2004), pp. 675–681. 8.7. exercises 433 8.7.5 outcome-based sampling: case-control studies 8.41 cvd and diabetes. an investigator asked for the records of patients diagnosed with diabetes in his practice, then sampled 20 patients with cardiovascular disease (cvd) and 80 patients without cvd. for the sampled patients, he then recorded whether or not the age of onset of diabetes was at age 50 or younger. of the 40 patients whose age of onset of diabetes was 50 years of age or earlier, 15 had cardiovascular disease. in the remaining 60 patients, 5 had cardiovascular disease. (a) write the contingency table that summarizes the result of this study. (b) what is the relative odds of cardiovascular disease, comparing the older patients to those less than 50 years old at onset of diabetes? (c) interpret the relative odds of cardiovascular disease and comment on whether the relative odds cohere with what you might expect. (d) in statistical terms, state the null hypothesis of no association between the presence of cardiovascular disease and age of onset of diabetes. (e) what test can be used to test the null hypothesis? are the assumptions for the test reasonably satisfied? (f) the value of the chi-square test statistic for this table is 11. identify the logical flaw in the following statement: \"in this retrospective study of cardiovascular disease and diabetes, our study has demonstrated statistically significant evidence that diabetes increases the risk of cardiovascular disease.\" 8.42 blood thinners. cardiopulmonary resuscitation (cpr) is a procedure commonly used on individuals suffering a heart attack when other emergency resources are not available. this procedure is helpful in maintaining some blood circulation, but the chest compressions involved can also cause internal injuries. internal bleeding and other injuries complicate additional treatment e fforts following arrival at a hospital. for instance, while blood thinners may be used to help release a clot that is causing a heart attack, the blood thinner would have negative repercussions on any internal injuries. this problem uses data from a study in which patients who underwent cpr for a heart attack and were subsequently admitted to a hospital. these patients were randomly divided into a treatment group where they received a blood thinner or the control group where they did not receive the blood thinner. the outcome variable of interest was whether the patients survived for at least 24 hours. the study results are shown in the table below: treatment control total survived 14 11 25 died 26 39 65 total 40 50 90 (a) for this table, calculate the odds ratio for survival, comparing treatment to control, and the relative risk of survival, comparing treatment to control. (b) what is the interpretation of each of these two statistics? (c) in this study, which of the two summary statistics in part (a) is the better description of the treatment effect? why? 434 chapter 8. inference for categorical data 8.43 cns disorder. suppose an investigator has studied the possible association between the use of a weight loss drug and a rare central nervous system (cns) disorder. he samples from a group of volunteers with and without the disorder, and records whether they have used the weight loss drug. the data are summarized in the following table: drug use cns disorder yes no yes 10 2000 no 7 4000 (a) can these data be used to estimate the probability of a cns disorder for someone taking the weight loss drug? (b) for this study, what is an appropriate measure of association between the weight-loss drug and the presence of cns disorder? (c) calculate the measure of association specified in part (b). (d) interpret the calculation from part (c). (e) what test of significance is the best choice for analyzing the hypothesis of no association for these data? 8.44 asthma risk. asthma is a chronic lung disease characterized as hypersensitivity to a variety of stimuli, such as tobacco smoke, mold, and pollen. the prevalence of asthma has been increasing in recent decades, especially in children. some studies suggest that children who either live in a farm environment or have pets become less likely to develop asthma later in life, due to early exposure to elevated amounts of microorganisms. a large study was conducted in norway to investigate the association between early exposure to animals and subsequent risk for asthma. using data from national registers, researchers identified 11,585 children known to have asthma at age 6 years out of the 276,281 children born in norway between january 1, 2006 and december 31, 2009. children whose parents were registered as \"animal producers and related workers\" during the child’s first year of life were defined as being exposed to farm animals. of the 958 children exposed to farm animals, 19 had an asthma diagnosis at age 6. (a) do these data support previous findings that living in a farm environment is associated with lower risk of childhood asthma? conduct a formal analysis and summarize your findings. be sure to check any necessary assumptions. (b) is the relative risk an appropriate measure of association for these data? briefly explain your answer. (c) in language accessible to someone who has not taken a statistics course, explain whether these results represent evidence that exposure to farm animals reduces the risk of developing asthma. limit your answer to no more than seven sentences. 8.45 tea consumption and carcinoma. in a study examining the association between green tea consumption and esophageal carcinoma, researchers recruited 300 patients with carcinoma and 571 without carcinoma and administered a questionnaire about tea drinking habits. out of the 47 individuals who reported that they regularly drink green tea, 17 had carcinoma. out of the 824 individuals who reported they never drink green tea, 283 had carcinoma. (a) analyze the data to assess evidence for an association between green tea consumption and esophageal carcinoma from these data. summarize your results. (b) report and interpret an appropriate measure of association. 435 appendix a end of chapter exercise solutions 1 introduction to data 1.1 (a) treatment: 10 =43 = 0:23!23%. (b) control: 2 =46 = 0:04!4%. (c) a higher percentage of patients in the treatment group were pain free 24 hours after receiving acupuncture. (d) it is possible that the observed di fference between the two group percentages is due to chance. 1.3 (a) “is there an association between air pollution exposure and preterm births?\" (b) 143,196 births in southern california between 1989 and 1993. (c) measurements of carbon monoxide, nitrogen dioxide, ozone, and particulate matter less than 10 \u0016g=m3(pm 10) collected at air-quality-monitoring stations as well as length of gestation. continuous numerical variables. 1.5 (a) “does explicitly telling children not to cheat a ffect their likelihood to cheat?\". (b) 160 children between the ages of 5 and 15. (c) four variables: (1) age (numerical, continuous), (2) sex (categorical), (3) whether they were an only child or not (categorical), (4) whether they cheated or not (categorical). 1.7 (a) control: the group of 16 female birds that received no treatment. treatment: the group of 16 female birds that were given supplementary diets. (b) \"does egg coloration indicate the health of female collared flycatchers?\" (c) darkness of blue color in female birds’ eggs. continuous numerical variable. 1.9 (a) each row represents a participant. (b) the response variable is colon cancer stage. the explanatory variables are the abundance levels of the five bacterial species. (c) colon cancer stage: ordinal categorical variable. abundance levels of bacterial species: continuous numerical variable. 1.11 (a) the population of interest consists of babies born in southern california. the sample consists of the 143,196 babies born between 1989 and 1993 in southern california. (b) assuming that the sample is representative of the population of interest, the results of the study can be generalized to the population. the findings cannot be used to establish causal relationships because the study was an observational study, not an experiment. 1.13 (a) the population of interest consists of asthma patients who rely on medication for asthma treatment. the sample consists of the 600 asthma patients ages 18-69 who participated in the study. (b) the sample may not be representative of the population because study participants were recruited, an example of a convenience sample. thus, the results of the study may not be generalizable to the population. the findings can be used to establish causal relationships because the study is an experiment conducted with control, randomization, and a reasonably large sample size. 436 appendix a. end of chapter exercise solutions 1.15 (a) experiment. (b) the experimental group consists of the chicks that received vitamin supplements. the control group consists of the chicks that did not receive vitamin supplements. (c) randomization ensures that there are not systematic di fferences between the control and treatment groups. even if chicks may vary in ways that a ffect body mass and corticosterone levels, random allocation essentially evens out such di fferences, on average, between the two groups. this is essential for a causal interpretation of the results to be valid. 1.17 (a) observational study. (b) answers may vary. one possible confounding variable is the wealth of a country. a wealthy country’s citizens tend to have a higher life expectancy due to a higher quality of life, and the country tends to have a higher percentage of internet users because there is enough money for the required infrastructure and citizens can afford computers. wealth of a country is associated with both estimated life expectancy and percentage of internet users. omitting the confounder from the analysis distorts the relationship between the two variables, such that there may seem to be a direct relationship when there is not. 1.19 (a) simple random sampling is reasonable if 500 students is a large enough sample size relative to the total student population of the university. (b) since student habits may vary by field of study, stratifying by field of study would be a reasonable decision. (c) students in the same class year may have more similar habits. since clusters should be diverse with respect to the outcome of interest, this would not be a good approach. 1.21 (a) non-responders may have a di fferent response to this question, e.g. parents who returned the surveys likely don’t have di fficulty spending time with their children. (b) it is unlikely that the women who were reached at the same address 3 years later are a random sample. these missing responders are probably renters (as opposed to homeowners) which means that they might be in a lower socio-economic class than the respondents. (c) this is an observational study, not an experiment, so it is not advisable to draw conclusions about causal relationships. the relationship may be in the other direction; i.e., that these people go running precisely because they do not have joint problems. additionally, the data are not even su fficient to provide evidence of an association between running and joint problems because data have only been collected from individuals who go running regularly. instead, a sample of individuals should be collected that includes both people who do and do not regularly go running; the number of individuals in each group with joint problems can then be compared for evidence of an association. 1.23 the lead author’s statements are not accurate because he or she drew conclusions about causation (that increased alcohol sales taxes lower rates of sexually transmitted infections) from an observational study. in addition, although the study observed that there was a decline in gonorrhea rate, the lead author generalized the observation to all sexually transmitted infections. 1.25 (a) randomized controlled experiment. (b) explanatory: treatment group (categorical, with 3 levels). response variable: psychological well-being. (c) no, because the participants were volunteers. (d) yes, because it was an experiment. (e) the statement should say “evidence” instead of “proof”. 437 1.27 (a) the two distributions have the same median since they have the same middle number when ordered from least to greatest. distribution 2 has a higher iqr because its first and third quartiles are farther apart than in distribution 2. (b) distribution 2 has a higher median since it has a higher middle number when ordered from least to greatest. distribution 2 has a higher iqr because its first and third quartiles are farther apart than in distribution 1. (c) distribution 2 has a higher median since all values in this distribution are higher than in distribution 1. the two distributions have the same iqr since the distance between the first and third quartiles in each distribution is the same. (d) distribution 2 has a higher median since most values in this distribution are higher than those in distribution 1. distribution 2 has a higher iqr because its first and third quartiles are farther apart than those of distribution 1. 1.29 (a) the distribution is bimodal, with peaks between 15-20 and 25-30. values range from 0 to 65. (b) the median aqi is about 30. (c) i would expect the mean to be higher than the median, since there is some right skewing. 1.31 (a) the median is a much better measure of the typical amount earned by these 42 people. the mean is much higher than the income of 40 of the 42 people. this is because the mean is an arithmetic average and gets affected by the two extreme observations. the median does not get e ffected as much since it is robust to outliers. (b) the iqr is a much better measure of variability in the amounts earned by nearly all of the 42 people. the standard deviation gets a ffected greatly by the two high salaries, but the iqr is robust to these extreme observations. 1.33 (a) these data are categorical. they can be summarized numerically in either a frequency table or relative frequency table, and summarized graphically in a bar plot of either counts or proportions. (b) the results of these studies cannot be generalized to the larger population. individuals taking the survey represent a specific subset of the population that are conscious about dental health, since they are at the dentist’s o ffice for an appointment. additionally, there may be response bias; even though the surveys are anonymous, it is likely that respondents will feel some pressure to give a \"correct\" answer in such a setting, and claim to floss more often than they actually do. 1.35 (a) yes, there seems to be a positive association between lifespan and length of gestation. generally, as gestation increases, so does life span. (b) positive association. reversal of the plot axes does not change the nature of an association. 1.37 (a) 75% of the countries have an adolescent fertility rate less than or equal to 75.73 births per 1,000 adolescents. (b) it is likely that the observations are missing due to the iraq war and general instability in the region during this time period. it is unlikely that the five-number summary would have been a ffected very much, even if the values were extreme; the median and iqr are robust estimates, and the dataset is relatively large, with data from 188 other countries. (c) the median and iqr decreases each year, with q1 and q3 also decreasing. 1.39 (a) 4,371/8,474 = 0.56 !56% (b) 110/190 = 0.58 !58% (c) 27/633 = 0.04 !4% (d) 53/3,110 = 0.02 !2% (e) relative risk:27=633 53=3;110= 2:50. yes, since the relative risk is greater than 1. a relative risk of 2.50 indicates that individuals with high trait anger are 2.5 times more likely to experience a chd event than individuals with low trait anger. (f) side-by-side boxplots, since blood cholesterol level is a numerical variable and anger group is categorical. 438 appendix a. end of chapter exercise solutions 2 probability 2.1 (a) false. these are independent trials. (b) false. there are red face cards. (c) true. a card cannot be both a face card and an ace. 2.3 (a)1 4. solution 1: a colorblind male has genotype x\u0000y. he must have inherited x\u0000from his mother (probability of 1 2) andyfrom his father (probability of1 2). since these are two independent events, the probability of both occuring is (1 2)(1 2) =1 4. solution 2: determine the possibilities using a punnett square. there are 4 equally likely possibilities, one of which is a colorblind male. thus, the probability is1 4. x+y x+x+x+x+y x\u0000x+x\u0000x\u0000y (b) true. an o ffspring of this couple cannot be both female and colorblind. 2.5 (a) 0.25. let hrepresent the event of being a high school graduate and frepresent the event of being a woman.p(h) =p(handw) +p(handwc) =p(hjw)p(w) +p(hjwc)p(wc) = (0:20)(0:50) + (0:30)(0:50) = 0:25. (b) 0.91.(ac) =p(acandw) +p(acandwc) = (1\u00000:09) + (1\u00000:09) = 0:91. (c) 0.25. let xrepresent the event of having at least a bachelor’s degree, where brepresents the event of attaining at most a bachelor’s degree and gthe event of attaining at most a graduate or professional degree. p(xjwc) =p(bjwc) +p(gjwc) = 0:16 + 0:09 = 0:25. (d) 0.26.p(xjw) =p(bjw) +p(gjw) = 0:17 + 0:09 = 0:26. (e) 0.065. let xwbe the event that a woman has at least a bachelor’s degree, and xmbe the event that a man has at least a bachelor’s degree. assuming that the education levels of the husband and wife are independent, p(xwandxm) =p(xw)\u0002p(xm) = (0:25)(0:26) = 0:065. this assumption is probably not reasonable, because people tend to marry someone with a comparable level of education. 2.7 (a) letcrepresent the event that one urgent care center sees 300-449 patients in a week. assuming that the number of patient visits are independent between urgent care centers in a given county for a given week, the probability that three random urgent care centers see 300-449 patients in a week is [ p(c)]3= (0:288)3= 0:024. this assumption is not reasonable because a county is a small area with relatively few urgent care centers; if one urgent care center takes in more patients than usual during a given week, so might other urgent care centers in the same county (e.g., this could occur during flu season). (b) 2:32\u000210\u00007. letdrepresent the event that one urgent care center sees 450 or more patients in a week. assuming independence, the probability that 10 urgent care centers throughout a state all see 450 or more patients in a week is [ p(d)]10= (0:217)10= 2:32\u000210\u00007. this assumption is reasonable because a state is a large area that contains many urgent care centers; the number of patients one urgent care center takes in is likely independent of the number of patients another urgent care center in the state takes in. (c) no, it is not possible, because it is not reasonable to assume that the patient visits for a given week are independent of those for the following week. 2.9 (a) if the class is not graded on a curve, they are independent. if graded on a curve, then neither independent nor disjoint – unless the instructor will only give one a, which is a situation we will ignore in parts (b) and (c). (b) they are probably not independent: if you study together, your study habits would be related, which suggests your course performances are also related. (c) no. see the answer to part (a) when the course is not graded on a curve. more generally: if two things are unrelated (independent), then one occurring does not preclude the other from occurring. 439 2.11 (a) 0:60 + 0:20\u00000:18 = 0:62 (b) 0:18=0:20 = 0:90 (c) 0:11=0:33 = 0:33 (d) no, because the answers to parts (c) and (d) are not equal. if global warming belief were independent of political party, then among liberal democrats and conservative republicans, there would be equal proportions of people who believe the earth is warming. (e) 0:06=0:34 = 0:18 2.13 (a) 375;264=436;968 = 0:859 (b) 229;246=255;980 = 0:896 (c) 0.896. this is equivalent to (b). (d) 146;018=180;988 = 0:807 (e) 4;719=7;394 = 0:638 (f) no, because the answers to (c) and (d) are not equal. if gender and seat belt usage were independent, then among males and females, there would be the same proportion of people who always wear seat belts. 2.15 the ppv is 0.8248. the npv is 0.9728. p(djt+) =p(t+jd)p(d) p(t+jd)p(d)+p(t+jdc)p(dc)=(0:997)(0:0259) (0:997)(0:0259)+(1\u00000:926)(1\u00000:259)= 0:8248. p(dcjt\u0000) =p(t\u0000jdc)p(dc) p(t\u0000jdc)p(dc)+p(t\u0000jd)p(d)=(0:926)(1\u00000:259) (0:926)(1\u00000:259)+(1\u00000:997)(0:259)= 0:9728. hiv? result yes, 0.259positive, 0.9970.259*0.997 = 0.2582 negative, 0.0030.259*0.003 = 0.0008 no, 0.741positive, 0.0740.741*0.074 = 0.0548 negative, 0.9260.741*0.926 = 0.6862 2.17 0.0714. even when a patient tests positive for lupus, there is only a 7.14% chance that he actually has lupus. house may be right. lupus? result yes, 0.02positive, 0.980.02*0.98 = 0.0196 negative, 0.020.02*0.02 = 0.0004 no, 0.98positive, 0.260.98*0.26 = 0.2548 negative, 0.740.98*0.74 = 0.7252 2.19 (a) leterepresent the event of agreeing with the idea of evolution and dbe the event of being a democrat. from the problem statement, p(ejd) = 0:67.p(ecjd) = 1\u0000p(ejd) = 1\u00000:67 = 0:33. (b) letirepresent the event of being an independent. p(eji) = 0:65, as stated in the problem. (c) letrrepresent the event of being a republican. p(ejr) = 1\u0000p(ecjr) = 1\u00000:48 = 0:52. (d) 0.35.p(rje) =p(eandr) p(e)=p(r)p(ejr) p(e)=(0:40)(0:52) 0:60= 0:35. 2.21 mumps is the most likely disease state, since p(b3ja) = 0:563,p(b1ja) = 0:023, andp(b2ja) =:415. p(bija) =p(ajbi)p(bi) p(a).p(a) =p(aandb1)+p(aandb2)+p(aandb3) =p(ajb1)p(b1)+p(ajb2)p(b2)+p(ajb3)p(b3). 2.23 (a) letabe the event of knowing the answer and bbe the event of answering it correctly. assume that if a participant knows the correct answer, they answer correctly with probability 1: p(bja) = 1. if they guess randomly, they have 1 out of mchances to answer correctly, thus p(bjac) = 1=m.p(ajb) =1\u0001p (1\u0001p)+(1 m\u0001(1\u0000p))= p p+1\u0000p m. (b) 0.524. let abe the event of having an iq over 150 and bbe the event of receiving a score indicating an iq over 150. from the problem statement, p(bja) = 1 andp(bjac) = 0:001.p(acjb) =0:001\u0001(1\u00001 1;100) (1\u0001(1 1;100))+(0:001\u0001(1\u00001 1;100))= 0:524. 440 appendix a. end of chapter exercise solutions 2.25 (a) in descending order on the table, the ppv for each age group is 0.003, 0.064, 0.175, 0.270; the npv for each age group is 0.999, 0.983, 0.948, 0.914. (b) as prevalence of prostate cancer increases by age group, ppv also increases. however, with rising prevalence, npv decreases. (c) the probability that a man has prostate cancer, given a positive test, necessarily increases as the overall probability of having prostate cancer increases. if more men have the disease, the chance of a positive test result being a true positive increases (and the chances of the result being a false positive decreases). the decreasing npv values follow similar logic: if more men have the disease, the chance of a negative test being a true negative decreases (and the chances of the result being a false negative increases). (d) lowering the cuto fffor a positive test would result in more men testing positive, since men with psa values 2.5 ng/ml to 4.1 ng/ml were not previously classified as testing positive. since the sensitivity of a test is the proportion who test positive among those who have disease, and the number with disease does not change, the proportion will increase, except in the rare and unlikely situation where the additional positive tests are among only men without the disease. 2.27 (a) frequency of x+x+: 0.863. frequency of x+x\u0000: 0.132. frequency of x\u0000x\u0000: 0.005. frequency of x\u0000y: 0.07. frequency of x+y: 0.93. from frequency of x\u0000x\u0000, frequency of x\u0000allele isp 0:005 = 0:071; thus, frequency of x+allele is 1\u00000:071 = 0:929. frequency of x+yis 1\u00000:093 = 0:07. (b) 0.033. let abe the event that two parents are not colorblind, and brepresent the event of having a colorblind child. on the tree, \u0002represents a mating between two genotypes. p(bja) = [p(x+x+\u0002x+yja)\u0001 p(bjx+x+\u0002x+y)] + [p(x+x\u0000\u0002x+yja)\u0001p(bjx+x\u0000\u0002x+y)] = (0:867)(0) + (0:133)(1=4) = 0:033. a x+x+\u0002x+y bbcx+x\u0000\u0002x+y bbc 2.29 (a) calculate p(m\\b), the probability a dog has a facial mask and a black coat. note that the event m consists of having either a unilateral mask or a bilateral mask. p(m\\b) =p(m1\\b) +p(m2\\b) =p(m1jb)p(b) +p(m2jb)p(b) =(0:25)(0:40) + (0:35)(0:40) =0:24 the probability an australian cattle dog has a facial mask and a black coat is 0.31. (b) calculate p(m2), the prevalence of bilateral masks. the event of having a bilateral mask can be partitioned into either having a bilateral mask and a red coat or having a bilateral mask and a black coat. p(m2) =p(r\\m2) +p(b\\m2) =p(m2jr)p(r) +p(m2jb)p(b) =(0:10)(0:60) + (0:35)(0:40) =0:20 the prevalence of bilateral masks in australian cattle dogs is 0.20. (c) calculate p(rjm2), the probability of having a red coat given having a bilateral mask. apply the definition of conditional probability. 441 p(rjm2) =p(r\\m2) p(m2) =p(m2jr)p(r) p(m2) =(0:10)(0:60) 0:20 =0:30 the probability of being a red heeler among australian cattle dogs with bilateral facial masks is 0.30. (d) the following new information has been introduced: -p(d1jr;m 0) =p(d1jr;m 1) = 0:15,p(dcjr;m 0) =p(dcjr;m 1) = 0:60. -p(m2\\d2\\r) = 0:012,p(m2\\d1\\r) = 0:045 -p(m2\\d2\\b) = 0:012,p(m2\\d1\\b) = 0:045 -p(d1jm0;b) =p(d1jm1;b) = 0:05,p(d2jm0;b) =p(d2jm1;b) = 0:01 i. calculate p(m2\\dc\\r). p(m2\\dc\\r) =p(dcjm2;r)p(m2jr)p(r) =p(dcjm2;r)(0:10)(0:60) to calculate p(dcjm2;r), first calculate p(d1jm2;r) andp(d2jm2;r) from the joint probabilities given in the problem, then apply the complement rule. p(d1jm2;r) =p(m2\\d1\\r) p(m2\\r)=0:045 (0:10)(0:60)= 0:75 p(d2jm2;r) =p(m2\\d2\\r) p(m2\\r)=0:012 (0:10)(0:60)= 0:20 back to the original question... p(m2\\dc\\r) =p(dcjm2;r)p(m2jr)p(r) =p(dcjm2;r)(0:10)(0:60) =[1\u0000(0:75 + 0:20)](0:10)(0:60) =(0:05)(0:10)(0:60) =0:003 the probability that an australian cattle dog has a bilateral mask, no hearing deficits, and a red coat is 0.003. ii. calculate p(dcjm2;b). p(dcjm2;b) =1\u0000[p(d1jm2;b) +p(d2jm2;b)] =1\u0000\"p(d1\\m2\\b) p(m2\\b)+p(d2\\m2\\b) p(m2\\b)# =1\u0000\"0:045 p(m2jb)p(b)+0:012 p(m2jb)p(b)# =1\u0000\"0:045 (0:35)(0:40)+0:012 (0:35)(0:40)# =0:593 the proportion of bilaterally masked blue heelers without hearing deficits is 0.593. iii. calculate p(djr) andp(djb). 442 appendix a. end of chapter exercise solutions p(djr) =p(d\\m0jr) +p(d\\m1jr) +p(d\\m2jr) =[1\u0000p(dcjr;m 0)](p(m0jr) + [1\u0000p(dcjr;m 1)](p(m1jr) + [1\u0000p(dcjm2;r)](p(m2jr) =(1\u00000:60)(0:50) + (1\u00000:60)(0:40) + (1\u00000:05)(0:10) =0:455 p(djb) =p(d\\m0jb) +p(d\\m1jb) +p(d\\m2jb) =[p(d1jb;m 0) +p(d2jb;m 0)](p(m0jb) + [p(d1jb;m 0) +p(d2jb;m 0)](p(m1jb) + [1\u0000p(dcjm2;b)](p(m2jb) =(0:05 + 0:01)(0:40) + (0:05 + 0:01)(0:25) + (1\u00000:593)(0:35) =0:181 the prevalence of deafness among red heelers is higher, at 0.455 versus 0.181 in blue heelers. iv. calculate p(bjdc). p(bjdc) =p(b\\dc) p(dc) =p(dcjb)p(b) p(dc\\b) +p(dc\\r) =[1\u0000p(djb)]p(b) [1\u0000p(djb)]p(b) + [1\u0000p(djr)]p(r) =(1\u00000:181)(0:40) (1\u00000:181)(0:40) + (1\u00000:455)(0:60) =0:50 the probability that a dog is a blue heeler given that it is known to have no hearing deficits is 0.50. 3 distributions of random variables 3.1 (a) 13. (b) no, these 27 students are not a random sample from the university’s student population. for example, it might be argued that the proportion of smokers among students who go to the gym at 9 am on a saturday morning would be lower than the proportion of smokers in the university as a whole. 3.3 (a) the probability of drawing three hearts equals (13 =52)(12=51)(11=50) = 0:0129, and the probability of drawing three black cards equals (26 =52)(25=51)(24=50) = 0:1176; thus, the probability of any other draw is 1\u00000:0129\u00000:1176 = 0:8694.e(x) = 0:0129(50)+0 :1176(25)+0 :8694(0) = 3:589:var (x) = 0:0129(50\u00003:589)2+ 0:1176(25\u00003:589)2+ 0:8694(0\u00003:589)2= 93:007.sd(x) =p var(x) = 9:644: (b) letyrepresent the net profit/loss, where y=x\u00005.e(y) =e(x\u00005) =e(x)\u00005 =\u00001:412. standard deviation does not change from a shift of the distribution; sd(y) =sd(x) = 9:644. (c) it is not advantageous to play, since the expected winnings are lower than $5. 3.5 (a) 215 eggs. let xrepresent the number of eggs laid by one gull. e(x) = 0:25(1) + 0:40(2) + 0:30(3) + 0:05(4) = 2:15:e(100x) = 100e(x) = 215: (b) 85.29 eggs. var(x) = 0:25(1\u00002:15)2+0:40(2\u00002:15)2+0:30(3\u00002:15)2+0:05(4\u00002:15)2= 0:7275:var (100x) = 1002var(x) = 7275!p 7275 = 85:29: 443 3.7 (a) binomial conditions are met: (1) independent trials: in a random sample across the us, it is reasonable to assume that whether or not one 18-20 year old has consumed alcohol does not depend on whether or not another one has. (2) fixed number of trials: n= 10. (3) only two outcomes at each trial: consumed or did not consume alcohol. (4) probability of a success is the same for each trial: p= 0:697. (b) letxbe the number of 18-20 year olds who have consumed alcohol; x\u0018bin(10;0:697).p(x= 6) = 0:203. (c) letybe the number of 18-20 year olds who have not consumed alcohol; y\u0018bin(10;1\u00000:697).p(y= 4) = p(x= 6) = 0:203. (d)x\u0018bin(5;0:697).p(x\u00142) = 0:167. (e)x\u0018bin(5;0:697).p(x\u00151) = 1\u0000p(x= 0) = 0:997. 3.9 (a)\u001634:85,\u001b= 3:25 (b)z=45\u000034:85 3:25= 3:12. 45 is more than 3 standard deviations away from the mean, we can assume that it is an unusual observation. therefore yes, we would be surprised. (c) using the normal approximation, 0.0009. with 0.5 correction, 0.0015. 3.11 (a) both o+ and o- individuals can donate blood to a type o+ patient; n= 15,p= 0:45.\u0016=np= 6:75. \u001b=p np(1\u0000p) = 1:93. (b) only o- individuals can donate blood to a type o- patient; n= 15,p= 0:08.p(x\u00153) = 0:113. 3.13 0.132. letxbe the number of iv drug users who contract hepatitis c within a month; x\u0018bin(5;0:30), p(x= 3) = 0:132. 3.15 (a) letxrepresent the number of infected stocks in the sample; x\u0018bin(250;0:30).p(x= 60) = 0:006. (b)p(x\u001460) = 0:021. (c)p(x\u001580) = 0:735. (d) 40% of 250 is 100. p(x\u0014100) = 0:997. yes, this seems reasonable; it is essentially guaranteed that within a sample of 250, no more than 40% will be infected. 3.17 (a) (200)(0:12) = 24 cases of hyponatremia are expected during the marathon. (b) letxrepresent the number of cases of hyponatremia during the marathon. p(x>30) = 0:082. 3.19 (a) 8.85%. (b) 6.94%. (c) 58.86%. (d) 4.56%. (a)−1.350 (b)01.48 (c)0 (d)−2 02 3.21 (a) 0.005. (b) 0.911. (c) 0.954. (d) 1.036. (e) -0.842 3.23 (a) verbal:n(\u0016= 151;\u001b= 7), quant: n(\u0016= 153;\u001b= 7:67).zvr= 1:29,zqr= 0:52. she did better on the verbal reasoning section since her z-score on that section was higher. vr z = 1.29qr z = 0.52 (b)percvr= 0:9007\u001990%,percqr= 0:6990\u001970%. 100%\u000090% = 10% did better than her on vr, and 100%\u000070% = 30% did better than her on qr. (c) 159. (d) 147. 3.25 (a) 0.115. (b) the coldest 10% of days are colder than 70.59\u000ef. 3.27 (a) 0.023. (b) 72.66 mg/dl. 444 appendix a. end of chapter exercise solutions 3.29 (a) 82.4%. (b) about 38 years of age. 3.31 (a)n= 50, andp= 0:70.\u0016=np= 35.\u001b=p np(1\u0000p) = 3:24. (b) bothnpandn(1\u0000p) are greater than 10. thus, it is valid to approximate the distribution as x\u0018n(35;3:24), wherexis the number of 18-20 year olds who have consumed alcohol. p(x\u001545) = 0:001. 3.33 letxrepresent the number of students who accept the o ffer;x\u0018bin(2500;0:70). this distribution can be approximated by a n(1750;22:91). the approximate probability that the school does not have enough dorm room spots equals p(x\u00151;786) = 0:06. 3.35 the data appear to follow a normal distribution, since the points closely follow the line on the normal probability plot. there are some small deviations, but this is to be expected for such a small sample size. 3.37 (a)p(x= 2) =exp\u00002(22) 2!= 0:271. (b)p(x\u00142) =p(x= 0) +p(x= 1) +p(x= 2) = 0:677. (c)p(x\u00153) = 1\u0000p(x\u00142) = 0:323. 3.39 (a)\u0016=\u0015= 75,\u001b=p \u0015= 8:66. (b)z=\u00001:73. since 60 is within 2 standard deviations of the mean, it would not generally be considered unusual. note that we often use this rule of thumb even when the normal model does not apply. (c) using poisson with \u0015= 75: 0.0402. 3.41 (a) the expected number of cases of osteosarcoma in nyc in a given year is 11.2. (b) let xrepresent the number of osteosarcoma cases diagnosed. the probability that 15 or more cases will be diagnosed in a given year is the quantity p(x\u001515) = 1\u0000p(x<15) = 1\u0000p(x\u001414) = 0:161:(c) first, calculate \u0015bgiven that n= 450;000 for brooklyn: 3.6. the probability of observing 10 or more cases in brooklyn in a given year is the quantity p(xb\u001510) = 1\u0000p(xb<10) = 1\u0000p(xb\u00149) = 0:004:(d) no, he is not correct. the probability calculated in c) deals only with brooklyn: the probability that there are 10 or more cases in brooklyn for a single year. it does not say anything about cases in other boroughs. if we assume independence between boroughs, the probability that the o fficial is referring to is: p(x= 0 in other boroughs) \u0002p(x\u001510 in brooklyn). there is no reason to expect that p(x= 0 in other boroughs) should equal 1, so this probability is di fferent from the one in part c). (e) o, this probability is not equal to the probability calculated in part c). over five years, there are five opportunities for the event of 10 or more cases in brooklyn in a single year to occur. letyrepresent the event that in a single year, 10 or more cases of osteosarcoma are observed in brooklyn. if we assume independence between years, then yfollows a binomial distribution with n= 5 andpof success as caculated in part c); p(y= 1) = 0:020. 3.43 (a)\u0015for a population of 2,000,000 male births is 400. the probability of at most 380 newborn males with hemophilia is p(x\u0014380), where x\u0018pois(400): 0.165. (b)p(x\u0015450) = 0:0075. (c) the number of male births is (1/2)(1,500,000) = 750,000. the rate \u0015for one year is 150. over 5 years, the rate\u0015is 750. the expected number of hemophilia births over 5 years is 750 and the standard deviation isp 750 = 27:39. 3.45 (a) on average, 2 women would need to be sampled in order to select a married woman ( \u0016= 1=p= 2:123), with standard deviation 1.544 ( \u001b=r (1\u0000p) p2). (b)\u0016= 3:33.\u001b= 2:79. (c) decreasing the probability increases both the mean and the standard deviation. 3.47 (a) letxrepresent the number of stocks that must be sampled to find an infected stock; x\u0018geom(0:30). p(x\u00145) = 0:832. (b)p(x\u00146) = 0:882. (c)p(x\u00153) = 1\u0000p(x\u00142) = 0:49. 445 3.49 (a) 0:8752\u00020:125 = 0:096. (b)\u0016= 8,\u001b= 7:48. 3.51 (a) 0.0804. (b) 0.0322. (c) 0.0193. 3.53 (a) 0.102, geometric with p= 1994=14;604 = 0:137. (b) 0.854, binomial with n= 10,p= 0:137. (c) 0.109, binomial with n= 10,p= 0:137. (d) the mean and standard deviation of a negative binomial random variable with r= 4 andp= 0:137 are 29.30 and 13.61, respectively. 3.55 (a)\u0016= 2:05;\u001b2= 1:77. (b) letxrepresent the number of soapy-taste detectors; x\u0018hgeom(1994 ;14604\u00001994;15).p(x= 4) = 0:09435. (c)p(x\u00142) = 0:663. (d) 0.09437, from the binomial distribution. with a large sample size, sampling with replacement is highly unlikely to result in any particular individual being sampled again. in this case, the hypergeometric and binomial distributions will produce equal probabilities. 3.57 (a) the marginal distributions for xis obtained by summing across the two rows, and for yby summing the columns. the marginal probabilities for x= 0 andx= 1 are 0.60 and 0.40, and for y=\u00001 andy= 1 are both 0.50; i.e., px(0) = 0:60,px(1) = 0:40,py(\u00001) =py(1) = 0:50 (b) the mean and variance of xare calculated using the formulas in section 3.1.2 and 3.1.3 and are \u0016x= (0)(0:60) + (1)(0:40) = 0:40 \u001b2 x= (0\u00000:40)2(0:60) + (1\u00000:40)2(0:40) = 0:24 the standard deviation of xisp 0:24 = 0:49. (c) the two standardized values of xare obtained by subtracting the mean of xfrom each value and dividing by the standard deviation. the two standardized values are -0.82 and 1.23. (d) the correlation between xandyadds the 4 products of the standardized values, weighted by the values in the joint distribution: \u001ax;y= (\u00000:82)(\u00001)(0:20) + (\u00000:83)(1)(0:40) + (1:23)(\u00001)(0:30) + (1:23)(1)(0:10) =\u0000:41 (e) no. the correlation between xandyis not zero. 3.59 (a) sum over the margins to calculate the marginal distributions. py(\u00001) = 0:25py(0) = 0:20py(1) = 0:55 px(\u00001) = 0:45px(0) = 0:20px(1) = 0:35 (b) the expected value of xis calculated as follows: e(x) =x ixip(x=xi) = (\u00001)(0:45) + (0)(0:20) + (1)(0:35) =\u00000:10 (c) the variance of yis calculated by first calculating e(y), then using that in the formula for a variance of a random variable. e(y) =x iyip(y=yi) = (\u00001)(0:25) + (0)(0:20) + (1)(0:55) = 0:30 var(y) =x i(yi\u0000e(y))2p(y=yi) = (\u00001\u00000:30)2(0:25) + (0\u00000:30)2(0:20) + (1\u00000:30)2(0:55) = 0:71 (d)p(x=\u00001jy= 0) = 0=0:20 = 0;p(x= 0jy= 0) = 0:10=0:20 = 0:50;p(x= 1jy= 0) = 0:10=0:20 = 0:5. 446 appendix a. end of chapter exercise solutions 3.61 (a) no. the new marginal distributions for the costs for the two members of the couple are shown in the following table. the values and the marginal distribution for the partner’s cost do not change, so the expected value and standard deviation will not change. the previous values for the mean and standard deviation were $980 and $9.80. partner costs, y employee costs, x $968 $988 marg. dist., x $968 0.18 0.12 0.30 $1,008 0.15 0.25 0.40 $1,028 0.07 0.23 0.30 marg. dist., y 0.40 0.60 1.00 (b) the expected value and standard deviation of the employee’s costs are calculated as in example 3.6, but using the new marginal distribution. the new values for the mean and standard deviation are $1,002 and $23.75. (c) the expected total cost is $1,002 + $980 = $1,982. (d) the calculation correlation depends on the standardized costs for each member of the couple and the joint probabilities. the new standardized values for the employee costs are -1.43, 0.25, and 1.09; the corresponding values for the partner are -1.22 and 0.82. the correlation is the weighted sum of the 6 products, weighted by the joint probabilities: \u001ax;y= 0:29. (e) the new variance for the total cost will be (23 :80)2+ (9:80)2+ (2)(23:8)(9:80)(0:29) = 796:00 the new standard deviation isp 796:00 = $28:21. 4 foundations for inference 4.1 (a)x= 0:6052: (b)s= 0:0131. (c)z0:63=0:63\u00000:6052 0:0131= 1:893. no, this level of bgc is within 2 sd of the mean. (d) the standard error of the sample mean is given byspn=0:0131p 70= 0:00157. 4.3 (a) this is the sampling distribution of the sample mean. (b) the sampling distribution will be normal and symmetric, centered around the theoretical population mean \u0016of the number of eggs laid by this hen species during a breeding period. (c) the variability of the distribution is the standard error of the sample mean:spn=18:2p 45= 2:71. (d) the variability of the new distribution will be greater than the variability of the original distribution. conceptually, a smaller sample is less informative, which leads to a more uncertain estimate. this can be shown concretely with a calculation:18:2p 10= 5:76 is larger than 2.71. 4.5 (a) we are 95% confident that the mean number of hours that u.s. residents have to relax or pursue activities that they enjoy is between 3.53 and 3.83 hours. (b) a larger margin of error with the same sample occurs with a higher confidence level (i.e., larger critical value). (c) the margin of error of the new 95% confidence interval will be smaller, since a larger sample size results in a smaller standard error. (d) a 90% confidence interval will be smaller than the original 95% interval, since the critical value is smaller and results in a smaller margin of error. the interval will provide a more precise estimate, but have an associated lower confidence of capturing \u0016. 447 4.7 (a) false. provided the data distribution is not very strongly skewed ( n= 64 in this sample, so we can be slightly lenient with the skew), the distribution of the sample mean will be nearly normal, allowing for the normal approximation. (b) false. inference is made on the population parameter, not the point estimate. the point estimate is always in the confidence interval. (c) true. (d) false. the confidence interval is not about a sample mean. (e) false. a wider interval is required to be more confident about capturing the parameter. (f) true. the margin of error is half the width of the interval, and the sample mean is the midpoint of the interval. (g) false. to halve the margin of error requires sampling 22= 4 times the number of people in the initial sample. 4.9 (a) i. false. there is a 5% chance that any 95% confidence interval does not contain the true population mean days out of the past 30 days that u.s. adults experienced poor mental health. ii. false. the population parameter\u0016is either inside or outside the interval; there is no probability associated with whether the fixed value\u0016is in a certain calculated interval. the randomness is associated with the interval (and the method for calculating it), not the parameter \u0016. thus, it would not be reasonable to say there is a 95% chance that the particular interval (3.40, 4.24) contains \u0016; this interpretation is coherent with the statement in part iii. of this question. iii. true. this is the definition of what it means to be 95% confident. iv. true. the interval corresponds to a two-sided test, with h0:\u0016= 4:5 days andha:\u0016,4:5 days and = 1\u00000:95 = 0:05. since \u00160of 4.5 days is outside the interval, the sample provides su fficient evidence to reject the null hypothesis and accept the alternative hypothesis. v. false. we can only be confident that 95% of the time, the entire interval calculated contains \u0016. it is not possible to make this statement about xor any other point within the interval. vi. false. the confidence interval is a statement about the population parameter \u0016, the mean days out of the past 30 days that all us adults experienced poor mental health. the sample mean xis a known quantity. (b) the 90% confidence interval will be smaller than the 95% confidence interval. if we are less confident that an interval contains \u0016, this implies that the interval is less wide; if we are more confident, the interval is wider. think about a theoretical \"100%\" confidence interval—to be 100% confident of capturing \u0016, then the range must be all possible numbers that \u0016could be. (c) (3.47, 4.17) days 4.11 (a) the null hypothesis is that new yorkers sleep an average of 8 hours of night ( h0:\u0016= 8 hours). the alternative hypothesis is that new yorkers sleep less than 8 hours a night on average ( ha:\u0016<8 hours). (b) the null hypothesis is employees spend on average 15 minutes on non-business activities in a day ( h0: \u0016= 15 minutes). the alternative hypothesis is that employees spend on average more than 15 minutes on non-business activities in a day ( ha:\u0016>15 minutes). 4.13 hypotheses are always made about the population parameter \u0016, not the sample mean x. the correct value of\u00160is 10 hours, as based on the previous evidence; both hypotheses should include \u00160. the correct hypotheses are h0:\u0016= 10 hours and ha:\u0016>10 hours. 4.15 (a) this claim is not supported by the confidence interval. 3 hours corresponds to a time of 180 minutes; there is evidence that the average waiting time is lower than 3 hours. (b) 2.2 hours corresponds to 132 minutes, which is within the interval. it is plausible that \u0016is 132 minutes, since we are 95% confident that the interval (128 minutes, 147 minutes) contains the average wait time. (c) yes, the claim would be supported based on a 99% interval, since the 99% interval is wider than the 95% interval. 4.17h0:\u0016= 130 grams, ha:\u0016,130 grams. test the hypothesis by calculating the test statistic: t=x\u0000\u00160 s=pn= 130\u0000134 17=sqrt 35= 1:39. this results in a p-value of 0.17. there is insu fficient evidence to reject the null hypothesis. there is no evidence that the nutrition label does not provide an accurate measure of calories. 448 appendix a. end of chapter exercise solutions 4.19 (a) the 95% confidence interval is 3 ;150\u0006(1:96\u0002250=p 50) = (3080:7;3219:3) grams. (b) she will conduct a test of the null against the two-sided alternative ha:\u0016,3250 grams. calculate the test statistic: t=x\u0000\u00160 s=pn=3150\u00003250 250=p 50=\u00002:83. thep-value is 0.007. there is su fficient evidence to reject the null hypothesis and conclude that the mean birthweight of babies from inner-city teaching hospitals is lower than 3,260 grams. 4.21 (a)h0: anti-depressants do not help symptoms of fibromyalgia. ha: anti- depressants do treat symptoms of fibromyalgia. (b) concluding that anti-depressants work for the treatment of fibromyalgia symptoms when they actually do not. (c) concluding that anti-depressants do not work for the treatment of fibromyalgia symptoms when they actually do. (d) if she makes a type 1 error, she will continue taking medication that does not actually treat her disorder. if she makes a type 2 error, she will stop taking medication that could treat her disorder. 4.23 (a) the standard error is larger under scenario i; standard error is larger for smaller values of n. (b) the margin of error is larger under scenario i; to be more confidence of capturing the population parameter requires a larger confidence interval. (c) thep-value from a z-statistic only depends on the value of the z-statistic; the value is equal under the scenarios. (d) the probability of making a type ii error and falsely rejecting the alternative is higher under scenario i; it is easier to reject the alternative with a high . 5 inference for numerical data 5.1 (a)df= 6\u00001 = 5,t? 5= 2:02 (column with two tails of 0.10, row with df= 5). (b)df= 21\u00001 = 20, t? 20= 2:53 (column with two tails of 0.02, row with df= 20). (c)df= 28,t? 28= 2:05. (d)df= 11,t? 11= 3:11. 5.3 on az-distribution, the cuto ffvalue for the upper 5% of values is 1.96. a t-distribution has wider tails than a normal distribution but approaches the shape of a standard normal as degrees of freedom increases. thus, 1.98 corresponds to the cuto fffor at-distribution with 100 degrees of freedom, 2.01 the cuto fffor 50 degrees of freedom, and 2.23 the cuto fffor 10 degrees of freedom. 5.5 the mean is the midpoint: ̄x= 20. identify the margin of error: me = 1:015, then use t? 35= 2:03 and se=s=pnin the formula for margin of error to identify s= 3. 5.7 (a)h0:\u0016= 8 (new yorkers sleep 8 hrs per night on average.) ha:\u0016,8 (new yorkers sleep less or more than 8 hrs per night on average.) (b) independence: the sample is random. the min/max suggest there are no concerning outliers. t=\u00001:75.df= 25\u00001 = 24. (c) p-value = 0 :093. if in fact the true population mean of the amount new yorkers sleep per night was 8 hours, the probability of getting a random sample of 25 new yorkers where the average amount of sleep is 7.73 hours per night or less (or 8.27 hours or more) is 0.093. (d) since p-value >0.05, do not reject h0. the data do not provide strong evidence that new yorkers sleep more or less than 8 hours per night on average. (e) no, since the p-value is smaller than 1 \u00000:90 = 0:10. 5.9tis either -2.09 or 2.09. then ̄xis one of the following: \u00002:09 = ̄x\u000060 8p 20! ̄x= 56:26 2:09 = ̄x\u000060 8p 20! ̄x= 63:74 449 5.11 (a) we will conduct a 1-sample t-test.h0:\u0016= 5.ha:\u0016,5. we’ll use = 0:05. this is a random sample, so the observations are independent. to proceed, we assume the distribution of years of piano lessons is approximately normal. se= 2:2=p 20 = 0:4919. the test statistic is t= (4:6\u00005)=se=\u00000:81.df= 20\u00001 = 19. the one-tail area is about 0.21, so the p-value is about 0.42, which is bigger than = 0:05 and we do not rejecth0. that is, we do not have su fficiently strong evidence to reject the notion that the average is 5 years. (b) usingse= 0:4919 andt? df=19= 2:093, the confidence interval is (3.57, 5.63). we are 95% confident that the average number of years a child takes piano lessons in this city is 3.57 to 5.63 years. (c) they agree, since we did not reject the null hypothesis and the null value of 5 was in the t-interval. 5.13 if the sample is large, then the margin of error will be about 1 :96\u0002100=pn. we want this value to be less than 10, which leads to n\u0015384:16, meaning we need a sample size of at least 385 (round up for sample size calculations!). 5.15 (a) since it’s the same students at the beginning and the end of the semester, there is a pairing between the datasets; for a given student their beginning and end of semester grades are dependent. (b) since the subjects were sampled randomly, each observation in the men’s group does not have a special correspondence with exactly one observation in the other (women’s) group. (c) since it’s the same subjects at the beginning and the end of the study, there is a pairing between the datasets; for a subject their beginning and end of semester artery thickness are dependent. (d) since it’s the same subjects at the beginning and the end of the study, there is a pairing between the datasets; for a subject their beginning and end of semester weights are dependent. 5.17 (a) for each observation in one data set, there is exactly one specially corresponding observation in the other data set for the same geographic location. the data are paired. (b) h0:\u0016diff= 0 (there is no di fference in average number of days exceeding 90°f in 1948 and 2018 for noaa stations.) ha:\u0016diff,0 (there is a difference.) (c) locations were randomly sampled, so independence is reasonable. the sample size is at least 30, so we’re just looking for particularly extreme outliers: none are present (the observation o ffleft in the histogram would be considered a clear outlier, but not a particularly extreme one). therefore, the conditions are satisfied. (d) se= 17:2=p 197 = 1:23.t=2:9\u00000 1:23= 2:36 with degrees of freedom df= 197\u00001 = 196. this leads to a one-tail area of 0.0096 and a p-value of about 0.019. (e) since the p-value is less than 0.05, we reject h0. the data provide strong evidence that noaa stations observed more 90°f days in 2018 than in 1948. (f) type 1 error, since we may have incorrectly rejected h0. this error would mean that noaa stations did not actually observe a decrease, but the sample we took just so happened to make it appear that this was the case. (g) no, since we rejected h0, which had a null value of 0. 5.19 (a)se= 1:23 andt?= 1:65. 2:9\u00061:65\u00021:23!(0:87;4:93). (b) we are 90% confident that there was an increase of 0.87 to 4.93 in the average number of days that hit 90°f in 2018 relative to 1948 for noaa stations. (c) yes, since the interval lies entirely above 0. 5.21 (a) each of the 36 mothers is related to exactly one of the 36 fathers (and vice-versa), so there is a special correspondence between the mothers and fathers. (b) h0:\u0016diff = 0.ha:\u0016diff,0. independence: random sample from less than 10% of population. sample size of at least 30. the skew of the di fferences is, at worst, slight.z= 2:72!p-value = 0:0066. since p-value <0.05, rejecth0. the data provide strong evidence that the average iq scores of mothers and fathers of gifted children are di fferent, and the data indicate that mothers’ scores are higher than fathers’ scores for the parents of gifted children. 450 appendix a. end of chapter exercise solutions 5.23 (a) sincep <0:05, there is statistically significant evidence that the population di fference in bgc is not 0. since the observed mean bgc is higher in the food supplemented group, these data suggest that food supplemented birds have higher bgc on average than birds that are not food supplemented. (b) the 95% confidence interval is d\u0006t?sdpn. since the mean of the di fferences is equal to the di fference of the means, d= 1:70\u00000:586 = 1:114. the test statistic is t=d sd=pn, so the standard error ( sd=pn) can be solved for: sd=pn=d=t= 1:114=2:64 = 0:422:the critical t-value for a 95% confidence interval on a t-distribution with 16\u00001 = 15 degrees of freedom is 2.13. thus, the 95% confidence interval is 1 :114\u0006(2:13\u00020:422)!(0:215;2:01) grams. with 95% confidence, the interval (0.215, 2.01) grams contains the population mean di fference in egg mass between food supplemented birds and non supplemented birds. 5.25 (a) these data are paired. for example, the friday the 13th in say, september 1991, would probably be more similar to the friday the 6th in september 1991 than to friday the 6th in another month or year. (b) let\u0016diff=\u0016sixth\u0000\u0016thirteenth .h0:\u0016diff= 0.ha:\u0016diff,0. (c) independence: the months selected are not random. however, if we think these dates are roughly equivalent to a simple random sample of all such friday 6th/13th date pairs, then independence is reasonable. to proceed, we must make this strong assumption, though we should note this assumption in any reported results. normality: with fewer than 10 observations, we would need to see clear outliers to be concerned. there is a borderline outlier on the right of the histogram of the di fferences, so we would want to report this in formal analysis results. (d)t= 4:93 fordf= 10\u00001 = 9!p-value = 0.001. (e) since p-value <0.05, reject h0. the data provide strong evidence that the average number of cars at the intersection is higher on friday the 6ththan on friday the 13th. (we should exercise caution about generalizing the interpretation to all intersections or roads.) (f) if the average number of cars passing the intersection actually was the same on friday the 6thand 13th, then the probability that we would observe a test statistic so far from zero is less than 0.01. (g) we might have made a type 1 error, i.e. incorrectly rejected the null hypothesis. 5.27 (a)h0:\u0016diff = 0.ha:\u0016diff,0.t=\u00002:71.df= 5. p-value = 0 :042. since p-value <0.05, reject h0. the data provide strong evidence that the average number of tra ffic accident related emergency room admissions are di fferent between friday the 6thand friday the 13th. furthermore, the data indicate that the direction of that di fference is that accidents are lower on friday the 6threlative to friday the 13th. (b) (-6.49, -0.17). (c) this is an observational study, not an experiment, so we cannot so easily infer a causal intervention implied by this statement. it is true that there is a di fference. however, for example, this does not mean that a responsible adult going out on friday the 13thhas a higher chance of harm than on any other night. 5.29 (a) chicken fed linseed weighed an average of 218.75 grams while those fed horsebean weighed an average of 160.20 grams. both distributions are relatively symmetric with no apparent outliers. there is more variability in the weights of chicken fed linseed. (b) h0:\u0016ls=\u0016hb.ha:\u0016ls,\u0016hb. we leave the conditions to you to consider. t= 3:02,df=min(11;9) = 9!0:01<p-value<0:02. since p-value <0.05, reject h0. the data provide strong evidence that there is a significant di fference between the average weights of chickens that were fed linseed and horsebean. (c) type 1 error, since we rejected h0. (d) yes, since p-value >0.01, we would have failed to reject h0. 451 5.31h0:\u0016c=\u0016s.ha:\u0016c,\u0016s.t= 3:27,df= 11!p-value<0:01. since p-value <0:05, rejecth0. the data provide strong evidence that the average weight of chickens that were fed casein is di fferent than the average weight of chickens that were fed soybean (with weights from casein being higher). since this is a randomized experiment, the observed di fference can be attributed to the diet. 5.33h0:\u0016t=\u0016c.ha:\u0016t,\u0016c.t= 2:24,df= 21!0:02<p-value<0:05. since p-value <0.05, reject h0. the data provide strong evidence that the average food consumption by the patients in the treatment and control groups are di fferent. furthermore, the data indicate patients in the distracted eating (treatment) group consume more food than patients in the control group. 5.35 let\u0016diff =\u0016pre\u0000\u0016post.h0:\u0016diff = 0: treatment has no e ffect.ha:\u0016diff,0: treatment has an e ffect on p .d.t. scores, either positive or negative. conditions: the subjects are randomly assigned to treatments, so independence within and between groups is satisfied. all three sample sizes are smaller than 30, so we look for clear outliers. there is a borderline outlier in the first treatment group. since it is borderline, we will proceed, but we should report this caveat with any results. for all three groups: df= 13.t1= 1:89!p-value = 0.081,t2= 1:35!p-value = 0.200), t3=\u00001:40!(p-value = 0.185). we do not reject the null hypothesis for any of these groups. as earlier noted, there is some uncertainty about if the method applied is reasonable for the first group. 5.37 difference we care about: 40. single tail of 90%: 1 :28\u0002se. rejection region bounds: \u00061:96\u0002se(if 5% significance level). setting 3 :24\u0002se= 40, subbing in se=q 942 n+942 n, and solving for the sample size ngives 116 plots of land for each fertilizer. 5.39h0:\u00161=\u00162=\u0001\u0001\u0001=\u00166.ha: the average weight varies across some (or all) groups. independence: chicks are randomly assigned to feed types (presumably kept separate from one another), therefore independence of observations is reasonable. approx. normal: the distributions of weights within each feed type appear to be fairly symmetric. constant variance: based on the side-by-side box plots, the constant variance assumption appears to be reasonable. there are di fferences in the actual computed standard deviations, but these might be due to chance as these are quite small samples. f5;65= 15:36 and the p-value is approximately 0. with such a small p-value, we reject h0. the data provide convincing evidence that the average weight of chicks varies across some (or all) feed supplement groups. 5.41 (a)h0: the population mean of met for each group is equal to the others. ha: at least one pair of means is di fferent. (b) independence: we don’t have any information on how the data were collected, so we cannot assess independence. to proceed, we must assume the subjects in each group are independent. in practice, we would inquire for more details. normality: the data are bound below by zero and the standard deviations are larger than the means, indicating very strong skew. however, since the sample sizes are extremely large, even extreme skew is acceptable. constant variance: this condition is su fficiently met, as the standard deviations are reasonably consistent across groups. (c) see below, with the last column omitted: df sum sq mean sq f value coffee 4 10508 2627 5.2 residuals 50734 25564819 504 total 50738 25575327 (d) since p-value is very small, reject h0. the data provide convincing evidence that the average met di ffers between at least one pair of groups. 5.43 (a)h0: average gpa is the same for all majors. ha: at least one pair of means are di fferent. (b) since p-value>0.05, fail to reject h0. the data do not provide convincing evidence of a di fference between the average gpas across three groups of majors. (c) the total degrees of freedom is 195 + 2 = 197, so the sample size is 197 + 1 = 198. 452 appendix a. end of chapter exercise solutions 5.45 (a) false. as the number of groups increases, so does the number of comparisons and hence the modified significance level decreases. (b) true. (c) true. (d) false. we need observations to be independent regardless of sample size. 5.47 (a)h0: average score di fference is the same for all treatments. ha: at least one pair of means are different. (b) we should check conditions. if we look back to the earlier exercise, we will see that the patients were randomized, so independence is satisfied. there are some minor concerns about skew, especially with the third group, though this may be acceptable. the standard deviations across the groups are reasonably similar. since the p-value is less than 0.05, reject h0. the data provide convincing evidence of a di fference between the average reduction in score among treatments. (c) we determined that at least two means are different in part (b), so we now conduct k= 3\u00022=2 = 3 pairwise t-tests that each use = 0:05=3 = 0:0167 for a significance level. use the following hypotheses for each pairwise test. h0: the two means are equal. ha: the two means are di fferent. the sample sizes are equal and we use the pooled sd, so we can compute se= 3:7 with the pooled df= 39. the p-value for trmt 1 vs. trmt 3 is the only one under 0.05: p-value = 0.035 (or 0.024 if using spooled in place ofs1ands3, though this won’t a ffect the final conclusion). the p-value is larger than 0:05=3 = 1:67, so we do not have strong evidence to conclude that it is this particular pair of groups that are different. that is, we cannot identify if which particular pair of groups are actually di fferent, even though we’ve rejected the notion that they are all the same! 6 simple linear regression 6.1 (a) strong relationship, but a straight line would not fit the data. (b) strong relationship, and a linear fit would be reasonable. (c) weak relationship, and trying a linear fit would be reasonable. (d) moderate relationship, but a straight line would not fit the data. (e) strong relationship, and a linear fit would be reasonable. (f) weak relationship, and trying a linear fit would be reasonable. 6.3 (a) there is a moderate, positive, and linear relationship between shoulder girth and height. (b) changing the units, even if just for one of the variables, will not change the form, direction or strength of the relationship between the two variables. 6.5 over-estimate. since the residual is calculated as observed\u0000predicted , a negative residual means that the predicted value is higher than the observed value. 6.7 (a)murder =\u000029:901+2:559\u0002poverty %. (b) expected murder rate in metropolitan areas with no poverty is -29. 901 per million. this is obviously not a meaningful value, it just serves to adjust the height of the regression line. (c) for each additional percentage increase in poverty, we expect murders per million to be higher on average by 2.559. (e)p 0:7052 = 0:8398. 6.9 (a) the slope of -1.26 indicates that on average, an increase in age of 1 year is associated with a lower rfft score by 1.26 points. the intercept of 137.55 represents the predicted mean rfft score for an individual of age 0 years; this does not have interpretive meaning since the rfft cannot be reasonably administered to a newborn. (b) rfft score di ffers on average by 10( \u00001:26) = 12:6 points between an individual who is 60 years old versus 50 years old, with the older individual having the lower score. (c) according to the model, average rfft score for a 70-year-old is 137 :55\u00001:26(70) = 49:3 points. (d) no, it is not valid to use the linear model to estimate rfft score for a 20-year-old. as indicated in the plot, data are only available for individuals as young as about 40 years old. 6.11 (a) the residual plot will show randomly distributed residuals around 0. the variance is also approximately constant. (b) the residuals will show a fan shape, with higher variability for smaller x. there will also be many points on the right above the line. there is trouble with the model being fit here. 453 6.13 (a) the points with the lowest and highest values for height have relatively high leverage. they do not seem particularly influential because they are not outliers; the one with a low x-value has a low y-value and the one with a high x-value has a high y-value, which follows the positive trend visible in the data. (b) yes, since the data show a linear trend, it is appropriate to use r2as a metric for describing the strength of the model fit. (c) height explains about 72% of the observed variability in length. 6.15 there is an upwards trend. however, the variability is higher for higher calorie counts, and it looks like there might be two clusters of observations above and below the line on the right, so we should be cautious about fitting a linear model to these data. 6.17 (a) there is an outlier in the bottom right. since it is far from the center of the data, it is a point with high leverage. it is also an influential point since, without that observation, the regression line would have a very di fferent slope. (b) there is an outlier in the bottom right. since it is far from the center of the data, it is a point with high leverage. however, it does not appear to be a ffecting the line much, so it is not an influential point. (c) the observation is in the center of the data (in the x-axis direction), so this point does nothave high leverage. this means the point won’t have much e ffect on the slope of the line and so is not an influential point. 6.19 (a) linearity is satisfied; the data scatter about the horizontal line with no apparent pattern. the variability seems constant across the predicted length values. (b) the fish were randomly sampled from a river, so without additional details about the life cycle of the fish, it seems reasonable to assume the height and length of any one fish does not provide information about the height and length of another fish. this could be violated, if, for example, the fish in a river tend to be closely related and height and length are highly heritable. (c) the residuals are approximately normally distributed, with some small deviations from normality in the tails. there are more outliers in both tails than expected under a normal distribution. 6.21 one possible equation is price = 44:51 + 12:3(carat 1:00), where the explanatory variable is a binary variable taking on value 1if the diamond is 1 carat. 6.23 (a) the relationship is positive, moderate-to-strong, and linear. there are a few outliers but no points that appear to be influential. (b)weight =\u0000105:0113 + 1:0176\u0002height . slope: for each additional centimeter in height, the model predicts the average weight to be 1.0176 additional kilograms (about 2.2 pounds). intercept: people who are 0 centimeters tall are expected to weigh - 105.0113 kilograms. this is obviously not possible. here, the y- intercept serves only to adjust the height of the line and is meaningless by itself. (c)h0: the true slope coe fficient of height is zero ( 1= 0). ha: the true slope coe fficient of height is di fferent than zero ( 1,0). the p-value for the two-sided alternative hypothesis ( 1,0) is incredibly small, so we reject h0. the data provide convincing evidence that height and weight are positively correlated. the true slope parameter is indeed greater than 0. (d)r2= 0:722= 0:52. approximately 52% of the variability in weight can be explained by the height of individuals. 454 appendix a. end of chapter exercise solutions 6.25 (a)h0: 1= 0.ha: 1,0. the p-value, as reported in the table, is incredibly small and is smaller than 0.05, so we reject h0. the data provide convincing evidence that wives’ and husbands’ heights are positively correlated. (b)heightw= 43:5755 + 0:2863\u0002heighth. (c) slope: for each additional inch in husband’s height, the average wife’s height is expected to be an additional 0.2863 inches on average. intercept: men who are 0 inches tall are expected to have wives who are, on average, 43.5755 inches tall. the intercept here is meaningless, and it serves only to adjust the height of the line. (d) the slope is positive, so rmust also be positive. r=p 0:09 = 0:30. (e) 63.33. since r2is low, the prediction based on this regression model is not very reliable. (f) no, we should avoid extrapolating. (g) yes, the p-value for the slope parameter is less than = 0:05. there is su fficient evidence to accept the alternative hypothesis, ha: 1,0. these data suggest that wife height and husband height are positively associated at the population level. (h) no, a 95% confidence interval for 1would not be expected to contain the null value 0, since the p-value is less than 0.05. 6.27 (a) the point estimate and standard error are b1= 0:9112 andse= 0:0259. we can compute a tscore:t= (0:9112\u00001)=0:0259 =\u00003:43. usingdf= 168, the p-value is about 0.001, which is less than = 0:05. that is, the data provide strong evidence that the average di fference between husbands’ and wives’ ages has actually changed over time. (b) dagew= 1:5740 + 0:9112\u0002ageh. (c) slope: for each additional year in husband’s age, the model predicts an additional 0.9112 years in wife’s age. this means that wives’ ages tend to be lower for later ages, suggesting the average gap of husband and wife age is larger for older people. intercept: men who are 0 years old are expected to have wives who are on average 1.5740 years old. the intercept here is meaningless and serves only to adjust the height of the line. (d) r=p 0:88 = 0:94. the regression of wives’ ages on husbands’ ages has a positive slope, so the correlation coe fficient will be positive. (e)dagew= 1:5740 + 0:9112\u000255 = 51:69. sincer2is pretty high, the prediction based on this regression model is reliable. (f) no, we shouldn’t use the same model to predict an 85 year old man’s wife’s age. this would require extrapolation. the scatterplot from an earlier exercise shows that husbands in this data set are approximately 20 to 65 years old. the regression model may not be reasonable outside of this range. 6.29 (a) yes, since p<0:01.h0: 1= 0,ha: 1,0, where 1represents the population average change in rfft score associated with a change in 1 year of age. there is statistically significant evidence that age is negatively associated with rfft score. (b) with 99% confidence, the interval (-1.49, -1.03) points contains the population average di fference in rfft score between individuals who di ffer in age by 1 year; the older individual is predicted to have a lower rfft score. 6.31 (a) first, compute the standard error: s.e.(e(agewifejagehusband = 55)) = 3:95r 1 170+(55\u000042:92)2 (170\u00001)11:762= 0:435:the critical value is t? 0:975;df=169= 1:97. thus, the 95% confidence interval is 51 :69\u0006(1:97)(0:435) = (50:83;52:55) years. (b) first, compute the standard error: s.e.( agewifejagehusband = 55) = 3:95r 1 +1 170+(55\u000042:92)2 (170\u00001)11:762= 3:97. the 95% prediction interval is 51 :69\u0006(1:97)(3:97) = (43:85;59:54) years. (c) for the approximate 95% confidence interval, use s=pn= 3:95=p 170 = 0:303 as the approximate standard error: (51 :09;52:29) years. for the approximate 95% prediction interval, use sp 1 + 1=n= 3:95p 1 + 1=170 = 4:25 as the approximate standard error: (43:30;60:09) years. 455 7 multiple linear regression 7.1 although the use of statins appeared to be associated with lower rfft score when no adjustment was made for possible confounders, statin use is not significantly associated with rfft score in a model that adjusts for age. after adjusting for age, the estimated di fference in mean rfft score between statin users and non-users is 0.85 points; there is a 74% chance of observing such a di fference if there is no di fference between mean rfft score in the population of statin users and non-users. 7.3 (a)baby _weight = 123:57\u00008:96(smoke )\u00001:98(parity ) (b) a child born to a mother who smokes has a birth weight about 9 ounces less, on average, than one born to a mother who does not smoke, holding birth order constant. a child who is the first born has birth weight about 2 ounces less, on average, than one who is not first born, when comparing children whose mothers were either both smokers or both nonsmokers. the intercept represents the predicted mean birth weight for a child whose mother is not a smoker and who was not the first born. (c) the estimated di fference in mean birth weight for two infants born to non-smoking mothers, where one is first born and the other is not, is -1.98. (d) this is the same value as in part (c). (e) 123:57\u00008:96(0)\u00001:98(1) = 121:59 ounces. 7.5 (a)baby _weight =\u000080:41 + 0:44\u0002gestation\u00003:33\u0002parity\u00000:01\u0002age+ 1:15\u0002height + 0:05\u0002weight\u0000 8:40\u0002smoke . (b) gestation : the model predicts a 0.44 ounce increase in the birth weight of the baby for each additional day of pregnancy, all else held constant. age: the model predicts a 0.01 ounce decrease in the birth weight of the baby for each additional year in mother’s age, all else held constant. (c) parity might be correlated with one of the other variables in the model, which complicates model estimation. (d) baby _weight = 120:58. e= 120\u0000120:58 =\u00000:58. the model over-predicts this baby’s birth weight. (e) r2= 0:2504.r2 adj= 0:2468. 7.7 nearly normal residuals: with so many observations in the data set, we look for particularly extreme outliers in the histogram and do not see any. variability of residuals: the scatterplot of the residuals versus the fitted values does not show any overall structure. however, values that have very low or very high fitted values appear to also have somewhat larger outliers. in addition, the residuals do appear to have constant variability between the two parity and smoking status groups, though these items are relatively minor. independent residuals: the scatterplot of residuals versus the order of data collection shows a random scatter, suggesting that there is no apparent structures related to the order the data were collected. linear relationships between the response variable and numerical explanatory variables: the residuals vs. height and weight of mother are randomly distributed around 0. the residuals vs. length of gestation plot also does not show any clear or strong remaining structures, with the possible exception of very short or long gestations. the rest of the residuals do appear to be randomly distributed around 0. all concerns raised here are relatively mild. there are some outliers, but there is so much data that the influence of such observations will be minor. 7.9 (b) true. (c) false. this would only be the case if the data was from an experiment and x1was one of the variables set by the researchers. (multiple regression can be useful for forming hypotheses about causal relationships, but it o ffers zero guarantees.) (d) false. we should check normality like we would for inference for a single mean: we look for particularly extreme outliers if n\u001530 or for clear outliers if n<30. 456 appendix a. end of chapter exercise solutions 7.11 (a) (-0.32, 0.16). we are 95% confident that male students on average have gpas 0.32 points lower to 0.16 points higher than females when controlling for the other variables in the model. (b) yes, since the p-value is larger than 0.05 in all cases (not including the intercept). 7.13 (a)eggs:laid =\u000017:88 + 4:28(wolbachia ) + 0:272(tibia ) (b) an increase in wolbachia density of one unit is associated with on average 4.28 more eggs laid over a lifetime, assuming body size is held constant. (c) in a multiple regression model adjusting for body size as a potential confounder, increase in wolbachia density was significantly positively associated with realized fitness, measured as the number of eggs laid over a female’s full lifetime ( p= 0:002). these data are consistent with the scientific hypothesis that wolbachia is beneficial for its host in nature. (d) (1 :85;7:05) eggs (e) as a group, the predictors wolbachia density and tibia length are useful for predicting the number of eggs laid over a lifetime. 7.15 (a) since the di fference is taken in the direction (pre - post), a positive value for trt.effect indicates that the post-intervention score is lower than the pre-intervention score, which represents e fficacy of the intervention. a negative value would represent a patient’s deviant t scores increasing after the intervention. (b) let ybe the change in mmpi score for a participant in this study, xneutral a variable with value 1 for participants assigned to the neutral tape and 0 otherwise, and xtherapeutic a variable with value 1 for participants in the emotional neutral group and 0 otherwise. the population-level equation is e(y) = 0+ neutralxneutral + therapeuticxtherapeutic . for these data, the estimated model equation is by=\u00003:21 + 6:07xneutral + 9:43xtherapeutic . (c) the predicted di fference scores byfor a patient receiving the neutral tape will be by=b0+bneutralxneutral +btherapeuticxtherapeutic =\u00003:21 + 6:07 + 0 = 2:86:(d) yes. the intercept is the average of the score di fference for the group that did not hear a taped message. (e) the two slopes represent the change in average mmpi score di fference from the average for the group that did not receive a tape. the absent category is the reference group. (f) the p\u0000value for the intercept corresponds to a test of the null hypothesis that the average di fference score was 0 in the group that did not hear a taped message. the slope p-values correspond to tests of the null hypotheses of (on average) no change in di fference scores between the intervention with no tape and each of the other two interventions. 7.17 (a) letpreandpost denote the pre- and post-intervention scores, respectively. the estimated equation for the model is dpost = 28:41 + 0:66(pre)\u00005:73xneutral\u00009:75xtherapeutic . (b) since the coe fficient of the preintervention score is positive, post-intervention scores tend to increase as the pre-intervention score increases. (c) yes. the t-statistic for the coe fficient of preis 4.05 and is statistically significant. (d) in this model, treatment is a factor variable with three levels and the intervention with no tape is the baseline treatment that does not appear in the model. for a participant with pre= 70 and no tape, the predicted value of post is 28:41 + 0:66(73)\u00005:73(1) = 70:86 (e) for a given value of pre, the coe fficient of treatmentneutral is the predicted change in post between an participant without a tape and one with the emotionally neutral tape. the model implies that post will be 5.7 points lower with the emotionally neutral tape. the evidence for a treatment e ffect of the emotionally neutral tape is weak; the coe fficient is not statistically significant at = 0:05. 457 7.19 (a)dpost =\u000017:58+1:28(pre)+67:75(neutral )+64:42(therapeutic )\u00000:99(pre\u0002neutral )\u00001:01(pre\u0002therapeutic ) (b) the coe fficient for preis the predicted increase in post score associated with a 1 unit increase in pre-score for individuals in the absent arm, while the coe fficients of the interaction terms for neutral and therapeutic represent the di fference in association between pre and post scores for individuals in those groups. for example, an individual in the neutral group is expected to have a 1 :28\u00000:99 = 0:29 point increase in post score, on average, per 1 point increase in pre-score. the coe fficients of the slopes for neutral and therapeutic are differences in intercept values relative to the intercept for the model, which is for the baseline group (absent). (c) absent: dpost =\u000017:58 + 1:28(pre) neutral: dpost =\u000017:58 + 67:75 + 1:28(pre)\u00000:99(pre) = 50:17 + 0:29(pre) therapeutic: dpost =\u000017:58 + 64:42 + 1:28(pre)\u00001:01(pre) = 46:84 + 0:27(pre) (d) these data suggest there is a statistically significant di fference in association between pre- and post-intervention scores by treatment group relative to the group that did not receive any treatment. the coe fficients of both interaction terms are statistically significant at = 0:05. since the slopes are smaller than the slope for the treatment absent group, the data demonstrate that individuals in either treatment group show less increase in mmpi score than occurs when no treatment is applied. 7.21 (a)rfft = 140:20\u000013:97(statin )\u00001:31(age)+0:25(statin\u0002age) (b) the model intercept represents the predicted mean rfft score for a statin non-user of age 0 years; the intercept does not have a meaningful interpretation. the slope coe fficient for age represents the predicted change in rfft score for a statin non-user; for non-users, a one year increase in age is associated with a 1.32 decrease in rfft score. the slope coe fficient for statin use represents the di fference in intercept between the regression line for users and the regression line for non-users; the intercept for users is -13.97 points lower than that of non-users. the interaction term coefficient represents the di fference in the magnitude of association between rfft score and age between users and non-users; in users, the slope coe fficient representing predicted change in rfft score per 1 year change in age is higher by 0.25 points. (c) no, there is not evidence that the association between rfft score and age di ffers by statin use. the p-value of the interaction coe fficient is 0.32, which is higher than = 0:05. 7.23 age should be the first variable removed from the model. it has the highest p-value, and its removal results in an adjusted r2of 0.255, which is higher than the current adjusted r2. 7.25 (a) the strongest predictor of birth weight appears to be gestational age; these two variables show a strong positive association. both parity and smoker status show a slight association with gestational age; the first born child tends to be a lower birth weight and children from mothers who smoke tend to have lower birth weight. while there does not appear to be an association between birth weight and age of the mother, there may be a slight positive association between both birth weight and height and birth weight and weight. all predictor variables with exception of age seem potentially useful for inclusion in an initial model. (b) height and weight appear to be positively associated. 7.27 (a) thef-statistic for the model corresponds to a test of h0: neutral = therapeutic = 0. (b) the intercept coe fficient is the estimated mean di fference score for the no intervention group, and the estimated mean difference score for the other two groups can be calculated by adding each of the slope estimates to the intercept. (c) under the null hypothesis that the two slope coe fficients are 0, all three interventions would have the same mean di fference in mmpi scores. this is the same as the null hypothesis for an anov a with three groups (h0:\u00161=\u00162=\u00163), which states that all three population means are the same. (d) the assumptions for multiple regression and anov a are outlined in sections 7.3.1 and 5.5, respectively. the assumptions for the two models are the same, though they may be phrased di fferently. the first assumption in multiple regression is linear change of the mean response variable when one predictor changes and the others do not change. since each of the two predictor variables in this model can only change from 0 to 1, this assumption is simply that the means in the three groups are possibly di fferent, which is true in anov a. the second assumption in regression is that the variance of the residuals is approximately constant. since the predicted response for an intervention group is its mean, the constant variance assumption in regression is the equivalent assumption in anov a that the three groups have approximately constant variance. both models assume that the observations are independent and that the residuals follow a normal distribution. this is a very long way of saying that the two models are identical! 458 appendix a. end of chapter exercise solutions 8 inference for categorical data 8.1 (a) false. doesn’t satisfy success-failure condition. (b) true. the success-failure condition is not satisfied. in most samples we would expect ˆpto be close to 0.08, the true population proportion. while ˆpcan be much above 0.08, it is bound below by 0, suggesting it would take on a right skewed shape. plotting the sampling distribution would confirm this suspicion. (c) false. seˆp= 0:0243, and ˆp= 0:12 is only0:12\u00000:08 0:0243= 1:65 ses away from the mean, which would not be considered unusual. (d) true. ˆp= 0:12 is 2.32 standard errors away from the mean, which is often considered unusual. (e) false. decreases the se by a factor of 1 =p 2. 8.3 (a) false. a confidence interval is constructed to estimate the population proportion, not the sample proportion. (b) true. 95% ci: 82% \u00062%. (c) true. by the definition of the confidence level. (d) true. quadrupling the sample size decreases the se and me by a factor of 1 =p 4. (e) true. the 95% ci is entirely above 50%. 8.5 with a random sample, independence is satisfied. the success-failure condition is also satisfied. me = z?q ˆp(1\u0000ˆp) n= 1:96q 0:56\u00020:44 600= 0:0397\u00194% 8.7 (a) no. the sample only represents students who took the sat, and this was also an online survey. (b) (0.5289, 0.5711). we are 90% confident that 53% to 57% of high school seniors who took the sat are fairly certain that they will participate in a study abroad program in college. (c) 90% of such random samples would produce a 90% confidence interval that includes the true proportion. (d) yes. the interval lies entirely above 50%. 8.9 (a) we want to check for a majority (or minority), so we use the following hypotheses: h0:p= 0:5 ha:p,0:5 we have a sample proportion of ˆp= 0:55 and a sample size of n= 617 independents. since this is a random sample, independence is satisfied. the success-failure condition is also satisfied: 617 \u0002 0:5 and 617\u0002(1\u00000:5) are both at least 10 (we use the null proportion p0= 0:5 for this check in a one-proportion hypothesis test). therefore, we can model ˆpusing a normal distribution with a standard error of se=r p(1\u0000p) n= 0:02 (we use the null proportion p0= 0:5 to compute the standard error for a one-proportion hypothesis test.) next, we compute the test statistic: z=0:55\u00000:5 0:02= 2:5 this yields a one-tail area of 0.0062, and a p-value of 2 \u00020:0062 = 0:0124. because the p-value is smaller than 0.05, we reject the null hypothesis. we have strong evidence that the support is di fferent from 0.5, and since the data provide a point estimate above 0.5, we have strong evidence to support this claim by the tv pundit. (b) no. generally we expect a hypothesis test and a confidence interval to align, so we would expect the confidence interval to show a range of plausible values entirely above 0.5. however, if the confidence level is misaligned (e.g. a 99% confidence level and a = 0:05 significance level), then this is no longer generally true. 459 8.11 since a sample proportion ( ˆp= 0:55) is available, we use this for the sample size calculations. the margin of error for a 90% confidence interval is 1 :65\u0002se= 1:65\u0002q p(1\u0000p) n. we want this to be less than 0.01, where we use ˆpin place ofp: 1:65\u0002r 0:55(1\u00000:55) n\u00140:01 1:6520:55(1\u00000:55) 0:012\u0014n from this, we get that nmust be at least 6739. 8.13 (a)h0:p= 0:5.ha:p,0:5. independence (random sample) is satisfied, as is the success-failure conditions (using p0= 0:5, we expect 40 successes and 40 failures). z= 2:91!the one tail area is 0.0018, so the p-value is 0.0036. since the p-value <0:05, we reject the null hypothesis. since we rejected h0and the point estimate suggests people are better than random guessing, we can conclude the rate of correctly identifying a soda for these people is significantly better than just by random guessing. (b) if in fact people cannot tell the di fference between diet and regular soda and they were randomly guessing, the probability of getting a random sample of 80 people where 53 or more identify a soda correctly (or 53 or more identify a soda incorrectly) would be 0.0036. 8.15 (a) yes, it is reasonable to use the normal approximation to the binomial distribution. the sample observations are independent and the expected numbers of successes and failures are greater than 10: nˆp= (100)(:15) = 15 and n(1\u0000ˆp) = (100)(0:85) = 85. (b) an approximate 95% confidence interval is ˆp\u0006 1:96q ˆp(1\u0000ˆp) n!(0:08;0:22). (c) the interval does not support the claim. since the interval does not contain 0.05, there is statistically significant evidence at = 0:05 that the proportion of young women in the neighborhood who use birth control is di fferent than 0.05. the interval is above 0.05, which is indicative of evidence that more than 5% of young women in the neighborhood use birth control. 8.17 this is not a randomized experiment, and it is unclear whether people would be a ffected by the behavior of their peers. that is, independence may not hold. additionally, there are only 5 interventions under the provocative scenario, so the success-failure condition does not hold. even if we consider a hypothesis test where we pool the proportions, the success-failure condition will not be satisfied. since one condition is questionable and the other is not satisfied, the di fference in sample proportions will not follow a nearly normal distribution. 8.19 (a) standard error: se=r 0:79(1\u00000:79) 347+0:55(1\u00000:55) 617= 0:03 usingz?= 1:96, we get: 0:79\u00000:55\u00061:96\u00020:03!(0:181;0:299) we are 95% confident that the proportion of democrats who support the plan is 18.1% to 29.9% higher than the proportion of independents who support the plan. (b) true. 460 appendix a. end of chapter exercise solutions 8.21 (a) testh0:p1=p2againstha:p1,p2, wherep1represents the population proportion of clinical improvement in covid-19 patients treated with remdesivir and p2represents the population proportion of clinical improvement in covid-19 patients treated with placebo. let = 0:05. thep-value is 0.328, which is greater than ; there is insu fficient evidence to reject the null hypothesis of no di fference. even though the proportion of patients who experienced clinical improvement about 7% higher in the remdesivir group, this difference is not extreme enough to represent su fficient evidence that remdesivir is more e ffective than placebo. (b) the 95% confidence interval is (-0.067, 0.217); with 95% confidence, this interval captures the difference in population proportion of clinical mortality between covid-19 patients treated with remdesivir and those treated with placebo. the interval contains 0, which is consistent with no statistically significant evidence of a di fference. the interval reflects the lack of precision around the e ffect estimate that is characteristic of an insu fficiently large sample size. 8.23 (a) false. the entire confidence interval is above 0. (b) true. (c) true. (d) true. (e) false. it is simply the negated and reordered values: (-0.06,-0.02). 8.25 subscript cmeans control group. subscript tmeans truck drivers. h0:pc=pt.ha:pc,pt. independence is satisfied (random samples), as is the success-failure condition, which we would check using the pooled proportion ( ˆppool= 70=495 = 0:141).z=\u00001:65!p-value = 0 :0989. since the p-value is high (default to alpha = 0.05), we fail to reject h0. the data do not provide strong evidence that the rates of sleep deprivation are di fferent for non-transportation workers and truck drivers. 8.27 (a) false. the chi-square distribution has one parameter called degrees of freedom. (b) true. (c) true. (d) false. as the degrees of freedom increases, the shape of the chi-square distribution becomes more symmetric. 8.29 (a) two-way table: quit treatment yes no total patch + support group 40 110 150 only patch 30 120 150 total 70 230 300 (b-i)erow 1;col1=(row 1total )\u0002(col1total ) tabletotal= 35. this is lower than the observed value. (b-ii)erow 2;col2=(row 2total )\u0002(col2total ) tabletotal= 115. this is lower than the observed value. 8.31 (a)h0: there is no association between statin use and educational level. ha: there is an association between statin use and educational level (b) it is reasonable to assume the counts are independent. the smallest expected value in the table is 39.27, so the success-failure condition is reasonably met. (c) there is statistically significant evidence at = 0:05 of an association between educational level and statin use. individuals with a higher educational level are less likely to be statin users. 8.33 (a) no default default sum non-diabetic 1053 127 1180 diabetic 54 0 54 sum 1107 127 1234 (b)h0:p1=p2versusha:p1,p2, wherep1represents the population proportion of treatment default in diabetics and p2represents the population proportion of treatment default in non-diabetics. (c) it is reasonable to assume the counts are independent. the smallest expected value is 5.56, which is not smaller than 5. (d) the 2test statistic is 5.37, with 1 degree of freedom. the p-value of the test statistic is 0.02. there is sufficient evidence to conclude that the proportion of treatment default is higher in non-diabetics than in diabetics. 461 8.35 (a) one possible 2 \u00022 contingency table: mosquito nets no yes total malaria 30 22 52 no malaria 70 78 148 total 100 100 200 (b) expected number of infected children among 100 families who did receive a net:52\u0002100 200= 26. (c) the null hypothesis is h0: using a mosquito net and being infected with malaria are not associated. the alternative is ha: using a net and being infected with malaria are associated. the 2statistic (1.66) has 1 degree of freedom and the table a3 can be used to show that p>0:10. there is not statistically significant evidence of an association between malaria infection and use of a net in children. (d) because this is a prospective study, the relative risk can be calculated directly from the table. let pno nets be the probability that a child without a net will be infected with malaria: ˆpno nets =30 100= 0:30. letpnets be the probability that a child with a net will be infected with malaria: ˆpnets =22 100= 0:22. the estimated relative risk: crr=ˆpno nets ˆpnets=0:30 0:22= 1:36. the risk of malaria infection for children in the control group is 36% higher than risk for children in the treatment group. 8.37 (a) under the null hypothesis of no association, the expected cell counts are 9.07 and 7.93 in the wait together and wait alone groups, respectively, for those considered \"high anxiety\" and 6.93 and 6.07 in the wait together and wait alone groups, respectively, for those considered \"low anxiety\". (b) use the hypergeometric distribution with parameters n= 30,m= 16, andn= 17; calculate p(x= 12). consider the \"successes\" to be the individuals who wait together, and the \"number sampled\" to be the people randomized to the highanxiety group. the probability of the observed set of results, assuming the marginal totals are fixed and the null hypothesis is true, is 0.0304. (c) more individuals than expected in the high-anxiety group were observed to wait together; thus, tables that are more extreme in the same direction also consist of those where more people in the high-anxiety group wait together than observed. these are tables in which 13, 14, 15, or 16 individuals in the high-anxiety group wait together. wait together wait alone sum high-anxiety 13 4 17 low-anxiety 3 10 13 sum 16 14 30 wait together wait alone sum high-anxiety 14 3 17 low-anxiety 2 11 13 sum 16 14 30 wait together wait alone sum high-anxiety 15 2 17 low-anxiety 1 12 13 sum 16 14 30 wait together wait alone sum high-anxiety 16 1 17 low-anxiety 0 13 13 sum 16 14 30 (d) letp1represent the population proportion of individuals waiting together in the high-anxiety group andp2represent the population proportion of individuals waiting together in the low-anxiety group. test h0:p1=p2againstha:p1,p2. let = 0:05. the two-sided p-value is 0.063. there is insu fficient evidence to reject the null hypothesis; the data do not suggest there is an association between high anxiety and a person’s desire to be in the company of others. 462 appendix a. end of chapter exercise solutions 8.39 (a)h0: the distribution of the format of the book used by the students follows the professor’s predictions.ha: the distribution of the format of the book used by the students does not follow the professor’s predictions. (b) ehard copy = 126\u00020:60 = 75:6.eprint = 126\u00020:25 = 31:5.eonline = 126\u00020:15 = 18:9. (c) independence: the sample is not random. however, if the professor has reason to believe that the proportions are stable from one term to the next and students are not a ffecting each other’s study habits, independence is probably reasonable. sample size: all expected counts are at least 5. (d) 2= 2:32,df= 2, p-value = 0.313. (e) since the p-value is large, we fail to reject h0. the data do not provide strong evidence indicating the professor’s predictions were statistically inaccurate. 8.41 (a) cvd no cvd age onset\u001450 years 15 25 age onset>50 years 5 55 (b) the odds of cvd for patients older than 50 years when diagnosed with diabetes is 5/55 = 0.09. the odds of cvd for the patients younger than 50 years at diabetes onset is 15/25 = 0.60. the relative odds (or odds ratio, or) is 0.09/0.60 = 0.15. (c) the odds of cvd for someone with late onset diabetes is less than 1/5 that of people with earlier onset diabetes. this can be explained by the fact that people with diabetes tend to build up plaque in their arteries; with early onset diabetes, plaque has longer time to accumulate, eventually causing cvd. (d)h0:or= 1. (e) the chi-square test can be used to test h0as long as the conditions for the test have been met. the observations are likely independent; knowing one person’s age of diabetes onset and cvd status is unlikely to provide information about another person’s age of diabetes onset and cvd status. under h0, the expected cell count for the lower left cell is (60)(20)/100 = 12, which is bigger than 5; all other expected cell counts will be larger. (f) since the study is not a randomized experiment, it cannot demonstrate causality. it may be the case, for example, that cvd presence causes earlier onset of diabetes. the study only demonstrates an association between cardiovascular disease and diabetes. 8.43 (a) no. this is an example of outcome dependent sampling. subjects were first identified according to presence or absence of the cns disorder, then queried about use of the drug. it is only possible to estimate the probability that someone had used the drug, given they either did or did not have a cns disorder. (b) the appropriate measure of association is the odds ratio. (c) the easiest way of calculating the or for the table is the cross-product of the diagonal elements of the table: [(10)(4000) ]=[(2000)(7) ]= 2:86. using the definition, it can be calculated as: ˆor=ˆp(cnsjusage) 1\u0000ˆp(cnsjusage) ˆp(cnsjno usage) 1\u0000ˆp(cnsjno usage)=ad bc=(10)(4000) (2000)(7)= 2:86 (d) the odds ratio has the interpretation of the relative odds of presence of a cns disorder, comparing people who have used the weight loss drug to those who have not. people who have used the weight loss drug have odds of cns that are almost three times as large as those for people who have not used the drug. (e) fisher’s exact test is better than the chi-square test. the independence assumption is met, but the expected cell count corresponding the presence of a cns disorder and the use of the drug is 5.68, so not all the expected cell counts are less than 10. 8.45 (a) thep-value is 0.92; there is insu fficient evidence to reject the null hypothesis of no association. these data are plausible with the null hypothesis that green tea consumption is independent of esophageal carcinoma. (b) since the study uses outcome-dependent sampling, the odds ratio should be used as a measure of association rather than relative risk. the odds ratio of esophageal carcinoma, comparing green tea drinkers to non-drinkers, is 1.08; the odds of carcinoma for those who regularly drink green tea are 8% larger than the odds for those who never drink green tea. 463 appendix b distribution tables b.1 normal probability table the area to the left of zrepresents the percentile of the observation. the normal probability table always lists percentiles. negative z y positive z to find the area to the right, calculate 1 minus the area to the left. 1.0000 0.6664 0.3336 = for additional details about working with the normal distribution and the normal probability table, see section 3.3, which starts on . 464 appendix b. distribution tables negative z second decimal place of z 0.09 0.08 0.07 0.06 0.05 0.04 0.03 0.02 0.01 0.00 z 0.0002 0.0003 0.0003 0.0003 0.0003 0.0003 0.0003 0.0003 0.0003 0.0003 \u00003:4 0.0003 0.0004 0.0004 0.0004 0.0004 0.0004 0.0004 0.0005 0.0005 0.0005 \u00003:3 0.0005 0.0005 0.0005 0.0006 0.0006 0.0006 0.0006 0.0006 0.0007 0.0007 \u00003:2 0.0007 0.0007 0.0008 0.0008 0.0008 0.0008 0.0009 0.0009 0.0009 0.0010 \u00003:1 0.0010 0.0010 0.0011 0.0011 0.0011 0.0012 0.0012 0.0013 0.0013 0.0013 \u00003:0 0.0014 0.0014 0.0015 0.0015 0.0016 0.0016 0.0017 0.0018 0.0018 0.0019 \u00002:9 0.0019 0.0020 0.0021 0.0021 0.0022 0.0023 0.0023 0.0024 0.0025 0.0026 \u00002:8 0.0026 0.0027 0.0028 0.0029 0.0030 0.0031 0.0032 0.0033 0.0034 0.0035 \u00002:7 0.0036 0.0037 0.0038 0.0039 0.0040 0.0041 0.0043 0.0044 0.0045 0.0047 \u00002:6 0.0048 0.0049 0.0051 0.0052 0.0054 0.0055 0.0057 0.0059 0.0060 0.0062 \u00002:5 0.0064 0.0066 0.0068 0.0069 0.0071 0.0073 0.0075 0.0078 0.0080 0.0082 \u00002:4 0.0084 0.0087 0.0089 0.0091 0.0094 0.0096 0.0099 0.0102 0.0104 0.0107 \u00002:3 0.0110 0.0113 0.0116 0.0119 0.0122 0.0125 0.0129 0.0132 0.0136 0.0139 \u00002:2 0.0143 0.0146 0.0150 0.0154 0.0158 0.0162 0.0166 0.0170 0.0174 0.0179 \u00002:1 0.0183 0.0188 0.0192 0.0197 0.0202 0.0207 0.0212 0.0217 0.0222 0.0228 \u00002:0 0.0233 0.0239 0.0244 0.0250 0.0256 0.0262 0.0268 0.0274 0.0281 0.0287 \u00001:9 0.0294 0.0301 0.0307 0.0314 0.0322 0.0329 0.0336 0.0344 0.0351 0.0359 \u00001:8 0.0367 0.0375 0.0384 0.0392 0.0401 0.0409 0.0418 0.0427 0.0436 0.0446 \u00001:7 0.0455 0.0465 0.0475 0.0485 0.0495 0.0505 0.0516 0.0526 0.0537 0.0548 \u00001:6 0.0559 0.0571 0.0582 0.0594 0.0606 0.0618 0.0630 0.0643 0.0655 0.0668 \u00001:5 0.0681 0.0694 0.0708 0.0721 0.0735 0.0749 0.0764 0.0778 0.0793 0.0808 \u00001:4 0.0823 0.0838 0.0853 0.0869 0.0885 0.0901 0.0918 0.0934 0.0951 0.0968 \u00001:3 0.0985 0.1003 0.1020 0.1038 0.1056 0.1075 0.1093 0.1112 0.1131 0.1151 \u00001:2 0.1170 0.1190 0.1210 0.1230 0.1251 0.1271 0.1292 0.1314 0.1335 0.1357 \u00001:1 0.1379 0.1401 0.1423 0.1446 0.1469 0.1492 0.1515 0.1539 0.1562 0.1587 \u00001:0 0.1611 0.1635 0.1660 0.1685 0.1711 0.1736 0.1762 0.1788 0.1814 0.1841 \u00000:9 0.1867 0.1894 0.1922 0.1949 0.1977 0.2005 0.2033 0.2061 0.2090 0.2119 \u00000:8 0.2148 0.2177 0.2206 0.2236 0.2266 0.2296 0.2327 0.2358 0.2389 0.2420 \u00000:7 0.2451 0.2483 0.2514 0.2546 0.2578 0.2611 0.2643 0.2676 0.2709 0.2743 \u00000:6 0.2776 0.2810 0.2843 0.2877 0.2912 0.2946 0.2981 0.3015 0.3050 0.3085 \u00000:5 0.3121 0.3156 0.3192 0.3228 0.3264 0.3300 0.3336 0.3372 0.3409 0.3446 \u00000:4 0.3483 0.3520 0.3557 0.3594 0.3632 0.3669 0.3707 0.3745 0.3783 0.3821 \u00000:3 0.3859 0.3897 0.3936 0.3974 0.4013 0.4052 0.4090 0.4129 0.4168 0.4207 \u00000:2 0.4247 0.4286 0.4325 0.4364 0.4404 0.4443 0.4483 0.4522 0.4562 0.4602 \u00000:1 0.4641 0.4681 0.4721 0.4761 0.4801 0.4840 0.4880 0.4920 0.4960 0.5000 \u00000:0 \u0003forz\u0014\u00003:50, the probability is less than or equal to 0 :0002. 465 y positive z second decimal place of z z 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.0 0.5000 0.5040 0.5080 0.5120 0.5160 0.5199 0.5239 0.5279 0.5319 0.5359 0.1 0.5398 0.5438 0.5478 0.5517 0.5557 0.5596 0.5636 0.5675 0.5714 0.5753 0.2 0.5793 0.5832 0.5871 0.5910 0.5948 0.5987 0.6026 0.6064 0.6103 0.6141 0.3 0.6179 0.6217 0.6255 0.6293 0.6331 0.6368 0.6406 0.6443 0.6480 0.6517 0.4 0.6554 0.6591 0.6628 0.6664 0.6700 0.6736 0.6772 0.6808 0.6844 0.6879 0.5 0.6915 0.6950 0.6985 0.7019 0.7054 0.7088 0.7123 0.7157 0.7190 0.7224 0.6 0.7257 0.7291 0.7324 0.7357 0.7389 0.7422 0.7454 0.7486 0.7517 0.7549 0.7 0.7580 0.7611 0.7642 0.7673 0.7704 0.7734 0.7764 0.7794 0.7823 0.7852 0.8 0.7881 0.7910 0.7939 0.7967 0.7995 0.8023 0.8051 0.8078 0.8106 0.8133 0.9 0.8159 0.8186 0.8212 0.8238 0.8264 0.8289 0.8315 0.8340 0.8365 0.8389 1.0 0.8413 0.8438 0.8461 0.8485 0.8508 0.8531 0.8554 0.8577 0.8599 0.8621 1.1 0.8643 0.8665 0.8686 0.8708 0.8729 0.8749 0.8770 0.8790 0.8810 0.8830 1.2 0.8849 0.8869 0.8888 0.8907 0.8925 0.8944 0.8962 0.8980 0.8997 0.9015 1.3 0.9032 0.9049 0.9066 0.9082 0.9099 0.9115 0.9131 0.9147 0.9162 0.9177 1.4 0.9192 0.9207 0.9222 0.9236 0.9251 0.9265 0.9279 0.9292 0.9306 0.9319 1.5 0.9332 0.9345 0.9357 0.9370 0.9382 0.9394 0.9406 0.9418 0.9429 0.9441 1.6 0.9452 0.9463 0.9474 0.9484 0.9495 0.9505 0.9515 0.9525 0.9535 0.9545 1.7 0.9554 0.9564 0.9573 0.9582 0.9591 0.9599 0.9608 0.9616 0.9625 0.9633 1.8 0.9641 0.9649 0.9656 0.9664 0.9671 0.9678 0.9686 0.9693 0.9699 0.9706 1.9 0.9713 0.9719 0.9726 0.9732 0.9738 0.9744 0.9750 0.9756 0.9761 0.9767 2.0 0.9772 0.9778 0.9783 0.9788 0.9793 0.9798 0.9803 0.9808 0.9812 0.9817 2.1 0.9821 0.9826 0.9830 0.9834 0.9838 0.9842 0.9846 0.9850 0.9854 0.9857 2.2 0.9861 0.9864 0.9868 0.9871 0.9875 0.9878 0.9881 0.9884 0.9887 0.9890 2.3 0.9893 0.9896 0.9898 0.9901 0.9904 0.9906 0.9909 0.9911 0.9913 0.9916 2.4 0.9918 0.9920 0.9922 0.9925 0.9927 0.9929 0.9931 0.9932 0.9934 0.9936 2.5 0.9938 0.9940 0.9941 0.9943 0.9945 0.9946 0.9948 0.9949 0.9951 0.9952 2.6 0.9953 0.9955 0.9956 0.9957 0.9959 0.9960 0.9961 0.9962 0.9963 0.9964 2.7 0.9965 0.9966 0.9967 0.9968 0.9969 0.9970 0.9971 0.9972 0.9973 0.9974 2.8 0.9974 0.9975 0.9976 0.9977 0.9977 0.9978 0.9979 0.9979 0.9980 0.9981 2.9 0.9981 0.9982 0.9982 0.9983 0.9984 0.9984 0.9985 0.9985 0.9986 0.9986 3.0 0.9987 0.9987 0.9987 0.9988 0.9988 0.9989 0.9989 0.9989 0.9990 0.9990 3.1 0.9990 0.9991 0.9991 0.9991 0.9992 0.9992 0.9992 0.9992 0.9993 0.9993 3.2 0.9993 0.9993 0.9994 0.9994 0.9994 0.9994 0.9994 0.9995 0.9995 0.9995 3.3 0.9995 0.9995 0.9995 0.9996 0.9996 0.9996 0.9996 0.9996 0.9996 0.9997 3.4 0.9997 0.9997 0.9997 0.9997 0.9997 0.9997 0.9997 0.9997 0.9997 0.9998 \u0003forz\u00153:50, the probability is greater than or equal to 0 :9998. 466 appendix b. distribution tables b.2 t-probability table −3−2−10123 one tail−3−2−10123 one tail−3−2−10123 two tails figure b.1: tails for the t-distribution. one tail 0.100 0.050 0.025 0.010 0.005 two tails 0.200 0.100 0.050 0.020 0.010 df 1 3.08 6.31 12.71 31.82 63.66 2 1.89 2.92 4.30 6.96 9.92 3 1.64 2.35 3.18 4.54 5.84 4 1.53 2.13 2.78 3.75 4.60 5 1.48 2.02 2.57 3.36 4.03 6 1.44 1.94 2.45 3.14 3.71 7 1.41 1.89 2.36 3.00 3.50 8 1.40 1.86 2.31 2.90 3.36 9 1.38 1.83 2.26 2.82 3.25 10 1.37 1.81 2.23 2.76 3.17 11 1.36 1.80 2.20 2.72 3.11 12 1.36 1.78 2.18 2.68 3.05 13 1.35 1.77 2.16 2.65 3.01 14 1.35 1.76 2.14 2.62 2.98 15 1.34 1.75 2.13 2.60 2.95 16 1.34 1.75 2.12 2.58 2.92 17 1.33 1.74 2.11 2.57 2.90 18 1.33 1.73 2.10 2.55 2.88 19 1.33 1.73 2.09 2.54 2.86 20 1.33 1.72 2.09 2.53 2.85 21 1.32 1.72 2.08 2.52 2.83 22 1.32 1.72 2.07 2.51 2.82 23 1.32 1.71 2.07 2.50 2.81 24 1.32 1.71 2.06 2.49 2.80 25 1.32 1.71 2.06 2.49 2.79 26 1.31 1.71 2.06 2.48 2.78 27 1.31 1.70 2.05 2.47 2.77 28 1.31 1.70 2.05 2.47 2.76 29 1.31 1.70 2.05 2.46 2.76 30 1.31 1.70 2.04 2.46 2.75 467 one tail 0.100 0.050 0.025 0.010 0.005 two tails 0.200 0.100 0.050 0.020 0.010 df 31 1.31 1.70 2.04 2.45 2.74 32 1.31 1.69 2.04 2.45 2.74 33 1.31 1.69 2.03 2.44 2.73 34 1.31 1.69 2.03 2.44 2.73 35 1.31 1.69 2.03 2.44 2.72 36 1.31 1.69 2.03 2.43 2.72 37 1.30 1.69 2.03 2.43 2.72 38 1.30 1.69 2.02 2.43 2.71 39 1.30 1.68 2.02 2.43 2.71 40 1.30 1.68 2.02 2.42 2.70 41 1.30 1.68 2.02 2.42 2.70 42 1.30 1.68 2.02 2.42 2.70 43 1.30 1.68 2.02 2.42 2.70 44 1.30 1.68 2.02 2.41 2.69 45 1.30 1.68 2.01 2.41 2.69 46 1.30 1.68 2.01 2.41 2.69 47 1.30 1.68 2.01 2.41 2.68 48 1.30 1.68 2.01 2.41 2.68 49 1.30 1.68 2.01 2.40 2.68 50 1.30 1.68 2.01 2.40 2.68 60 1.30 1.67 2.00 2.39 2.66 70 1.29 1.67 1.99 2.38 2.65 80 1.29 1.66 1.99 2.37 2.64 90 1.29 1.66 1.99 2.37 2.63 100 1.29 1.66 1.98 2.36 2.63 150 1.29 1.66 1.98 2.35 2.61 200 1.29 1.65 1.97 2.35 2.60 300 1.28 1.65 1.97 2.34 2.59 400 1.28 1.65 1.97 2.34 2.59 500 1.28 1.65 1.96 2.33 2.59 1 1.28 1.65 1.96 2.33 2.58 468 appendix b. distribution tables b.3 chi-square probability table 0 5 10 15 figure b.2: areas in the chi-square table always refer to the right tail. upper tail 0.3 0.2 0.1 0.05 0.02 0.01 0.005 0.001 df 1 1.07 1.64 2.71 3.84 5.41 6.63 7.88 10.83 2 2.41 3.22 4.61 5.99 7.82 9.21 10.60 13.82 3 3.66 4.64 6.25 7.81 9.84 11.34 12.84 16.27 4 4.88 5.99 7.78 9.49 11.67 13.28 14.86 18.47 5 6.06 7.29 9.24 11.07 13.39 15.09 16.75 20.52 6 7.23 8.56 10.64 12.59 15.03 16.81 18.55 22.46 7 8.38 9.80 12.02 14.07 16.62 18.48 20.28 24.32 8 9.52 11.03 13.36 15.51 18.17 20.09 21.95 26.12 9 10.66 12.24 14.68 16.92 19.68 21.67 23.59 27.88 10 11.78 13.44 15.99 18.31 21.16 23.21 25.19 29.59 11 12.90 14.63 17.28 19.68 22.62 24.72 26.76 31.26 12 14.01 15.81 18.55 21.03 24.05 26.22 28.30 32.91 13 15.12 16.98 19.81 22.36 25.47 27.69 29.82 34.53 14 16.22 18.15 21.06 23.68 26.87 29.14 31.32 36.12 15 17.32 19.31 22.31 25.00 28.26 30.58 32.80 37.70 16 18.42 20.47 23.54 26.30 29.63 32.00 34.27 39.25 17 19.51 21.61 24.77 27.59 31.00 33.41 35.72 40.79 18 20.60 22.76 25.99 28.87 32.35 34.81 37.16 42.31 19 21.69 23.90 27.20 30.14 33.69 36.19 38.58 43.82 20 22.77 25.04 28.41 31.41 35.02 37.57 40.00 45.31 25 28.17 30.68 34.38 37.65 41.57 44.31 46.93 52.62 30 33.53 36.25 40.26 43.77 47.96 50.89 53.67 59.70 40 44.16 47.27 51.81 55.76 60.44 63.69 66.77 73.40 50 54.72 58.16 63.17 67.50 72.61 76.15 79.49 86.66 469 index ac, 100 adjustedr2(r2 adj),341, 341 adjusted r-squared, 341 alternative hypothesis ( ha),212 analysis of variance (anov a), 264, 264–270 association, 38 bar plot, 37 segmented bar plot, 45 bayes’ theorem, 114, 111–116 blocking, 26 blocks, 26 bonferroni correction, 269 boxplot, 34 case-control studies, 416–419 tests for association, 417 categorical variable, 16 levels, 16 nominal, 16 ordinal, 16 central limit theorem, 202 chi-square distribution, 404 chi-square statistic, 403 chi-square table, 405 cohort, 24 collections, 94 column totals, 43 complement, 100 conditional distribution, 178, 179 conditional distribution for random variables, 177–183 conditional probability, 108–116 confidence interval, 205, 210 confidence level, 207–208 difference of two means, 248–249 difference of two proportions, 396 interpretation, 209–210regression coe fficient, 309 single proportion, 389 confident, 205 confounder, 28,332,350 confounding factor, 28 confounding variable, 28 contingency table, 43 continuous probability distribution, 98 continuous random variable, 141 control, 26 correlated random variables, 180 correlation, 40 correlation coe fficient, 40 data, 11 arabidopsis thaliana, 415 births, 249–251 breast cancer, 397–400 cdc, 200 congress approval rating, 394 developmental disability support, 253–255, 339–340 dolphins and mercury, 241–242 famuss, 15, 37–48, 406–407, 414–415 fcid, 98–99 fecal infusion, 409–413 forest birds, 359–367 frog, 14–15, 30–38, 48 glioblastoma, 392 golub, 59–67 health care, 396 hiv, 402, 407 leap, 12–13, 26, 408 life.expectancy, 40 mammography, 397–400 nhanes, 38–39, 297, 391 470 index persistent pulmonary hypertension in newborns (pphn), 417 prevend, 293–295, 332–337, 350–351 stem cells, heart function, 247–249 swim suit velocities, 244 white fish and mercury, 243 data density, 33 data fishing, 266 data matrix, 15 deck of cards, 95 deviation, 31 discrete probability distributions, 98 discrete random variable, 141 disjoint events, 93 distribution, 30 t, 238–240 bernoulli, 147, 147–148 binomial, 149, 147–151 normal approximation, 161–162 geometric, 171, 170–171 hypergeometric, 175–176 negative binomial, 172, 172–174 normal, 152, 152–167 poisson, 168, 168–169 dot plot, 32 effect size, 235 empirical rule, 36 estimate, 199 event, 94, 94–95 expectation, 142 expected counts, 401 expected value, 142 experiment, 24 explanatory variable, 17,291 exponentially, 170 f-statistic, 267 factor variables, 16 factorial, 149 failure, 147 false negative, 112 false positive, 112 fisher’s exact test, 409 frequency table, 37 general addition rule, 96 general multiplication rule, 110 goodness-of-fit test, 414 greek lambda (\u0015), 168 mu (\u0016), 142sigma (\u001b), 143 high leverage, 305 histogram, 33,48 hypothesis testing, 212–222 decision errors, 221 significance level, 221–222 single proportion, 390 independent, 38,101 independent random variables, 180 influential, 306 interaction, 352 interquartile range, 32 joint distribution, 177,181 joint distribution for random variables, 177–183 joint probabilities, 107 joint probability, 107, 106–107 law of large numbers, 92 least squares regression, 295–297 r-squared (r2),302 least squares regression line, 295, 296 linear association, 38 linear model, 295 lurking variable, 28 margin of error, 205,393, 393–394 marginal distribution, 178 marginal distribution for random variables, 177–183 marginal probabilities, 107 marginal probability, 107, 106–107 marginal totals, 43 mean, 30 average, 30 mean square between groups ( msg ),266 mean square error ( mse ),267,345 median, 30 milgram, stanley, 147 modality bimodal, 34 multimodal, 34 unimodal, 34 mode, 34 model sum of squares (msm), 345 moderate relationships, 291 multiple linear regression, 331 f-statistic, 345 anov a, connection with, 368–369 471 assumptions, 338 categorical predictors, 347–348 confidence interval for the mean, 346 confidence intervals, 344 general model, 342 hypothesis tests, 344 interaction, 352–356 model selection, 358 prediction, 336 prediction interval, 346 residual plots, 338 residuals, 338, 343 multiplication rule, 103 mutually exclusive events, 93 n choose k, 149 negative predictive value, 113 negatively associated, 38 non-response, 21 non-response bias, 21 normal probability plot, 163, 163–167, 301, 339 normal probability table, 156 null hypothesis ( h0),212 numerical variable, 15 continuous, 15 discrete, 15 observational study, 24 odds, 418 odds ratio, 46,418 case-control studies, 418 outlier, 35 outlier in regression, 306 p-value, 214 paired data, 244, 244 parameter, 148 percentile, 156 placebo, 24 point estimate, 201, 201–204 difference of two means, 247–248 difference of two proportions, 395 population mean, 201 single proportion, 389 pooled standard deviation, 256 population, 18, 18–21 population e ffect size, 259 population parameter, 199 population regression model, 296 positive predictive value, 111,113 positively associated, 38power of a test, 257,259 prediction interval, 314 predictor, 291 prevalence, 113 probability, 92, 89–116 probability density function, 98 probability distribution, 97 probability of a success, 147 prospective study, 29 quantile-quantile plot, 163 random phenomena, 93 random variable, 139, 139–146 rejection region, 217,258 relative frequency table, 37 relative odds, 419 relative risk, 46 replication, 26 residual, 295 residual confounders, 351 residuals, 298–301 contingency table, 406 regression, 295, 338 response variable, 17,291 retrospective study, 29 robust estimates, 32 row totals, 43 s, 100 s, 31 sample, 18 cluster, 24 cluster sample, 24 cluster sampling, 25 convenience sample, 20 multistage sample, 24 multistage sampling, 25 non-response, 21 non-response bias, 21 outcome-dependent, 416 random sample, 20–21 representative sample, 20 simple random, 22 simple random sampling, 23 strata, 22 stratified sampling, 22, 23 sample proportion, 147,388 sample size estimating a proportion, 393–394 sample space, 100 sampling distribution 472 index difference of two proportions, 395 regression coe fficient, 308, 344 sample mean, 202 sample proportion, 389 sampling variation, 201 scatter plots, 294 scatterplot, 38 scatterplot matrix, 362 scatterplots, 293 se, 203 sensitivity, 113 sets, 94 shape, 33 side-by-side boxplots, 48 significance level, 213, 221–222 multiple comparisons, 269–270 simple linear regression, 291 assumptions, 294 categorical predictors, 303 interpretation, 298 outliers, 305 prediction intervals, 314 r-squared (r2), 302 simple random sample, 20 simpson’s paradox, 57 skew example: strong, 250 example: very strong, 165 left skewed, 33 right skewed, 33 specificity, 113 standard deviation, 31, 143 standard error (se), 203 difference in means, 248 difference in proportions, 395 regression coe fficient, 309 single proportion, 389standard normal distribution, 152 strata, 22 stratification, 26 strong relationships, 291 success, 147 success-failure condition, 389 sum of squared errors, 267 sum of squares between groups, 266 symmetric, 33 t-distribution, 238–240 t-table, 239 t-test one-sample, 241–243 paired data, 244–245 two independent groups, 249–251 time series, 294 transformation, 36 tree diagram, 112, 116 trial, 147 two-by-two tables, 46 two-sided alternative, 213 two-sided confidence intervals, 205 two-way tables, 43 type i error, 220 type ii error, 220 uncorrelated, 38 variables, 14 variance, 31, 143 venn diagrams, 95 weak relationship, 291 whiskers, 35 z, 153 z-score, 153",
  "metadata": {
    "filename": "biostat.pdf",
    "filepath": "C:\\Users\\Albert\\aa-660-ai-doc-classification-deduplication\\data\\raw\\biostat.pdf",
    "hash": "63ef75170a122ba8ecfee64304b464368fc01d3d78aaaeb0a06be87788910d4b",
    "filesize": 7214768,
    "language": "en"
  }
}